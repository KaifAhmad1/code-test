{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48fa6068c2de4496abce067600272f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f313a2cfb43438383a82af3ce214a03",
              "IPY_MODEL_77299d33cdee440dba969816e74ec4cf",
              "IPY_MODEL_64946689d3374a65a328b0028949ad3e"
            ],
            "layout": "IPY_MODEL_a529b376c56d4063b32b2a11076a93f9"
          }
        },
        "4f313a2cfb43438383a82af3ce214a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".wav,.mp3,.flac,.m4a",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_205b70b9d2ef40438824efbd4f0890de",
            "metadata": [
              {
                "name": "trump-to-taylor.wav",
                "type": "audio/wav",
                "size": 105840150,
                "lastModified": 1743840200240
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_b76aa3ed1a44415e94479c3c7a627483"
          }
        },
        "77299d33cdee440dba969816e74ec4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Detect Deepfake",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e55ba9766cea4dbbafa51a7f8280db8e",
            "style": "IPY_MODEL_5b9a56587949471f81da3995fc861520",
            "tooltip": ""
          }
        },
        "64946689d3374a65a328b0028949ad3e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0cf504f0661b4dff980f379dedcfc427",
            "msg_id": "",
            "outputs": []
          }
        },
        "a529b376c56d4063b32b2a11076a93f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205b70b9d2ef40438824efbd4f0890de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76aa3ed1a44415e94479c3c7a627483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e55ba9766cea4dbbafa51a7f8280db8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9a56587949471f81da3995fc861520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0cf504f0661b4dff980f379dedcfc427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Audio_Deepfake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensice Forensics**"
      ],
      "metadata": {
        "id": "P4GCDUZqAlnl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71zcprttAcJh",
        "outputId": "7e54c27e-b02b-4ed9-e301-736efae10f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pycld3 (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pycld3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pycld3)\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for coverage (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q numpy scipy librosa soundfile pydub torch torchaudio transformers vllm huggingface_hub scikit-learn speechbrain pyAudioAnalysis openai\n",
        "%pip install -q pydantic langchain praat-parselmouth webrtcvad resampy inaSpeechSegmenter pycld3 noisereduce audiomentations\n",
        "%pip install -q webrtcvad hmmlearn audiomentations langchain-community langgraph eyed3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import webrtcvad\n",
        "from pydub import AudioSegment\n",
        "from audiomentations import Compose, AddGaussianNoise\n",
        "\n",
        "from pyAudioAnalysis import audioSegmentation as aS, ShortTermFeatures\n",
        "import speechbrain as sb\n",
        "from speechbrain.inference.speaker import SpeakerRecognition, EncoderClassifier\n",
        "\n",
        "# For forensic report generation using LangChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import openai\n",
        "\n",
        "# For LangGraph orchestration\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# For vLLM integration\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "\n",
        "# For UI in Colab\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML"
      ],
      "metadata": {
        "id": "WJ1SfiSpCKgS",
        "outputId": "4914f367-d691-4311-a04e-49d986e99f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-17 06:24:09 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Forensic Report Data Model\n",
        "#############################################\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file = kwargs.get(\"file\")\n",
        "        self.verdict = kwargs.get(\"verdict\")\n",
        "        self.mean_score = kwargs.get(\"mean_score\")\n",
        "        self.confidence = kwargs.get(\"confidence\")\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\")\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\")\n",
        "        self.natural_summary = kwargs.get(\"natural_summary\")\n",
        "        self.asr_transcript = kwargs.get(\"asr_transcript\")\n",
        "        self.asr_lang = kwargs.get(\"asr_lang\")\n",
        "        self.speaker_identities = kwargs.get(\"speaker_identities\")\n",
        "        self.speaker_spoof_score = kwargs.get(\"speaker_spoof_score\")\n",
        "        self.noise_quality_score = kwargs.get(\"noise_quality_score\")\n",
        "        self.gender_distribution = kwargs.get(\"gender_distribution\")\n",
        "        self.detailed_results = kwargs.get(\"detailed_results\")\n",
        "        self.timestamp = kwargs.get(\"timestamp\")\n",
        "        self.extra_info = kwargs.get(\"extra_info\", {})\n",
        "        self.vllm_model_outputs = kwargs.get(\"vllm_model_outputs\", {})\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        return json.dumps(self.__dict__, indent=indent)"
      ],
      "metadata": {
        "id": "JmbGsBsOCOss"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Audio Preprocessing & Feature Extraction\n",
        "#############################################\n",
        "def preprocess_audio(audio_path: str, out_sr: int = 16000, mono: bool = True, reduce_noise: bool = True) -> Tuple[np.ndarray, int]:\n",
        "    print(\"[Step 1] Preprocessing: Loading audio...\")\n",
        "    ext = os.path.splitext(audio_path)[1].lower()\n",
        "    if ext == \".wav\":\n",
        "        try:\n",
        "            audio, sr = sf.read(audio_path, dtype='float32', always_2d=False, mmap=True)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error reading WAV file {audio_path}: {e}\")\n",
        "        if mono and audio.ndim > 1:\n",
        "            audio = np.mean(audio, axis=1)\n",
        "        if sr != out_sr:\n",
        "            print(\"[Step 1] Resampling audio...\")\n",
        "            audio = librosa.resample(audio, orig_sr=sr, target_sr=out_sr)\n",
        "            sr = out_sr\n",
        "    else:\n",
        "        print(\"[Step 1] Converting non-WAV file to WAV...\")\n",
        "        audio_seg = AudioSegment.from_file(audio_path)\n",
        "        audio_seg = audio_seg.set_frame_rate(out_sr).set_channels(1 if mono else 2)\n",
        "        temp_wav = \"temp_input.wav\"\n",
        "        audio_seg.export(temp_wav, format=\"wav\")\n",
        "        audio, sr = sf.read(temp_wav, dtype='float32', always_2d=False, mmap=True)\n",
        "        if mono and audio.ndim > 1:\n",
        "            audio = np.mean(audio, axis=1)\n",
        "        os.remove(temp_wav)\n",
        "    audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
        "    if reduce_noise:\n",
        "        print(\"[Step 1] Reducing noise...\")\n",
        "        try:\n",
        "            audio = nr.reduce_noise(y=audio, sr=sr)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"[Step 1] Audio loaded successfully.\")\n",
        "    return audio, sr\n",
        "\n",
        "def extract_features(audio: np.ndarray, sr: int) -> Dict[str, float]:\n",
        "    print(\"[Step 2] Extracting audio features...\")\n",
        "    feat = {}\n",
        "    feat['duration'] = len(audio) / sr\n",
        "    feat['energy'] = np.sqrt(np.mean(audio ** 2))\n",
        "    feat['zcr'] = np.mean(librosa.feature.zero_crossing_rate(y=audio))\n",
        "    feat['rmse'] = np.mean(librosa.feature.rms(y=audio))\n",
        "    feat['spec_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
        "    feat['spec_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr))\n",
        "    feat['spec_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    feat['mfcc_mean'] = np.mean(mfccs)\n",
        "    feat['mfcc_std'] = np.std(mfccs)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    feat['chroma_mean'] = np.mean(chroma)\n",
        "    feat['chroma_std'] = np.std(chroma)\n",
        "    st_feats, _ = ShortTermFeatures.feature_extraction(audio, sr, int(0.050 * sr), int(0.025 * sr))\n",
        "    feat['st_energy_std'] = np.std(st_feats[1, :])\n",
        "    feat['spectral_flatness'] = np.mean(librosa.feature.spectral_flatness(y=audio))\n",
        "    try:\n",
        "        signal_power = np.mean(audio ** 2)\n",
        "        noise_est = audio - librosa.effects.hpss(audio)[1]\n",
        "        noise_power = np.mean(noise_est ** 2)\n",
        "        feat['snr_est'] = 10 * np.log10((signal_power + 1e-6) / (noise_power + 1e-6))\n",
        "    except Exception:\n",
        "        feat['snr_est'] = 0\n",
        "    print(\"[Step 2] Feature extraction complete.\")\n",
        "    return feat\n",
        "\n",
        "def extract_vad_ratio(audio: np.ndarray, sr: int) -> float:\n",
        "    print(\"[Step 3] Computing voice activity (VAD) ratio...\")\n",
        "    try:\n",
        "        vad = webrtcvad.Vad(2)\n",
        "        audio_bytes = (audio * 32768).astype(np.int16).tobytes()\n",
        "        speech_frames = 0\n",
        "        total_frames = 0\n",
        "        frame_length = 320\n",
        "        for i in range(0, len(audio_bytes), frame_length):\n",
        "            if i + frame_length > len(audio_bytes):\n",
        "                break\n",
        "            total_frames += 1\n",
        "            if vad.is_speech(audio_bytes[i:i+frame_length], sr):\n",
        "                speech_frames += 1\n",
        "        ratio = speech_frames / (total_frames + 1e-8)\n",
        "    except Exception:\n",
        "        ratio = 0\n",
        "    print(f\"[Step 3] VAD ratio: {ratio:.3f}\")\n",
        "    return ratio\n",
        "\n",
        "def extract_gender_distribution(audio_path: str) -> Tuple[int, int, Dict[str, float]]:\n",
        "    print(\"[Step 4] Estimating gender distribution via diarization...\")\n",
        "    try:\n",
        "        segs, classes = py_audio_segmentation(audio_path)\n",
        "        male_segs = sum(1 for s in segs if \"male\" in s.lower())\n",
        "        female_segs = sum(1 for s in segs if \"female\" in s.lower())\n",
        "        total = male_segs + female_segs\n",
        "        distribution = {\"male\": male_segs / total if total > 0 else 0,\n",
        "                        \"female\": female_segs / total if total > 0 else 0}\n",
        "        print(\"[Step 4] Gender distribution estimated.\")\n",
        "        return (len(segs), male_segs, distribution)\n",
        "    except Exception:\n",
        "        return (0, 0, {})\n",
        "\n",
        "def py_audio_segmentation(audio_path: str) -> Tuple[List[str], List[Any]]:\n",
        "    try:\n",
        "        segs, classes, _ = aS.speaker_diarization(audio_path, 2, plot_res=False)\n",
        "        seg_labels = [\"male\" if c == 0 else \"female\" for c in classes]\n",
        "        return seg_labels, classes\n",
        "    except Exception:\n",
        "        return [], []"
      ],
      "metadata": {
        "id": "Vu-oEkj_ZKyB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Forensic Agent Functions (SpeechBrain)\n",
        "#############################################\n",
        "def run_speechbrain_speaker(audio_path: str) -> Tuple[List[str], List[Any]]:\n",
        "    print(\"[Step 5] Running speaker diarization...\")\n",
        "    segs, _ = py_audio_segmentation(audio_path)\n",
        "    speakers = list(set(segs))\n",
        "    print(f\"[Step 5] Speakers detected: {speakers}\")\n",
        "    return speakers, segs\n",
        "\n",
        "def run_speechbrain_verification(audio_path: str) -> float:\n",
        "    print(\"[Step 6] Running speaker verification...\")\n",
        "    spkr_model = SpeakerRecognition.from_hparams(\n",
        "        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "        savedir=\"tmp_spkrec\"\n",
        "    )\n",
        "    try:\n",
        "        result = spkr_model.verify_files(audio_path, audio_path)\n",
        "        score = float(result['score'])\n",
        "    except Exception:\n",
        "        score = 0.0\n",
        "    print(f\"[Step 6] Verification score: {score:.3f}\")\n",
        "    return score\n",
        "\n",
        "def run_speechbrain_spoof(audio_path: str) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 7] Running spoof detection...\")\n",
        "    try:\n",
        "        model = EncoderClassifier.from_hparams(\n",
        "            source=\"speechbrain/anti-spoofing-ecapa-voxceleb\",\n",
        "            savedir=\"tmp_spoof\"\n",
        "        )\n",
        "        output = model.classify_file(audio_path)[0]\n",
        "        score = float(output.detach().cpu().numpy()[1])\n",
        "        anomalies = [\"SpeechBrain: spoof detected\"] if score > 0.5 else []\n",
        "    except Exception:\n",
        "        score, anomalies = 0.3, []\n",
        "    print(f\"[Step 7] Spoof score: {score:.3f}\")\n",
        "    return score, anomalies\n",
        "\n",
        "def run_language_id(audio_path: str) -> Tuple[str, float]:\n",
        "    print(\"[Step 8] Identifying language...\")\n",
        "    try:\n",
        "        langid = LanguageIdentification.from_hparams(\n",
        "            source=\"speechbrain/lang-id-commonlanguage_ecapa\",\n",
        "            savedir=\"tmp_langid\"\n",
        "        )\n",
        "        result = langid.classify_file(audio_path)\n",
        "        lang = result[3][0] if result[3] else \"unknown\"\n",
        "        conf = float(result[1][0]) if result[1] else 0.0\n",
        "    except Exception:\n",
        "        lang, conf = \"unknown\", 0.0\n",
        "    print(f\"[Step 8] Language: {lang} (confidence: {conf:.2f})\")\n",
        "    return lang, conf\n",
        "\n",
        "def run_wave2vec_fake_detection(audio_path: str) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 9] Running wave2vec-based fake detection...\")\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "    if zcr > 0.2:\n",
        "        return 0.8, [\"High ZCR – potential synthetic voice.\"]\n",
        "    return 0.3, []\n",
        "\n",
        "def run_replay_attack_detection(audio: np.ndarray, sr: int) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 10] Running replay attack detection...\")\n",
        "    rms = np.mean(librosa.feature.rms(y=audio))\n",
        "    if rms < 0.01:\n",
        "        return 0.7, [\"Low RMS – replay attack indicator.\"]\n",
        "    return 0.2, []\n",
        "\n",
        "def run_augmentation_tests(audio: np.ndarray, sr: int) -> Dict[str, float]:\n",
        "    print(\"[Step 11] Running augmentation tests...\")\n",
        "    aug = Compose([AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.05, p=1.0)])\n",
        "    aug_audio = aug(samples=audio, sample_rate=sr)\n",
        "    zcr_orig = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "    zcr_aug = np.mean(librosa.feature.zero_crossing_rate(aug_audio))\n",
        "    score_diff = abs(zcr_orig - zcr_aug)\n",
        "    return {'zcr_aug_diff': score_diff}\n",
        "\n",
        "def run_enhanced_emotion_detection(audio: np.ndarray, sr: int) -> Tuple[float, List[str], str]:\n",
        "    print(\"[Step 12] Running enhanced emotion detection...\")\n",
        "    try:\n",
        "        classifier = EncoderClassifier.from_hparams(\n",
        "            \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
        "            savedir=\"tmp_emotion\"\n",
        "        )\n",
        "        import torch\n",
        "        out_prob, score, index, text_lab = classifier.classify_batch(torch.tensor(audio).unsqueeze(0))\n",
        "        emotion = text_lab[0]\n",
        "        conf = float(score[0])\n",
        "        anomalies = [f\"Emotion detected: {emotion} (conf: {conf:.2f})\"]\n",
        "        desc = f\"Emotion: {emotion} (confidence: {conf:.2f})\"\n",
        "    except Exception:\n",
        "        conf, anomalies, desc = 0.1, [\"Enhanced emotion detection failed; using default value.\"], \"emotion:unknown\"\n",
        "    print(f\"[Step 12] Detected emotion: {desc}\")\n",
        "    return conf, anomalies, desc\n",
        "\n",
        "def run_asr_transcription(audio_path: str) -> str:\n",
        "    print(\"[Step 13] Running ASR transcription...\")\n",
        "    try:\n",
        "        asr_model = EncoderDecoderASR.from_hparams(\n",
        "            source=\"speechbrain/asr-crdnn-rnnlm-librispeech\",\n",
        "            savedir=\"tmp_asr\"\n",
        "        )\n",
        "        transcript = asr_model.transcribe_file(audio_path)\n",
        "    except Exception as e:\n",
        "        transcript = f\"ASR transcription failed: {e}\"\n",
        "    print(\"[Step 13] ASR transcript complete.\")\n",
        "    return transcript"
      ],
      "metadata": {
        "id": "8B5h9lSMZTKl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# vLLM Audio Model Implementations\n",
        "#############################################\n",
        "def run_minicpmo(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running MiniCPM-o...\")\n",
        "    model_name = \"openbmb/MiniCPM-o-2_6\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
        "    stop_token_ids = [tokenizer.convert_tokens_to_ids(tok) for tok in stop_tokens]\n",
        "    audio_placeholder = \"()\" * audio_count\n",
        "    messages = [{'role': 'user', 'content': f'{audio_placeholder}\\n{question}'}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128, stop_token_ids=stop_token_ids)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM MiniCPM-o error: no output]\"\n",
        "    print(\"[vLLM] MiniCPM-o result obtained.\")\n",
        "    return text\n",
        "\n",
        "def run_phi4_mm(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Phi-4 MM...\")\n",
        "    from huggingface_hub import snapshot_download\n",
        "    model_path = snapshot_download(\"microsoft/Phi-4-multimodal-instruct\")\n",
        "    placeholders = \"\".join([f\"<|audio_{i+1}|>\" for i in range(audio_count)])\n",
        "    prompt = f\"<|user|>{placeholders}{question}<|end|><|assistant|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_path,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        enable_lora=True,\n",
        "        max_lora_rank=320,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Phi-4 MM error: no output]\"\n",
        "    print(\"[vLLM] Phi-4 MM result obtained.\")\n",
        "    return text\n",
        "\n",
        "def run_qwen2_audio(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Qwen2-Audio...\")\n",
        "    model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    audio_in_prompt = \"\".join([f\"Audio {i+1}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n\" for i in range(audio_count)])\n",
        "    prompt = (\"<|im_start|>system\\nYou are a forensic audio analysis assistant.<|im_end|>\\n\"\n",
        "              \"<|im_start|>user\\n\" + audio_in_prompt + question + \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>assistant\\n\")\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Qwen2-Audio error: no output]\"\n",
        "    print(\"[vLLM] Qwen2-Audio result obtained.\")\n",
        "    return text\n",
        "\n",
        "def run_ultravox(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Ultravox...\")\n",
        "    model_name = \"fixie-ai/ultravox-v0_5-llama-3_2-1b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    messages = [{'role': 'user', 'content': (\"<|audio|>\\n\" * audio_count) + question}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        trust_remote_code=True,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Ultravox error: no output]\"\n",
        "    print(\"[vLLM] Ultravox result obtained.\")\n",
        "    return text\n",
        "\n",
        "def run_whisper(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Whisper...\")\n",
        "    if audio_count != 1:\n",
        "        return \"Whisper supports only one audio input.\"\n",
        "    model_name = \"openai/whisper-large-v3-turbo\"\n",
        "    prompt = \"<|startoftranscript|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=448,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Whisper error: no output]\"\n",
        "    print(\"[vLLM] Whisper result obtained.\")\n",
        "    return text\n",
        "\n",
        "def get_vllm_audio_model_configs() -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"minicpmo\": run_minicpmo,\n",
        "        \"phi4_mm\": run_phi4_mm,\n",
        "        \"qwen2_audio\": run_qwen2_audio,\n",
        "        \"ultravox\": run_ultravox,\n",
        "        \"whisper\": run_whisper,\n",
        "    }"
      ],
      "metadata": {
        "id": "i5IypX-6Zc5K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Groq LLM Integration using Real API Calls\n",
        "#############################################\n",
        "def run_groq_model1(question: str, audio_count: int) -> str:\n",
        "    print(\"[Groq] Running Groq Model 1...\")\n",
        "    url = \"https://api.groq.ai/v1/inference/model1\"\n",
        "    payload = {\"question\": question, \"audio_count\": audio_count}\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('GROQ_API_KEY', '')}\"}\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json().get(\"text\", \"\")\n",
        "    else:\n",
        "        result = f\"Groq model1 error: {response.status_code}\"\n",
        "    print(\"[Groq] Groq Model 1 result obtained.\")\n",
        "    return result\n",
        "\n",
        "def run_groq_model2(question: str, audio_count: int) -> str:\n",
        "    print(\"[Groq] Running Groq Model 2...\")\n",
        "    url = \"https://api.groq.ai/v1/inference/model2\"\n",
        "    payload = {\"question\": question, \"audio_count\": audio_count}\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('GROQ_API_KEY', '')}\"}\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json().get(\"text\", \"\")\n",
        "    else:\n",
        "        result = f\"Groq model2 error: {response.status_code}\"\n",
        "    print(\"[Groq] Groq Model 2 result obtained.\")\n",
        "    return result\n",
        "\n",
        "def get_groq_audio_model_configs() -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"groq_model1\": run_groq_model1,\n",
        "        \"groq_model2\": run_groq_model2,\n",
        "    }\n",
        "\n",
        "def run_vllm_inference(audio_path: str, question: str) -> Dict[str, str]:\n",
        "    print(\"[Step 14] Running vLLM and Groq integrations...\")\n",
        "    results = {}\n",
        "    for model_name, model_fn in get_vllm_audio_model_configs().items():\n",
        "        try:\n",
        "            result = model_fn(question, audio_count=1)\n",
        "            results[model_name] = result\n",
        "        except Exception as e:\n",
        "            results[model_name] = f\"vLLM error: {e}\"\n",
        "    for model_name, model_fn in get_groq_audio_model_configs().items():\n",
        "        try:\n",
        "            result = model_fn(question, audio_count=1)\n",
        "            results[model_name] = result\n",
        "        except Exception as e:\n",
        "            results[model_name] = f\"Groq error: {e}\"\n",
        "    print(\"[Step 14] vLLM and Groq integration completed.\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "1FgwqziLZjTE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# LangGraph Integration for Forensic Report\n",
        "#############################################\n",
        "def langgraph_forensic_report(report_data: Dict[str, Any]) -> str:\n",
        "    print(\"[Step 15] Running LangGraph analysis...\")\n",
        "    graph = StateGraph(\"forensic_analysis\")\n",
        "    graph.add_node(\"evidence_aggregation\", lambda data: f\"Aggregated anomalies: {data['all_anomalies']}. Features: {data['features']}\")\n",
        "    graph.add_node(\"analysis\", lambda data: langchain_llm_report(data))\n",
        "    graph.add_node(\"evidence_table\", lambda data: f\"Model scores: {data['all_model_scores']}\")\n",
        "    def combine(data):\n",
        "        return f\"{data['evidence_aggregation']}\\n{data['analysis']}\\n{data['evidence_table']}\"\n",
        "    graph.add_node(\"final_report\", combine, dependencies=[\"evidence_aggregation\", \"analysis\", \"evidence_table\"])\n",
        "    graph.add_edge(\"start\", \"evidence_aggregation\")\n",
        "    graph.add_edge(\"start\", \"analysis\")\n",
        "    graph.add_edge(\"start\", \"evidence_table\")\n",
        "    graph.add_edge(\"evidence_aggregation\", \"final_report\")\n",
        "    graph.add_edge(\"analysis\", \"final_report\")\n",
        "    graph.add_edge(\"evidence_table\", \"final_report\")\n",
        "    result = graph.run(report_data)\n",
        "    final_report = result.get(END, \"LangGraph did not produce a final report.\")\n",
        "    print(\"[Step 15] LangGraph analysis completed.\")\n",
        "    return final_report"
      ],
      "metadata": {
        "id": "JvGDv8OKZoF4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Concurrency Helpers\n",
        "#############################################\n",
        "async def async_run_in_executor(func, *args):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        return await loop.run_in_executor(executor, func, *args)\n",
        "\n",
        "async def gather_agents(audio: np.ndarray, sr: int, audio_path: str) -> Dict[str, Any]:\n",
        "    print(\"[Step 16] Gathering forensic agent results concurrently...\")\n",
        "    tasks = {\n",
        "        \"wave2vec\": async_run_in_executor(run_wave2vec_fake_detection, audio_path),\n",
        "        \"replay\": async_run_in_executor(run_replay_attack_detection, audio, sr),\n",
        "        \"emotion\": async_run_in_executor(run_enhanced_emotion_detection, audio, sr),\n",
        "        \"speechbrain_spoof\": async_run_in_executor(run_speechbrain_spoof, audio_path)\n",
        "    }\n",
        "    results = {}\n",
        "    for key, task in tasks.items():\n",
        "        results[key] = await task\n",
        "    print(\"[Step 16] Forensic agents completed.\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "dlvi9vtkZs6M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# LangChain Forensic Report Generation\n",
        "#############################################\n",
        "def langchain_llm_report(report_data: Dict[str, Any]) -> str:\n",
        "    print(\"[Step 17] Generating LangChain forensic report...\")\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"verdict\", \"mean_score\", \"anomalies\", \"asr\", \"asr_lang\",\n",
        "                           \"speakers\", \"spoof\", \"features\", \"noise_quality\", \"gender_dist\", \"extra\", \"vllm_outputs\"],\n",
        "        template=(\n",
        "            \"You are a digital audio forensics expert. Given the detailed analysis below, please explain each step, \"\n",
        "            \"create a comprehensive evidence table, and recommend actionable remediation.\\n\\n\"\n",
        "            \"Verdict: {verdict}\\n\"\n",
        "            \"Mean Deepfake Score: {mean_score:.2f}\\n\"\n",
        "            \"Detected Anomalies: {anomalies}\\n\"\n",
        "            \"ASR Transcript: {asr}\\n\"\n",
        "            \"ASR Language: {asr_lang}\\n\"\n",
        "            \"Speaker Identities: {speakers}\\n\"\n",
        "            \"Spoof Score: {spoof}\\n\"\n",
        "            \"Feature Summary: {features}\\n\"\n",
        "            \"Noise/Quality Score: {noise_quality}\\n\"\n",
        "            \"Gender Distribution: {gender_dist}\\n\"\n",
        "            \"Additional Analysis: {extra}\\n\"\n",
        "            \"vLLM Audio Model Outputs: {vllm_outputs}\\n\\n\"\n",
        "            \"Provide a risk assessment, detailed evidence breakdown, and actionable recommendations.\"\n",
        "        )\n",
        "    )\n",
        "    llm = OpenAI(temperature=0.2, max_tokens=700)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    summary = chain.run(\n",
        "        verdict=report_data['verdict'],\n",
        "        mean_score=report_data['mean_score'],\n",
        "        anomalies=\", \".join(report_data['all_anomalies']),\n",
        "        asr=report_data['asr_transcript'][:400] + \"...\" if report_data['asr_transcript'] else \"N/A\",\n",
        "        asr_lang=report_data.get('asr_lang', 'unknown'),\n",
        "        speakers=\", \".join(report_data['speaker_identities']),\n",
        "        spoof=str(report_data['speaker_spoof_score']),\n",
        "        features=\"; \".join(f\"{k}: {v:.3f}\" for k, v in report_data['features'].items()),\n",
        "        noise_quality=str(report_data.get('noise_quality_score', 'N/A')),\n",
        "        gender_dist=json.dumps(report_data.get('gender_distribution', {})),\n",
        "        extra=json.dumps(report_data.get('extra_info', {})),\n",
        "        vllm_outputs=json.dumps(report_data.get('vllm_outputs', {}))\n",
        "    )\n",
        "    print(\"[Step 17] LangChain report generated.\")\n",
        "    return summary"
      ],
      "metadata": {
        "id": "cao23N_WZyoU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Aggregation & Final Report\n",
        "#############################################\n",
        "def aggregate_and_report(audio_path: str, results: Dict[str, Any],\n",
        "                         feats: Dict[str, float], asr: str,\n",
        "                         asr_lang: str, speaker_identities: List[str],\n",
        "                         spk_score: float, noise_quality_score: float,\n",
        "                         gender_dist: Dict[str, float],\n",
        "                         vllm_model_outputs: Dict[str, str],\n",
        "                         extra_results: Dict[str, Any] = {}) -> ForensicReport:\n",
        "    print(\"[Step 18] Aggregating all analysis results...\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "    detailed = {}\n",
        "    for agent, (score, anomalies, detail) in results.items():\n",
        "        scores.append(score)\n",
        "        model_scores[agent] = score\n",
        "        all_anomalies.extend(anomalies)\n",
        "        detailed[agent] = {\"score\": score, \"anomalies\": anomalies, \"detail\": detail}\n",
        "    speakers, diar_segments = run_speechbrain_speaker(audio_path)\n",
        "    spk_verif_score = run_speechbrain_verification(audio_path)\n",
        "    spoof_score, _ = results.get(\"speechbrain_spoof\", (0.0, []))\n",
        "    combined_scores = scores + [spk_verif_score]\n",
        "    mean_score = float(np.mean(combined_scores))\n",
        "    confidence = 1.0 - float(np.std(combined_scores))\n",
        "    verdict = (\"Likely FAKE (spoof/scam/deepfake detected)\" if mean_score > 0.7\n",
        "               else \"Possibly FAKE (review anomalies)\" if mean_score > 0.5\n",
        "               else \"Likely REAL\")\n",
        "    extra_info = {\n",
        "        \"speaker_diarization\": diar_segments,\n",
        "        \"augmentation_tests\": run_augmentation_tests(librosa.util.normalize(feats.get('energy')), 16000)\n",
        "    }\n",
        "    try:\n",
        "        llm_summary = langchain_llm_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities if speaker_identities else speakers,\n",
        "            \"speaker_spoof_score\": spoof_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"extra_info\": extra_info,\n",
        "            \"vllm_outputs\": vllm_model_outputs\n",
        "        })\n",
        "        lg_summary = langgraph_forensic_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities if speaker_identities else speakers,\n",
        "            \"speaker_spoof_score\": spoof_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"extra_info\": extra_info,\n",
        "            \"vllm_outputs\": vllm_model_outputs,\n",
        "            \"all_model_scores\": model_scores\n",
        "        })\n",
        "        natural_summary = f\"{llm_summary}\\n\\nLangGraph Analysis:\\n{lg_summary}\"\n",
        "    except Exception as e:\n",
        "        natural_summary = f\"Verdict: {verdict} (Error generating summary: {e})\"\n",
        "    detailed.update(extra_results)\n",
        "    print(\"[Step 18] Aggregation complete. Forensic report ready.\")\n",
        "    return ForensicReport(\n",
        "        file=audio_path,\n",
        "        verdict=verdict,\n",
        "        mean_score=mean_score,\n",
        "        confidence=confidence,\n",
        "        all_model_scores=model_scores,\n",
        "        all_anomalies=list(set(all_anomalies)),\n",
        "        natural_summary=natural_summary,\n",
        "        asr_transcript=asr,\n",
        "        asr_lang=asr_lang,\n",
        "        speaker_identities=speaker_identities if speaker_identities else speakers,\n",
        "        speaker_spoof_score=spoof_score,\n",
        "        noise_quality_score=noise_quality_score,\n",
        "        gender_distribution=gender_dist,\n",
        "        detailed_results=detailed,\n",
        "        timestamp=datetime.utcnow().isoformat(),\n",
        "        extra_info=extra_info,\n",
        "        vllm_model_outputs=vllm_model_outputs\n",
        "    )"
      ],
      "metadata": {
        "id": "d5uXGKoCZ5Oj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Main Pipeline Function (Async)\n",
        "#############################################\n",
        "async def deepfake_defensive_pipeline(audio_path: str, vllm_question: str = None) -> ForensicReport:\n",
        "    if vllm_question is None:\n",
        "        vllm_question = (\n",
        "            \"Analyze this audio for deepfake, spoofing, scam, or synthetic voice. \"\n",
        "            \"Extract evidence including speaker details, replay attacks, spoof indicators, emotion, ASR, spectral features, and augmentation artifacts. \"\n",
        "            \"Provide a risk score (0 to 1), a detailed evidence table, and actionable recommendations.\"\n",
        "        )\n",
        "    audio, sr = await async_run_in_executor(preprocess_audio, audio_path)\n",
        "    feats = extract_features(audio, sr)\n",
        "    feats['vad_ratio'] = extract_vad_ratio(audio, sr)\n",
        "    nb_segments, _, gender_dist = extract_gender_distribution(audio_path)\n",
        "    feats['nb_segments'] = nb_segments\n",
        "    for gender, ratio in gender_dist.items():\n",
        "        feats[f'gender_{gender}_ratio'] = ratio\n",
        "    langid_label, _ = run_language_id(audio_path)\n",
        "    feats['langid_label'] = langid_label\n",
        "    agent_results = await gather_agents(audio, sr, audio_path)\n",
        "    asr_transcript = run_asr_transcription(audio_path)\n",
        "    asr_lang = langid_label\n",
        "    vllm_outputs = run_vllm_inference(audio_path, vllm_question)\n",
        "    report = aggregate_and_report(\n",
        "        audio_path,\n",
        "        agent_results,\n",
        "        feats,\n",
        "        asr_transcript,\n",
        "        asr_lang,\n",
        "        speaker_identities=[],\n",
        "        spk_score=run_speechbrain_verification(audio_path),\n",
        "        noise_quality_score=nr.reduce_noise(y=audio, sr=sr).std(),\n",
        "        gender_dist=gender_dist,\n",
        "        vllm_model_outputs=vllm_outputs,\n",
        "        extra_results={}\n",
        "    )\n",
        "    return report"
      ],
      "metadata": {
        "id": "mWlU_Qbdb77x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Mermaid.js Flowchart Generation\n",
        "#############################################\n",
        "def generate_mermaid_flowchart() -> str:\n",
        "    mermaid_code = \"\"\"\n",
        "    %% Mermaid Flowchart for Deepfake Audio Forensic Pipeline\n",
        "    graph TD\n",
        "      A[Upload Audio File] --> B[Preprocess Audio]\n",
        "      B --> C[Extract Features]\n",
        "      C --> D[Compute VAD Ratio]\n",
        "      D --> E[Estimate Gender Distribution]\n",
        "      E --> F[Speaker Diarization & Verification]\n",
        "      F --> G[ASR Transcription]\n",
        "      G --> H[vLLM & Groq Analysis]\n",
        "      H --> I[Aggregation & Report Generation]\n",
        "      I --> J[Forensic Report]\n",
        "    \"\"\"\n",
        "    return mermaid_code\n",
        "\n",
        "def display_mermaid_chart(mermaid_code: str):\n",
        "    html_content = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "    <script type=\"module\">\n",
        "      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.js';\n",
        "      mermaid.initialize({{startOnLoad:true}});\n",
        "    </script>\n",
        "    </head>\n",
        "    <body>\n",
        "    <div class=\"mermaid\">\n",
        "    {mermaid_code}\n",
        "    </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    display(HTML(html_content))"
      ],
      "metadata": {
        "id": "m-kiP3x0cAAA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# UI for Google Colab using ipywidgets\n",
        "#############################################\n",
        "def display_audio_file(audio_path: str, sr: int = 16000):\n",
        "    audio, _ = librosa.load(audio_path, sr=sr)\n",
        "    plt.figure(figsize=(14, 3))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "def run_pipeline_ui():\n",
        "    upload_widget = widgets.FileUpload(accept=\".wav,.mp3,.flac,.m4a\", multiple=False)\n",
        "    detect_button = widgets.Button(description=\"Detect Deepfake\", button_style='success')\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_detect_clicked(change):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            if not upload_widget.value:\n",
        "                print(\"Please upload an audio file.\")\n",
        "                return\n",
        "            print(\"Starting the deepfake detection pipeline...\")\n",
        "            # Save uploaded file locally\n",
        "            uploaded_filename = list(upload_widget.value.keys())[0]\n",
        "            content = upload_widget.value[uploaded_filename]['content']\n",
        "            local_filename = \"uploaded_audio\" + os.path.splitext(uploaded_filename)[1]\n",
        "            with open(local_filename, \"wb\") as f:\n",
        "                f.write(content)\n",
        "            print(f\"Audio file '{uploaded_filename}' saved as '{local_filename}'.\")\n",
        "            display_audio_file(local_filename)\n",
        "            # Generate and display the Mermaid flowchart for the pipeline\n",
        "            print(\"\\nGenerating pipeline flowchart with Mermaid.js...\")\n",
        "            mermaid_code = generate_mermaid_flowchart()\n",
        "            display_mermaid_chart(mermaid_code)\n",
        "            print(\"\\nExecuting forensic analysis...\\n\")\n",
        "            loop = asyncio.get_event_loop()\n",
        "            report = loop.run_until_complete(deepfake_defensive_pipeline(local_filename))\n",
        "            print(\"\\n=== Forensic Report ===\\n\")\n",
        "            print(report.natural_summary)\n",
        "            print(\"\\n=== Raw JSON Report ===\\n\")\n",
        "            print(report.json(indent=2))\n",
        "\n",
        "    detect_button.on_click(on_detect_clicked)\n",
        "    ui = widgets.VBox([upload_widget, detect_button, output_area])\n",
        "    display(ui)\n",
        "\n",
        "# Run the UI when this cell is executed in Colab\n",
        "run_pipeline_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "48fa6068c2de4496abce067600272f61",
            "4f313a2cfb43438383a82af3ce214a03",
            "77299d33cdee440dba969816e74ec4cf",
            "64946689d3374a65a328b0028949ad3e",
            "a529b376c56d4063b32b2a11076a93f9",
            "205b70b9d2ef40438824efbd4f0890de",
            "b76aa3ed1a44415e94479c3c7a627483",
            "e55ba9766cea4dbbafa51a7f8280db8e",
            "5b9a56587949471f81da3995fc861520",
            "0cf504f0661b4dff980f379dedcfc427"
          ]
        },
        "id": "lU2jSVfwcEI5",
        "outputId": "da2014da-ddee-45ad-dfba-f971d7a949b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(FileUpload(value={}, accept='.wav,.mp3,.flac,.m4a', description='Upload'), Button(button_style=…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48fa6068c2de4496abce067600272f61"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRQR3qS7cJnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}