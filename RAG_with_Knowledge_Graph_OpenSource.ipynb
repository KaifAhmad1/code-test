{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcLEAt3+YkuMdmecVBE89L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/RAG_with_Knowledge_Graph_OpenSource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RAG with Knowledge Graph**\n",
        "- This notebook demonstrates how to build a knowledge graph using website data, visualize it, and answer complex queries that cannot be handled by traditional naive RAG.\n",
        "- It includes optimizations for cybersecurity data, utilizing the ReLiK model for entity detection and relation extraction, and integrating with Kuzu DB for efficient querying."
      ],
      "metadata": {
        "id": "Zg5xeCTNFGEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installations**"
      ],
      "metadata": {
        "id": "z-DurpBnFND8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9zDcGynOE93O"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-experimental langchain-core langchain-community langchain-groq pandas networkx\n",
        "!pip install -q mpi4py pyvis ampligraph transformers relik deepspeed kuzu bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Data Loading**\n",
        "- This function loads documents from a list of URLs.\n",
        "- Loading data from websites is the first step in any data processing pipeline. It ensures that we have the raw data needed for further analysis."
      ],
      "metadata": {
        "id": "vzBUQy_cFUos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "def load_data_from_websites(urls):\n",
        "    \"\"\"Load documents from a list of URLs.\"\"\"\n",
        "    web_base_loader = WebBaseLoader(urls)\n",
        "    documents = web_base_loader.load()\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    return documents\n",
        "\n",
        "# List of websites to load data from\n",
        "websites = [\n",
        "    \"https://www.scmagazine.com/home/security-news/\",\n",
        "    \"https://thehackernews.com/\",\n",
        "    \"https://www.securityweek.com/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://krebsonsecurity.com/\",\n",
        "    \"https://www.bleepingcomputer.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "    \"https://www.cyberscoop.com/\",\n",
        "    \"https://www.infosecurity-magazine.com/\",\n",
        "    \"https://www.zdnet.com/topic/security/\",\n",
        "]\n",
        "\n",
        "documents = load_data_from_websites(websites)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3tx4EP_FZsj",
        "outputId": "f67dc8b6-adfd-4c4b-af37-61c6726f2f8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Documents into Chunks**\n",
        "- This function splits the loaded documents into smaller chunks for easier processing.\n",
        "- Splitting documents into chunks helps in managing large texts and ensures that each chunk can be processed independently.\n",
        "- This function is used to break down long documents into manageable pieces for entity detection and triplet extraction."
      ],
      "metadata": {
        "id": "s9OM2kVrGXor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_data(documents, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"Split documents into smaller chunks.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Number of chunks created: {len(chunks)}\")\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_data(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGkVKgTFFc17",
        "outputId": "4ed2983c-e20a-4a61-82b5-12d3e2939404"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks created: 362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a DataFrame of Chunks**\n",
        "- This function converts the list of chunks into a pandas DataFrame.\n",
        "- Converting chunks into a DataFrame allows for easier data manipulation and analysis using pandas.\n",
        "- This function is used to create a structured format for the chunks, which will be used in subsequent steps."
      ],
      "metadata": {
        "id": "yZ5HGJyXGoHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_chunks_dataframe(chunks):\n",
        "    \"\"\"Create a DataFrame from document chunks.\"\"\"\n",
        "    data = {'content': [chunk.page_content for chunk in chunks]}\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "chunks_df = create_chunks_dataframe(chunks)\n",
        "chunks_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XQlI0t8sGmxQ",
        "outputId": "aeb76c9f-e4f4-45bf-e0dd-7254863bd800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content\n",
              "0  404: Not FoundCISO StoriesTopicsEventsPodcasts...\n",
              "1  in any form without prior authorization.\\n    ...\n",
              "2  The Hacker News | #1 Trusted Cybersecurity New...\n",
              "3  Contact/Tip Us\\n\\n\\n\\nReach out to get featur...\n",
              "4  In 2023, no fewer than 94 percent of businesse..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55464802-49d5-45fc-b7da-2d7c87635ec1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>404: Not FoundCISO StoriesTopicsEventsPodcasts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in any form without prior authorization.\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Hacker News | #1 Trusted Cybersecurity New...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Contact/Tip Us\\n\\n\\n\\nReach out to get featur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In 2023, no fewer than 94 percent of businesse...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55464802-49d5-45fc-b7da-2d7c87635ec1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55464802-49d5-45fc-b7da-2d7c87635ec1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55464802-49d5-45fc-b7da-2d7c87635ec1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6131ed63-1c36-4225-8488-69d34e6b550e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6131ed63-1c36-4225-8488-69d34e6b550e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6131ed63-1c36-4225-8488-69d34e6b550e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "chunks_df",
              "summary": "{\n  \"name\": \"chunks_df\",\n  \"rows\": 362,\n  \"fields\": [\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 362,\n        \"samples\": [\n          \"AT&T Corp. disclosed today that a new data breach has exposed phone call and text message records for roughly 110 million people \\u2014 nearly all of its customers. AT&T said it delayed disclosing the incident in response to \\u201cnational security and public safety concerns,\\u201d noting that some of the records included data that could be used to determine where a call was made or text message sent. AT&T also acknowledged the customer records were exposed in a cloud database that was protected only by a\",\n          \"slight naming discrepancy has been leveraged by a threat actor who published a 'solana-py' project on PyPI.\\\" The malicious \\\"solana-py\\\" package attracted a total of 1,122 downloads since it was published on August 4, 2024. It's no longer available for download from PyPI. The most striking aspect of the library is that it carried the version numbers 0.34.3, 0.34.4, and 0.34.5. The latest version of the legitimate \\\"solana\\\" package is 0.34.3. This clearly indicates an attempt o\",\n          \"SANS Institute 35th Anniversary Complimentary Cyber Bundle ($1700 Value) at Network Security 2024SANS InstituteArtificial Intelligence / Network SecurityRegister to attend in-person training at Network Security 2024 and receive a complimentary cyber bundle! Bundle includes bonus SANS course AIS247, OnDemand bundle, AND @Night pass to the AI Cybersecurity Summit (Sept 8-9)!\\n\\n\\n\\n\\n\\n\\n\\n\\nFreeBSD Releases Urgent Patch for High-Severity OpenSSH Vulnerability\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect and Classify Entities**\n",
        "- This function uses a Named Entity Recognition (NER) pipeline to detect entities in the text chunks.\n",
        "- Detecting entities helps in identifying key concepts and entities in the text, which is crucial for understanding the content.\n",
        "- This function is used to extract entities from the text chunks and create a DataFrame of detected entities."
      ],
      "metadata": {
        "id": "4TO6AaDIGxAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from relik import Relik\n",
        "from langchain.schema.document import Document\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "def extract_entities_with_relik(chunks_df, max_retries=3):\n",
        "    \"\"\"Extract entities from document chunks using the ReLiK entity extraction model.\"\"\"\n",
        "    session = requests.Session()\n",
        "    retry = Retry(total=max_retries, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "\n",
        "    relik = Relik.from_pretrained(\n",
        "        \"relik-ie/relik-cie-tiny\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        precision=\"fp8\" if torch.cuda.is_available() else \"fp32\",\n",
        "        skip_metadata=True,  # don't load index metadata to keep low memory requirements\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=chunk) for chunk in chunks_df['content']]\n",
        "    entities = []\n",
        "\n",
        "    for doc in documents:\n",
        "        # Assuming `relik.extract` is the correct method for entity extraction\n",
        "        results = relik.extract(doc.page_content)\n",
        "        for entity in results['entities']:\n",
        "            entities.append({\n",
        "                'Entity': entity['text'],\n",
        "                'Type': entity['label']\n",
        "            })\n",
        "\n",
        "    entities_df = pd.DataFrame(entities)\n",
        "    return entities_df\n",
        "\n",
        "entities_df = extract_entities_with_relik(chunks_df)\n",
        "entities_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__b_BOjEGs2Z",
        "outputId": "f1cc936c-3b4e-47ed-c71e-3777c362c689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ___              __         \n",
            "               /\\_ \\      __    /\\ \\        \n",
            " _ __     __   \\//\\ \\    /\\_\\   \\ \\ \\/'\\    \n",
            "/\\`'__\\ /'__`\\   \\ \\ \\   \\/\\ \\   \\ \\ , <    \n",
            "\\ \\ \\/ /\\  __/    \\_\\ \\_  \\ \\ \\   \\ \\ \\\\`\\  \n",
            " \\ \\_\\ \\ \\____\\   /\\____\\  \\ \\_\\   \\ \\_\\ \\_\\\n",
            "  \\/_/  \\/____/   \\/____/   \\/_/    \\/_/\\/_/\n",
            "                                            \n",
            "                                            \n",
            "\n",
            "[2024-08-12 12:48:27,069] [INFO] [relik.inference.annotator.from_pretrained:700] [PID:4299] [RANK:0] Loading Relik from relik-ie/relik-cie-tiny\u001b[39m\n",
            "[2024-08-12 12:48:27,073] [INFO] [relik.inference.annotator.from_pretrained:701] [PID:4299] [RANK:0] {\n",
            "    '_target_': 'relik.inference.annotator.Relik',\n",
            "    'index': {\n",
            "        'span': {\n",
            "            '_target_': 'relik.retriever.indexers.inmemory.InMemoryDocumentIndex.from_pretrained',\n",
            "            'name_or_path': 'relik-ie/index-e5-small-v2-wikipedia-matryoshka',\n",
            "        },\n",
            "        'triplet': {\n",
            "            '_target_': 'relik.retriever.indexers.inmemory.InMemoryDocumentIndex.from_pretrained',\n",
            "            'name_or_path': 'relik-ie/encoder-e5-small-v2-wikipedia-relations-index',\n",
            "        },\n",
            "    },\n",
            "    'metadata_fields': [],\n",
            "    'reader': {\n",
            "        '_target_': 'relik.reader.pytorch_modules.triplet.RelikReaderForTripletExtraction',\n",
            "        'transformer_model': 'relik-ie/relik-reader-deberta-v3-small-cie-wikipedia',\n",
            "        'use_nme': True,\n",
            "    },\n",
            "    'retriever': {\n",
            "        'span': {\n",
            "            '_target_': 'relik.retriever.pytorch_modules.model.GoldenRetriever',\n",
            "            'question_encoder': 'relik-ie/encoder-e5-small-v2-wikipedia-matryoshka',\n",
            "        },\n",
            "        'triplet': {\n",
            "            '_target_': 'relik.retriever.pytorch_modules.model.GoldenRetriever',\n",
            "            'question_encoder': 'relik-ie/encoder-e5-small-v2-wikipedia-relations',\n",
            "        },\n",
            "    },\n",
            "    'task': 'BOTH',\n",
            "    'top_k': 100,\n",
            "    'window_size': 64,\n",
            "    'window_stride': 32,\n",
            "}\u001b[39m\n",
            "\u001b[33m[2024-08-12 12:48:29,540] [WARNING] [relik.common.utils.download_from_hf:342] [PID:4299] [RANK:0] Couldn't download index.faiss from relik-ie/index-e5-small-v2-wikipedia-matryoshka, ignoring\u001b[39m\n",
            "[2024-08-12 12:48:29,546] [INFO] [relik.retriever.indexers.base.from_pretrained:482] [PID:4299] [RANK:0] Loading Index from config:\u001b[39m\n",
            "[2024-08-12 12:48:29,548] [INFO] [relik.retriever.indexers.base.from_pretrained:483] [PID:4299] [RANK:0] {\n",
            "    '_target_': 'relik.retriever.indexers.inmemory.InMemoryDocumentIndex',\n",
            "    'device': 'cuda',\n",
            "    'metadata_fields': [],\n",
            "    'name_or_path': '/media/ssd/perelluis/relik_experiments/indexes/index-e5-small-v2-wikipedia-matryoshka',\n",
            "    'precision': 'bf16',\n",
            "    'separator': None,\n",
            "    'use_faiss': False,\n",
            "}\u001b[39m\n",
            "[2024-08-12 12:48:29,549] [INFO] [relik.retriever.indexers.base.from_pretrained:490] [PID:4299] [RANK:0] Loading documents from /root/.cache/huggingface/hub/models--relik-ie--index-e5-small-v2-wikipedia-matryoshka/snapshots/8d119e710a7a8be5000b789dfcde3e661767982b/documents.jsonl\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing LLM for Knowledge Extraction**\n",
        "- Initialize the Mistral LLM using Groq for knowledge extraction."
      ],
      "metadata": {
        "id": "96vgsQvnG-_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "GROQ_API_KEY = \"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\"\n",
        "\n",
        "# Initialize the Mistral LLM using Groq\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "wt5pQd8wG3Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Triplets using ReLiK**\n",
        "- Use the ReLiK model for relation extraction.\n",
        "- Extracting triplets helps in understanding the relationships between entities, which is essential for building a knowledge graph.\n",
        "- This function is used to extract triplets from the text chunks and create a list of triplets."
      ],
      "metadata": {
        "id": "1o3RR941HQAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Extract Triplets using ReLiK\n",
        "def extract_triplets_relik(chunks, max_retries=3):\n",
        "    \"\"\"Extract triplets using the ReLiK model.\"\"\"\n",
        "    model_name = \"relik-ie/relik-relation-extraction-small-wikipedia\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    ds_config = {\n",
        "        \"train_micro_batch_size_per_gpu\": 1,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"steps_per_print\": 2000,\n",
        "        \"fp16\": {\n",
        "            \"enabled\": True,\n",
        "            \"loss_scale\": 0,\n",
        "            \"loss_scale_window\": 1000,\n",
        "            \"hysteresis\": 2,\n",
        "            \"min_loss_scale\": 1\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"type\": \"Adam\",\n",
        "            \"params\": {\n",
        "                \"lr\": 0.0005,\n",
        "                \"betas\": [0.8, 0.999],\n",
        "                \"eps\": 1e-8,\n",
        "                \"weight_decay\": 3e-7\n",
        "            }\n",
        "        },\n",
        "        \"scheduler\": {\n",
        "            \"type\": \"WarmupLR\",\n",
        "            \"params\": {\n",
        "                \"warmup_min_lr\": 0,\n",
        "                \"warmup_max_lr\": 0.0005,\n",
        "                \"warmup_num_steps\": 1000\n",
        "            }\n",
        "        },\n",
        "        \"zero_optimization\": {\n",
        "            \"stage\": 2,\n",
        "            \"offload_optimizer\": {\n",
        "                \"device\": \"cpu\",\n",
        "                \"pin_memory\": True\n",
        "            },\n",
        "            \"allgather_partitions\": True,\n",
        "            \"allgather_bucket_size\": 2e8,\n",
        "            \"overlap_comm\": True,\n",
        "            \"contiguous_gradients\": True,\n",
        "            \"reduce_bucket_size\": 2e8,\n",
        "            \"reduce_scatter\": True\n",
        "        },\n",
        "        \"gradient_clipping\": 1.0\n",
        "    }\n",
        "    model, optimizer, _, scheduler = deepspeed.initialize(model=model, config=ds_config)\n",
        "\n",
        "    def extract_triplets_from_text(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        triplets = [f\"({tokenizer.decode(logit.argmax(dim=-1))})\" for logit in logits]\n",
        "        return triplets\n",
        "\n",
        "    all_triplets = []\n",
        "    for chunk in chunks:\n",
        "        triplets = extract_triplets_from_text(chunk.page_content)\n",
        "        all_triplets.extend(triplets)\n",
        "    return all_triplets\n",
        "\n",
        "triplets = extract_triplets_relik(chunks)\n",
        "print(f\"Number of triplets extracted: {len(triplets)}\")"
      ],
      "metadata": {
        "id": "4vegiBkuHCM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze Relationships**\n",
        "- This function converts the list of triplets into a DataFrame.\n",
        "- Converting triplets into a DataFrame allows for easier analysis and manipulation of the relationships between entities.\n",
        "- This function is used to create a structured format for the triplets, which will be used in subsequent steps."
      ],
      "metadata": {
        "id": "mDXV2LiaHZvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze Relationships\n",
        "def analyze_relationships(triplets):\n",
        "    \"\"\"Convert list of triplets into a DataFrame.\"\"\"\n",
        "    def create_triplets_dataframe(triplets):\n",
        "        triplet_data = []\n",
        "        for triplet in triplets:\n",
        "            subject, predicate, obj = triplet.strip(\"()\").split(\", \")\n",
        "            triplet_data.append({\n",
        "                \"subject\": subject.strip(),\n",
        "                \"predicate\": predicate.strip(),\n",
        "                \"object\": obj.strip()\n",
        "            })\n",
        "        return pd.DataFrame(triplet_data)\n",
        "    triplets_df = create_triplets_dataframe(triplets)\n",
        "    return triplets_df\n",
        "\n",
        "triplets_df = analyze_relationships(triplets)"
      ],
      "metadata": {
        "id": "cotQDPf5HUPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Contextual Proximity**\n",
        "- This function calculates the contextual proximity between nodes by merging the DataFrame with itself and counting the occurrences of node pairs within the same chunk.\n",
        "- Contextual proximity helps in understanding the relationships between entities that appear together in the same context.\n",
        "- This function is used to create a DataFrame of contextual proximity relationships between entities.\n"
      ],
      "metadata": {
        "id": "prJYzTDGHeEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_contextual_proximity(df):\n",
        "    \"\"\"Calculate contextual proximity between nodes.\"\"\"\n",
        "    long_format_df = pd.melt(\n",
        "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
        "    )\n",
        "    long_format_df.drop(columns=[\"variable\"], inplace=True)\n",
        "    merged_df = pd.merge(long_format_df, long_format_df, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
        "    self_loops_index = merged_df[merged_df[\"node_1\"] == merged_df[\"node_2\"]].index\n",
        "    merged_df = merged_df.drop(index=self_loops_index).reset_index(drop=True)\n",
        "    grouped_df = (\n",
        "        merged_df.groupby([\"node_1\", \"node_2\"])\n",
        "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
        "        .reset_index()\n",
        "    )\n",
        "    grouped_df.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
        "    grouped_df.replace(\"\", np.nan, inplace=True)\n",
        "    grouped_df.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
        "    grouped_df = grouped_df[grouped_df[\"count\"] != 1]\n",
        "    grouped_df[\"edge\"] = \"contextual proximity\"\n",
        "    return grouped_df\n",
        "\n",
        "# Calculate contextual proximity\n",
        "contextual_proximity_df = calculate_contextual_proximity(triplets_df)\n",
        "contextual_proximity_df.head()"
      ],
      "metadata": {
        "id": "0MpTtBekHhLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge DataFrames**\n",
        "- This function merges the concepts DataFrame with the contextual proximity DataFrame and aggregates the data.\n",
        "- Merging the DataFrames allows for a comprehensive view of the relationships between entities.\n",
        "- This function is used to create a merged DataFrame that will be used to create the graph."
      ],
      "metadata": {
        "id": "kzPIPZ0NHlVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dataframes(concepts_df, contextual_proximity_df):\n",
        "    \"\"\"Merge the concepts DataFrame with the contextual proximity DataFrame.\"\"\"\n",
        "    merged_df = pd.concat([concepts_df, contextual_proximity_df], ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "# Merge DataFrames\n",
        "merged_df = merge_dataframes(triplets_df, contextual_proximity_df)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "QiwotiM7HoHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create NetworkX Graph**\n",
        "- This function creates a NetworkX graph from the merged DataFrame, with nodes and edges representing the relationships between entities.\n",
        "- Creating a graph allows for visualizing and analyzing the relationships between entities.\n",
        "- This function is used to create a graph that will be used for further analysis and visualization."
      ],
      "metadata": {
        "id": "-GsL-ZKuHtYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvis.network import Network\n",
        "import networkx as nx\n",
        "\n",
        "def create_network_graph(triplets_df):\n",
        "    \"\"\"Create a network graph using Pyvis.\"\"\"\n",
        "    G = nx.MultiDiGraph()\n",
        "\n",
        "    for _, row in triplets_df.iterrows():\n",
        "        G.add_node(row['subject'], label=row['subject'], title=row['subject'])\n",
        "        G.add_node(row['object'], label=row['object'], title=row['object'])\n",
        "        G.add_edge(row['subject'], row['object'], label=row['predicate'], title=row['predicate'])\n",
        "\n",
        "    net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
        "    net.from_nx(G)\n",
        "\n",
        "    net.set_options(\"\"\"\n",
        "    var options = {\n",
        "      \"nodes\": {\n",
        "        \"shape\": \"dot\",\n",
        "        \"size\": 16,\n",
        "        \"font\": {\n",
        "          \"size\": 12\n",
        "        }\n",
        "      },\n",
        "      \"edges\": {\n",
        "        \"width\": 2,\n",
        "        \"color\": {\n",
        "          \"inherit\": true\n",
        "        }\n",
        "      },\n",
        "      \"physics\": {\n",
        "        \"forceAtlas2Based\": {\n",
        "          \"gravitationalConstant\": -50,\n",
        "          \"centralGravity\": 0.01,\n",
        "          \"springLength\": 230,\n",
        "          \"springConstant\": 0.18\n",
        "        },\n",
        "        \"maxVelocity\": 50,\n",
        "        \"solver\": \"forceAtlas2Based\",\n",
        "        \"timestep\": 0.22,\n",
        "        \"stabilization\": {\n",
        "          \"iterations\": 150\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    return net\n",
        "\n",
        "# Create network graph\n",
        "net = create_network_graph(triplets_df)\n",
        "net.show(\"network_graph.html\")"
      ],
      "metadata": {
        "id": "s3X7Wk2bHuK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Graph Embeddings of these Metrics**\n",
        "- This function calculates various centrality metrics for the graph and sets them as node attributes.\n",
        "- Centrality metrics help in understanding the importance and influence of nodes in the graph.\n",
        "- This function is used to enrich the graph with additional metrics that will be used for analysis and visualization.\n"
      ],
      "metadata": {
        "id": "YbmeJkOjH1pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ComplEx\n",
        "from ampligraph.evaluation import evaluate_performance, mr_score, mrr_score, hits_at_n_score\n",
        "\n",
        "def create_knowledge_graph_embeddings(triplets_df):\n",
        "    \"\"\"Create knowledge graph embeddings using AmpliGraph.\"\"\"\n",
        "    X = triplets_df[['subject', 'predicate', 'object']].values\n",
        "    model = ComplEx(batches_count=10, epochs=50, k=100, eta=5, optimizer='adam', optimizer_params={'lr': 0.0005},\n",
        "                    loss='multiclass_nll', regularizer='LP', regularizer_params={'p': 3, 'lambda': 0.1},\n",
        "                    seed=0, verbose=True)\n",
        "    model.fit(X)\n",
        "    return model\n",
        "\n",
        "# Create knowledge graph embeddings\n",
        "model = create_knowledge_graph_embeddings(triplets_df)"
      ],
      "metadata": {
        "id": "b2a1ohvnH4-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**\n",
        "- We'll evaluate the knowledge graph embedding model:\n"
      ],
      "metadata": {
        "id": "cCYNE3jdH836"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, triplets_df):\n",
        "    \"\"\"Evaluate the knowledge graph embedding model.\"\"\"\n",
        "    X = triplets_df[['subject', 'predicate', 'object']].values\n",
        "    filter_triples = np.ones(X.shape[0])\n",
        "    ranks = evaluate_performance(X, model=model, filter_triples=filter_triples, use_default_protocol=True, verbose=True)\n",
        "    mr = mr_score(ranks)\n",
        "    mrr = mrr_score(ranks)\n",
        "    hits_at_10 = hits_at_n_score(ranks, n=10)\n",
        "    metrics = {\n",
        "        'MR': mr,\n",
        "        'MRR': mrr,\n",
        "        'Hits@10': hits_at_10\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_model(model, triplets_df)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "b2_2EKqzH_MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up Kuzu DB and Create the Schema**\n",
        "- We'll set up the Kuzu DB and create the schema:\n"
      ],
      "metadata": {
        "id": "eT7aNPgIICJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kuzu\n",
        "\n",
        "def setup_kuzu_db(db_name):\n",
        "    \"\"\"Set up Kuzu DB and create the schema.\"\"\"\n",
        "    db = kuzu.Database(db_name)\n",
        "    conn = kuzu.Connection(db)\n",
        "\n",
        "    conn.execute(\"CREATE NODE TABLE Movie (name STRING, PRIMARY KEY(name))\")\n",
        "    conn.execute(\"CREATE NODE TABLE Person (name STRING, birthDate STRING, PRIMARY KEY(name))\")\n",
        "    conn.execute(\"CREATE REL TABLE ActedIn (FROM Person TO Movie)\")\n",
        "\n",
        "    # Create additional tables and relationships\n",
        "    conn.execute(\"CREATE NODE TABLE Incident (name STRING, date STRING, type STRING, PRIMARY KEY(name))\")\n",
        "    conn.execute(\"CREATE REL TABLE InvolvedIn (FROM Person TO Incident)\")\n",
        "    conn.execute(\"CREATE REL TABLE Targeted (FROM Incident TO Movie)\")\n",
        "\n",
        "    return conn\n",
        "\n",
        "# Set up Kuzu DB\n",
        "conn = setup_kuzu_db(\"test_db\")"
      ],
      "metadata": {
        "id": "xXDcT9hvIEVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insert Data into Kuzu DB**\n",
        "- We'll insert data into the Kuzu DB"
      ],
      "metadata": {
        "id": "DBr6uc1BIGsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_data_into_kuzu(conn, triplets_df):\n",
        "    \"\"\"Insert data into Kuzu DB.\"\"\"\n",
        "    for _, row in triplets_df.iterrows():\n",
        "        subject, predicate, obj = row['subject'], row['predicate'], row['object']\n",
        "        if predicate == 'ActedIn':\n",
        "            conn.execute(f\"CREATE (:Person {{name: '{subject}'}})-[:ActedIn]->(:Movie {{name: '{obj}'}})\")\n",
        "        elif predicate == 'InvolvedIn':\n",
        "            conn.execute(f\"CREATE (:Person {{name: '{subject}'}})-[:InvolvedIn]->(:Incident {{name: '{obj}'}})\")\n",
        "        elif predicate == 'Targeted':\n",
        "            conn.execute(f\"CREATE (:Incident {{name: '{subject}'}})-[:Targeted]->(:Movie {{name: '{obj}'}})\")\n",
        "\n",
        "# Insert data into Kuzu DB\n",
        "insert_data_into_kuzu(conn, triplets_df)"
      ],
      "metadata": {
        "id": "P8P6-ru9IJCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create KuzuQAChain and Query the Graph**\n",
        "- We'll create a KuzuQAChain for querying the graph\n"
      ],
      "metadata": {
        "id": "m9JVfqfOIMQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import KuzuQAChain\n",
        "from langchain_community.graphs import KuzuGraph\n",
        "\n",
        "def create_kuzu_qa_chain(db, api_key):\n",
        "    \"\"\"Create KuzuQAChain for querying the graph.\"\"\"\n",
        "    graph = KuzuGraph(db)\n",
        "    chain = KuzuQAChain.from_llm(llm, graph=graph, verbose=True)\n",
        "    return chain\n",
        "\n",
        "# Create KuzuQAChain\n",
        "chain = create_kuzu_qa_chain(db, GROQ_API_KEY)\n",
        "\n",
        "# Process queries\n",
        "queries = [\n",
        "    \"List all details on BFSI security incidents in India.\",\n",
        "    \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "    \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "    \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "]\n",
        "\n",
        "# Query the graph\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    result = chain.run(query)\n",
        "    print(result)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "jc8WERzCIOer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Refresh Schema Information**\n",
        "- We'll refresh the schema information\n",
        "\n"
      ],
      "metadata": {
        "id": "sdpOONWsIR2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refresh_schema(graph):\n",
        "    \"\"\"Refresh the schema information needed to generate Cypher statements.\"\"\"\n",
        "    graph.refresh_schema()\n",
        "    print(\"Schema information refreshed.\")\n",
        "\n",
        "# Refresh schema\n",
        "refresh_schema(graph)"
      ],
      "metadata": {
        "id": "8_ogD8jyIUUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add Indexing**\n",
        "- We'll create indexes on the Kuzu DB"
      ],
      "metadata": {
        "id": "vbuBvSXVIX00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_indexes(conn):\n",
        "    \"\"\"Create indexes on the Kuzu DB.\"\"\"\n",
        "    safe_execute(conn, \"CREATE INDEX ON Movie(name)\")\n",
        "    safe_execute(conn, \"CREATE INDEX ON Person(name)\")\n",
        "    safe_execute(conn, \"CREATE INDEX ON Incident(name)\")\n",
        "\n",
        "# Create indexes\n",
        "create_indexes(conn)"
      ],
      "metadata": {
        "id": "O1Q7dVYPIZ4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add More Complex Queries**\n",
        "- We'll run more complex queries using the KuzuQAChain"
      ],
      "metadata": {
        "id": "j4FoE_tuIdLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complex_queries(chain):\n",
        "    \"\"\"Run more complex queries using the KuzuQAChain.\"\"\"\n",
        "    complex_queries = [\n",
        "        \"Which actors were involved in incidents related to Lockbit Ransomware?\",\n",
        "        \"List all movies targeted by ransomware attacks in the last month.\",\n",
        "        \"Who is the oldest actor involved in any incident?\"\n",
        "    ]\n",
        "\n",
        "    for query in complex_queries:\n",
        "        print(f\"Query: {query}\")\n",
        "        result = chain.run(query)\n",
        "        print(result)\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Run complex queries\n",
        "run_complex_queries(chain)"
      ],
      "metadata": {
        "id": "zBX0ThPOIfQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}