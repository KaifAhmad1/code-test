{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjX23Gd/Fm+ZB/n/tqeU7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Threat_Intelligence_Information_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Threat Intelligence Information Extraction Pipeline**"
      ],
      "metadata": {
        "id": "h6HnCOCDiiI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rake_nltk keybert pyvis"
      ],
      "metadata": {
        "id": "d-h5ESnXWaL1",
        "outputId": "fbd45ffc-0c99-4a95-ecc9-1868914834f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qK3Z5nEMZ-nC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6fb049-ad09-4341-8341-b633e93da187"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rake_nltk import Rake\n",
        "from keybert import KeyBERT\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize global models to avoid reloading\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "bert_model = KeyBERT()\n",
        "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text while preserving important technical indicators.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Preserve technical indicators\n",
        "    text = re.sub(r'[^\\w\\s\\.-:/@]', ' ', text)  # Keep special chars for IPs/URLs/paths\n",
        "\n",
        "    # Split camelCase and PascalCase\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Xn7AkTVsWS1z"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_technical_patterns(text):\n",
        "    \"\"\"\n",
        "    Extract technical indicators commonly found in threat intelligence.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of found patterns categorized by type\n",
        "    \"\"\"\n",
        "    patterns = {\n",
        "        'ip_addresses': r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
        "        'domains': r'\\b[a-zA-Z0-9][a-zA-Z0-9-._]+\\.[a-zA-Z]{2,}\\b',\n",
        "        'md5_hashes': r'\\b[a-fA-F0-9]{32}\\b',\n",
        "        'sha256_hashes': r'\\b[a-fA-F0-9]{64}\\b',\n",
        "        'cve_ids': r'CVE-\\d{4}-\\d{4,7}',\n",
        "        'urls': r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[^\\s]*',\n",
        "        'file_paths': r'(?:[a-zA-Z]:\\|/)[^\\s/:\\\"|<>]+',\n",
        "        'registry_keys': r'HKEY_[A-Z_]+\\[^\\s]+'\n",
        "    }\n",
        "\n",
        "    results = defaultdict(list)\n",
        "    for pattern_type, pattern in patterns.items():\n",
        "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "        results[pattern_type].extend([match.group() for match in matches])\n",
        "\n",
        "    return dict(results)"
      ],
      "metadata": {
        "id": "s_bimxolYETz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_domain_specific_terms(text):\n",
        "    \"\"\"\n",
        "    Extract cyber security and threat intelligence specific terminology.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of domain-specific terms categorized by type\n",
        "    \"\"\"\n",
        "    threat_indicators = {\n",
        "        'malware_terms': ['malware', 'ransomware', 'trojan', 'botnet', 'worm', 'rootkit', 'keylogger'],\n",
        "        'attack_patterns': ['phishing', 'spear-phishing', 'ddos', 'bruteforce', 'zero-day', 'exploit'],\n",
        "        'threat_actors': ['apt', 'threat actor', 'threat group', 'adversary'],\n",
        "        'security_terms': ['vulnerability', 'payload', 'backdoor', 'c2', 'command and control']\n",
        "    }\n",
        "\n",
        "    results = defaultdict(list)\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    # Extract terms based on categories\n",
        "    for category, terms in threat_indicators.items():\n",
        "        for term in terms:\n",
        "            if term in text.lower():\n",
        "                # Find the complete phrase containing the term\n",
        "                matches = re.finditer(r'\\b\\w+\\s*' + term + r'\\s*\\w+\\b', text.lower())\n",
        "                results[category].extend([match.group() for match in matches])\n",
        "                # Also add single term matches\n",
        "                if term in text.lower():\n",
        "                    results[category].append(term)\n",
        "\n",
        "    # Extract potential new terms using NER\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'PRODUCT']:\n",
        "            results['potential_threats'].append(ent.text)\n",
        "\n",
        "    return dict(results)"
      ],
      "metadata": {
        "id": "kbQurgoUYMXP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_contextual_keywords(text, top_n=15):\n",
        "    \"\"\"\n",
        "    Extract keywords using multiple methods including statistical and contextual approaches.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        top_n (int): Number of top keywords to extract\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing keywords extracted through different methods\n",
        "    \"\"\"\n",
        "    # TF-IDF extraction\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
        "                                  stop_words='english',\n",
        "                                  max_features=100)\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform([text])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        tfidf_scores = dict(zip(feature_names, tfidf_matrix.toarray()[0]))\n",
        "        tfidf_keywords = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    except:\n",
        "        tfidf_keywords = []\n",
        "\n",
        "    # BERT-based extraction\n",
        "    try:\n",
        "        bert_keywords = bert_model.extract_keywords(text,\n",
        "                                                    keyphrase_ngram_range=(1,3),\n",
        "                                                    stop_words='english',\n",
        "                                                    top_n=top_n)\n",
        "    except:\n",
        "        bert_keywords = []\n",
        "\n",
        "    # Spacy NER and noun phrases\n",
        "    doc = nlp(text)\n",
        "    ner_terms = [ent.text for ent in doc.ents]\n",
        "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "    return {\n",
        "        'tfidf_keywords': tfidf_keywords,\n",
        "        'bert_keywords': bert_keywords,\n",
        "        'ner_terms': ner_terms,\n",
        "        'noun_phrases': noun_phrases\n",
        "    }"
      ],
      "metadata": {
        "id": "aG9gN_jlYQ39"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_terms(keywords, similarity_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Find similar terms using multiple similarity measures.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords to analyze\n",
        "        similarity_threshold (float): Threshold for considering terms similar\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of terms and their similar variants\n",
        "    \"\"\"\n",
        "    similar_terms = defaultdict(set)\n",
        "\n",
        "    # WordNet synonyms\n",
        "    for word in keywords:\n",
        "        synsets = wordnet.synsets(word)\n",
        "        for syn in synsets:\n",
        "            similar_terms[word].update(lemma.name() for lemma in syn.lemmas())\n",
        "\n",
        "    # Sentence transformer similarity\n",
        "    if len(keywords) > 1:\n",
        "        try:\n",
        "            embeddings = sentence_transformer.encode(keywords)\n",
        "            similarity_matrix = np.inner(embeddings, embeddings)\n",
        "            for i, word in enumerate(keywords):\n",
        "                similar_indices = np.where(similarity_matrix[i] > similarity_threshold)[0]\n",
        "                similar_terms[word].update([keywords[idx] for idx in similar_indices if idx != i])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Clean up similar terms\n",
        "    for word in similar_terms:\n",
        "        similar_terms[word] = list(similar_terms[word])\n",
        "\n",
        "    return dict(similar_terms)"
      ],
      "metadata": {
        "id": "QX8fJ-osYVBJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_keywords(keywords, n_clusters=None, min_cluster_size=2):\n",
        "    \"\"\"\n",
        "    Cluster keywords based on semantic similarity.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords to cluster\n",
        "        n_clusters (int): Number of clusters (optional)\n",
        "        min_cluster_size (int): Minimum size for a cluster\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of clusters and their keywords\n",
        "    \"\"\"\n",
        "    if len(keywords) < min_cluster_size:\n",
        "        return {'single_cluster': keywords}\n",
        "\n",
        "    try:\n",
        "        # Generate embeddings\n",
        "        embeddings = sentence_transformer.encode(keywords)\n",
        "        # Determine optimal number of clusters\n",
        "        if n_clusters is None:\n",
        "            n_clusters = min(max(2, len(keywords) // 5), 10)\n",
        "        # Perform clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        clusters = kmeans.fit_predict(embeddings)\n",
        "        # Calculate cluster centers and distances\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        distances = np.linalg.norm(embeddings[:, np.newaxis] - cluster_centers, axis=2)\n",
        "        # Organize results with confidence scores\n",
        "        clustered_keywords = defaultdict(list)\n",
        "        for keyword, cluster_id, dist in zip(keywords, clusters, distances):\n",
        "            confidence = 1 - (dist[cluster_id] / np.max(dist))\n",
        "            clustered_keywords[f'cluster_{cluster_id}'].append({\n",
        "                'keyword': keyword,\n",
        "                'confidence': float(confidence)\n",
        "            })\n",
        "        return dict(clustered_keywords)\n",
        "    except:\n",
        "        return {'error_cluster': keywords}"
      ],
      "metadata": {
        "id": "tKtF0aDMYZlQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_threat_intelligence(text):\n",
        "    \"\"\"\n",
        "    Analyze threat intelligence text and extract relevant information.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input threat intelligence text\n",
        "\n",
        "    Returns:\n",
        "        dict: Comprehensive analysis results\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    processed_text = preprocess_text(text)\n",
        "    # Extract different types of information\n",
        "    technical_indicators = extract_technical_patterns(text)\n",
        "    domain_terms = extract_domain_specific_terms(processed_text)\n",
        "    contextual_kw = extract_contextual_keywords(processed_text)\n",
        "    # Combine all unique keywords\n",
        "    all_keywords = set()\n",
        "    # Add technical indicators\n",
        "    for indicator_type in technical_indicators.values():\n",
        "        all_keywords.update(indicator_type)\n",
        "    # Add domain terms\n",
        "    for term_type in domain_terms.values():\n",
        "        all_keywords.update(term_type)\n",
        "    # Add contextual keywords\n",
        "    all_keywords.update([kw[0] for kw in contextual_kw['bert_keywords']])\n",
        "    all_keywords.update(contextual_kw['ner_terms'])\n",
        "    # Convert to list and find similar terms\n",
        "    all_keywords = list(all_keywords)\n",
        "    similar_terms = find_similar_terms(all_keywords)\n",
        "    # Cluster keywords\n",
        "    clusters = cluster_keywords(all_keywords)\n",
        "    return {\n",
        "        'technical_indicators': technical_indicators,\n",
        "        'domain_specific_terms': domain_terms,\n",
        "        'contextual_keywords': contextual_kw,\n",
        "        'similar_terms': similar_terms,\n",
        "        'clustered_keywords': clusters\n",
        "    }"
      ],
      "metadata": {
        "id": "DeKgfXV_Yh5e"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the analysis immediately after initialization\n",
        "sample_text = \"\"\"\n",
        "APT29 (also known as Cozy Bear) has been observed using WellMess malware targeting COVID-19 research organizations.\n",
        "The threat actor employs sophisticated spear-phishing campaigns and exploits CVE-2020-0688 to deploy backdoors.\n",
        "Command and control traffic was observed at 192.168.1.1 and malicious.domain.com.\n",
        "The attacks involved SHA256 hash abc123def456 and registry modifications at HKEY_LOCAL_MACHINE\\Software\\Microsoft.\n",
        "\"\"\"\n",
        "\n",
        "results = analyze_threat_intelligence(sample_text)\n",
        "\n",
        "# Print results in a structured way\n",
        "print(\"\\nTechnical Indicators:\")\n",
        "for indicator_type, indicators in results['technical_indicators'].items():\n",
        "    if indicators:\n",
        "        print(f\"\\n{indicator_type.replace('_', ' ').title()}:\")\n",
        "        for indicator in indicators:\n",
        "            print(f\"- {indicator}\")\n",
        "\n",
        "print(\"\\nDomain-Specific Terms:\")\n",
        "for term_type, terms in results['domain_specific_terms'].items():\n",
        "    if terms:\n",
        "        print(f\"\\n{term_type.replace('_', ' ').title()}:\")\n",
        "        for term in terms:\n",
        "            print(f\"- {term}\")\n",
        "\n",
        "print(\"\\nClustered Keywords:\")\n",
        "for cluster_name, keywords in results['clustered_keywords'].items():\n",
        "    print(f\"\\n{cluster_name}:\")\n",
        "    for kw in keywords:\n",
        "        if isinstance(kw, dict):\n",
        "            print(f\"- {kw['keyword']} (confidence: {kw['confidence']:.2f})\")\n",
        "        else:\n",
        "            print(f\"- {kw}\")"
      ],
      "metadata": {
        "id": "CH8_4OX2cw8y",
        "outputId": "d209d437-a7f5-483e-ccf9-9e50391507b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Technical Indicators:\n",
            "\n",
            "Ip Addresses:\n",
            "- 192.168.1.1\n",
            "\n",
            "Domains:\n",
            "- malicious.domain.com\n",
            "\n",
            "Cve Ids:\n",
            "- CVE-2020-0688\n",
            "\n",
            "Domain-Specific Terms:\n",
            "\n",
            "Malware Terms:\n",
            "- wellmess malware targeting\n",
            "- malware\n",
            "\n",
            "Attack Patterns:\n",
            "- spear phishing campaigns\n",
            "- phishing\n",
            "- and exploits\n",
            "- exploit\n",
            "\n",
            "Threat Actors:\n",
            "- apt\n",
            "- the threat actor employs\n",
            "- threat actor\n",
            "\n",
            "Security Terms:\n",
            "- deploy backdoors\n",
            "- backdoor\n",
            "- command and control\n",
            "\n",
            "Potential Threats:\n",
            "- cozy bear\n",
            "- hkey_local_machine\n",
            "- microsoft\n",
            "\n",
            "Clustered Keywords:\n",
            "\n",
            "cluster_4:\n",
            "- 192.168.1.1 (confidence: 0.35)\n",
            "- spear phishing campaigns (confidence: 0.50)\n",
            "- phishing (confidence: 0.50)\n",
            "\n",
            "cluster_2:\n",
            "- apt29 known cozy (confidence: 0.64)\n",
            "- cozy bear (confidence: 0.36)\n",
            "- apt29 known (confidence: 0.48)\n",
            "- apt (confidence: 0.46)\n",
            "\n",
            "cluster_1:\n",
            "- using wellmess malware (confidence: 0.60)\n",
            "- hkey_local_machine (confidence: 0.19)\n",
            "- malware targeting (confidence: 0.55)\n",
            "- malware targeting covid (confidence: 0.43)\n",
            "- wellmess malware (confidence: 0.61)\n",
            "- wellmess malware targeting (confidence: 0.63)\n",
            "- malware (confidence: 0.53)\n",
            "\n",
            "cluster_5:\n",
            "- malicious (confidence: 0.44)\n",
            "- and exploits (confidence: 0.31)\n",
            "- microsoft (confidence: 0.25)\n",
            "- command and control (confidence: 0.21)\n",
            "- malicious.domain.com (confidence: 0.41)\n",
            "- the threat actor employs (confidence: 0.27)\n",
            "- malicious domain (confidence: 0.48)\n",
            "- threat actor (confidence: 0.29)\n",
            "- 168 malicious (confidence: 0.41)\n",
            "- 168 malicious domain (confidence: 0.43)\n",
            "- exploit (confidence: 0.32)\n",
            "\n",
            "cluster_0:\n",
            "- CVE-2020-0688 (confidence: 0.45)\n",
            "- 19 (confidence: 0.45)\n",
            "- 2020 0688 (confidence: 0.51)\n",
            "\n",
            "cluster_3:\n",
            "- backdoors command control (confidence: 0.57)\n",
            "- backdoor (confidence: 0.51)\n",
            "- deploy backdoors (confidence: 0.67)\n",
            "- 0688 deploy backdoors (confidence: 0.55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9kAGBtnc2DG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}