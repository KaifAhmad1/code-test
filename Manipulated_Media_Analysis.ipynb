{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbidoa4T3qyNTuCym52sSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Manipulated_Media_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Multimodal Manipulated Media Analysis Pipeline for Defensive Forensic**\n",
        "\n",
        "\n",
        "\n",
        "This end-to-end pipeline uses LangChain-based VLLM and Groq LLM integration\n",
        "along with LangGraph multiagent orchestration to analyze audio, video, and image modalities.\n",
        "Each modality has its own processing and analysis steps, and the results are aggregated\n",
        "into a consolidated report"
      ],
      "metadata": {
        "id": "2Fqt0MDqomfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iYbnoslNm-tQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy opencv-python librosa pydantic mediapipe moviepy face_recognition scikit-image dtw-python scipy langchain langchain_community langchain_core langgraph nest_asyncio ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "###  IMPORTS SECTION  ###\n",
        "#########################\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# Media processing libraries\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# For interactive notebook usage\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# For image quality metrics\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from dtw import dtw\n",
        "import face_recognition\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Langchain and LangGraph imports\n",
        "from langchain_community.llms import VLLM\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Apply nest_asyncio to allow running async code in notebooks\n",
        "nest_asyncio.apply()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysWFk98_nTbX",
        "outputId": "3040f6c8-6694-4c5b-84e0-db8f864e0aa4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "### GLOBAL DEBUG SETTINGS ###\n",
        "#############################\n",
        "DEBUG = True\n",
        "def debug_print(msg: str):\n",
        "    \"\"\"Print debug messages when DEBUG is enabled.\"\"\"\n",
        "    if DEBUG:\n",
        "        print(f\"[DEBUG] {msg}\")"
      ],
      "metadata": {
        "id": "G9XItUIFo_8a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "### MODEL INITIALIZATION SECTION  ###\n",
        "######################################\n",
        "COMMON_PARAMS = {\n",
        "    \"task\": \"generate\",\n",
        "    \"max_model_len\": 4096,\n",
        "    \"dtype\": \"half\",\n",
        "    \"gpu_memory_utilization\": 0.85,\n",
        "    \"cpu_offload_gb\": 8,\n",
        "    \"enforce_eager\": True,\n",
        "    \"trust_remote_code\": True\n",
        "}\n",
        "\n",
        "def init_vllm_model(name: str, model_id: str, **overrides):\n",
        "    \"\"\"Initialize a VLLM model with specified parameters.\"\"\"\n",
        "    params = {**COMMON_PARAMS, **overrides}\n",
        "    debug_print(f\"Initializing VLLM model '{name}' with id '{model_id}' and params: {params}\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"params\": params}\n",
        "\n",
        "def init_groq_model(name: str, model_id: str):\n",
        "    \"\"\"Initialize a Groq model with API key.\"\"\"\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\", \"your_groq_api_key\")\n",
        "    debug_print(f\"Initializing Groq model '{name}' with id '{model_id}' using API key.\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"api_key\": api_key}\n",
        "\n",
        "# Groq LLM wrapper class for LangChain\n",
        "class GroqLLM:\n",
        "    \"\"\"A wrapper for Groq LLM API.\"\"\"\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def call_as_llm(self, prompt: str) -> str:\n",
        "        \"\"\"Process a prompt through the Groq API.\"\"\"\n",
        "        debug_print(f\"GroqLLM processing prompt with {self.model_data['name']}\")\n",
        "        # In a real implementation, this would make an API call to Groq\n",
        "        # Here we simulate a response for demonstration\n",
        "        if \"audio\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.82\\nAnomalies: [\\\"unnatural voice transitions\\\", \\\"inconsistent background noise\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.77\\nAnomalies: [\\\"facial landmark inconsistencies\\\", \\\"unnatural eye movements\\\"]\"\n",
        "        elif \"image\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.68\\nAnomalies: [\\\"lighting inconsistencies\\\", \\\"unusual facial proportions\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.75\\nAnomalies: [\\\"inconsistent narrative\\\", \\\"unusual phrasing patterns\\\"]\"\n",
        "\n",
        "class GroqLLMWrapper(LLM):\n",
        "    \"\"\"LangChain compatible wrapper for GroqLLM.\"\"\"\n",
        "    def __init__(self, groq_llm: GroqLLM, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._groq_llm = groq_llm\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop=None) -> str:\n",
        "        return self._groq_llm.call_as_llm(prompt)\n",
        "\n",
        "# Simulated VLLM model wrapper\n",
        "class VLLMSimulated:\n",
        "    \"\"\"A simulated VLLM model for demonstration.\"\"\"\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        \"\"\"Process a prompt through the simulated VLLM.\"\"\"\n",
        "        debug_print(f\"VLLM simulated processing with {self.model_data['name']}\")\n",
        "        # Simulate different responses based on the model name\n",
        "        if \"wav2vec\" in self.model_data[\"name\"] or \"whisper\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.79\\nAnomalies: [\\\"frequency anomalies\\\", \\\"unnatural pauses\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.81\\nAnomalies: [\\\"temporal inconsistencies\\\", \\\"blending artifacts\\\"]\"\n",
        "        elif \"llava\" in self.model_data[\"name\"] or \"clip\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.73\\nAnomalies: [\\\"compression artifacts\\\", \\\"unusual texture patterns\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.78\\nAnomalies: [\\\"stylistic inconsistencies\\\"]\"\n",
        "\n",
        "# Initialize models for each modality\n",
        "debug_print(\"Initializing modality-specific models...\")\n",
        "models = {\n",
        "    \"audio\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"wav2vec2\", \"facebook/wav2vec2-large-robust-ft-swbd-300h\", tensor_parallel_size=1)),\n",
        "            VLLMSimulated(init_vllm_model(\"whisper\", \"openai/whisper-large-v3\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_audio_model\", \"whisper-large-v3-turbo\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_audio\", \"meta-llama/llama-4-audio-17b-16e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"video\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_next_video\", \"llava-hf/LLaVA-NeXT-Video-7B-hf\", tensor_parallel_size=2, max_tokens=1024)),\n",
        "            VLLMSimulated(init_vllm_model(\"videomae\", \"MCG-NJU/videomae-large-static\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_video_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_video_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"image\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_image\", \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", tensor_parallel_size=2)),\n",
        "            VLLMSimulated(init_vllm_model(\"clip\", \"openai/clip-vit-large-patch14\", tensor_parallel_size=1))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_image_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_image_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"text\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llama3\", \"meta-llama/Llama-3-70b-hf\", tensor_parallel_size=4))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCrOCZ4_poed",
        "outputId": "581778f5-2f6c-48a3-b170-3525383011c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Initializing modality-specific models...\n",
            "[DEBUG] Initializing VLLM model 'wav2vec2' with id 'facebook/wav2vec2-large-robust-ft-swbd-300h' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing VLLM model 'whisper' with id 'openai/whisper-large-v3' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_audio_model' with id 'whisper-large-v3-turbo' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_llama_audio' with id 'meta-llama/llama-4-audio-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_next_video' with id 'llava-hf/LLaVA-NeXT-Video-7B-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2, 'max_tokens': 1024}\n",
            "[DEBUG] Initializing VLLM model 'videomae' with id 'MCG-NJU/videomae-large-static' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_video_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_video_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_image' with id 'llava-hf/llava-onevision-qwen2-7b-ov-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing VLLM model 'clip' with id 'openai/clip-vit-large-patch14' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing Groq model 'groq_image_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_image_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llama3' with id 'meta-llama/Llama-3-70b-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 4}\n",
            "[DEBUG] Initializing Groq model 'groq_text_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_text_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "### DATA MODELS & REPORT STRUCTURES SECTION ###\n",
        "###############################################\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    \"\"\"Model for storing deepfake analysis results per modality.\"\"\"\n",
        "    score: float\n",
        "    label: str\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    explanation: Optional[str] = None\n",
        "    model_scores: Dict[str, float] = Field(default_factory=dict)\n",
        "\n",
        "class Evidence(BaseModel):\n",
        "    \"\"\"Model for storing specific evidence of manipulation.\"\"\"\n",
        "    type: str\n",
        "    description: str\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: Optional[float] = None\n",
        "    location: Optional[Dict[str, int]] = None\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    \"\"\"Comprehensive report aggregating all modality analyses.\"\"\"\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Evidence]\n",
        "    metadata: Dict[str, Any]\n",
        "    recommendations: List[str] = Field(default_factory=list)\n",
        "    confidence_matrix: Dict[str, Dict[str, float]] = Field(default_factory=dict)\n",
        "    processing_time: float"
      ],
      "metadata": {
        "id": "Otpsa4nDpx5s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "### PROCESSING & ANALYSIS FUNCTIONS ###\n",
        "#######################################\n",
        "# --- Shared utility functions ---\n",
        "def parse_model_output(output: str) -> Tuple[float, List[str]]:\n",
        "    \"\"\"Parse model output to extract score and anomalies.\"\"\"\n",
        "    debug_print(f\"Parsing model output: {output}\")\n",
        "    score_match = re.search(r\"Score:?\\s*(0\\.\\d+|1\\.0)\", output, re.IGNORECASE)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    anomalies = []\n",
        "    anomalies_pattern = r\"Anomalies:?\\s*\\[(.*?)\\]|Anomalies:?\\s*(.*?)(?=\\n|$)\"\n",
        "    anomalies_match = re.search(anomalies_pattern, output, re.IGNORECASE | re.DOTALL)\n",
        "    if anomalies_match:\n",
        "        anomalies_text = anomalies_match.group(1) or anomalies_match.group(2)\n",
        "        if anomalies_text:\n",
        "            anomalies = [a.strip().strip('\"\\'') for a in anomalies_text.split(',') if a.strip()]\n",
        "    debug_print(f\"Parsed output: score={score}, anomalies={anomalies}\")\n",
        "    return score, anomalies\n",
        "\n",
        "async def aggregate_llm_outputs(prompt: str, content: str, modality: str=\"generic\") -> Tuple[float, List[str], Dict[str, float]]:\n",
        "    \"\"\"Aggregate outputs from multiple LLMs for a given modality.\"\"\"\n",
        "    debug_print(f\"Aggregating LLM outputs for modality {modality}\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "\n",
        "    try:\n",
        "        # Get responses from VLLM models\n",
        "        for i, model in enumerate(models[modality][\"vllm\"]):\n",
        "            model_name = f\"vllm_{modality}_{i}\"\n",
        "            try:\n",
        "                response = model(f\"{prompt}\\n\\nContent: {content}\")\n",
        "                s, a = parse_model_output(response)\n",
        "                scores.append(s)\n",
        "                all_anomalies.extend(a)\n",
        "                model_scores[model_name] = s\n",
        "                debug_print(f\"{model_name} score: {s}, anomalies: {a}\")\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error with {model_name}: {e}\")\n",
        "\n",
        "        # Get responses from Groq models\n",
        "        for i, model in enumerate(models[modality][\"groq\"]):\n",
        "            model_name = f\"groq_{modality}_{i}\"\n",
        "            try:\n",
        "                chat_prompt = ChatPromptTemplate.from_messages([\n",
        "                    (\"system\", f\"You are a forensic {modality} deepfake detection expert.\"),\n",
        "                    (\"user\", f\"{prompt}\\n\\nAnalyze this content: {content}\")\n",
        "                ])\n",
        "                chain = LLMChain(prompt=chat_prompt, llm=model)\n",
        "                response = await asyncio.to_thread(chain.run, {\"content\": content})\n",
        "                s, a = parse_model_output(response)\n",
        "                scores.append(s)\n",
        "                all_anomalies.extend(a)\n",
        "                model_scores[model_name] = s\n",
        "                debug_print(f\"{model_name} score: {s}, anomalies: {a}\")\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error with {model_name}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in aggregate_llm_outputs: {e}\")\n",
        "\n",
        "    # Compute aggregate score and deduplicate anomalies\n",
        "    agg_score = float(np.mean(scores)) if scores else 0.5\n",
        "    unique_anomalies = list(set(all_anomalies))\n",
        "    debug_print(f\"Aggregated score for {modality}: {agg_score}, anomalies: {unique_anomalies}\")\n",
        "    return agg_score, unique_anomalies, model_scores"
      ],
      "metadata": {
        "id": "CfDQ_Ffzp4cW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AUDIO PROCESSING & ANALYSIS ---\n",
        "def audio_preprocessing(audio_path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load and preprocess audio data.\"\"\"\n",
        "    debug_print(f\"Loading and preprocessing audio from {audio_path} ...\")\n",
        "    try:\n",
        "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        # Basic preprocessing steps\n",
        "        # 1. Normalization\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
        "\n",
        "        # 2. Simple noise reduction (high-pass filter)\n",
        "        if len(audio_data) > 0:\n",
        "            b, a = librosa.filters.butter(4, 100/(sr/2), btype='highpass')\n",
        "            audio_data = librosa.filters.filtfilt(b, a, audio_data)\n",
        "\n",
        "        debug_print(f\"Audio loaded and preprocessed. Sample rate: {sr}, Duration: {len(audio_data)/sr:.2f}s\")\n",
        "        return audio_data, sr\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in audio preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sr: int) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from audio data for deepfake detection.\"\"\"\n",
        "    debug_print(\"Extracting audio features...\")\n",
        "    features = {}\n",
        "\n",
        "    # Short-time Fourier transform\n",
        "    stft = np.abs(librosa.stft(audio_data))\n",
        "    features[\"stft\"] = stft\n",
        "\n",
        "    # Mel spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
        "    features[\"mel_spectrogram\"] = mel_spec\n",
        "\n",
        "    # MFCC features\n",
        "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
        "    features[\"mfccs\"] = mfccs\n",
        "\n",
        "    # Spectral contrast\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
        "    features[\"spectral_contrast\"] = spectral_contrast\n",
        "\n",
        "    # Chroma features\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
        "    features[\"chroma\"] = chroma\n",
        "\n",
        "    # Zero crossing rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
        "    features[\"zero_crossing_rate\"] = zcr\n",
        "\n",
        "    # Tempo estimation\n",
        "    onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr)\n",
        "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "    features[\"tempo\"] = tempo[0]\n",
        "\n",
        "    debug_print(f\"Audio features extracted: {list(features.keys())}\")\n",
        "    return features\n",
        "\n",
        "def detect_audio_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in audio features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting audio anomalies from features...\")\n",
        "    anomalies = []\n",
        "\n",
        "    # Example anomaly detection logic (would be much more sophisticated in practice)\n",
        "\n",
        "    # Check for unusual spectral patterns\n",
        "    mel_spec = features[\"mel_spectrogram\"]\n",
        "    if np.std(mel_spec) < 0.1 or np.std(mel_spec) > 10:\n",
        "        anomalies.append(\"unusual spectral distribution\")\n",
        "\n",
        "    # Check for unnatural MFCC patterns\n",
        "    mfccs = features[\"mfccs\"]\n",
        "    if np.max(np.diff(mfccs, axis=1)) > 5:\n",
        "        anomalies.append(\"abrupt MFCC transitions\")\n",
        "\n",
        "    # Check for unusual tempo\n",
        "    tempo = features[\"tempo\"]\n",
        "    if tempo < 40 or tempo > 240:\n",
        "        anomalies.append(\"unusual speech tempo\")\n",
        "\n",
        "    # Check for unusual zero-crossing rate\n",
        "    zcr = features[\"zero_crossing_rate\"]\n",
        "    if np.mean(zcr) > 0.3:\n",
        "        anomalies.append(\"unusually high zero-crossing rate\")\n",
        "\n",
        "    debug_print(f\"Detected audio anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_audio_analysis(audio_data: np.ndarray, sr: int, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive audio analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced audio analysis...\")\n",
        "\n",
        "    # Extract audio features\n",
        "    audio_features = extract_audio_features(audio_data, sr)\n",
        "\n",
        "    # Detect anomalies from traditional signal processing\n",
        "    signal_anomalies = detect_audio_anomalies(audio_features)\n",
        "    signal_score = 0.8 if not signal_anomalies else 0.6  # Simplified scoring\n",
        "\n",
        "    # Format feature content for LLM analysis\n",
        "    feature_summary = (\n",
        "        f\"Audio duration: {len(audio_data)/sr:.2f}s, Sample rate: {sr}Hz\\n\"\n",
        "        f\"Mean amplitude: {np.mean(np.abs(audio_data)):.4f}, Max amplitude: {np.max(np.abs(audio_data)):.4f}\\n\"\n",
        "        f\"Detected tempo: {audio_features['tempo']:.2f} BPM\\n\"\n",
        "        f\"Mean zero-crossing rate: {np.mean(audio_features['zero_crossing_rate']):.4f}\\n\"\n",
        "        f\"Signal-to-noise ratio estimate: {np.mean(audio_data**2) / np.std(audio_data):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {signal_anomalies}\"\n",
        "    )\n",
        "\n",
        "    # Get LLM-based analysis\n",
        "    prompt = \"Based on the audio features, provide a deepfake confidence score (0-1) and list specific audio anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"audio\")\n",
        "\n",
        "    # Combine traditional and LLM analysis\n",
        "    combined_anomalies = list(set(signal_anomalies + llm_anomalies))\n",
        "    final_score = (signal_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([signal_score, llm_score])  # Higher agreement = higher confidence\n",
        "\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_audio_analysis (signal processing + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced audio analysis combining signal processing features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "kFWN6Eh-qAQB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VIDEO PROCESSING & ANALYSIS ---\n",
        "def video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load and preprocess video data.\"\"\"\n",
        "    debug_print(f\"Loading video from {video_path} ...\")\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "\n",
        "        # Extract frames at regular intervals\n",
        "        frame_count = int(video.fps * video.duration)\n",
        "        sample_rate = max(1, frame_count // 30)  # Extract up to 30 frames\n",
        "        frames = [video.get_frame(i/video.fps) for i in range(0, frame_count, sample_rate)]\n",
        "\n",
        "        # Extract audio if available\n",
        "        audio = video.audio.to_soundarray() if video.audio else None\n",
        "\n",
        "        # Collect metadata\n",
        "        metadata = {\n",
        "            \"duration\": video.duration,\n",
        "            \"fps\": video.fps,\n",
        "            \"frame_count\": frame_count,\n",
        "            \"resolution\": f\"{video.size[0]}x{video.size[1]}\",\n",
        "            \"file_size\": os.path.getsize(video_path),\n",
        "            \"file_path\": video_path\n",
        "        }\n",
        "\n",
        "        debug_print(f\"Video preprocessed: {len(frames)} frames extracted, Audio present: {'Yes' if audio is not None else 'No'}\")\n",
        "        return {\n",
        "            \"frames\": frames,\n",
        "            \"audio\": audio,\n",
        "            \"metadata\": metadata,\n",
        "            \"image\": frames[0] if frames else None,\n",
        "            \"text\": \"Extracted text placeholder\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in video preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_video_features(frames: List[np.ndarray]) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from video frames for deepfake detection.\"\"\"\n",
        "    debug_print(f\"Extracting video features from {len(frames)} frames...\")\n",
        "    features = {}\n",
        "\n",
        "    # Initialize face mesh detector for facial landmarks\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=True,\n",
        "        max_num_faces=1,\n",
        "        min_detection_confidence=0.5\n",
        "    )\n",
        "\n",
        "    # Process frames\n",
        "    face_landmarks_sequence = []\n",
        "    face_embeddings = []\n",
        "    optical_flow_metrics = []\n",
        "    frame_diffs = []\n",
        "    blur_metrics = []\n",
        "    compression_metrics = []\n",
        "\n",
        "    for i in range(len(frames)):\n",
        "        # Convert to RGB for face detection\n",
        "        frame_rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB) if frames[i].shape[2] == 3 else frames[i]\n",
        "\n",
        "        # Get facial landmarks\n",
        "        face_landmarks = face_mesh.process(frame_rgb).multi_face_landmarks\n",
        "        if face_landmarks:\n",
        "            face_landmarks_sequence.append(face_landmarks[0].landmark)\n",
        "\n",
        "            # Get face location for face recognition\n",
        "            face_locations = face_recognition.face_locations(frame_rgb)\n",
        "            if face_locations:\n",
        "                # Get face encoding\n",
        "                face_encoding = face_recognition.face_encodings(frame_rgb, face_locations)[0]\n",
        "                face_embeddings.append(face_encoding)\n",
        "\n",
        "        # Calculate frame differences (if not the first frame)\n",
        "        if i > 0:\n",
        "            # Convert frames to grayscale for simpler comparison\n",
        "            prev_gray = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY)\n",
        "            curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Frame difference\n",
        "            diff = cv2.absdiff(prev_gray, curr_gray)\n",
        "            frame_diffs.append(np.mean(diff))\n",
        "\n",
        "            # Simple optical flow metric\n",
        "            flow = np.mean(diff)  # In a real implementation, use proper optical flow\n",
        "            optical_flow_metrics.append(flow)\n",
        "\n",
        "        # Calculate blur metric (variance of Laplacian)\n",
        "        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        blur_metric = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        blur_metrics.append(blur_metric)\n",
        "\n",
        "        # Simple compression artifacts metric\n",
        "        compression_metric = np.mean(cv2.Canny(gray, 100, 200))\n",
        "        compression_metrics.append(compression_metric)\n",
        "\n",
        "    # Store features\n",
        "    features[\"face_landmarks_sequence\"] = face_landmarks_sequence\n",
        "    features[\"face_embeddings\"] = face_embeddings\n",
        "    features[\"optical_flow_metrics\"] = optical_flow_metrics\n",
        "    features[\"frame_diffs\"] = frame_diffs\n",
        "    features[\"blur_metrics\"] = blur_metrics\n",
        "    features[\"compression_metrics\"] = compression_metrics\n",
        "\n",
        "    debug_print(f\"Video features extracted: {list(features.keys())}\")\n",
        "    debug_print(f\"Found facial landmarks in {len(face_landmarks_sequence)}/{len(frames)} frames\")\n",
        "\n",
        "    face_mesh.close()\n",
        "    return features\n",
        "\n",
        "def detect_video_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in video features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting video anomalies from features...\")\n",
        "    anomalies = []\n",
        "\n",
        "    # Check for facial landmark consistency\n",
        "    face_landmarks = features.get(\"face_landmarks_sequence\", [])\n",
        "    if len(face_landmarks) > 1:\n",
        "        # Check for abrupt changes in landmarks\n",
        "        landmark_diffs = []\n",
        "        for i in range(1, len(face_landmarks)):\n",
        "            diff = sum(np.sqrt((face_landmarks[i][j].x - face_landmarks[i-1][j].x)**2 +\n",
        "                      (face_landmarks[i][j].y - face_landmarks[i-1][j].y)**2)\n",
        "                     for j in range(min(5, len(face_landmarks[i]))))  # Use first 5 landmarks\n",
        "            landmark_diffs.append(diff)\n",
        "\n",
        "        if np.max(landmark_diffs) > 0.5:  # Threshold for unnatural movement\n",
        "            anomalies.append(\"abrupt facial landmark movements\")\n",
        "\n",
        "    # Check face embedding consistency\n",
        "    face_embeddings = features.get(\"face_embeddings\", [])\n",
        "    if len(face_embeddings) > 1:\n",
        "        embedding_diffs = []\n",
        "        for i in range(1, len(face_embeddings)):\n",
        "            diff = cosine(face_embeddings[i], face_embeddings[i-1])\n",
        "            embedding_diffs.append(diff)\n",
        "\n",
        "        if np.max(embedding_diffs) > 0.3:  # Threshold for identity shift\n",
        "            anomalies.append(\"inconsistent face identity\")\n",
        "\n",
        "    # Check for unnatural motion\n",
        "    frame_diffs = features.get(\"frame_diffs\", [])\n",
        "    if frame_diffs:\n",
        "        if np.std(frame_diffs) < 0.01:  # Too smooth\n",
        "            anomalies.append(\"unnaturally smooth motion\")\n",
        "        if np.max(frame_diffs) / (np.mean(frame_diffs) + 1e-6) > 10:  # Erratic changes\n",
        "            anomalies.append(\"erratic frame changes\")\n",
        "\n",
        "    # Check for inconsistent blur levels\n",
        "    blur_metrics = features.get(\"blur_metrics\", [])\n",
        "    if blur_metrics:\n",
        "        if np.std(blur_metrics) / (np.mean(blur_metrics) + 1e-6) > 0.8:  # Inconsistent blur\n",
        "            anomalies.append(\"inconsistent blur levels\")\n",
        "\n",
        "    # Check for compression artifact patterns\n",
        "    compression_metrics = features.get(\"compression_metrics\", [])\n",
        "    if compression_metrics:\n",
        "        if np.std(compression_metrics) / (np.mean(compression_metrics) + 1e-6) > 0.5:\n",
        "            anomalies.append(\"inconsistent compression artifacts\")\n",
        "\n",
        "    debug_print(f\"Detected video anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive video analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced video analysis...\")\n",
        "\n",
        "    frames = video_data.get(\"frames\", [])\n",
        "    if not frames:\n",
        "        debug_print(\"No frames available for analysis\")\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.5,\n",
        "            label=\"INCONCLUSIVE\",\n",
        "            confidence=0.0,\n",
        "            method=\"advanced_video_analysis\",\n",
        "            anomalies=[\"no frames available for analysis\"],\n",
        "            explanation=\"Could not perform video analysis due to lack of frame data.\"\n",
        "        )\n",
        "\n",
        "    # Extract video features\n",
        "    video_features = extract_video_features(frames)\n",
        "\n",
        "    # Detect anomalies from traditional computer vision\n",
        "    cv_anomalies = detect_video_anomalies(video_features)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6  # Simplified scoring\n",
        "\n",
        "    # Format feature content for LLM analysis\n",
        "    feature_summary = (\n",
        "        f\"Video features: {len(frames)} frames, Resolution: {video_data['metadata']['resolution']}\\n\"\n",
        "        f\"Facial landmarks detected in {len(video_features.get('face_landmarks_sequence', []))}/{len(frames)} frames\\n\"\n",
        "        f\"Mean frame difference: {np.mean(video_features.get('frame_diffs', [0])):.4f}\\n\"\n",
        "        f\"Blur consistency: {np.std(video_features.get('blur_metrics', [0]))/(np.mean(video_features.get('blur_metrics', [1]))+1e-6):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\"\n",
        "    )\n",
        "\n",
        "    # Get LLM-based analysis\n",
        "    prompt = \"Based on the video features, provide a deepfake confidence score (0-1) and list specific video anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"video\")\n",
        "\n",
        "    # Combine traditional and LLM analysis\n",
        "    combined_anomalies = list(set(cv_anomalies + llm_anomalies))\n",
        "    final_score = (cv_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([cv_score, llm_score])  # Higher agreement = higher confidence\n",
        "\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_video_analysis (computer vision + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced video analysis combining computer vision features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "-QJAMVIQqHkn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMAGE PROCESSING & ANALYSIS ---\n",
        "def image_preprocessing(image_path: str) -> np.ndarray:\n",
        "    \"\"\"Load and preprocess image data.\"\"\"\n",
        "    debug_print(f\"Loading and preprocessing image from {image_path} ...\")\n",
        "    try:\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
        "\n",
        "        # Resize if too large\n",
        "        max_dim = 1024\n",
        "        h, w = image.shape[:2]\n",
        "        if max(h, w) > max_dim:\n",
        "            scale = max_dim / max(h, w)\n",
        "            image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
        "            debug_print(f\"Image resized from {w}x{h} to {int(w * scale)}x{int(h * scale)}\")\n",
        "\n",
        "        # Convert to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        debug_print(f\"Image loaded and preprocessed. Shape: {image_rgb.shape}\")\n",
        "        return image_rgb\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in image preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from an image for deepfake detection.\"\"\"\n",
        "    debug_print(\"Extracting image features...\")\n",
        "    features = {}\n",
        "\n",
        "    # Initialize face detection\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    features[\"face_count\"] = len(faces)\n",
        "\n",
        "    # If a face is detected, extract face-specific features\n",
        "    if len(faces) > 0:\n",
        "        # Extract the largest face\n",
        "        x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        features[\"face_roi\"] = face_roi\n",
        "\n",
        "        # Face landmarks using face_recognition\n",
        "        face_landmarks = face_recognition.face_landmarks(image)\n",
        "        features[\"face_landmarks\"] = face_landmarks\n",
        "\n",
        "        # Face encoding (for identity verification)\n",
        "        face_encodings = face_recognition.face_encodings(image)\n",
        "        if face_encodings:\n",
        "            features[\"face_encoding\"] = face_encodings[0]\n",
        "\n",
        "    # General image features\n",
        "    # Error Level Analysis (ELA) - simplified version\n",
        "    # Save image with low quality and reload\n",
        "    temp_path = \"temp_ela.jpg\"\n",
        "    cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "    low_qual = cv2.imread(temp_path)\n",
        "    low_qual_rgb = cv2.cvtColor(low_qual, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Calculate difference\n",
        "    ela = cv2.absdiff(image, low_qual_rgb)\n",
        "    features[\"ela\"] = cv2.cvtColor(ela, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Remove temp file\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "\n",
        "    # Noise analysis\n",
        "    noise = cv2.medianBlur(gray, 5) - gray\n",
        "    features[\"noise_pattern\"] = noise\n",
        "\n",
        "    # Edge detection\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features[\"edges\"] = edges\n",
        "\n",
        "    # JPEG quantization tables - proxy using block artifacts\n",
        "    dct_blocks = np.zeros_like(gray, dtype=float)\n",
        "    block_size = 8\n",
        "    for i in range(0, gray.shape[0] - block_size, block_size):\n",
        "        for j in range(0, gray.shape[1] - block_size, block_size):\n",
        "            block = gray[i:i+block_size, j:j+block_size].astype(float)\n",
        "            dct_block = cv2.dct(block)\n",
        "            dct_blocks[i:i+block_size, j:j+block_size] = dct_block\n",
        "\n",
        "    features[\"dct_blocks\"] = dct_blocks\n",
        "\n",
        "    debug_print(f\"Image features extracted. Faces found: {features['face_count']}\")\n",
        "    return features\n",
        "\n",
        "def detect_image_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in image features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting image anomalies from features...\")\n",
        "    anomalies = []\n",
        "\n",
        "    # Check for face-specific anomalies\n",
        "    if features[\"face_count\"] > 0:\n",
        "        # Check face landmarks if available\n",
        "        face_landmarks = features.get(\"face_landmarks\", [])\n",
        "        if face_landmarks:\n",
        "            # Analyze symmetry (simplified)\n",
        "            landmarks = face_landmarks[0]  # First face\n",
        "            left_eye = np.mean(landmarks.get(\"left_eye\", [[0, 0]]), axis=0)\n",
        "            right_eye = np.mean(landmarks.get(\"right_eye\", [[0, 0]]), axis=0)\n",
        "\n",
        "            # Check horizontal alignment\n",
        "            if abs(left_eye[1] - right_eye[1]) > 10:  # Eyes not horizontally aligned\n",
        "                anomalies.append(\"asymmetric eye alignment\")\n",
        "\n",
        "            # Check proportions (simplified)\n",
        "            nose_tip = landmarks.get(\"nose_tip\", [[0, 0]])[0]\n",
        "            top_lip = np.mean(landmarks.get(\"top_lip\", [[0, 0]]), axis=0)\n",
        "            if abs((nose_tip[1] - left_eye[1]) - (top_lip[1] - nose_tip[1])) > 15:\n",
        "                anomalies.append(\"unusual facial proportions\")\n",
        "\n",
        "        # ELA analysis\n",
        "        ela = features.get(\"ela\")\n",
        "        if ela is not None:\n",
        "            ela_mean = np.mean(ela)\n",
        "            ela_std = np.std(ela)\n",
        "\n",
        "            # High ELA values can indicate manipulation\n",
        "            if ela_mean > 10 or ela_std > 15:\n",
        "                anomalies.append(\"potential image manipulation (ELA)\")\n",
        "\n",
        "        # Noise pattern analysis\n",
        "        noise = features.get(\"noise_pattern\")\n",
        "        if noise is not None:\n",
        "            noise_mean = np.mean(np.abs(noise))\n",
        "            noise_std = np.std(noise)\n",
        "\n",
        "            # Unnatural noise patterns\n",
        "            if noise_std / (noise_mean + 1e-6) > 3 or noise_mean < 0.5:\n",
        "                anomalies.append(\"unnatural noise pattern\")\n",
        "\n",
        "        # DCT block artifact analysis\n",
        "        dct_blocks = features.get(\"dct_blocks\")\n",
        "        if dct_blocks is not None:\n",
        "            # Look for unusual block patterns that could indicate splicing\n",
        "            block_diff = cv2.Laplacian(dct_blocks, cv2.CV_64F).var()\n",
        "            if block_diff > 500:\n",
        "                anomalies.append(\"unusual compression artifacts\")\n",
        "    else:\n",
        "        anomalies.append(\"no faces detected for analysis\")\n",
        "\n",
        "    debug_print(f\"Detected image anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_image_analysis(image: np.ndarray, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive image analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced image analysis...\")\n",
        "\n",
        "    # Extract image features\n",
        "    image_features = extract_image_features(image)\n",
        "\n",
        "    # Detect anomalies from traditional computer vision\n",
        "    cv_anomalies = detect_image_anomalies(image_features)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6  # Simplified scoring\n",
        "\n",
        "    # Format feature content for LLM analysis\n",
        "    feature_summary = (\n",
        "        f\"Image features: Resolution: {image.shape[1]}x{image.shape[0]}, Faces detected: {image_features['face_count']}\\n\"\n",
        "        f\"ELA mean: {np.mean(image_features.get('ela', np.zeros(1))):.4f}, std: {np.std(image_features.get('ela', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Noise pattern: mean: {np.mean(np.abs(image_features.get('noise_pattern', np.zeros(1)))):.4f}, std: {np.std(image_features.get('noise_pattern', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\"\n",
        "    )\n",
        "\n",
        "    # Get LLM-based analysis\n",
        "    prompt = \"Based on the image features, provide a deepfake confidence score (0-1) and list specific image anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"image\")\n",
        "\n",
        "    # Combine traditional and LLM analysis\n",
        "    combined_anomalies = list(set(cv_anomalies + llm_anomalies))\n",
        "    final_score = (cv_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([cv_score, llm_score])  # Higher agreement = higher confidence\n",
        "\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_image_analysis (computer vision + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced image analysis combining computer vision features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "Az8f7cUtqxz3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEXT ANALYSIS ---\n",
        "async def analyze_text_content(text: str) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Analyze text for indicators of AI generation or manipulation.\"\"\"\n",
        "    debug_print(\"Starting text content analysis...\")\n",
        "\n",
        "    # Basic text analysis metrics\n",
        "    word_count = len(text.split())\n",
        "    sent_count = len(re.split(r'[.!?]+', text))\n",
        "    avg_word_len = sum(len(word) for word in text.split()) / max(1, word_count)\n",
        "\n",
        "    # Feature extraction for text\n",
        "    feature_summary = (\n",
        "        f\"Text statistics: {word_count} words, {sent_count} sentences\\n\"\n",
        "        f\"Average word length: {avg_word_len:.2f}\\n\"\n",
        "        f\"Sample text (truncated): {text[:500]}...\\n\"\n",
        "    )\n",
        "\n",
        "    # Get LLM-based analysis\n",
        "    prompt = \"Based on the text features, provide a score (0-1) indicating if this text was AI-generated or manipulated and list specific text anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"text\")\n",
        "\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=llm_score,\n",
        "        label=\"HUMAN\" if llm_score > 0.7 else \"AI-GENERATED\",\n",
        "        confidence=0.8,  # Fixed confidence for demonstration\n",
        "        method=\"text_analysis (LLM-based)\",\n",
        "        anomalies=llm_anomalies,\n",
        "        explanation=\"Text analysis using multiagent LLM insights to detect AI-generated content.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "RHmIO_rRrCAl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###   MULTI-MODAL FUSION SECTION   ###\n",
        "#######################################\n",
        "def calculate_multimodal_score(results: Dict[str, DeepfakeAnalysisResult]) -> float:\n",
        "    \"\"\"Calculate an aggregate score from multiple modality results.\"\"\"\n",
        "    debug_print(\"Calculating multimodal score...\")\n",
        "\n",
        "    # Extract scores and weights based on confidence\n",
        "    scores = []\n",
        "    weights = []\n",
        "\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            scores.append(result.score)\n",
        "            weights.append(result.confidence)\n",
        "\n",
        "    # If no valid results, return default\n",
        "    if not scores:\n",
        "        debug_print(\"No valid results found for multimodal scoring\")\n",
        "        return 0.5\n",
        "\n",
        "    # Normalize weights\n",
        "    total_weight = sum(weights)\n",
        "    if total_weight == 0:\n",
        "        weighted_avg = sum(scores) / len(scores)\n",
        "    else:\n",
        "        norm_weights = [w / total_weight for w in weights]\n",
        "        weighted_avg = sum(s * w for s, w in zip(scores, norm_weights))\n",
        "\n",
        "    debug_print(f\"Multimodal weighted score: {weighted_avg:.4f}\")\n",
        "    return weighted_avg\n",
        "\n",
        "def generate_verdict(score: float, results: Dict[str, DeepfakeAnalysisResult]) -> str:\n",
        "    \"\"\"Generate a final verdict based on the multimodal score and individual results.\"\"\"\n",
        "    if score > 0.8:\n",
        "        return \"AUTHENTIC - High confidence that this content is genuine\"\n",
        "    elif score > 0.6:\n",
        "        return \"LIKELY AUTHENTIC - Moderate confidence that this content is genuine\"\n",
        "    elif score > 0.4:\n",
        "        return \"INCONCLUSIVE - Analysis unable to determine authenticity\"\n",
        "    elif score > 0.2:\n",
        "        return \"LIKELY MANIPULATED - Moderate confidence that this content contains manipulations\"\n",
        "    else:\n",
        "        return \"MANIPULATED - High confidence that this content is manipulated or synthetic\"\n",
        "\n",
        "def collect_evidence(results: Dict[str, DeepfakeAnalysisResult]) -> List[Evidence]:\n",
        "    \"\"\"Collect evidence from all modality results.\"\"\"\n",
        "    evidence_list = []\n",
        "\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            for anomaly in result.anomalies:\n",
        "                evidence_list.append(\n",
        "                    Evidence(\n",
        "                        type=modality,\n",
        "                        description=anomaly,\n",
        "                        confidence=result.confidence,\n",
        "                        method=result.method\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    return evidence_list\n",
        "\n",
        "def generate_recommendations(report: MultimodalAnalysisReport) -> List[str]:\n",
        "    \"\"\"Generate recommendations based on the analysis report.\"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    # Add general recommendation\n",
        "    if report.multimodal_score < 0.4:\n",
        "        recommendations.append(\"This content should be treated as potentially manipulated and not be used as evidence without further forensic analysis.\")\n",
        "\n",
        "    # Add modality-specific recommendations\n",
        "    if report.video_analysis and report.video_analysis.score < 0.5:\n",
        "        recommendations.append(\"Video content shows significant signs of manipulation and should be verified through alternative sources.\")\n",
        "\n",
        "    if report.audio_analysis and report.audio_analysis.score < 0.5:\n",
        "        recommendations.append(\"Audio content appears to be synthetic or manipulated. Verify the source and content through other means.\")\n",
        "\n",
        "    if report.image_analysis and report.image_analysis.score < 0.5:\n",
        "        recommendations.append(\"Image analysis indicates potential manipulation. Consider requesting the original, unprocessed image file.\")\n",
        "\n",
        "    if report.text_analysis and report.text_analysis.score < 0.5:\n",
        "        recommendations.append(\"Text content appears to be AI-generated. Verify authorship through additional means.\")\n",
        "\n",
        "    # Add standard recommendations for all reports\n",
        "    recommendations.append(\"For definitive analysis, consider submitting this content to a professional forensic lab specialized in digital media authentication.\")\n",
        "\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "s4nJFQ3FrFqY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###   MAIN PIPELINE ORCHESTRATION  ###\n",
        "#######################################\n",
        "class DeepfakePipeline:\n",
        "    \"\"\"End-to-end pipeline for deepfake detection across multiple modalities.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the deepfake detection pipeline.\"\"\"\n",
        "        debug_print(\"Initializing DeepfakePipeline...\")\n",
        "        # Initialize device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        debug_print(f\"Using device: {self.device}\")\n",
        "\n",
        "    async def analyze_file(self, file_path: str) -> MultimodalAnalysisReport:\n",
        "        \"\"\"Analyze a file for potential deepfake manipulation.\"\"\"\n",
        "        debug_print(f\"Starting analysis for file: {file_path}\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Determine file type\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        file_info = {\n",
        "            \"path\": file_path,\n",
        "            \"size\": os.path.getsize(file_path),\n",
        "            \"type\": \"unknown\",\n",
        "            \"extension\": file_ext\n",
        "        }\n",
        "\n",
        "        # Initialize results\n",
        "        results = {\n",
        "            \"video_analysis\": None,\n",
        "            \"audio_analysis\": None,\n",
        "            \"image_analysis\": None,\n",
        "            \"text_analysis\": None\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Process based on file type\n",
        "            if file_ext in ['.mp4', '.mov', '.avi', '.mkv', '.webm']:\n",
        "                file_info[\"type\"] = \"video\"\n",
        "                video_data = video_preprocessing(file_path)\n",
        "\n",
        "                # Run modality analysis tasks concurrently\n",
        "                tasks = []\n",
        "\n",
        "                # Video analysis\n",
        "                tasks.append(advanced_video_analysis(video_data, self.device))\n",
        "\n",
        "                # Audio analysis if available\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    audio_data, sr = video_data[\"audio\"], 44100  # Assuming standard sample rate\n",
        "                    tasks.append(advanced_audio_analysis(audio_data, sr, self.device))\n",
        "\n",
        "                # Extract representative image from first frame\n",
        "                if video_data.get(\"frames\"):\n",
        "                    image = video_data[\"frames\"][0]\n",
        "                    tasks.append(advanced_image_analysis(image, self.device))\n",
        "\n",
        "                # Extract and analyze text if available (e.g., from speech recognition)\n",
        "                if video_data.get(\"text\"):\n",
        "                    tasks.append(analyze_text_content(video_data[\"text\"]))\n",
        "\n",
        "                # Wait for all analysis tasks to complete\n",
        "                completed_tasks = await asyncio.gather(*tasks)\n",
        "\n",
        "                # Assign results\n",
        "                results[\"video_analysis\"] = completed_tasks[0]\n",
        "                task_idx = 1\n",
        "\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    results[\"audio_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "\n",
        "                if video_data.get(\"frames\"):\n",
        "                    results[\"image_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "\n",
        "                if video_data.get(\"text\"):\n",
        "                    results[\"text_analysis\"] = completed_tasks[task_idx]\n",
        "\n",
        "            elif file_ext in ['.wav', '.mp3', '.ogg', '.flac']:\n",
        "                file_info[\"type\"] = \"audio\"\n",
        "                audio_data, sr = audio_preprocessing(file_path)\n",
        "                results[\"audio_analysis\"] = await advanced_audio_analysis(audio_data, sr, self.device)\n",
        "\n",
        "                # Also analyze for potential text content (speech)\n",
        "                text_content = \"Audio transcript would be here in a real implementation.\"\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "\n",
        "            elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
        "                file_info[\"type\"] = \"image\"\n",
        "                image = image_preprocessing(file_path)\n",
        "                results[\"image_analysis\"] = await advanced_image_analysis(image, self.device)\n",
        "\n",
        "            elif file_ext in ['.txt', '.md', '.rtf', '.doc', '.docx', '.pdf']:\n",
        "                file_info[\"type\"] = \"text\"\n",
        "                # In a real implementation, extract text based on file type\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    text_content = f.read()\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "\n",
        "            else:\n",
        "                debug_print(f\"Unsupported file type: {file_ext}\")\n",
        "                raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
        "\n",
        "            # Calculate multimodal score\n",
        "            multimodal_score = calculate_multimodal_score(results)\n",
        "\n",
        "            # Generate verdict\n",
        "            verdict = generate_verdict(multimodal_score, results)\n",
        "\n",
        "            # Collect evidence\n",
        "            evidence = collect_evidence(results)\n",
        "\n",
        "            # Calculate processing time\n",
        "            processing_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "            # Create confidence matrix\n",
        "            confidence_matrix = {\n",
        "                modality: {\n",
        "                    \"score\": result.score if result else None,\n",
        "                    \"confidence\": result.confidence if result else None\n",
        "                }\n",
        "                for modality, result in results.items()\n",
        "            }\n",
        "\n",
        "            # Create report\n",
        "            report = MultimodalAnalysisReport(\n",
        "                case_id=f\"case_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "                file_info=file_info,\n",
        "                video_analysis=results[\"video_analysis\"],\n",
        "                audio_analysis=results[\"audio_analysis\"],\n",
        "                image_analysis=results[\"image_analysis\"],\n",
        "                text_analysis=results[\"text_analysis\"],\n",
        "                multimodal_score=multimodal_score,\n",
        "                verdict=verdict,\n",
        "                evidence=evidence,\n",
        "                metadata={\n",
        "                    \"pipeline_version\": \"1.0.0\",\n",
        "                    \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "                    \"device\": str(self.device)\n",
        "                },\n",
        "                confidence_matrix=confidence_matrix,\n",
        "                processing_time=processing_time\n",
        "            )\n",
        "\n",
        "            # Generate and add recommendations\n",
        "            report.recommendations = generate_recommendations(report)\n",
        "\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in analyze_file: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "Jxls6PLJrKb7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###  LANGGRAPH ORCHESTRATION GRAPH  ###\n",
        "#######################################\n",
        "class PipelineState(BaseModel):\n",
        "    \"\"\"State for the deepfake analysis pipeline graph.\"\"\"\n",
        "    file_path: str\n",
        "    report: Optional[MultimodalAnalysisReport] = None\n",
        "    current_step: str = \"initialize\"\n",
        "    error: Optional[str] = None\n",
        "    results: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "async def initialize_analysis(state: PipelineState) -> PipelineState:\n",
        "    \"\"\"Initialize the analysis process.\"\"\"\n",
        "    debug_print(f\"Initializing analysis for {state.file_path}\")\n",
        "    try:\n",
        "        state.current_step = \"processing\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "async def process_file(state: PipelineState) -> PipelineState:\n",
        "    \"\"\"Process the file using the DeepfakePipeline.\"\"\"\n",
        "    try:\n",
        "        pipeline = DeepfakePipeline()\n",
        "        state.report = await pipeline.analyze_file(state.file_path)\n",
        "        state.current_step = \"complete\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "def create_pipeline_graph() -> StateGraph:\n",
        "    \"\"\"Create the LangGraph for the deepfake analysis pipeline.\"\"\"\n",
        "    workflow = StateGraph(PipelineState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"initialize\", initialize_analysis)\n",
        "    workflow.add_node(\"processing\", process_file)\n",
        "\n",
        "    # Add edges\n",
        "    workflow.add_edge(\"initialize\", \"processing\")\n",
        "    workflow.add_edge(\"processing\", END)\n",
        "\n",
        "    # Add conditional edge for error handling\n",
        "    workflow.add_conditional_edges(\n",
        "        \"initialize\",\n",
        "        lambda state: \"error\" if state.error else \"processing\"\n",
        "    )\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"processing\",\n",
        "        lambda state: \"error\" if state.error else END\n",
        "    )\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "fv-KVsHarQdP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###    MAIN APPLICATION SECTION     ###\n",
        "#######################################\n",
        "async def main():\n",
        "    \"\"\"Main entry point for the deepfake analysis pipeline.\"\"\"\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Deepfake Detection Pipeline\")\n",
        "    parser.add_argument('file_path', help='Path to the file to analyze')\n",
        "    parser.add_argument('--debug', action='store_true', help='Enable debug output')\n",
        "    parser.add_argument('--output', help='Path to save the JSON report')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set debug mode\n",
        "    global DEBUG\n",
        "    DEBUG = args.debug\n",
        "\n",
        "    debug_print(f\"Starting deepfake analysis for file: {args.file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Create and run the pipeline graph\n",
        "        workflow = create_pipeline_graph()\n",
        "        state = PipelineState(file_path=args.file_path)\n",
        "        final_state = await workflow.ainvoke(state)\n",
        "\n",
        "        if final_state.error:\n",
        "            print(f\"Error: {final_state.error}\")\n",
        "            return 1\n",
        "\n",
        "        # Display results\n",
        "        report = final_state.report\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"DEEPFAKE ANALYSIS REPORT - {report.case_id}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"File: {os.path.basename(args.file_path)} ({report.file_info['type']})\")\n",
        "        print(f\"Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)\")\n",
        "        print(f\"Verdict: {report.verdict}\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # Display modality results\n",
        "        for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "            result = getattr(report, f\"{modality}_analysis\")\n",
        "            if result:\n",
        "                print(f\"{modality.upper()} Analysis: {result.score:.2f} ({result.label})\")\n",
        "                if result.anomalies:\n",
        "                    print(f\"  Detected anomalies: {', '.join(result.anomalies)}\")\n",
        "\n",
        "        print(\"-\"*50)\n",
        "        print(\"RECOMMENDATIONS:\")\n",
        "        for i, rec in enumerate(report.recommendations, 1):\n",
        "            print(f\"{i}. {rec}\")\n",
        "\n",
        "        # Save JSON report if requested\n",
        "        if args.output:\n",
        "            with open(args.output, 'w') as f:\n",
        "                f.write(report.json(indent=2))\n",
        "            print(f\"\\nDetailed report saved to {args.output}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Tc9fGfIIrVKP",
        "outputId": "3a06c77e-0ca3-4034-87ec-7b2efcd2ae28"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--debug] [--output OUTPUT] file_path\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code at the end of the file, after the main() function\n",
        "\n",
        "async def run_in_colab():\n",
        "    \"\"\"Run the deepfake detection pipeline in Google Colab with an interactive interface.\"\"\"\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import json\n",
        "    import time\n",
        "\n",
        "    # Header\n",
        "    display(HTML(\"\"\"\n",
        "    <div style=\"background-color:#4285F4; padding:20px; border-radius:10px; margin-bottom:20px;\">\n",
        "        <h1 style=\"color:white; text-align:center;\">Deepfake Detection Pipeline</h1>\n",
        "        <p style=\"color:white; text-align:center;\">Upload media files to analyze for potential manipulations</p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # Upload widget\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload File',\n",
        "        disabled=False,\n",
        "        button_style='primary',\n",
        "        tooltip='Click to upload a file for analysis',\n",
        "        icon='upload'\n",
        "    )\n",
        "\n",
        "    debug_checkbox = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Enable debug output',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "    progress_bar = widgets.IntProgress(\n",
        "        value=0,\n",
        "        min=0,\n",
        "        max=10,\n",
        "        description='Processing:',\n",
        "        bar_style='info',\n",
        "        orientation='horizontal'\n",
        "    )\n",
        "\n",
        "    display(widgets.HBox([upload_button, debug_checkbox]))\n",
        "    display(progress_bar)\n",
        "    display(output_area)\n",
        "\n",
        "    # Function to handle file upload and processing\n",
        "    def on_upload_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(\"Please select a file to upload...\")\n",
        "\n",
        "            # Set global debug flag\n",
        "            global DEBUG\n",
        "            DEBUG = debug_checkbox.value\n",
        "\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "\n",
        "                if not uploaded:\n",
        "                    print(\"No file was uploaded.\")\n",
        "                    return\n",
        "\n",
        "                file_path = list(uploaded.keys())[0]\n",
        "                print(f\"Analyzing file: {file_path}\")\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.value = 1\n",
        "\n",
        "                # Create temporary path for the uploaded file\n",
        "                temp_path = f\"/tmp/{file_path}\"\n",
        "                with open(temp_path, \"wb\") as f:\n",
        "                    f.write(uploaded[file_path])\n",
        "\n",
        "                # Create and run the pipeline graph\n",
        "                progress_bar.value = 3\n",
        "                workflow = create_pipeline_graph()\n",
        "                state = PipelineState(file_path=temp_path)\n",
        "\n",
        "                # Run analysis\n",
        "                print(\"Starting analysis... This may take a few minutes.\")\n",
        "                progress_bar.value = 5\n",
        "\n",
        "                # Run the analysis asynchronously\n",
        "                loop = asyncio.get_event_loop()\n",
        "                final_state = loop.run_until_complete(workflow.ainvoke(state))\n",
        "\n",
        "                progress_bar.value = 8\n",
        "\n",
        "                if final_state.error:\n",
        "                    print(f\"Error: {final_state.error}\")\n",
        "                    return\n",
        "\n",
        "                # Display results\n",
        "                report = final_state.report\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(f\"DEEPFAKE ANALYSIS REPORT\")\n",
        "                print(\"=\"*50)\n",
        "                print(f\"File: {file_path} ({report.file_info['type']})\")\n",
        "                print(f\"Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)\")\n",
        "                print(f\"Verdict: {report.verdict}\")\n",
        "                print(\"-\"*50)\n",
        "\n",
        "                # Display modality results\n",
        "                for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                    result = getattr(report, f\"{modality}_analysis\")\n",
        "                    if result:\n",
        "                        print(f\"{modality.upper()} Analysis: {result.score:.2f} ({result.label})\")\n",
        "                        if result.anomalies:\n",
        "                            print(f\"  Detected anomalies: {', '.join(result.anomalies)}\")\n",
        "\n",
        "                print(\"-\"*50)\n",
        "                print(\"RECOMMENDATIONS:\")\n",
        "                for i, rec in enumerate(report.recommendations, 1):\n",
        "                    print(f\"{i}. {rec}\")\n",
        "\n",
        "                # Save JSON report to downloadable file\n",
        "                report_json = report.json(indent=2)\n",
        "                report_filename = f\"deepfake_report_{int(time.time())}.json\"\n",
        "                with open(report_filename, 'w') as f:\n",
        "                    f.write(report_json)\n",
        "\n",
        "                print(\"\\nDetailed JSON report available for download:\")\n",
        "                files.download(report_filename)\n",
        "\n",
        "                # Create visualization of the results\n",
        "                display(HTML(f\"\"\"\n",
        "                <div style=\"background-color:#f5f5f5; padding:15px; border-radius:5px; margin-top:20px;\">\n",
        "                    <h3>Analysis Summary Visualization</h3>\n",
        "                    <div style=\"display: flex; margin-bottom: 20px;\">\n",
        "                        <div style=\"flex: 1; text-align: center;\">\n",
        "                            <div style=\"font-size: 48px; font-weight: bold; color: {'green' if report.multimodal_score > 0.7 else 'red' if report.multimodal_score < 0.3 else 'orange'}\">\n",
        "                                {report.multimodal_score:.2f}\n",
        "                            </div>\n",
        "                            <div>Overall Score</div>\n",
        "                        </div>\n",
        "                        <div style=\"flex: 2; padding-left: 20px;\">\n",
        "                            <div style=\"font-size: 24px; margin-bottom: 10px;\">{report.verdict}</div>\n",
        "                            <div>{' Likely authentic' if report.multimodal_score > 0.7 else ' Likely manipulated' if report.multimodal_score < 0.3 else ' Results inconclusive'}</div>\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <h4>Modality Scores</h4>\n",
        "                    <div style=\"display: flex; flex-wrap: wrap;\">\n",
        "                \"\"\"))\n",
        "\n",
        "                # Display modality scores as progress bars\n",
        "                for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                    result = getattr(report, f\"{modality}_analysis\")\n",
        "                    if result:\n",
        "                        score = result.score\n",
        "                        color = 'green' if score > 0.7 else 'red' if score < 0.3 else 'orange'\n",
        "                        display(HTML(f\"\"\"\n",
        "                        <div style=\"flex: 1; min-width: 200px; margin: 10px;\">\n",
        "                            <div style=\"text-transform: uppercase; font-weight: bold;\">{modality}</div>\n",
        "                            <div style=\"background-color: #ddd; border-radius: 5px; height: 20px; margin: 5px 0;\">\n",
        "                                <div style=\"background-color: {color}; width: {int(score*100)}%; height: 100%; border-radius: 5px; text-align: right;\">\n",
        "                                    <span style=\"color: white; padding-right: 5px;\">{score:.2f}</span>\n",
        "                                </div>\n",
        "                            </div>\n",
        "                        </div>\n",
        "                        \"\"\"))\n",
        "\n",
        "                display(HTML(\"</div></div>\"))\n",
        "\n",
        "                progress_bar.value = 10\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(f\"Error in processing: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    # Attach the handler to the button\n",
        "    upload_button.on_click(on_upload_clicked)\n",
        "\n",
        "    print(\" Ready! Click 'Upload File' to begin analysis.\")\n",
        "\n",
        "# Try to detect if we're running in Colab and run the appropriate interface\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if IN_COLAB:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        run_in_colab()\n",
        "    else:\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "id": "QvmmW-lErYvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}