{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Manipulated_Media_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Multimodal Manipulated Media Analysis Pipeline for Defensive Forensic**\n",
        "\n",
        "\n",
        "\n",
        "This end-to-end pipeline uses LangChain-based VLLM and Groq LLM integration\n",
        "along with LangGraph multiagent orchestration to analyze audio, video, and image modalities.\n",
        "Each modality has its own processing and analysis steps, and the results are aggregated\n",
        "into a consolidated report"
      ],
      "metadata": {
        "id": "2Fqt0MDqomfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iYbnoslNm-tQ",
        "outputId": "aca70eeb-d09f-4d75-abde-2298a54102cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch numpy opencv-python librosa pydantic mediapipe moviepy face_recognition scikit-image dtw-python scipy langchain langchain_community langchain_core langgraph nest_asyncio ipywidgets mcp_use langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# Media processing libraries\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# For interactive notebook usage\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "# For image quality metrics\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from dtw import dtw\n",
        "import face_recognition\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Integration for LLM inference using vLLM and Groq.\n",
        "from langchain_community.llms import VLLM\n",
        "from langchain_groq import ChatGroq\n",
        "# MCP-Use integration supporting multi-server configuration.\n",
        "from mcp_use import MCPAgent, MCPClient\n",
        "\n",
        "# Graph orchestration for pipeline steps.\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Apply nest_asyncio to allow running async code in notebooks.\n",
        "nest_asyncio.apply()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ysWFk98_nTbX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = True\n",
        "def debug_print(msg: str):\n",
        "    \"\"\"Print debug messages when DEBUG is enabled.\"\"\"\n",
        "    if DEBUG:\n",
        "        print(f\"[DEBUG] {msg}\")"
      ],
      "metadata": {
        "id": "G9XItUIFo_8a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMMON_PARAMS = {\n",
        "    \"task\": \"generate\",\n",
        "    \"max_model_len\": 4096,\n",
        "    \"dtype\": \"half\",\n",
        "    \"gpu_memory_utilization\": 0.85,\n",
        "    \"cpu_offload_gb\": 8,\n",
        "    \"enforce_eager\": True,\n",
        "    \"trust_remote_code\": True\n",
        "}\n",
        "\n",
        "def init_vllm_model(name: str, model_id: str, **overrides):\n",
        "    params = {**COMMON_PARAMS, **overrides}\n",
        "    debug_print(f\"Initializing VLLM model '{name}' with id '{model_id}' and params: {params}\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"params\": params}\n",
        "\n",
        "def init_groq_model(name: str, model_id: str):\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\", \"your_groq_api_key\")\n",
        "    debug_print(f\"Initializing Groq model '{name}' with id '{model_id}' using API key.\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"api_key\": api_key}\n",
        "\n",
        "# Groq LLM wrapper. In production you might call Groq’s specialized endpoints.\n",
        "class GroqLLMWrapper:\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        debug_print(f\"GroqLLMWrapper processing prompt with {self.model_data['name']}\")\n",
        "        # Simulated response from Groq engine.\n",
        "        if \"audio\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.82\\nAnomalies: [\\\"unnatural voice transitions\\\", \\\"inconsistent background noise\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.77\\nAnomalies: [\\\"facial landmark inconsistencies\\\", \\\"unnatural eye movements\\\"]\"\n",
        "        elif \"image\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.68\\nAnomalies: [\\\"lighting inconsistencies\\\", \\\"unusual facial proportions\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.75\\nAnomalies: [\\\"inconsistent narrative\\\", \\\"unusual phrasing patterns\\\"]\"\n",
        "\n",
        "# Simulated vLLM wrapper using the VLLM library.\n",
        "class VLLMSimulated:\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        debug_print(f\"VLLMSimulated processing with {self.model_data['name']}\")\n",
        "        if \"wav2vec\" in self.model_data[\"name\"] or \"whisper\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.79\\nAnomalies: [\\\"frequency anomalies\\\", \\\"unnatural pauses\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.81\\nAnomalies: [\\\"temporal inconsistencies\\\", \\\"blending artifacts\\\"]\"\n",
        "        elif \"llava\" in self.model_data[\"name\"] or \"clip\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.73\\nAnomalies: [\\\"compression artifacts\\\", \\\"unusual texture patterns\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.78\\nAnomalies: [\\\"stylistic inconsistencies\\\"]\"\n",
        "\n",
        "# Initialize modality-specific models using vLLM and Groq wrappers.\n",
        "debug_print(\"Initializing modality-specific models...\")\n",
        "models = {\n",
        "    \"audio\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"wav2vec2\", \"facebook/wav2vec2-large-robust-ft-swbd-300h\", tensor_parallel_size=1)),\n",
        "            VLLMSimulated(init_vllm_model(\"whisper\", \"openai/whisper-large-v3\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_audio_model\", \"whisper-large-v3-turbo\")),\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_llama_audio\", \"meta-llama/llama-4-audio-17b-16e-instruct\"))\n",
        "        ]\n",
        "    },\n",
        "    \"video\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_next_video\", \"llava-hf/LLaVA-NeXT-Video-7B-hf\", tensor_parallel_size=2, max_tokens=1024)),\n",
        "            VLLMSimulated(init_vllm_model(\"videomae\", \"MCG-NJU/videomae-large-static\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_video_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\")),\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_video_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\"))\n",
        "        ]\n",
        "    },\n",
        "    \"image\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_image\", \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", tensor_parallel_size=2)),\n",
        "            VLLMSimulated(init_vllm_model(\"clip\", \"openai/clip-vit-large-patch14\", tensor_parallel_size=1))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_image_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\")),\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_image_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\"))\n",
        "        ]\n",
        "    },\n",
        "    \"text\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llama3\", \"meta-llama/Llama-3-70b-hf\", tensor_parallel_size=4))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_text_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\")),\n",
        "            GroqLLMWrapper(init_groq_model(\"groq_text_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\"))\n",
        "        ]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCrOCZ4_poed",
        "outputId": "75c1ede5-6454-40e8-da58-1faf5edeb2a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Initializing modality-specific models...\n",
            "[DEBUG] Initializing VLLM model 'wav2vec2' with id 'facebook/wav2vec2-large-robust-ft-swbd-300h' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing VLLM model 'whisper' with id 'openai/whisper-large-v3' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_audio_model' with id 'whisper-large-v3-turbo' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_llama_audio' with id 'meta-llama/llama-4-audio-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_next_video' with id 'llava-hf/LLaVA-NeXT-Video-7B-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2, 'max_tokens': 1024}\n",
            "[DEBUG] Initializing VLLM model 'videomae' with id 'MCG-NJU/videomae-large-static' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_video_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_video_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_image' with id 'llava-hf/llava-onevision-qwen2-7b-ov-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing VLLM model 'clip' with id 'openai/clip-vit-large-patch14' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing Groq model 'groq_image_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_image_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llama3' with id 'meta-llama/Llama-3-70b-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 4}\n",
            "[DEBUG] Initializing Groq model 'groq_text_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_text_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float\n",
        "    label: str\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    explanation: Optional[str] = None\n",
        "    model_scores: Dict[str, float] = Field(default_factory=dict)\n",
        "\n",
        "class Evidence(BaseModel):\n",
        "    type: str\n",
        "    description: str\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: Optional[float] = None\n",
        "    location: Optional[Dict[str, int]] = None\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Evidence]\n",
        "    metadata: Dict[str, Any]\n",
        "    recommendations: List[str] = Field(default_factory=list)\n",
        "    confidence_matrix: Dict[str, Dict[str, float]] = Field(default_factory=dict)\n",
        "    processing_time: float"
      ],
      "metadata": {
        "id": "Otpsa4nDpx5s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face_forgery(image: np.ndarray) -> Tuple[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Additional check on facial regions to detect forgery artifacts.\n",
        "    \"\"\"\n",
        "    score = 0.75\n",
        "    anomalies = []\n",
        "    avg_brightness = np.mean(image)\n",
        "    if avg_brightness < 100:\n",
        "        anomalies.append(\"Low brightness may indicate face forgery artifacts\")\n",
        "        score = 0.6\n",
        "    return score, anomalies\n",
        "\n",
        "def check_background_consistency(frames: List[np.ndarray]) -> Tuple[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Check for consistency in background color across video frames.\n",
        "    \"\"\"\n",
        "    score = 0.8\n",
        "    anomalies = []\n",
        "    if len(frames) >= 2:\n",
        "        bg_first = np.median(frames[0], axis=(0,1))\n",
        "        bg_last = np.median(frames[-1], axis=(0,1))\n",
        "        color_diff = np.linalg.norm(bg_first - bg_last)\n",
        "        if color_diff > 30:\n",
        "            anomalies.append(\"Inconsistent background detected across frames\")\n",
        "            score = 0.65\n",
        "    return score, anomalies\n",
        "\n",
        "def additional_audio_checks(audio_data: np.ndarray, sr: int) -> Tuple[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Additional audio checks for echoes and background noise.\n",
        "    \"\"\"\n",
        "    score = 0.8\n",
        "    anomalies = []\n",
        "    variance = np.var(audio_data)\n",
        "    if variance < 0.001:\n",
        "        anomalies.append(\"Audio may suffer from echo or lack of clarity\")\n",
        "        score = 0.65\n",
        "    if np.std(np.abs(audio_data)) > 0.5:\n",
        "        anomalies.append(\"High level of background noise detected\")\n",
        "        score = 0.6\n",
        "    return score, anomalies\n",
        "\n",
        "def cross_modality_consistency_check(results: Dict[str, DeepfakeAnalysisResult]) -> Tuple[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Check for consistency across modalities (e.g., audio and text analysis).\n",
        "    \"\"\"\n",
        "    score = 0.8\n",
        "    anomalies = []\n",
        "    if results.get(\"audio_analysis\") and results.get(\"text_analysis\"):\n",
        "         if results[\"audio_analysis\"].label != results[\"text_analysis\"].label:\n",
        "             anomalies.append(\"Mismatch between audio and text analysis results\")\n",
        "             score = 0.6\n",
        "    return score, anomalies"
      ],
      "metadata": {
        "id": "mKvRhIj9PC4x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_model_output(output: str) -> Tuple[float, List[str]]:\n",
        "    debug_print(f\"Parsing model output: {output}\")\n",
        "    score_match = re.search(r\"Score:?\\s*(0\\.\\d+|1\\.0)\", output, re.IGNORECASE)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    anomalies = []\n",
        "    anomalies_pattern = r\"Anomalies:?\\s*(.*?)(?:\\n|$)\"\n",
        "    anomalies_match = re.search(anomalies_pattern, output, re.IGNORECASE | re.DOTALL)\n",
        "    if anomalies_match:\n",
        "        anomalies_text = anomalies_match.group(1)\n",
        "        if anomalies_text:\n",
        "            anomalies_text = anomalies_text.strip(\"[]\")\n",
        "            anomalies = [a.strip().strip('\"\\'') for a in anomalies_text.split(',') if a.strip()]\n",
        "    debug_print(f\"Parsed output: score={score}, anomalies={anomalies}\")\n",
        "    return score, anomalies\n",
        "\n",
        "async def aggregate_llm_outputs(prompt: str, content: str, modality: str=\"generic\") -> Tuple[float, List[str], Dict[str, float]]:\n",
        "    debug_print(f\"Aggregating LLM outputs for modality {modality}\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "\n",
        "    def process_model(model, model_name):\n",
        "        try:\n",
        "            response = model(f\"{prompt}\\n\\nContent: {content}\")\n",
        "            s, a = parse_model_output(response)\n",
        "            scores.append(s)\n",
        "            all_anomalies.extend(a)\n",
        "            model_scores[model_name] = s\n",
        "            debug_print(f\"{model_name} score: {s}, anomalies: {a}\")\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error with {model_name}: {e}\")\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for i, model in enumerate(models[modality][\"vllm\"]):\n",
        "            model_name = f\"vllm_{modality}_{i}\"\n",
        "            futures.append(executor.submit(process_model, model, model_name))\n",
        "        for i, model in enumerate(models[modality][\"groq\"]):\n",
        "            model_name = f\"groq_{modality}_{i}\"\n",
        "            futures.append(executor.submit(process_model, model, model_name))\n",
        "        for future in as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    agg_score = float(np.mean(scores)) if scores else 0.5\n",
        "    unique_anomalies = list(set(all_anomalies))\n",
        "    debug_print(f\"Aggregated score for {modality}: {agg_score}, anomalies: {unique_anomalies}\")\n",
        "    return agg_score, unique_anomalies, model_scores"
      ],
      "metadata": {
        "id": "CfDQ_Ffzp4cW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AUDIO PROCESSING & ANALYSIS ---\n",
        "def audio_preprocessing(audio_path: str) -> Tuple[np.ndarray, int]:\n",
        "    debug_print(f\"Loading and preprocessing audio from {audio_path} ...\")\n",
        "    try:\n",
        "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
        "        if len(audio_data) > 0:\n",
        "            b, a = librosa.filters.butter(4, 100/(sr/2), btype='highpass')\n",
        "            audio_data = librosa.filters.filtfilt(b, a, audio_data)\n",
        "        debug_print(f\"Audio loaded and preprocessed. Sample rate: {sr}, Duration: {len(audio_data)/sr:.2f}s\")\n",
        "        return audio_data, sr\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in audio preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sr: int) -> Dict[str, Any]:\n",
        "    debug_print(\"Extracting audio features...\")\n",
        "    features = {}\n",
        "    stft = np.abs(librosa.stft(audio_data))\n",
        "    features[\"stft\"] = stft\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
        "    features[\"mel_spectrogram\"] = mel_spec\n",
        "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
        "    features[\"mfccs\"] = mfccs\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
        "    features[\"spectral_contrast\"] = spectral_contrast\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
        "    features[\"chroma\"] = chroma\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
        "    features[\"zero_crossing_rate\"] = zcr\n",
        "    onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr)\n",
        "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "    features[\"tempo\"] = tempo[0]\n",
        "    debug_print(f\"Audio features extracted: {list(features.keys())}\")\n",
        "    return features\n",
        "\n",
        "def detect_audio_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    debug_print(\"Detecting audio anomalies from features...\")\n",
        "    anomalies = []\n",
        "    mel_spec = features[\"mel_spectrogram\"]\n",
        "    if np.std(mel_spec) < 0.1 or np.std(mel_spec) > 10:\n",
        "        anomalies.append(\"unusual spectral distribution\")\n",
        "    mfccs = features[\"mfccs\"]\n",
        "    if np.max(np.diff(mfccs, axis=1)) > 5:\n",
        "        anomalies.append(\"abrupt MFCC transitions\")\n",
        "    tempo = features[\"tempo\"]\n",
        "    if tempo < 40 or tempo > 240:\n",
        "        anomalies.append(\"unusual speech tempo\")\n",
        "    zcr = features[\"zero_crossing_rate\"]\n",
        "    if np.mean(zcr) > 0.3:\n",
        "        anomalies.append(\"unusually high zero-crossing rate\")\n",
        "    debug_print(f\"Detected audio anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_audio_analysis(audio_data: np.ndarray, sr: int, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    debug_print(\"Starting advanced audio analysis...\")\n",
        "    audio_features = extract_audio_features(audio_data, sr)\n",
        "    signal_anomalies = detect_audio_anomalies(audio_features)\n",
        "    signal_score = 0.8 if not signal_anomalies else 0.6\n",
        "    # Additional audio checks for echo and noise\n",
        "    additional_score, additional_anomalies = additional_audio_checks(audio_data, sr)\n",
        "    combined_score = (signal_score + additional_score) / 2\n",
        "    combined_anomalies = list(set(signal_anomalies + additional_anomalies))\n",
        "    feature_summary = (\n",
        "        f\"Audio duration: {len(audio_data)/sr:.2f}s, Sample rate: {sr}Hz\\n\"\n",
        "        f\"Mean amplitude: {np.mean(np.abs(audio_data)):.4f}, Max amplitude: {np.max(np.abs(audio_data)):.4f}\\n\"\n",
        "        f\"Detected tempo: {audio_features['tempo']:.2f} BPM\\n\"\n",
        "        f\"Mean zero-crossing rate: {np.mean(audio_features['zero_crossing_rate']):.4f}\\n\"\n",
        "        f\"Signal-to-noise ratio estimate: {np.mean(audio_data**2) / np.std(audio_data):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {signal_anomalies}\\n\"\n",
        "        f\"Additional audio checks: {additional_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the audio features and additional checks, provide a deepfake confidence score (0-1) and list specific audio anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"audio\")\n",
        "    final_score = (combined_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([combined_score, llm_score])\n",
        "    final_anomalies = list(set(combined_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_audio_analysis (signal processing + LLM + additional checks)\",\n",
        "        anomalies=final_anomalies,\n",
        "        explanation=\"Advanced audio analysis combining traditional feature extraction, additional echo/noise checks, and LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "kFWN6Eh-qAQB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VIDEO PROCESSING & ANALYSIS ---\n",
        "def video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    debug_print(f\"Loading video from {video_path} ...\")\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        frame_count = int(video.fps * video.duration)\n",
        "        sample_rate = max(1, frame_count // 30)\n",
        "        frames = [video.get_frame(i / video.fps) for i in range(0, frame_count, sample_rate)]\n",
        "        audio = video.audio.to_soundarray() if video.audio else None\n",
        "        metadata = {\n",
        "            \"duration\": video.duration,\n",
        "            \"fps\": video.fps,\n",
        "            \"frame_count\": frame_count,\n",
        "            \"resolution\": f\"{video.size[0]}x{video.size[1]}\",\n",
        "            \"file_size\": os.path.getsize(video_path),\n",
        "            \"file_path\": video_path\n",
        "        }\n",
        "        debug_print(f\"Video preprocessed: {len(frames)} frames extracted, Audio present: {'Yes' if audio is not None else 'No'}\")\n",
        "        return {\n",
        "            \"frames\": frames,\n",
        "            \"audio\": audio,\n",
        "            \"metadata\": metadata,\n",
        "            \"image\": frames[0] if frames else None,\n",
        "            \"text\": \"Extracted text placeholder\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in video preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_video_features(frames: List[np.ndarray]) -> Dict[str, Any]:\n",
        "    debug_print(f\"Extracting video features from {len(frames)} frames...\")\n",
        "    features = {}\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "    face_landmarks_sequence = []\n",
        "    face_embeddings = []\n",
        "    optical_flow_metrics = []\n",
        "    frame_diffs = []\n",
        "    blur_metrics = []\n",
        "    compression_metrics = []\n",
        "    for i in range(len(frames)):\n",
        "        frame_rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB) if frames[i].shape[2] == 3 else frames[i]\n",
        "        face_landmarks = face_mesh.process(frame_rgb).multi_face_landmarks\n",
        "        if face_landmarks:\n",
        "            face_landmarks_sequence.append(face_landmarks[0].landmark)\n",
        "            face_locations = face_recognition.face_locations(frame_rgb)\n",
        "            if face_locations:\n",
        "                face_encoding = face_recognition.face_encodings(frame_rgb, face_locations)[0]\n",
        "                face_embeddings.append(face_encoding)\n",
        "        if i > 0:\n",
        "            prev_gray = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY)\n",
        "            curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "            diff = cv2.absdiff(prev_gray, curr_gray)\n",
        "            frame_diffs.append(np.mean(diff))\n",
        "            flow = np.mean(diff)\n",
        "            optical_flow_metrics.append(flow)\n",
        "        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        blur_metric = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        blur_metrics.append(blur_metric)\n",
        "        compression_metric = np.mean(cv2.Canny(gray, 100, 200))\n",
        "        compression_metrics.append(compression_metric)\n",
        "    features[\"face_landmarks_sequence\"] = face_landmarks_sequence\n",
        "    features[\"face_embeddings\"] = face_embeddings\n",
        "    features[\"optical_flow_metrics\"] = optical_flow_metrics\n",
        "    features[\"frame_diffs\"] = frame_diffs\n",
        "    features[\"blur_metrics\"] = blur_metrics\n",
        "    features[\"compression_metrics\"] = compression_metrics\n",
        "    debug_print(f\"Video features extracted: {list(features.keys())}\")\n",
        "    debug_print(f\"Found facial landmarks in {len(face_landmarks_sequence)}/{len(frames)} frames\")\n",
        "    face_mesh.close()\n",
        "    return features\n",
        "\n",
        "def detect_video_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    debug_print(\"Detecting video anomalies from features...\")\n",
        "    anomalies = []\n",
        "    face_landmarks = features.get(\"face_landmarks_sequence\", [])\n",
        "    if len(face_landmarks) > 1:\n",
        "        landmark_diffs = []\n",
        "        for i in range(1, len(face_landmarks)):\n",
        "            diff = sum(\n",
        "                np.sqrt((face_landmarks[i][j].x - face_landmarks[i-1][j].x)**2 +\n",
        "                        (face_landmarks[i][j].y - face_landmarks[i-1][j].y)**2)\n",
        "                for j in range(min(5, len(face_landmarks[i])))\n",
        "            )\n",
        "            landmark_diffs.append(diff)\n",
        "        if np.max(landmark_diffs) > 0.5:\n",
        "            anomalies.append(\"abrupt facial landmark movements\")\n",
        "    face_embeddings = features.get(\"face_embeddings\", [])\n",
        "    if len(face_embeddings) > 1:\n",
        "        embedding_diffs = []\n",
        "        for i in range(1, len(face_embeddings)):\n",
        "            diff = cosine(face_embeddings[i], face_embeddings[i-1])\n",
        "            embedding_diffs.append(diff)\n",
        "        if np.max(embedding_diffs) > 0.3:\n",
        "            anomalies.append(\"inconsistent face identity\")\n",
        "    frame_diffs = features.get(\"frame_diffs\", [])\n",
        "    if frame_diffs:\n",
        "        if np.std(frame_diffs) < 0.01:\n",
        "            anomalies.append(\"unnaturally smooth motion\")\n",
        "        if np.max(frame_diffs) / (np.mean(frame_diffs) + 1e-6) > 10:\n",
        "            anomalies.append(\"erratic frame changes\")\n",
        "    blur_metrics = features.get(\"blur_metrics\", [])\n",
        "    if blur_metrics:\n",
        "        if np.std(blur_metrics) / (np.mean(blur_metrics) + 1e-6) > 0.8:\n",
        "            anomalies.append(\"inconsistent blur levels\")\n",
        "    compression_metrics = features.get(\"compression_metrics\", [])\n",
        "    if compression_metrics:\n",
        "        if np.std(compression_metrics) / (np.mean(compression_metrics) + 1e-6) > 0.5:\n",
        "            anomalies.append(\"inconsistent compression artifacts\")\n",
        "    debug_print(f\"Detected video anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    debug_print(\"Starting advanced video analysis...\")\n",
        "    frames = video_data.get(\"frames\", [])\n",
        "    if not frames:\n",
        "        debug_print(\"No frames available for analysis\")\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.5,\n",
        "            label=\"INCONCLUSIVE\",\n",
        "            confidence=0.0,\n",
        "            method=\"advanced_video_analysis\",\n",
        "            anomalies=[\"no frames available for analysis\"],\n",
        "            explanation=\"Could not perform video analysis due to lack of frame data.\"\n",
        "        )\n",
        "    video_features = extract_video_features(frames)\n",
        "    cv_anomalies = detect_video_anomalies(video_features)\n",
        "    # Additional background consistency check\n",
        "    bg_score, bg_anomalies = check_background_consistency(frames)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6\n",
        "    # Combine scores for video analysis (simple average)\n",
        "    combined_video_score = (cv_score + bg_score) / 2\n",
        "    combined_anomalies = list(set(cv_anomalies + bg_anomalies))\n",
        "    feature_summary = (\n",
        "        f\"Video features: {len(frames)} frames, Resolution: {video_data['metadata']['resolution']}\\n\"\n",
        "        f\"Facial landmarks detected in {len(video_features.get('face_landmarks_sequence', []))}/{len(frames)} frames\\n\"\n",
        "        f\"Mean frame difference: {np.mean(video_features.get('frame_diffs', [0])):.4f}\\n\"\n",
        "        f\"Blur consistency: {np.std(video_features.get('blur_metrics', [0]))/(np.mean(video_features.get('blur_metrics', [1]))+1e-6):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\\n\"\n",
        "        f\"Background consistency anomalies: {bg_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the video features, background consistency, and additional checks, provide a deepfake confidence score (0-1) and list specific video anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"video\")\n",
        "    final_score = (combined_video_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([combined_video_score, llm_score])\n",
        "    final_anomalies = list(set(combined_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_video_analysis (computer vision + background consistency + LLM)\",\n",
        "        anomalies=final_anomalies,\n",
        "        explanation=\"Advanced video analysis combining computer vision features, background consistency checks, and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "-QJAMVIQqHkn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMAGE PROCESSING & ANALYSIS ---\n",
        "def image_preprocessing(image_path: str) -> np.ndarray:\n",
        "    debug_print(f\"Loading and preprocessing image from {image_path} ...\")\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
        "        max_dim = 1024\n",
        "        h, w = image.shape[:2]\n",
        "        if max(h, w) > max_dim:\n",
        "            scale = max_dim / max(h, w)\n",
        "            image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
        "            debug_print(f\"Image resized from {w}x{h} to {int(w * scale)}x{int(h * scale)}\")\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        debug_print(f\"Image loaded and preprocessed. Shape: {image_rgb.shape}\")\n",
        "        return image_rgb\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in image preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    debug_print(\"Extracting image features...\")\n",
        "    features = {}\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    features[\"face_count\"] = len(faces)\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        features[\"face_roi\"] = face_roi\n",
        "        face_landmarks = face_recognition.face_landmarks(image)\n",
        "        features[\"face_landmarks\"] = face_landmarks\n",
        "        face_encodings = face_recognition.face_encodings(image)\n",
        "        if face_encodings:\n",
        "            features[\"face_encoding\"] = face_encodings[0]\n",
        "    temp_path = \"temp_ela.jpg\"\n",
        "    cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "    low_qual = cv2.imread(temp_path)\n",
        "    low_qual_rgb = cv2.cvtColor(low_qual, cv2.COLOR_BGR2RGB)\n",
        "    ela = cv2.absdiff(image, low_qual_rgb)\n",
        "    features[\"ela\"] = cv2.cvtColor(ela, cv2.COLOR_RGB2GRAY)\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "    noise = cv2.medianBlur(gray, 5) - gray\n",
        "    features[\"noise_pattern\"] = noise\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features[\"edges\"] = edges\n",
        "    dct_blocks = np.zeros_like(gray, dtype=float)\n",
        "    block_size = 8\n",
        "    for i in range(0, gray.shape[0] - block_size, block_size):\n",
        "        for j in range(0, gray.shape[1] - block_size, block_size):\n",
        "            block = gray[i:i+block_size, j:j+block_size].astype(float)\n",
        "            dct_block = cv2.dct(block)\n",
        "            dct_blocks[i:i+block_size, j:j+block_size] = dct_block\n",
        "    features[\"dct_blocks\"] = dct_blocks\n",
        "    debug_print(f\"Image features extracted. Faces found: {features['face_count']}\")\n",
        "    return features\n",
        "\n",
        "def detect_image_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    debug_print(\"Detecting image anomalies from features...\")\n",
        "    anomalies = []\n",
        "    if features[\"face_count\"] > 0:\n",
        "        face_landmarks = features.get(\"face_landmarks\", [])\n",
        "        if face_landmarks:\n",
        "            landmarks = face_landmarks[0]\n",
        "            left_eye = np.mean(landmarks.get(\"left_eye\", [[0, 0]]), axis=0)\n",
        "            right_eye = np.mean(landmarks.get(\"right_eye\", [[0, 0]]), axis=0)\n",
        "            if abs(left_eye[1] - right_eye[1]) > 10:\n",
        "                anomalies.append(\"asymmetric eye alignment\")\n",
        "            nose_tip = landmarks.get(\"nose_tip\", [[0, 0]])[0]\n",
        "            top_lip = np.mean(landmarks.get(\"top_lip\", [[0, 0]]), axis=0)\n",
        "            if abs((nose_tip[1] - left_eye[1]) - (top_lip[1] - nose_tip[1])) > 15:\n",
        "                anomalies.append(\"unusual facial proportions\")\n",
        "        ela = features.get(\"ela\")\n",
        "        if ela is not None:\n",
        "            ela_mean = np.mean(ela)\n",
        "            ela_std = np.std(ela)\n",
        "            if ela_mean > 10 or ela_std > 15:\n",
        "                anomalies.append(\"potential image manipulation (ELA)\")\n",
        "        noise = features.get(\"noise_pattern\")\n",
        "        if noise is not None:\n",
        "            noise_mean = np.mean(np.abs(noise))\n",
        "            noise_std = np.std(noise)\n",
        "            if noise_std / (noise_mean + 1e-6) > 3 or noise_mean < 0.5:\n",
        "                anomalies.append(\"unnatural noise pattern\")\n",
        "        dct_blocks = features.get(\"dct_blocks\")\n",
        "        if dct_blocks is not None:\n",
        "            block_diff = cv2.Laplacian(dct_blocks, cv2.CV_64F).var()\n",
        "            if block_diff > 500:\n",
        "                anomalies.append(\"unusual compression artifacts\")\n",
        "    else:\n",
        "        anomalies.append(\"no faces detected for analysis\")\n",
        "    debug_print(f\"Detected image anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_image_analysis(image: np.ndarray, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    debug_print(\"Starting advanced image analysis...\")\n",
        "    image_features = extract_image_features(image)\n",
        "    cv_anomalies = detect_image_anomalies(image_features)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6\n",
        "    # Additional face forgery detection step\n",
        "    forgery_score, forgery_anomalies = detect_face_forgery(image)\n",
        "    combined_score = (cv_score + forgery_score) / 2\n",
        "    combined_anomalies = list(set(cv_anomalies + forgery_anomalies))\n",
        "    feature_summary = (\n",
        "        f\"Image features: Resolution: {image.shape[1]}x{image.shape[0]}, Faces detected: {image_features['face_count']}\\n\"\n",
        "        f\"ELA mean: {np.mean(image_features.get('ela', np.zeros(1))):.4f}, std: {np.std(image_features.get('ela', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Noise pattern: mean: {np.mean(np.abs(image_features.get('noise_pattern', np.zeros(1)))):.4f}, std: {np.std(image_features.get('noise_pattern', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\\n\"\n",
        "        f\"Face forgery anomalies: {forgery_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the image features, including face forgery detection, provide a deepfake confidence score (0-1) and list specific image anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"image\")\n",
        "    final_score = (combined_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([combined_score, llm_score])\n",
        "    final_anomalies = list(set(combined_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_image_analysis (computer vision + face forgery detection + LLM)\",\n",
        "        anomalies=final_anomalies,\n",
        "        explanation=\"Advanced image analysis combining computer vision features, additional face forgery checks, and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "Az8f7cUtqxz3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEXT ANALYSIS ---\n",
        "async def analyze_text_content(text: str) -> DeepfakeAnalysisResult:\n",
        "    debug_print(\"Starting text content analysis...\")\n",
        "    word_count = len(text.split())\n",
        "    sent_count = len(re.split(r'[.!?]+', text))\n",
        "    avg_word_len = sum(len(word) for word in text.split()) / max(1, word_count)\n",
        "    feature_summary = (\n",
        "        f\"Text statistics: {word_count} words, {sent_count} sentences\\n\"\n",
        "        f\"Average word length: {avg_word_len:.2f}\\n\"\n",
        "        f\"Sample text (truncated): {text[:500]}...\\n\"\n",
        "    )\n",
        "    prompt = \"Based on the text features, provide a score (0-1) indicating if this text was AI-generated or manipulated and list specific text anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"text\")\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=llm_score,\n",
        "        label=\"HUMAN\" if llm_score > 0.7 else \"AI-GENERATED\",\n",
        "        confidence=0.8,\n",
        "        method=\"text_analysis (LLM-based)\",\n",
        "        anomalies=llm_anomalies,\n",
        "        explanation=\"Text analysis using multiagent LLM insights to detect AI-generated content.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "RHmIO_rRrCAl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_modality_consistency_check(results: Dict[str, DeepfakeAnalysisResult]) -> Tuple[float, List[str]]:\n",
        "    score = 0.8\n",
        "    anomalies = []\n",
        "    if results.get(\"audio_analysis\") and results.get(\"text_analysis\"):\n",
        "         if results[\"audio_analysis\"].label != results[\"text_analysis\"].label:\n",
        "             anomalies.append(\"Mismatch between audio and text analysis results\")\n",
        "             score = 0.6\n",
        "    return score, anomalies"
      ],
      "metadata": {
        "id": "6ob08MnmP3gb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_multimodal_score(results: Dict[str, DeepfakeAnalysisResult]) -> float:\n",
        "    debug_print(\"Calculating multimodal score...\")\n",
        "    scores = []\n",
        "    weights = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            scores.append(result.score)\n",
        "            weights.append(result.confidence)\n",
        "    if not scores:\n",
        "        debug_print(\"No valid results found for multimodal scoring\")\n",
        "        return 0.5\n",
        "    total_weight = sum(weights)\n",
        "    if total_weight == 0:\n",
        "        weighted_avg = sum(scores) / len(scores)\n",
        "    else:\n",
        "        norm_weights = [w / total_weight for w in weights]\n",
        "        weighted_avg = sum(s * w for s, w in zip(scores, norm_weights))\n",
        "    debug_print(f\"Multimodal weighted score: {weighted_avg:.4f}\")\n",
        "    return weighted_avg\n",
        "\n",
        "def generate_verdict(score: float, results: Dict[str, DeepfakeAnalysisResult]) -> str:\n",
        "    if score > 0.8:\n",
        "        return \"AUTHENTIC - High confidence that this content is genuine\"\n",
        "    elif score > 0.6:\n",
        "        return \"LIKELY AUTHENTIC - Moderate confidence that this content is genuine\"\n",
        "    elif score > 0.4:\n",
        "        return \"INCONCLUSIVE - Analysis unable to determine authenticity\"\n",
        "    elif score > 0.2:\n",
        "        return \"LIKELY MANIPULATED - Moderate confidence that this content contains manipulations\"\n",
        "    else:\n",
        "        return \"MANIPULATED - High confidence that this content is manipulated or synthetic\"\n",
        "\n",
        "def collect_evidence(results: Dict[str, DeepfakeAnalysisResult]) -> List[Evidence]:\n",
        "    evidence_list = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            for anomaly in result.anomalies:\n",
        "                evidence_list.append(\n",
        "                    Evidence(\n",
        "                        type=modality,\n",
        "                        description=anomaly,\n",
        "                        confidence=result.confidence,\n",
        "                        method=result.method\n",
        "                    )\n",
        "                )\n",
        "    return evidence_list\n",
        "\n",
        "def generate_recommendations(report: MultimodalAnalysisReport) -> List[str]:\n",
        "    recommendations = []\n",
        "    if report.multimodal_score < 0.4:\n",
        "        recommendations.append(\"This content should be treated as potentially manipulated and not be used as evidence without further forensic analysis.\")\n",
        "    if report.video_analysis and report.video_analysis.score < 0.5:\n",
        "        recommendations.append(\"Video content shows significant signs of manipulation and should be verified through alternative sources.\")\n",
        "    if report.audio_analysis and report.audio_analysis.score < 0.5:\n",
        "        recommendations.append(\"Audio content appears to be synthetic or manipulated. Verify the source and content through other means.\")\n",
        "    if report.image_analysis and report.image_analysis.score < 0.5:\n",
        "        recommendations.append(\"Image analysis indicates potential manipulation. Consider requesting the original, unprocessed image file.\")\n",
        "    if report.text_analysis and report.text_analysis.score < 0.5:\n",
        "        recommendations.append(\"Text content appears to be AI-generated. Verify authorship through additional means.\")\n",
        "    recommendations.append(\"For definitive analysis, consider submitting this content to a professional forensic lab specialized in digital media authentication.\")\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "s4nJFQ3FrFqY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def enhance_report_with_mcp(report: MultimodalAnalysisReport) -> str:\n",
        "    config = {\n",
        "        \"mcpServers\": {\n",
        "            \"http\": {\n",
        "                \"url\": \"http://localhost:8931/sse\"  # Adjust this according to your MCP server settings.\n",
        "            },\n",
        "            \"playwright\": {\n",
        "                \"command\": \"npx\",\n",
        "                \"args\": [\"@playwright/mcp@latest\"],\n",
        "                \"env\": {\n",
        "                    \"DISPLAY\": \":1\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    client = MCPClient.from_dict(config)\n",
        "    llm = VLLM(\n",
        "        model=\"meta-llama/Llama-3-70b-hf\",\n",
        "        trust_remote_code=True,\n",
        "        max_new_tokens=128,\n",
        "        top_k=10,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "    agent = MCPAgent(llm=llm, client=client, max_steps=30)\n",
        "    query = (\"Using Google search, please find extra contextual information about recent deepfake detection trends \"\n",
        "             \"and forensic analysis in digital media. Provide a concise summary with relevant links.\")\n",
        "    result = await agent.run(query, max_steps=30)\n",
        "    report.metadata[\"mcp_enhancement\"] = result\n",
        "    if client.sessions:\n",
        "        await client.close_all_sessions()\n",
        "    return result"
      ],
      "metadata": {
        "id": "_em2CillR5k3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###   MAIN PIPELINE ORCHESTRATION  ###\n",
        "#######################################\n",
        "class DeepfakePipeline:\n",
        "    def __init__(self):\n",
        "        debug_print(\"Initializing DeepfakePipeline...\")\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        debug_print(f\"Using device: {self.device}\")\n",
        "\n",
        "    async def analyze_file(self, file_path: str) -> MultimodalAnalysisReport:\n",
        "        debug_print(f\"Starting analysis for file: {file_path}\")\n",
        "        start_time = datetime.now()\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        file_info = {\n",
        "            \"path\": file_path,\n",
        "            \"size\": os.path.getsize(file_path),\n",
        "            \"type\": \"unknown\",\n",
        "            \"extension\": file_ext\n",
        "        }\n",
        "        results = {\n",
        "            \"video_analysis\": None,\n",
        "            \"audio_analysis\": None,\n",
        "            \"image_analysis\": None,\n",
        "            \"text_analysis\": None\n",
        "        }\n",
        "        try:\n",
        "            if file_ext in ['.mp4', '.mov', '.avi', '.mkv', '.webm']:\n",
        "                file_info[\"type\"] = \"video\"\n",
        "                video_data = video_preprocessing(file_path)\n",
        "                tasks = []\n",
        "                tasks.append(advanced_video_analysis(video_data, self.device))\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    audio_data, sr = video_data[\"audio\"], 44100\n",
        "                    tasks.append(advanced_audio_analysis(audio_data, sr, self.device))\n",
        "                if video_data.get(\"frames\"):\n",
        "                    image = video_data[\"frames\"][0]\n",
        "                    tasks.append(advanced_image_analysis(image, self.device))\n",
        "                if video_data.get(\"text\"):\n",
        "                    tasks.append(analyze_text_content(video_data[\"text\"]))\n",
        "                completed_tasks = await asyncio.gather(*tasks)\n",
        "                results[\"video_analysis\"] = completed_tasks[0]\n",
        "                task_idx = 1\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    results[\"audio_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "                if video_data.get(\"frames\"):\n",
        "                    results[\"image_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "                if video_data.get(\"text\"):\n",
        "                    results[\"text_analysis\"] = completed_tasks[task_idx]\n",
        "            elif file_ext in ['.wav', '.mp3', '.ogg', '.flac']:\n",
        "                file_info[\"type\"] = \"audio\"\n",
        "                audio_data, sr = audio_preprocessing(file_path)\n",
        "                results[\"audio_analysis\"] = await advanced_audio_analysis(audio_data, sr, self.device)\n",
        "                text_content = \"Audio transcript would be here in a real implementation.\"\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "            elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
        "                file_info[\"type\"] = \"image\"\n",
        "                image = image_preprocessing(file_path)\n",
        "                results[\"image_analysis\"] = await advanced_image_analysis(image, self.device)\n",
        "            elif file_ext in ['.txt', '.md', '.rtf', '.doc', '.docx', '.pdf']:\n",
        "                file_info[\"type\"] = \"text\"\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    text_content = f.read()\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "            else:\n",
        "                debug_print(f\"Unsupported file type: {file_ext}\")\n",
        "                raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
        "            multimodal_score = calculate_multimodal_score(results)\n",
        "            # Cross modality consistency check\n",
        "            cross_score, cross_anomalies = cross_modality_consistency_check(results)\n",
        "            multimodal_score = (multimodal_score + cross_score) / 2\n",
        "            verdict = generate_verdict(multimodal_score, results)\n",
        "            evidence = collect_evidence(results)\n",
        "            processing_time = (datetime.now() - start_time).total_seconds()\n",
        "            confidence_matrix = {\n",
        "                modality: {\n",
        "                    \"score\": result.score if result else None,\n",
        "                    \"confidence\": result.confidence if result else None\n",
        "                }\n",
        "                for modality, result in results.items()\n",
        "            }\n",
        "            report = MultimodalAnalysisReport(\n",
        "                case_id=f\"case_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "                file_info=file_info,\n",
        "                video_analysis=results[\"video_analysis\"],\n",
        "                audio_analysis=results[\"audio_analysis\"],\n",
        "                image_analysis=results[\"image_analysis\"],\n",
        "                text_analysis=results[\"text_analysis\"],\n",
        "                multimodal_score=multimodal_score,\n",
        "                verdict=verdict,\n",
        "                evidence=evidence,\n",
        "                metadata={\n",
        "                    \"pipeline_version\": \"1.0.0\",\n",
        "                    \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "                    \"device\": str(self.device),\n",
        "                    \"cross_modality_consistency\": {\n",
        "                        \"score\": cross_score,\n",
        "                        \"anomalies\": cross_anomalies\n",
        "                    }\n",
        "                },\n",
        "                confidence_matrix=confidence_matrix,\n",
        "                processing_time=processing_time\n",
        "            )\n",
        "            report.recommendations = generate_recommendations(report)\n",
        "            # Enhance report with additional online context using MCP.\n",
        "            mcp_context = await enhance_report_with_mcp(report)\n",
        "            debug_print(f\"MCP enhancement result added to report metadata: {mcp_context}\")\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in analyze_file: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "Jxls6PLJrKb7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PipelineState(BaseModel):\n",
        "    file_path: str\n",
        "    report: Optional[MultimodalAnalysisReport] = None\n",
        "    current_step: str = \"initialize\"\n",
        "    error: Optional[str] = None\n",
        "    results: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "async def initialize_analysis(state: PipelineState) -> PipelineState:\n",
        "    debug_print(f\"Initializing analysis for {state.file_path}\")\n",
        "    try:\n",
        "        state.current_step = \"processing\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "async def process_file(state: PipelineState) -> PipelineState:\n",
        "    try:\n",
        "        pipeline = DeepfakePipeline()\n",
        "        state.report = await pipeline.analyze_file(state.file_path)\n",
        "        state.current_step = \"complete\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "def create_pipeline_graph() -> StateGraph:\n",
        "    workflow = StateGraph(PipelineState)\n",
        "    workflow.add_node(\"initialize\", initialize_analysis)\n",
        "    workflow.add_node(\"processing\", process_file)\n",
        "    workflow.add_edge(\"initialize\", \"processing\")\n",
        "    workflow.add_edge(\"processing\", END)\n",
        "    workflow.add_conditional_edges(\"initialize\", lambda state: \"error\" if state.error else \"processing\")\n",
        "    workflow.add_conditional_edges(\"processing\", lambda state: \"error\" if state.error else END)\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "fv-KVsHarQdP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description=\"Deepfake Detection Pipeline with MCP Enhancement using Groq and vLLM\")\n",
        "    parser.add_argument('file_path', help='Path to the file to analyze')\n",
        "    parser.add_argument('--debug', action='store_true', help='Enable debug output')\n",
        "    parser.add_argument('--output', help='Path to save the JSON report')\n",
        "    args = parser.parse_args()\n",
        "    global DEBUG\n",
        "    DEBUG = args.debug\n",
        "    debug_print(f\"Starting deepfake analysis for file: {args.file_path}\")\n",
        "    try:\n",
        "        workflow = create_pipeline_graph()\n",
        "        state = PipelineState(file_path=args.file_path)\n",
        "        final_state = await workflow.ainvoke(state)\n",
        "        if final_state.error:\n",
        "            print(f\"Error: {final_state.error}\")\n",
        "            return 1\n",
        "        report = final_state.report\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"DEEPFAKE ANALYSIS REPORT - {report.case_id}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"File: {os.path.basename(args.file_path)} ({report.file_info['type']})\")\n",
        "        print(f\"Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)\")\n",
        "        print(f\"Verdict: {report.verdict}\")\n",
        "        print(\"-\"*50)\n",
        "        for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "            result = getattr(report, f\"{modality}_analysis\")\n",
        "            if result:\n",
        "                print(f\"{modality.upper()} Analysis: {result.score:.2f} ({result.label})\")\n",
        "                if result.anomalies:\n",
        "                    print(f\"  Detected anomalies: {', '.join(result.anomalies)}\")\n",
        "        print(\"-\"*50)\n",
        "        print(\"RECOMMENDATIONS:\")\n",
        "        for i, rec in enumerate(report.recommendations, 1):\n",
        "            print(f\"{i}. {rec}\")\n",
        "        if args.output:\n",
        "            with open(args.output, 'w') as f:\n",
        "                f.write(report.json(indent=2))\n",
        "            print(f\"\\nDetailed report saved to {args.output}\")\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 1"
      ],
      "metadata": {
        "id": "dRxJL7L9wgnY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_deepfake_enhanced_ui():\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import json, time, matplotlib.pyplot as plt\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    import numpy as np, io, base64\n",
        "    from datetime import datetime\n",
        "\n",
        "    colors = [(0.8, 0.2, 0.2), (1, 0.6, 0), (0.2, 0.8, 0.2)]\n",
        "    cmap = LinearSegmentedColormap.from_list(\"deepfake_cmap\", colors, N=100)\n",
        "    dashboard_html = \"\"\"\n",
        "    Deepfake Detection Pipeline with MCP Enhancement using Groq and vLLM\n",
        "    Upload media files to analyze for potential manipulations across multiple modalities.\n",
        "    \"\"\"\n",
        "    display(HTML(dashboard_html))\n",
        "    tabs = widgets.Tab()\n",
        "    tab_analysis = widgets.VBox()\n",
        "    tab_results = widgets.VBox()\n",
        "    tab_logs = widgets.VBox()\n",
        "    tabs.children = [tab_analysis, tab_results, tab_logs]\n",
        "    tabs.set_title(0, \"Analysis\")\n",
        "    tabs.set_title(1, \"Results\")\n",
        "    tabs.set_title(2, \"Debug Logs\")\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload File',\n",
        "        disabled=False,\n",
        "        button_style='primary',\n",
        "        tooltip='Click to upload a file for analysis',\n",
        "        icon='upload',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "    debug_checkbox = widgets.Checkbox(value=True, description='Enable debug output', disabled=False)\n",
        "    file_info = widgets.HTML(value=\"No file selected\", layout=widgets.Layout(margin=\"10px 0px\"))\n",
        "    status_label = widgets.HTML(value=\"Status: Ready\", layout=widgets.Layout(margin=\"10px 0px\"))\n",
        "    progress_bar = widgets.IntProgress(value=0, min=0, max=100, description='Overall:',\n",
        "                                       bar_style='info', style={'bar_color': '#007bff'},\n",
        "                                       orientation='horizontal', layout=widgets.Layout(width='100%', margin=\"10px 0px\"))\n",
        "    step_progress = widgets.HTML(value=\"Waiting for file upload...\", layout=widgets.Layout(margin=\"10px 0px\"))\n",
        "    log_output = widgets.Output(layout=widgets.Layout(height='300px', border='1px solid #ddd', overflow_y='auto',\n",
        "                                                      padding='10px', margin=\"10px 0px\"))\n",
        "    result_output = widgets.Output(layout=widgets.Layout(height='500px', border='1px solid #ddd', overflow_y='auto',\n",
        "                                                         padding='10px', margin=\"10px 0px\"))\n",
        "    visualization_area = widgets.Output(layout=widgets.Layout(height='auto', padding='10px', margin=\"20px 0px\"))\n",
        "    export_options = widgets.HBox([\n",
        "        widgets.Button(description='Download JSON', icon='download', button_style='success', layout=widgets.Layout(width='150px')),\n",
        "        widgets.Button(description='Download PDF', icon='file-pdf', button_style='info', layout=widgets.Layout(width='150px')),\n",
        "        widgets.Button(description='View Full Report', icon='eye', button_style='warning', layout=widgets.Layout(width='150px'))\n",
        "    ], layout=widgets.Layout(display='none'))\n",
        "    tab_analysis.children = [widgets.HBox([upload_button, debug_checkbox]), file_info, status_label,\n",
        "                             progress_bar, step_progress, log_output]\n",
        "    tab_results.children = [visualization_area, result_output, export_options]\n",
        "    debug_controls = widgets.HBox([widgets.Button(description='Clear Logs', icon='trash', button_style='danger',\n",
        "                                                  layout=widgets.Layout(width='120px')), widgets.Checkbox(value=True, description='Auto-scroll', disabled=False)])\n",
        "    debug_output = widgets.Output(layout=widgets.Layout(height='400px', border='1px solid #ddd', overflow_y='auto',\n",
        "                                                       padding='10px', font_family='monospace', margin=\"10px 0px\"))\n",
        "    tab_logs.children = [debug_controls, debug_output]\n",
        "    display(tabs)\n",
        "    current_file_path = None\n",
        "    process_stages = {\n",
        "        \"preprocessing\": {\"started\": False, \"completed\": False, \"weight\": 10},\n",
        "        \"feature_extraction\": {\"started\": False, \"completed\": False, \"weight\": 20},\n",
        "        \"model_processing\": {\"started\": False, \"completed\": False, \"weight\": 40},\n",
        "        \"analysis\": {\"started\": False, \"completed\": False, \"weight\": 20},\n",
        "        \"report_generation\": {\"started\": False, \"completed\": False, \"weight\": 10}\n",
        "    }\n",
        "    def update_progress(stage, status, message=None, substage=None):\n",
        "        if stage in process_stages:\n",
        "            if status == \"start\":\n",
        "                process_stages[stage][\"started\"] = True\n",
        "            elif status == \"complete\":\n",
        "                process_stages[stage][\"started\"] = True\n",
        "                process_stages[stage][\"completed\"] = True\n",
        "        total_weight = sum(stage_info[\"weight\"] for stage_info in process_stages.values())\n",
        "        completed_weight = sum(stage_info[\"weight\"] for stage_info in process_stages.values() if stage_info[\"completed\"])\n",
        "        in_progress_weight = sum(stage_info[\"weight\"] * 0.5 for stage_info in process_stages.values() if stage_info[\"started\"] and not stage_info[\"completed\"])\n",
        "        progress_value = int((completed_weight + in_progress_weight) / total_weight * 100)\n",
        "        progress_bar.value = progress_value\n",
        "        stage_display = stage.replace(\"_\", \" \").title()\n",
        "        if status == \"start\":\n",
        "            status_label.value = f\"Status: Processing - {stage_display}\"\n",
        "            step_progress.value = f\"Current step: {stage_display}{' - ' + substage if substage else ''}\"\n",
        "        elif status == \"complete\":\n",
        "            step_progress.value = f\"Completed: {stage_display}{' - ' + substage if substage else ''}\"\n",
        "        elif status == \"error\":\n",
        "            status_label.value = f\"Status: Error in {stage_display}\"\n",
        "            step_progress.value = f\"Error: {message}\"\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        with debug_output:\n",
        "            print(f\"[{timestamp}] {stage_display}: {status.upper()} {message if message else ''}\")\n",
        "    def ui_debug_print(msg: str):\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        with debug_output:\n",
        "            print(f\"[{timestamp}] [DEBUG] {msg}\")\n",
        "    global debug_print\n",
        "    debug_print = ui_debug_print\n",
        "    def visualize_results(report):\n",
        "        with visualization_area:\n",
        "            clear_output()\n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            ax_overall = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n",
        "            overall_score = report.multimodal_score\n",
        "            overall_color = cmap(overall_score)\n",
        "            ax_overall.barh(['Overall'], [overall_score], color=overall_color, height=0.5)\n",
        "            ax_overall.set_xlim(0, 1)\n",
        "            ax_overall.set_title(f'Overall Analysis: {report.verdict}', fontsize=14)\n",
        "            ax_overall.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "            ax_modalities = plt.subplot2grid((3, 3), (1, 0), colspan=3)\n",
        "            modalities = []\n",
        "            scores = []\n",
        "            colors = []\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result:\n",
        "                    modalities.append(modality.capitalize())\n",
        "                    scores.append(result.score)\n",
        "                    colors.append(cmap(result.score))\n",
        "            ax_modalities.barh(modalities, scores, color=colors, height=0.5)\n",
        "            ax_modalities.set_xlim(0, 1)\n",
        "            ax_modalities.set_title('Analysis by Modality', fontsize=12)\n",
        "            ax_modalities.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "            for i, score in enumerate(scores):\n",
        "                ax_modalities.text(min(max(score + 0.05, 0.1), 0.95), i, f\"{score:.2f}\",\n",
        "                                   va='center', ha='center', fontweight='bold')\n",
        "            ax_anomalies = plt.subplot2grid((3, 3), (2, 0), colspan=3, rowspan=1)\n",
        "            ax_anomalies.axis('off')\n",
        "            anomaly_text = \"Key Anomalies Detected:\\n\"\n",
        "            anomaly_count = 0\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result and result.anomalies:\n",
        "                    anomaly_text += f\"\\n{modality.capitalize()}: {', '.join(result.anomalies[:3])}\"\n",
        "                    if len(result.anomalies) > 3:\n",
        "                        anomaly_text += f\" (+ {len(result.anomalies) - 3} more)\"\n",
        "                    anomaly_count += len(result.anomalies)\n",
        "            if anomaly_count == 0:\n",
        "                anomaly_text += \"\\nNo significant anomalies detected.\"\n",
        "            ax_anomalies.text(0.5, 0.5, anomaly_text,\n",
        "                              ha='center', va='center', fontsize=10,\n",
        "                              bbox=dict(boxstyle='round', facecolor='#f8f9fa', alpha=0.8))\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    def display_report(report):\n",
        "        with result_output:\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <h2>Deepfake Analysis Report</h2>\n",
        "            <b>Case ID:</b> {report.case_id}<br>\n",
        "            <b>VERDICT:</b> {report.verdict}<br>\n",
        "            <b>Overall Score:</b> {report.multimodal_score:.2f} (0=Fake, 1=Real)<br>\n",
        "            <h3>File Information</h3>\n",
        "            <b>File Type:</b> {report.file_info['type'].upper()}<br>\n",
        "            <b>File Size:</b> {report.file_info['size'] / (1024*1024):.2f} MB<br>\n",
        "            <b>Extension:</b> {report.file_info['extension']}<br>\n",
        "            <h3>Analysis Results</h3>\n",
        "            \"\"\"))\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result:\n",
        "                    score = result.score\n",
        "                    color = '#d4edda' if score > 0.7 else '#f8d7da' if score < 0.3 else '#fff3cd'\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <h4>{modality.upper()} Analysis: {score:.2f} ({result.label})</h4>\n",
        "                    <b>Confidence:</b> {result.confidence:.2f}<br>\n",
        "                    <b>Method:</b> {result.method}<br>\n",
        "                    <b>Detected Anomalies:</b> {', '.join(result.anomalies) if result.anomalies else \"No anomalies detected\"}<br>\n",
        "                    \"\"\"))\n",
        "            display(HTML(f\"\"\"\n",
        "            <h3>Evidence Summary</h3>\n",
        "            Total evidence items: {len(report.evidence)}<br>\n",
        "            <h3>Recommendations</h3>\n",
        "            {\"<br>\".join(report.recommendations)}<br>\n",
        "            <b>Processing time:</b> {report.processing_time:.2f} seconds<br>\n",
        "            <b>Report generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "            \"\"\"))\n",
        "    def generate_pdf_report(report):\n",
        "        try:\n",
        "            return f\"deepfake_report_{int(time.time())}.pdf\"\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error generating PDF: {e}\")\n",
        "            return None\n",
        "    def download_json_report(report, b):\n",
        "        try:\n",
        "            report_json = report.json(indent=2)\n",
        "            report_filename = f\"deepfake_report_{int(time.time())}.json\"\n",
        "            with open(report_filename, 'w') as f:\n",
        "                f.write(report_json)\n",
        "            files.download(report_filename)\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error downloading JSON report: {e}\")\n",
        "    def download_pdf_report(report, b):\n",
        "        try:\n",
        "            with log_output:\n",
        "                print(\"PDF export functionality is a placeholder - would be implemented with a PDF library\")\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error downloading PDF report: {e}\")\n",
        "    def show_full_report(report, b):\n",
        "        with result_output:\n",
        "            display(HTML(f\"\"\"\n",
        "            <h3>Full Technical Report</h3>\n",
        "            <pre>{json.dumps(json.loads(report.json()), indent=2)}</pre>\n",
        "            \"\"\"))\n",
        "    def on_upload_clicked(b):\n",
        "        with log_output:\n",
        "            clear_output()\n",
        "            print(\"Please select a file to upload...\")\n",
        "        global DEBUG, current_file_path\n",
        "        DEBUG = debug_checkbox.value\n",
        "        for stage in process_stages:\n",
        "            process_stages[stage][\"started\"] = False\n",
        "            process_stages[stage][\"completed\"] = False\n",
        "        progress_bar.value = 0\n",
        "        try:\n",
        "            status_label.value = \"Status: Uploading File\"\n",
        "            export_options.layout.display = 'none'\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                with log_output:\n",
        "                    print(\"No file was uploaded.\")\n",
        "                status_label.value = \"Status: Ready\"\n",
        "                step_progress.value = \"Waiting for file upload...\"\n",
        "                return\n",
        "            file_path = list(uploaded.keys())[0]\n",
        "            file_size_mb = uploaded[file_path].size / (1024*1024)\n",
        "            current_file_path = file_path\n",
        "            file_info.value = f\"\"\"\n",
        "            <b>File:</b> {file_path}<br>\n",
        "            <b>Size:</b> {file_size_mb:.2f} MB<br>\n",
        "            <b>Type:</b> {file_path.split('.')[-1].upper()}\n",
        "            \"\"\"\n",
        "            with log_output:\n",
        "                print(f\"File uploaded: {file_path} ({file_size_mb:.2f} MB)\")\n",
        "                print(\"Preparing for analysis...\")\n",
        "            temp_path = f\"/tmp/{file_path}\"\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                f.write(uploaded[file_path])\n",
        "            update_progress(\"preprocessing\", \"start\", f\"Initializing for {file_path}\")\n",
        "            workflow = create_pipeline_graph()\n",
        "            state = PipelineState(file_path=temp_path)\n",
        "            with log_output:\n",
        "                print(\"Starting analysis workflow...\")\n",
        "                print(\"This process may take several minutes depending on file size and complexity.\")\n",
        "            asyncio.create_task(run_analysis(workflow, state))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            with log_output:\n",
        "                print(f\"Error starting analysis: {e}\")\n",
        "                traceback.print_exc()\n",
        "            status_label.value = f\"Status: Error - {str(e)[:50]}...\"\n",
        "            step_progress.value = \"Analysis failed\"\n",
        "    async def run_analysis(workflow, state):\n",
        "        global extract_video_features, extract_audio_features, extract_image_features\n",
        "        try:\n",
        "            await asyncio.sleep(1)\n",
        "            update_progress(\"preprocessing\", \"complete\")\n",
        "            update_progress(\"feature_extraction\", \"start\")\n",
        "            original_extract_video_features = extract_video_features\n",
        "            original_extract_audio_features = extract_audio_features\n",
        "            original_extract_image_features = extract_image_features\n",
        "            def wrapped_extract_video_features(frames):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Video Features\")\n",
        "                result = original_extract_video_features(frames)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Video Features\")\n",
        "                return result\n",
        "            def wrapped_extract_audio_features(audio_data, sr):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Audio Features\")\n",
        "                result = original_extract_audio_features(audio_data, sr)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Audio Features\")\n",
        "                return result\n",
        "            def wrapped_extract_image_features(image):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Image Features\")\n",
        "                result = original_extract_image_features(image)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Image Features\")\n",
        "                return result\n",
        "            extract_video_features = wrapped_extract_video_features\n",
        "            extract_audio_features = wrapped_extract_audio_features\n",
        "            extract_image_features = wrapped_extract_image_features\n",
        "            update_progress(\"model_processing\", \"start\")\n",
        "            final_state = await workflow.ainvoke(state)\n",
        "            extract_video_features = original_extract_video_features\n",
        "            extract_audio_features = original_extract_audio_features\n",
        "            extract_image_features = original_extract_image_features\n",
        "            if final_state.error:\n",
        "                update_progress(\"model_processing\", \"error\", final_state.error)\n",
        "                with log_output:\n",
        "                    print(f\"Error in analysis: {final_state.error}\")\n",
        "                status_label.value = \"Status: Analysis Failed\"\n",
        "                return\n",
        "            update_progress(\"model_processing\", \"complete\")\n",
        "            update_progress(\"analysis\", \"start\")\n",
        "            await asyncio.sleep(1)\n",
        "            update_progress(\"analysis\", \"complete\")\n",
        "            update_progress(\"report_generation\", \"start\")\n",
        "            report = final_state.report\n",
        "            tabs.selected_index = 1\n",
        "            visualize_results(report)\n",
        "            display_report(report)\n",
        "            export_options.layout.display = 'flex'\n",
        "            export_options.children[0].on_click(lambda b: download_json_report(report, b))\n",
        "            export_options.children[1].on_click(lambda b: download_pdf_report(report, b))\n",
        "            export_options.children[2].on_click(lambda b: show_full_report(report, b))\n",
        "            update_progress(\"report_generation\", \"complete\")\n",
        "            status_label.value = \"Status: Analysis Complete\"\n",
        "            step_progress.value = \"Analysis completed. See Results for details.\"\n",
        "            with log_output:\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(\"Analysis completed successfully!\")\n",
        "                print(f\"Overall score: {report.multimodal_score:.2f} - Verdict: {report.verdict}\")\n",
        "                print(\"=\"*50)\n",
        "                print(\"View the Results tab for detailed report and visualizations.\")\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            with log_output:\n",
        "                print(f\"Error in analysis process: {e}\")\n",
        "                traceback.print_exc()\n",
        "            status_label.value = \"Status: Error\"\n",
        "            step_progress.value = f\"Analysis failed: {str(e)[:100]}\"\n",
        "    tab_logs.children[0].children[0].on_click(lambda b: debug_output.clear_output())\n",
        "    upload_button.on_click(on_upload_clicked)\n",
        "    with log_output:\n",
        "        print(\"🚀 Deepfake Detection Pipeline with MCP Enhancement using Groq and vLLM initialized\")\n",
        "        print(\"Click 'Upload File' to begin analysis\")\n",
        "    with debug_output:\n",
        "        print(\"[INFO] Debug logs will appear here\")\n",
        "        print(\"[INFO] Pipeline ready for file upload\")\n",
        "\n",
        "# Execute the UI\n",
        "run_deepfake_enhanced_ui()"
      ],
      "metadata": {
        "id": "K0MWVUCWTZHh",
        "outputId": "a38c9d3e-f5ba-4f71-e70b-8db11a3042af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<coroutine object run_deepfake_enhanced_ui at 0x17121a20>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glIZ6pvWTeQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}