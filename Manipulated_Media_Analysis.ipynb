{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Manipulated_Media_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Multimodal Manipulated Media Analysis Pipeline for Defensive Forensic**\n",
        "\n",
        "\n",
        "\n",
        "This end-to-end pipeline uses LangChain-based VLLM and Groq LLM integration\n",
        "along with LangGraph multiagent orchestration to analyze audio, video, and image modalities.\n",
        "Each modality has its own processing and analysis steps, and the results are aggregated\n",
        "into a consolidated report"
      ],
      "metadata": {
        "id": "2Fqt0MDqomfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iYbnoslNm-tQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy opencv-python librosa pydantic mediapipe moviepy face_recognition scikit-image dtw-python scipy langchain langchain_community langchain_core langgraph nest_asyncio ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# Media processing libraries\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# For interactive notebook usage\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "# For image quality metrics\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from dtw import dtw\n",
        "import face_recognition\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Langchain and LangGraph imports\n",
        "from langchain_community.llms import VLLM\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Apply nest_asyncio to allow running async code in notebooks\n",
        "nest_asyncio.apply()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ysWFk98_nTbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da218cd2-b900-42db-9d00-e49c55df064b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "### GLOBAL DEBUG SETTINGS ###\n",
        "#########################\n",
        "DEBUG = True\n",
        "def debug_print(msg: str):\n",
        "    \"\"\"Print debug messages when DEBUG is enabled.\"\"\"\n",
        "    if DEBUG:\n",
        "        print(f\"[DEBUG] {msg}\")"
      ],
      "metadata": {
        "id": "G9XItUIFo_8a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "### MODEL INITIALIZATION SECTION  ###\n",
        "######################################\n",
        "COMMON_PARAMS = {\n",
        "    \"task\": \"generate\",\n",
        "    \"max_model_len\": 4096,\n",
        "    \"dtype\": \"half\",\n",
        "    \"gpu_memory_utilization\": 0.85,\n",
        "    \"cpu_offload_gb\": 8,\n",
        "    \"enforce_eager\": True,\n",
        "    \"trust_remote_code\": True\n",
        "}\n",
        "\n",
        "def init_vllm_model(name: str, model_id: str, **overrides):\n",
        "    \"\"\"Initialize a VLLM model with specified parameters.\"\"\"\n",
        "    params = {**COMMON_PARAMS, **overrides}\n",
        "    debug_print(f\"Initializing VLLM model '{name}' with id '{model_id}' and params: {params}\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"params\": params}\n",
        "\n",
        "def init_groq_model(name: str, model_id: str):\n",
        "    \"\"\"Initialize a Groq model with API key.\"\"\"\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\", \"your_groq_api_key\")\n",
        "    debug_print(f\"Initializing Groq model '{name}' with id '{model_id}' using API key.\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"api_key\": api_key}\n",
        "\n",
        "# Groq LLM wrapper class for LangChain\n",
        "class GroqLLM:\n",
        "    \"\"\"A wrapper for Groq LLM API.\"\"\"\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def call_as_llm(self, prompt: str) -> str:\n",
        "        \"\"\"Process a prompt through the Groq API.\"\"\"\n",
        "        debug_print(f\"GroqLLM processing prompt with {self.model_data['name']}\")\n",
        "        # In a real implementation, this would make an API call to Groq.\n",
        "        # Here we simulate a response for demonstration.\n",
        "        if \"audio\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.82\\nAnomalies: [\\\"unnatural voice transitions\\\", \\\"inconsistent background noise\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.77\\nAnomalies: [\\\"facial landmark inconsistencies\\\", \\\"unnatural eye movements\\\"]\"\n",
        "        elif \"image\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.68\\nAnomalies: [\\\"lighting inconsistencies\\\", \\\"unusual facial proportions\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.75\\nAnomalies: [\\\"inconsistent narrative\\\", \\\"unusual phrasing patterns\\\"]\"\n",
        "\n",
        "class GroqLLMWrapper(LLM):\n",
        "    \"\"\"LangChain compatible wrapper for GroqLLM.\"\"\"\n",
        "    def __init__(self, groq_llm: GroqLLM, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._groq_llm = groq_llm\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop=None) -> str:\n",
        "        return self._groq_llm.call_as_llm(prompt)\n",
        "\n",
        "# Simulated VLLM model wrapper\n",
        "class VLLMSimulated:\n",
        "    \"\"\"A simulated VLLM model for demonstration.\"\"\"\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        \"\"\"Process a prompt through the simulated VLLM.\"\"\"\n",
        "        debug_print(f\"VLLM simulated processing with {self.model_data['name']}\")\n",
        "        # Simulate different responses based on the model name.\n",
        "        if \"wav2vec\" in self.model_data[\"name\"] or \"whisper\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.79\\nAnomalies: [\\\"frequency anomalies\\\", \\\"unnatural pauses\\\"]\"\n",
        "        elif \"video\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.81\\nAnomalies: [\\\"temporal inconsistencies\\\", \\\"blending artifacts\\\"]\"\n",
        "        elif \"llava\" in self.model_data[\"name\"] or \"clip\" in self.model_data[\"name\"]:\n",
        "            return \"Score: 0.73\\nAnomalies: [\\\"compression artifacts\\\", \\\"unusual texture patterns\\\"]\"\n",
        "        else:\n",
        "            return \"Score: 0.78\\nAnomalies: [\\\"stylistic inconsistencies\\\"]\"\n",
        "\n",
        "# Initialize models for each modality\n",
        "debug_print(\"Initializing modality-specific models...\")\n",
        "models = {\n",
        "    \"audio\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"wav2vec2\", \"facebook/wav2vec2-large-robust-ft-swbd-300h\", tensor_parallel_size=1)),\n",
        "            VLLMSimulated(init_vllm_model(\"whisper\", \"openai/whisper-large-v3\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_audio_model\", \"whisper-large-v3-turbo\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_audio\", \"meta-llama/llama-4-audio-17b-16e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"video\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_next_video\", \"llava-hf/LLaVA-NeXT-Video-7B-hf\", tensor_parallel_size=2, max_tokens=1024)),\n",
        "            VLLMSimulated(init_vllm_model(\"videomae\", \"MCG-NJU/videomae-large-static\", tensor_parallel_size=2))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_video_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_video_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"image\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llava_image\", \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", tensor_parallel_size=2)),\n",
        "            VLLMSimulated(init_vllm_model(\"clip\", \"openai/clip-vit-large-patch14\", tensor_parallel_size=1))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_image_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_image_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    },\n",
        "    \"text\": {\n",
        "        \"vllm\": [\n",
        "            VLLMSimulated(init_vllm_model(\"llama3\", \"meta-llama/Llama-3-70b-hf\", tensor_parallel_size=4))\n",
        "        ],\n",
        "        \"groq\": [\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "            GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "        ]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCrOCZ4_poed",
        "outputId": "69d01713-3312-433d-bc35-0515da3effa3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Initializing modality-specific models...\n",
            "[DEBUG] Initializing VLLM model 'wav2vec2' with id 'facebook/wav2vec2-large-robust-ft-swbd-300h' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing VLLM model 'whisper' with id 'openai/whisper-large-v3' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_audio_model' with id 'whisper-large-v3-turbo' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_llama_audio' with id 'meta-llama/llama-4-audio-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_next_video' with id 'llava-hf/LLaVA-NeXT-Video-7B-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2, 'max_tokens': 1024}\n",
            "[DEBUG] Initializing VLLM model 'videomae' with id 'MCG-NJU/videomae-large-static' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing Groq model 'groq_video_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_video_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llava_image' with id 'llava-hf/llava-onevision-qwen2-7b-ov-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing VLLM model 'clip' with id 'openai/clip-vit-large-patch14' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing Groq model 'groq_image_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_image_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing VLLM model 'llama3' with id 'meta-llama/Llama-3-70b-hf' and params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 4}\n",
            "[DEBUG] Initializing Groq model 'groq_text_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Initializing Groq model 'groq_text_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "### DATA MODELS & REPORT STRUCTURES SECTION ###\n",
        "###############################################\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    \"\"\"Model for storing deepfake analysis results per modality.\"\"\"\n",
        "    score: float\n",
        "    label: str\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    explanation: Optional[str] = None\n",
        "    model_scores: Dict[str, float] = Field(default_factory=dict)\n",
        "\n",
        "class Evidence(BaseModel):\n",
        "    \"\"\"Model for storing specific evidence of manipulation.\"\"\"\n",
        "    type: str\n",
        "    description: str\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: Optional[float] = None\n",
        "    location: Optional[Dict[str, int]] = None\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    \"\"\"Comprehensive report aggregating all modality analyses.\"\"\"\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Evidence]\n",
        "    metadata: Dict[str, Any]\n",
        "    recommendations: List[str] = Field(default_factory=list)\n",
        "    confidence_matrix: Dict[str, Dict[str, float]] = Field(default_factory=dict)\n",
        "    processing_time: float"
      ],
      "metadata": {
        "id": "Otpsa4nDpx5s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "### PROCESSING & ANALYSIS FUNCTIONS ###\n",
        "#######################################\n",
        "# --- Shared utility functions ---\n",
        "def parse_model_output(output: str) -> Tuple[float, List[str]]:\n",
        "    \"\"\"Parse model output to extract score and anomalies.\"\"\"\n",
        "    debug_print(f\"Parsing model output: {output}\")\n",
        "    score_match = re.search(r\"Score:?\\s*(0\\.\\d+|1\\.0)\", output, re.IGNORECASE)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    anomalies = []\n",
        "    # Allow anomalies to appear on the same or next line after 'Anomalies:'\n",
        "    anomalies_pattern = r\"Anomalies:?\\s*(\\[.*\\]|.*)\"\n",
        "    anomalies_match = re.search(anomalies_pattern, output, re.IGNORECASE | re.DOTALL)\n",
        "    if anomalies_match:\n",
        "        anomalies_text = anomalies_match.group(1)\n",
        "        if anomalies_text:\n",
        "            # Remove brackets if present and split by comma\n",
        "            anomalies_text = anomalies_text.strip(\"[]\")\n",
        "            anomalies = [a.strip().strip('\"\\'') for a in anomalies_text.split(',') if a.strip()]\n",
        "    debug_print(f\"Parsed output: score={score}, anomalies={anomalies}\")\n",
        "    return score, anomalies\n",
        "\n",
        "async def aggregate_llm_outputs(prompt: str, content: str, modality: str=\"generic\") -> Tuple[float, List[str], Dict[str, float]]:\n",
        "    \"\"\"Aggregate outputs from multiple LLMs for a given modality.\"\"\"\n",
        "    debug_print(f\"Aggregating LLM outputs for modality {modality}\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "\n",
        "    try:\n",
        "        # Get responses from VLLM models\n",
        "        for i, model in enumerate(models[modality][\"vllm\"]):\n",
        "            model_name = f\"vllm_{modality}_{i}\"\n",
        "            try:\n",
        "                response = model(f\"{prompt}\\n\\nContent: {content}\")\n",
        "                s, a = parse_model_output(response)\n",
        "                scores.append(s)\n",
        "                all_anomalies.extend(a)\n",
        "                model_scores[model_name] = s\n",
        "                debug_print(f\"{model_name} score: {s}, anomalies: {a}\")\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error with {model_name}: {e}\")\n",
        "\n",
        "        # Get responses from Groq models\n",
        "        for i, model in enumerate(models[modality][\"groq\"]):\n",
        "            model_name = f\"groq_{modality}_{i}\"\n",
        "            try:\n",
        "                chat_prompt = ChatPromptTemplate.from_messages([\n",
        "                    (\"system\", f\"You are a forensic {modality} deepfake detection expert.\"),\n",
        "                    (\"user\", f\"{prompt}\\n\\nAnalyze this content: {content}\")\n",
        "                ])\n",
        "                chain = LLMChain(prompt=chat_prompt, llm=model)\n",
        "                response = await asyncio.to_thread(chain.run, {\"content\": content})\n",
        "                s, a = parse_model_output(response)\n",
        "                scores.append(s)\n",
        "                all_anomalies.extend(a)\n",
        "                model_scores[model_name] = s\n",
        "                debug_print(f\"{model_name} score: {s}, anomalies: {a}\")\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error with {model_name}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in aggregate_llm_outputs: {e}\")\n",
        "\n",
        "    # Compute aggregate score and deduplicate anomalies\n",
        "    agg_score = float(np.mean(scores)) if scores else 0.5\n",
        "    unique_anomalies = list(set(all_anomalies))\n",
        "    debug_print(f\"Aggregated score for {modality}: {agg_score}, anomalies: {unique_anomalies}\")\n",
        "    return agg_score, unique_anomalies, model_scores"
      ],
      "metadata": {
        "id": "CfDQ_Ffzp4cW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AUDIO PROCESSING & ANALYSIS ---\n",
        "def audio_preprocessing(audio_path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load and preprocess audio data.\"\"\"\n",
        "    debug_print(f\"Loading and preprocessing audio from {audio_path} ...\")\n",
        "    try:\n",
        "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
        "        # Basic preprocessing: normalization\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
        "        # Simple noise reduction (using high-pass filter)\n",
        "        if len(audio_data) > 0:\n",
        "            b, a = librosa.filters.butter(4, 100/(sr/2), btype='highpass')\n",
        "            audio_data = librosa.filters.filtfilt(b, a, audio_data)\n",
        "        debug_print(f\"Audio loaded and preprocessed. Sample rate: {sr}, Duration: {len(audio_data)/sr:.2f}s\")\n",
        "        return audio_data, sr\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in audio preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sr: int) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from audio data for deepfake detection.\"\"\"\n",
        "    debug_print(\"Extracting audio features...\")\n",
        "    features = {}\n",
        "    stft = np.abs(librosa.stft(audio_data))\n",
        "    features[\"stft\"] = stft\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
        "    features[\"mel_spectrogram\"] = mel_spec\n",
        "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
        "    features[\"mfccs\"] = mfccs\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
        "    features[\"spectral_contrast\"] = spectral_contrast\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
        "    features[\"chroma\"] = chroma\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
        "    features[\"zero_crossing_rate\"] = zcr\n",
        "    onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr)\n",
        "    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "    features[\"tempo\"] = tempo[0]\n",
        "    debug_print(f\"Audio features extracted: {list(features.keys())}\")\n",
        "    return features\n",
        "\n",
        "def detect_audio_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in audio features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting audio anomalies from features...\")\n",
        "    anomalies = []\n",
        "    mel_spec = features[\"mel_spectrogram\"]\n",
        "    if np.std(mel_spec) < 0.1 or np.std(mel_spec) > 10:\n",
        "        anomalies.append(\"unusual spectral distribution\")\n",
        "    mfccs = features[\"mfccs\"]\n",
        "    if np.max(np.diff(mfccs, axis=1)) > 5:\n",
        "        anomalies.append(\"abrupt MFCC transitions\")\n",
        "    tempo = features[\"tempo\"]\n",
        "    if tempo < 40 or tempo > 240:\n",
        "        anomalies.append(\"unusual speech tempo\")\n",
        "    zcr = features[\"zero_crossing_rate\"]\n",
        "    if np.mean(zcr) > 0.3:\n",
        "        anomalies.append(\"unusually high zero-crossing rate\")\n",
        "    debug_print(f\"Detected audio anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_audio_analysis(audio_data: np.ndarray, sr: int, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive audio analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced audio analysis...\")\n",
        "    audio_features = extract_audio_features(audio_data, sr)\n",
        "    signal_anomalies = detect_audio_anomalies(audio_features)\n",
        "    signal_score = 0.8 if not signal_anomalies else 0.6\n",
        "    feature_summary = (\n",
        "        f\"Audio duration: {len(audio_data)/sr:.2f}s, Sample rate: {sr}Hz\\n\"\n",
        "        f\"Mean amplitude: {np.mean(np.abs(audio_data)):.4f}, Max amplitude: {np.max(np.abs(audio_data)):.4f}\\n\"\n",
        "        f\"Detected tempo: {audio_features['tempo']:.2f} BPM\\n\"\n",
        "        f\"Mean zero-crossing rate: {np.mean(audio_features['zero_crossing_rate']):.4f}\\n\"\n",
        "        f\"Signal-to-noise ratio estimate: {np.mean(audio_data**2) / np.std(audio_data):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {signal_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the audio features, provide a deepfake confidence score (0-1) and list specific audio anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"audio\")\n",
        "    combined_anomalies = list(set(signal_anomalies + llm_anomalies))\n",
        "    final_score = (signal_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([signal_score, llm_score])\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_audio_analysis (signal processing + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced audio analysis combining signal processing features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "kFWN6Eh-qAQB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VIDEO PROCESSING & ANALYSIS ---\n",
        "def video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load and preprocess video data.\"\"\"\n",
        "    debug_print(f\"Loading video from {video_path} ...\")\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        frame_count = int(video.fps * video.duration)\n",
        "        sample_rate = max(1, frame_count // 30)\n",
        "        frames = [video.get_frame(i/video.fps) for i in range(0, frame_count, sample_rate)]\n",
        "        audio = video.audio.to_soundarray() if video.audio else None\n",
        "        metadata = {\n",
        "            \"duration\": video.duration,\n",
        "            \"fps\": video.fps,\n",
        "            \"frame_count\": frame_count,\n",
        "            \"resolution\": f\"{video.size[0]}x{video.size[1]}\",\n",
        "            \"file_size\": os.path.getsize(video_path),\n",
        "            \"file_path\": video_path\n",
        "        }\n",
        "        debug_print(f\"Video preprocessed: {len(frames)} frames extracted, Audio present: {'Yes' if audio is not None else 'No'}\")\n",
        "        return {\n",
        "            \"frames\": frames,\n",
        "            \"audio\": audio,\n",
        "            \"metadata\": metadata,\n",
        "            \"image\": frames[0] if frames else None,\n",
        "            \"text\": \"Extracted text placeholder\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in video preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_video_features(frames: List[np.ndarray]) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from video frames for deepfake detection.\"\"\"\n",
        "    debug_print(f\"Extracting video features from {len(frames)} frames...\")\n",
        "    features = {}\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "    face_landmarks_sequence = []\n",
        "    face_embeddings = []\n",
        "    optical_flow_metrics = []\n",
        "    frame_diffs = []\n",
        "    blur_metrics = []\n",
        "    compression_metrics = []\n",
        "    for i in range(len(frames)):\n",
        "        frame_rgb = cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB) if frames[i].shape[2] == 3 else frames[i]\n",
        "        face_landmarks = face_mesh.process(frame_rgb).multi_face_landmarks\n",
        "        if face_landmarks:\n",
        "            face_landmarks_sequence.append(face_landmarks[0].landmark)\n",
        "            face_locations = face_recognition.face_locations(frame_rgb)\n",
        "            if face_locations:\n",
        "                face_encoding = face_recognition.face_encodings(frame_rgb, face_locations)[0]\n",
        "                face_embeddings.append(face_encoding)\n",
        "        if i > 0:\n",
        "            prev_gray = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY)\n",
        "            curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "            diff = cv2.absdiff(prev_gray, curr_gray)\n",
        "            frame_diffs.append(np.mean(diff))\n",
        "            flow = np.mean(diff)\n",
        "            optical_flow_metrics.append(flow)\n",
        "        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        blur_metric = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        blur_metrics.append(blur_metric)\n",
        "        compression_metric = np.mean(cv2.Canny(gray, 100, 200))\n",
        "        compression_metrics.append(compression_metric)\n",
        "    features[\"face_landmarks_sequence\"] = face_landmarks_sequence\n",
        "    features[\"face_embeddings\"] = face_embeddings\n",
        "    features[\"optical_flow_metrics\"] = optical_flow_metrics\n",
        "    features[\"frame_diffs\"] = frame_diffs\n",
        "    features[\"blur_metrics\"] = blur_metrics\n",
        "    features[\"compression_metrics\"] = compression_metrics\n",
        "    debug_print(f\"Video features extracted: {list(features.keys())}\")\n",
        "    debug_print(f\"Found facial landmarks in {len(face_landmarks_sequence)}/{len(frames)} frames\")\n",
        "    face_mesh.close()\n",
        "    return features\n",
        "\n",
        "def detect_video_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in video features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting video anomalies from features...\")\n",
        "    anomalies = []\n",
        "    face_landmarks = features.get(\"face_landmarks_sequence\", [])\n",
        "    if len(face_landmarks) > 1:\n",
        "        landmark_diffs = []\n",
        "        for i in range(1, len(face_landmarks)):\n",
        "            diff = sum(np.sqrt((face_landmarks[i][j].x - face_landmarks[i-1][j].x)**2 +\n",
        "                      (face_landmarks[i][j].y - face_landmarks[i-1][j].y)**2)\n",
        "                     for j in range(min(5, len(face_landmarks[i]))))\n",
        "            landmark_diffs.append(diff)\n",
        "        if np.max(landmark_diffs) > 0.5:\n",
        "            anomalies.append(\"abrupt facial landmark movements\")\n",
        "    face_embeddings = features.get(\"face_embeddings\", [])\n",
        "    if len(face_embeddings) > 1:\n",
        "        embedding_diffs = []\n",
        "        for i in range(1, len(face_embeddings)):\n",
        "            diff = cosine(face_embeddings[i], face_embeddings[i-1])\n",
        "            embedding_diffs.append(diff)\n",
        "        if np.max(embedding_diffs) > 0.3:\n",
        "            anomalies.append(\"inconsistent face identity\")\n",
        "    frame_diffs = features.get(\"frame_diffs\", [])\n",
        "    if frame_diffs:\n",
        "        if np.std(frame_diffs) < 0.01:\n",
        "            anomalies.append(\"unnaturally smooth motion\")\n",
        "        if np.max(frame_diffs) / (np.mean(frame_diffs) + 1e-6) > 10:\n",
        "            anomalies.append(\"erratic frame changes\")\n",
        "    blur_metrics = features.get(\"blur_metrics\", [])\n",
        "    if blur_metrics:\n",
        "        if np.std(blur_metrics) / (np.mean(blur_metrics) + 1e-6) > 0.8:\n",
        "            anomalies.append(\"inconsistent blur levels\")\n",
        "    compression_metrics = features.get(\"compression_metrics\", [])\n",
        "    if compression_metrics:\n",
        "        if np.std(compression_metrics) / (np.mean(compression_metrics) + 1e-6) > 0.5:\n",
        "            anomalies.append(\"inconsistent compression artifacts\")\n",
        "    debug_print(f\"Detected video anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive video analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced video analysis...\")\n",
        "    frames = video_data.get(\"frames\", [])\n",
        "    if not frames:\n",
        "        debug_print(\"No frames available for analysis\")\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.5,\n",
        "            label=\"INCONCLUSIVE\",\n",
        "            confidence=0.0,\n",
        "            method=\"advanced_video_analysis\",\n",
        "            anomalies=[\"no frames available for analysis\"],\n",
        "            explanation=\"Could not perform video analysis due to lack of frame data.\"\n",
        "        )\n",
        "    video_features = extract_video_features(frames)\n",
        "    cv_anomalies = detect_video_anomalies(video_features)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6\n",
        "    feature_summary = (\n",
        "        f\"Video features: {len(frames)} frames, Resolution: {video_data['metadata']['resolution']}\\n\"\n",
        "        f\"Facial landmarks detected in {len(video_features.get('face_landmarks_sequence', []))}/{len(frames)} frames\\n\"\n",
        "        f\"Mean frame difference: {np.mean(video_features.get('frame_diffs', [0])):.4f}\\n\"\n",
        "        f\"Blur consistency: {np.std(video_features.get('blur_metrics', [0]))/(np.mean(video_features.get('blur_metrics', [1]))+1e-6):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the video features, provide a deepfake confidence score (0-1) and list specific video anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"video\")\n",
        "    combined_anomalies = list(set(cv_anomalies + llm_anomalies))\n",
        "    final_score = (cv_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([cv_score, llm_score])\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_video_analysis (computer vision + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced video analysis combining computer vision features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "-QJAMVIQqHkn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMAGE PROCESSING & ANALYSIS ---\n",
        "def image_preprocessing(image_path: str) -> np.ndarray:\n",
        "    \"\"\"Load and preprocess image data.\"\"\"\n",
        "    debug_print(f\"Loading and preprocessing image from {image_path} ...\")\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
        "        max_dim = 1024\n",
        "        h, w = image.shape[:2]\n",
        "        if max(h, w) > max_dim:\n",
        "            scale = max_dim / max(h, w)\n",
        "            image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
        "            debug_print(f\"Image resized from {w}x{h} to {int(w * scale)}x{int(h * scale)}\")\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        debug_print(f\"Image loaded and preprocessed. Shape: {image_rgb.shape}\")\n",
        "        return image_rgb\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in image preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    \"\"\"Extract relevant features from an image for deepfake detection.\"\"\"\n",
        "    debug_print(\"Extracting image features...\")\n",
        "    features = {}\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    features[\"face_count\"] = len(faces)\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        features[\"face_roi\"] = face_roi\n",
        "        face_landmarks = face_recognition.face_landmarks(image)\n",
        "        features[\"face_landmarks\"] = face_landmarks\n",
        "        face_encodings = face_recognition.face_encodings(image)\n",
        "        if face_encodings:\n",
        "            features[\"face_encoding\"] = face_encodings[0]\n",
        "    temp_path = \"temp_ela.jpg\"\n",
        "    cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "    low_qual = cv2.imread(temp_path)\n",
        "    low_qual_rgb = cv2.cvtColor(low_qual, cv2.COLOR_BGR2RGB)\n",
        "    ela = cv2.absdiff(image, low_qual_rgb)\n",
        "    features[\"ela\"] = cv2.cvtColor(ela, cv2.COLOR_RGB2GRAY)\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "    noise = cv2.medianBlur(gray, 5) - gray\n",
        "    features[\"noise_pattern\"] = noise\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features[\"edges\"] = edges\n",
        "    dct_blocks = np.zeros_like(gray, dtype=float)\n",
        "    block_size = 8\n",
        "    for i in range(0, gray.shape[0] - block_size, block_size):\n",
        "        for j in range(0, gray.shape[1] - block_size, block_size):\n",
        "            block = gray[i:i+block_size, j:j+block_size].astype(float)\n",
        "            dct_block = cv2.dct(block)\n",
        "            dct_blocks[i:i+block_size, j:j+block_size] = dct_block\n",
        "    features[\"dct_blocks\"] = dct_blocks\n",
        "    debug_print(f\"Image features extracted. Faces found: {features['face_count']}\")\n",
        "    return features\n",
        "\n",
        "def detect_image_anomalies(features: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Detect anomalies in image features that might indicate deepfakes.\"\"\"\n",
        "    debug_print(\"Detecting image anomalies from features...\")\n",
        "    anomalies = []\n",
        "    if features[\"face_count\"] > 0:\n",
        "        face_landmarks = features.get(\"face_landmarks\", [])\n",
        "        if face_landmarks:\n",
        "            landmarks = face_landmarks[0]\n",
        "            left_eye = np.mean(landmarks.get(\"left_eye\", [[0, 0]]), axis=0)\n",
        "            right_eye = np.mean(landmarks.get(\"right_eye\", [[0, 0]]), axis=0)\n",
        "            if abs(left_eye[1] - right_eye[1]) > 10:\n",
        "                anomalies.append(\"asymmetric eye alignment\")\n",
        "            nose_tip = landmarks.get(\"nose_tip\", [[0, 0]])[0]\n",
        "            top_lip = np.mean(landmarks.get(\"top_lip\", [[0, 0]]), axis=0)\n",
        "            if abs((nose_tip[1] - left_eye[1]) - (top_lip[1] - nose_tip[1])) > 15:\n",
        "                anomalies.append(\"unusual facial proportions\")\n",
        "        ela = features.get(\"ela\")\n",
        "        if ela is not None:\n",
        "            ela_mean = np.mean(ela)\n",
        "            ela_std = np.std(ela)\n",
        "            if ela_mean > 10 or ela_std > 15:\n",
        "                anomalies.append(\"potential image manipulation (ELA)\")\n",
        "        noise = features.get(\"noise_pattern\")\n",
        "        if noise is not None:\n",
        "            noise_mean = np.mean(np.abs(noise))\n",
        "            noise_std = np.std(noise)\n",
        "            if noise_std / (noise_mean + 1e-6) > 3 or noise_mean < 0.5:\n",
        "                anomalies.append(\"unnatural noise pattern\")\n",
        "        dct_blocks = features.get(\"dct_blocks\")\n",
        "        if dct_blocks is not None:\n",
        "            block_diff = cv2.Laplacian(dct_blocks, cv2.CV_64F).var()\n",
        "            if block_diff > 500:\n",
        "                anomalies.append(\"unusual compression artifacts\")\n",
        "    else:\n",
        "        anomalies.append(\"no faces detected for analysis\")\n",
        "    debug_print(f\"Detected image anomalies: {anomalies}\")\n",
        "    return anomalies\n",
        "\n",
        "async def advanced_image_analysis(image: np.ndarray, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Perform comprehensive image analysis for deepfake detection.\"\"\"\n",
        "    debug_print(\"Starting advanced image analysis...\")\n",
        "    image_features = extract_image_features(image)\n",
        "    cv_anomalies = detect_image_anomalies(image_features)\n",
        "    cv_score = 0.8 if not cv_anomalies else 0.6\n",
        "    feature_summary = (\n",
        "        f\"Image features: Resolution: {image.shape[1]}x{image.shape[0]}, Faces detected: {image_features['face_count']}\\n\"\n",
        "        f\"ELA mean: {np.mean(image_features.get('ela', np.zeros(1))):.4f}, std: {np.std(image_features.get('ela', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Noise pattern: mean: {np.mean(np.abs(image_features.get('noise_pattern', np.zeros(1)))):.4f}, std: {np.std(image_features.get('noise_pattern', np.zeros(1))):.4f}\\n\"\n",
        "        f\"Traditional analysis anomalies: {cv_anomalies}\"\n",
        "    )\n",
        "    prompt = \"Based on the image features, provide a deepfake confidence score (0-1) and list specific image anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"image\")\n",
        "    combined_anomalies = list(set(cv_anomalies + llm_anomalies))\n",
        "    final_score = (cv_score + llm_score) / 2\n",
        "    confidence = 1.0 - np.std([cv_score, llm_score])\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=confidence,\n",
        "        method=\"advanced_image_analysis (computer vision + LLM)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Advanced image analysis combining computer vision features and multiagent LLM insights.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "Az8f7cUtqxz3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEXT ANALYSIS ---\n",
        "async def analyze_text_content(text: str) -> DeepfakeAnalysisResult:\n",
        "    \"\"\"Analyze text for indicators of AI generation or manipulation.\"\"\"\n",
        "    debug_print(\"Starting text content analysis...\")\n",
        "    word_count = len(text.split())\n",
        "    sent_count = len(re.split(r'[.!?]+', text))\n",
        "    avg_word_len = sum(len(word) for word in text.split()) / max(1, word_count)\n",
        "    feature_summary = (\n",
        "        f\"Text statistics: {word_count} words, {sent_count} sentences\\n\"\n",
        "        f\"Average word length: {avg_word_len:.2f}\\n\"\n",
        "        f\"Sample text (truncated): {text[:500]}...\\n\"\n",
        "    )\n",
        "    prompt = \"Based on the text features, provide a score (0-1) indicating if this text was AI-generated or manipulated and list specific text anomalies.\"\n",
        "    llm_score, llm_anomalies, model_scores = await aggregate_llm_outputs(prompt, feature_summary, modality=\"text\")\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=llm_score,\n",
        "        label=\"HUMAN\" if llm_score > 0.7 else \"AI-GENERATED\",\n",
        "        confidence=0.8,\n",
        "        method=\"text_analysis (LLM-based)\",\n",
        "        anomalies=llm_anomalies,\n",
        "        explanation=\"Text analysis using multiagent LLM insights to detect AI-generated content.\",\n",
        "        model_scores=model_scores\n",
        "    )"
      ],
      "metadata": {
        "id": "RHmIO_rRrCAl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###   MULTI-MODAL FUSION SECTION   ###\n",
        "#######################################\n",
        "def calculate_multimodal_score(results: Dict[str, DeepfakeAnalysisResult]) -> float:\n",
        "    \"\"\"Calculate an aggregate score from multiple modality results.\"\"\"\n",
        "    debug_print(\"Calculating multimodal score...\")\n",
        "    scores = []\n",
        "    weights = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            scores.append(result.score)\n",
        "            weights.append(result.confidence)\n",
        "    if not scores:\n",
        "        debug_print(\"No valid results found for multimodal scoring\")\n",
        "        return 0.5\n",
        "    total_weight = sum(weights)\n",
        "    if total_weight == 0:\n",
        "        weighted_avg = sum(scores) / len(scores)\n",
        "    else:\n",
        "        norm_weights = [w / total_weight for w in weights]\n",
        "        weighted_avg = sum(s * w for s, w in zip(scores, norm_weights))\n",
        "    debug_print(f\"Multimodal weighted score: {weighted_avg:.4f}\")\n",
        "    return weighted_avg\n",
        "\n",
        "def generate_verdict(score: float, results: Dict[str, DeepfakeAnalysisResult]) -> str:\n",
        "    \"\"\"Generate a final verdict based on the multimodal score and individual results.\"\"\"\n",
        "    if score > 0.8:\n",
        "        return \"AUTHENTIC - High confidence that this content is genuine\"\n",
        "    elif score > 0.6:\n",
        "        return \"LIKELY AUTHENTIC - Moderate confidence that this content is genuine\"\n",
        "    elif score > 0.4:\n",
        "        return \"INCONCLUSIVE - Analysis unable to determine authenticity\"\n",
        "    elif score > 0.2:\n",
        "        return \"LIKELY MANIPULATED - Moderate confidence that this content contains manipulations\"\n",
        "    else:\n",
        "        return \"MANIPULATED - High confidence that this content is manipulated or synthetic\"\n",
        "\n",
        "def collect_evidence(results: Dict[str, DeepfakeAnalysisResult]) -> List[Evidence]:\n",
        "    \"\"\"Collect evidence from all modality results.\"\"\"\n",
        "    evidence_list = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            for anomaly in result.anomalies:\n",
        "                evidence_list.append(\n",
        "                    Evidence(\n",
        "                        type=modality,\n",
        "                        description=anomaly,\n",
        "                        confidence=result.confidence,\n",
        "                        method=result.method\n",
        "                    )\n",
        "                )\n",
        "    return evidence_list\n",
        "\n",
        "def generate_recommendations(report: MultimodalAnalysisReport) -> List[str]:\n",
        "    \"\"\"Generate recommendations based on the analysis report.\"\"\"\n",
        "    recommendations = []\n",
        "    if report.multimodal_score < 0.4:\n",
        "        recommendations.append(\"This content should be treated as potentially manipulated and not be used as evidence without further forensic analysis.\")\n",
        "    if report.video_analysis and report.video_analysis.score < 0.5:\n",
        "        recommendations.append(\"Video content shows significant signs of manipulation and should be verified through alternative sources.\")\n",
        "    if report.audio_analysis and report.audio_analysis.score < 0.5:\n",
        "        recommendations.append(\"Audio content appears to be synthetic or manipulated. Verify the source and content through other means.\")\n",
        "    if report.image_analysis and report.image_analysis.score < 0.5:\n",
        "        recommendations.append(\"Image analysis indicates potential manipulation. Consider requesting the original, unprocessed image file.\")\n",
        "    if report.text_analysis and report.text_analysis.score < 0.5:\n",
        "        recommendations.append(\"Text content appears to be AI-generated. Verify authorship through additional means.\")\n",
        "    recommendations.append(\"For definitive analysis, consider submitting this content to a professional forensic lab specialized in digital media authentication.\")\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "s4nJFQ3FrFqY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###   MAIN PIPELINE ORCHESTRATION  ###\n",
        "#######################################\n",
        "class DeepfakePipeline:\n",
        "    \"\"\"End-to-end pipeline for deepfake detection across multiple modalities.\"\"\"\n",
        "    def __init__(self):\n",
        "        debug_print(\"Initializing DeepfakePipeline...\")\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        debug_print(f\"Using device: {self.device}\")\n",
        "\n",
        "    async def analyze_file(self, file_path: str) -> MultimodalAnalysisReport:\n",
        "        debug_print(f\"Starting analysis for file: {file_path}\")\n",
        "        start_time = datetime.now()\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        file_info = {\n",
        "            \"path\": file_path,\n",
        "            \"size\": os.path.getsize(file_path),\n",
        "            \"type\": \"unknown\",\n",
        "            \"extension\": file_ext\n",
        "        }\n",
        "        results = {\n",
        "            \"video_analysis\": None,\n",
        "            \"audio_analysis\": None,\n",
        "            \"image_analysis\": None,\n",
        "            \"text_analysis\": None\n",
        "        }\n",
        "        try:\n",
        "            if file_ext in ['.mp4', '.mov', '.avi', '.mkv', '.webm']:\n",
        "                file_info[\"type\"] = \"video\"\n",
        "                video_data = video_preprocessing(file_path)\n",
        "                tasks = []\n",
        "                tasks.append(advanced_video_analysis(video_data, self.device))\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    audio_data, sr = video_data[\"audio\"], 44100\n",
        "                    tasks.append(advanced_audio_analysis(audio_data, sr, self.device))\n",
        "                if video_data.get(\"frames\"):\n",
        "                    image = video_data[\"frames\"][0]\n",
        "                    tasks.append(advanced_image_analysis(image, self.device))\n",
        "                if video_data.get(\"text\"):\n",
        "                    tasks.append(analyze_text_content(video_data[\"text\"]))\n",
        "                completed_tasks = await asyncio.gather(*tasks)\n",
        "                results[\"video_analysis\"] = completed_tasks[0]\n",
        "                task_idx = 1\n",
        "                if video_data.get(\"audio\") is not None:\n",
        "                    results[\"audio_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "                if video_data.get(\"frames\"):\n",
        "                    results[\"image_analysis\"] = completed_tasks[task_idx]\n",
        "                    task_idx += 1\n",
        "                if video_data.get(\"text\"):\n",
        "                    results[\"text_analysis\"] = completed_tasks[task_idx]\n",
        "            elif file_ext in ['.wav', '.mp3', '.ogg', '.flac']:\n",
        "                file_info[\"type\"] = \"audio\"\n",
        "                audio_data, sr = audio_preprocessing(file_path)\n",
        "                results[\"audio_analysis\"] = await advanced_audio_analysis(audio_data, sr, self.device)\n",
        "                text_content = \"Audio transcript would be here in a real implementation.\"\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "            elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
        "                file_info[\"type\"] = \"image\"\n",
        "                image = image_preprocessing(file_path)\n",
        "                results[\"image_analysis\"] = await advanced_image_analysis(image, self.device)\n",
        "            elif file_ext in ['.txt', '.md', '.rtf', '.doc', '.docx', '.pdf']:\n",
        "                file_info[\"type\"] = \"text\"\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    text_content = f.read()\n",
        "                results[\"text_analysis\"] = await analyze_text_content(text_content)\n",
        "            else:\n",
        "                debug_print(f\"Unsupported file type: {file_ext}\")\n",
        "                raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
        "            multimodal_score = calculate_multimodal_score(results)\n",
        "            verdict = generate_verdict(multimodal_score, results)\n",
        "            evidence = collect_evidence(results)\n",
        "            processing_time = (datetime.now() - start_time).total_seconds()\n",
        "            confidence_matrix = {\n",
        "                modality: {\n",
        "                    \"score\": result.score if result else None,\n",
        "                    \"confidence\": result.confidence if result else None\n",
        "                }\n",
        "                for modality, result in results.items()\n",
        "            }\n",
        "            report = MultimodalAnalysisReport(\n",
        "                case_id=f\"case_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "                file_info=file_info,\n",
        "                video_analysis=results[\"video_analysis\"],\n",
        "                audio_analysis=results[\"audio_analysis\"],\n",
        "                image_analysis=results[\"image_analysis\"],\n",
        "                text_analysis=results[\"text_analysis\"],\n",
        "                multimodal_score=multimodal_score,\n",
        "                verdict=verdict,\n",
        "                evidence=evidence,\n",
        "                metadata={\n",
        "                    \"pipeline_version\": \"1.0.0\",\n",
        "                    \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "                    \"device\": str(self.device)\n",
        "                },\n",
        "                confidence_matrix=confidence_matrix,\n",
        "                processing_time=processing_time\n",
        "            )\n",
        "            report.recommendations = generate_recommendations(report)\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in analyze_file: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "Jxls6PLJrKb7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###  LANGGRAPH ORCHESTRATION GRAPH  ###\n",
        "#######################################\n",
        "class PipelineState(BaseModel):\n",
        "    \"\"\"State for the deepfake analysis pipeline graph.\"\"\"\n",
        "    file_path: str\n",
        "    report: Optional[MultimodalAnalysisReport] = None\n",
        "    current_step: str = \"initialize\"\n",
        "    error: Optional[str] = None\n",
        "    results: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "async def initialize_analysis(state: PipelineState) -> PipelineState:\n",
        "    \"\"\"Initialize the analysis process.\"\"\"\n",
        "    debug_print(f\"Initializing analysis for {state.file_path}\")\n",
        "    try:\n",
        "        state.current_step = \"processing\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "async def process_file(state: PipelineState) -> PipelineState:\n",
        "    \"\"\"Process the file using the DeepfakePipeline.\"\"\"\n",
        "    try:\n",
        "        pipeline = DeepfakePipeline()\n",
        "        state.report = await pipeline.analyze_file(state.file_path)\n",
        "        state.current_step = \"complete\"\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        state.error = str(e)\n",
        "        state.current_step = \"error\"\n",
        "        return state\n",
        "\n",
        "def create_pipeline_graph() -> StateGraph:\n",
        "    \"\"\"Create the LangGraph for the deepfake analysis pipeline.\"\"\"\n",
        "    workflow = StateGraph(PipelineState)\n",
        "    workflow.add_node(\"initialize\", initialize_analysis)\n",
        "    workflow.add_node(\"processing\", process_file)\n",
        "    workflow.add_edge(\"initialize\", \"processing\")\n",
        "    workflow.add_edge(\"processing\", END)\n",
        "    workflow.add_conditional_edges(\n",
        "        \"initialize\",\n",
        "        lambda state: \"error\" if state.error else \"processing\"\n",
        "    )\n",
        "    workflow.add_conditional_edges(\n",
        "        \"processing\",\n",
        "        lambda state: \"error\" if state.error else END\n",
        "    )\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "fv-KVsHarQdP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###    MAIN APPLICATION SECTION     ###\n",
        "#######################################\n",
        "async def main():\n",
        "    \"\"\"Main entry point for the deepfake analysis pipeline.\"\"\"\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description=\"Deepfake Detection Pipeline\")\n",
        "    parser.add_argument('file_path', help='Path to the file to analyze')\n",
        "    parser.add_argument('--debug', action='store_true', help='Enable debug output')\n",
        "    parser.add_argument('--output', help='Path to save the JSON report')\n",
        "    args = parser.parse_args()\n",
        "    global DEBUG\n",
        "    DEBUG = args.debug\n",
        "    debug_print(f\"Starting deepfake analysis for file: {args.file_path}\")\n",
        "    try:\n",
        "        workflow = create_pipeline_graph()\n",
        "        state = PipelineState(file_path=args.file_path)\n",
        "        final_state = await workflow.ainvoke(state)\n",
        "        if final_state.error:\n",
        "            print(f\"Error: {final_state.error}\")\n",
        "            return 1\n",
        "        report = final_state.report\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"DEEPFAKE ANALYSIS REPORT - {report.case_id}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"File: {os.path.basename(args.file_path)} ({report.file_info['type']})\")\n",
        "        print(f\"Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)\")\n",
        "        print(f\"Verdict: {report.verdict}\")\n",
        "        print(\"-\"*50)\n",
        "        for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "            result = getattr(report, f\"{modality}_analysis\")\n",
        "            if result:\n",
        "                print(f\"{modality.upper()} Analysis: {result.score:.2f} ({result.label})\")\n",
        "                if result.anomalies:\n",
        "                    print(f\"  Detected anomalies: {', '.join(result.anomalies)}\")\n",
        "        print(\"-\"*50)\n",
        "        print(\"RECOMMENDATIONS:\")\n",
        "        for i, rec in enumerate(report.recommendations, 1):\n",
        "            print(f\"{i}. {rec}\")\n",
        "        if args.output:\n",
        "            with open(args.output, 'w') as f:\n",
        "                f.write(report.json(indent=2))\n",
        "            print(f\"\\nDetailed report saved to {args.output}\")\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For non-interactive environments\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "    if IN_COLAB:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        # Run the interactive Colab interface\n",
        "        async def run_deepfake():\n",
        "            \"\"\"Run the deepfake detection pipeline in Google Colab with an interactive interface.\"\"\"\n",
        "            from google.colab import files\n",
        "            import ipywidgets as widgets\n",
        "            from IPython.display import display, HTML, clear_output\n",
        "            import json\n",
        "            import time\n",
        "            display(HTML(\"\"\"\n",
        "                <h2>Deepfake Detection Pipeline</h2>\n",
        "                <p>Upload media files to analyze for potential manipulations</p>\n",
        "            \"\"\"))\n",
        "            upload_button = widgets.Button(\n",
        "                description='Upload File',\n",
        "                disabled=False,\n",
        "                button_style='primary',\n",
        "                tooltip='Click to upload a file for analysis',\n",
        "                icon='upload'\n",
        "            )\n",
        "            debug_checkbox = widgets.Checkbox(\n",
        "                value=True,\n",
        "                description='Enable debug output',\n",
        "                disabled=False\n",
        "            )\n",
        "            output_area = widgets.Output()\n",
        "            progress_bar = widgets.IntProgress(\n",
        "                value=0,\n",
        "                min=0,\n",
        "                max=10,\n",
        "                description='Processing:',\n",
        "                bar_style='info',\n",
        "                orientation='horizontal'\n",
        "            )\n",
        "            display(widgets.HBox([upload_button, debug_checkbox]))\n",
        "            display(progress_bar)\n",
        "            display(output_area)\n",
        "            def on_upload_clicked(b):\n",
        "                with output_area:\n",
        "                    clear_output()\n",
        "                    print(\"Please select a file to upload...\")\n",
        "                    global DEBUG\n",
        "                    DEBUG = debug_checkbox.value\n",
        "                    try:\n",
        "                        uploaded = files.upload()\n",
        "                        if not uploaded:\n",
        "                            print(\"No file was uploaded.\")\n",
        "                            return\n",
        "                        file_path = list(uploaded.keys())[0]\n",
        "                        print(f\"Analyzing file: {file_path}\")\n",
        "                        progress_bar.value = 1\n",
        "                        temp_path = f\"/tmp/{file_path}\"\n",
        "                        with open(temp_path, \"wb\") as f:\n",
        "                            f.write(uploaded[file_path])\n",
        "                        progress_bar.value = 3\n",
        "                        workflow = create_pipeline_graph()\n",
        "                        state = PipelineState(file_path=temp_path)\n",
        "                        print(\"Starting analysis... This may take a few minutes.\")\n",
        "                        progress_bar.value = 5\n",
        "                        loop = asyncio.get_event_loop()\n",
        "                        final_state = loop.run_until_complete(workflow.ainvoke(state))\n",
        "                        progress_bar.value = 8\n",
        "                        if final_state.error:\n",
        "                            print(f\"Error: {final_state.error}\")\n",
        "                            return\n",
        "                        report = final_state.report\n",
        "                        print(\"\\n\" + \"=\"*50)\n",
        "                        print(f\"DEEPFAKE ANALYSIS REPORT\")\n",
        "                        print(\"=\"*50)\n",
        "                        print(f\"File: {file_path} ({report.file_info['type']})\")\n",
        "                        print(f\"Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)\")\n",
        "                        print(f\"Verdict: {report.verdict}\")\n",
        "                        print(\"-\"*50)\n",
        "                        for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                            result = getattr(report, f\"{modality}_analysis\")\n",
        "                            if result:\n",
        "                                print(f\"{modality.upper()} Analysis: {result.score:.2f} ({result.label})\")\n",
        "                                if result.anomalies:\n",
        "                                    print(f\"  Detected anomalies: {', '.join(result.anomalies)}\")\n",
        "                        print(\"-\"*50)\n",
        "                        print(\"RECOMMENDATIONS:\")\n",
        "                        for i, rec in enumerate(report.recommendations, 1):\n",
        "                            print(f\"{i}. {rec}\")\n",
        "                        report_json = report.json(indent=2)\n",
        "                        report_filename = f\"deepfake_report_{int(time.time())}.json\"\n",
        "                        with open(report_filename, 'w') as f:\n",
        "                            f.write(report_json)\n",
        "                        print(\"\\nDetailed JSON report available for download:\")\n",
        "                        files.download(report_filename)\n",
        "                        display(HTML(f\"\"\"\n",
        "                            <h3>Analysis Summary Visualization</h3>\n",
        "                            <p><strong>Overall Score:</strong> {report.multimodal_score:.2f}</p>\n",
        "                            <p><strong>Verdict:</strong> {report.verdict}</p>\n",
        "                        \"\"\"))\n",
        "                        for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                            result = getattr(report, f\"{modality}_analysis\")\n",
        "                            if result:\n",
        "                                score = result.score\n",
        "                                color = 'green' if score > 0.7 else 'red' if score < 0.3 else 'orange'\n",
        "                                display(HTML(f\"\"\"\n",
        "                                    <p><strong>{modality.capitalize()} Analysis Score:</strong> <span style=\"color:{color}\">{score:.2f}</span></p>\n",
        "                                \"\"\"))\n",
        "                        progress_bar.value = 10\n",
        "                    except Exception as e:\n",
        "                        import traceback\n",
        "                        print(f\"Error in processing: {e}\")\n",
        "                        traceback.print_exc()\n",
        "            upload_button.on_click(on_upload_clicked)\n",
        "            print(\" Ready! Click 'Upload File' to begin analysis.\")\n",
        "        run_deepfake()\n",
        "    else:\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc9fGfIIrVKP",
        "outputId": "e901d855-a6a2-4b12-cd27-32e37d9ed88f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-14-acb1facab3ed>:169: RuntimeWarning: coroutine 'run_deepfake' was never awaited\n",
            "  run_deepfake()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced UI for Deepfake Detection Pipeline\n",
        "# Integrates with your existing pipeline code with improved UI\n",
        "\n",
        "async def run_deepfake_enhanced_ui():\n",
        "    \"\"\"Run the deepfake detection pipeline in Google Colab with an advanced interactive interface.\"\"\"\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import json\n",
        "    import time\n",
        "    import asyncio\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    import numpy as np\n",
        "    import io\n",
        "    import base64\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create a custom color map for score visualization\n",
        "    colors = [(0.8, 0.2, 0.2), (1, 0.6, 0), (0.2, 0.8, 0.2)]  # Red -> Orange -> Green\n",
        "    cmap = LinearSegmentedColormap.from_list(\"deepfake_cmap\", colors, N=100)\n",
        "\n",
        "    # Create the main dashboard layout\n",
        "    dashboard_html = \"\"\"\n",
        "    <div style=\"padding: 20px; background-color: #f8f9fa; border-radius: 10px; margin-bottom: 20px;\">\n",
        "        <h1 style=\"text-align: center; color: #343a40;\">Deepfake Detection Pipeline</h1>\n",
        "        <p style=\"text-align: center; color: #495057; font-size: 16px;\">\n",
        "            Upload media files to analyze for potential manipulations across multiple modalities\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(dashboard_html))\n",
        "\n",
        "    # Create tabbed interface\n",
        "    tabs = widgets.Tab()\n",
        "    tab_analysis = widgets.VBox()\n",
        "    tab_results = widgets.VBox()\n",
        "    tab_logs = widgets.VBox()\n",
        "    tabs.children = [tab_analysis, tab_results, tab_logs]\n",
        "    tabs.set_title(0, \"Analysis\")\n",
        "    tabs.set_title(1, \"Results\")\n",
        "    tabs.set_title(2, \"Debug Logs\")\n",
        "\n",
        "    # Upload interface elements\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload File',\n",
        "        disabled=False,\n",
        "        button_style='primary',\n",
        "        tooltip='Click to upload a file for analysis',\n",
        "        icon='upload',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    debug_checkbox = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Enable debug output',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    file_info = widgets.HTML(\n",
        "        value=\"<p>No file selected</p>\",\n",
        "        layout=widgets.Layout(margin=\"10px 0px\")\n",
        "    )\n",
        "\n",
        "    # Processing status elements\n",
        "    status_label = widgets.HTML(\n",
        "        value=\"<h3>Status: Ready</h3>\",\n",
        "        layout=widgets.Layout(margin=\"10px 0px\")\n",
        "    )\n",
        "\n",
        "    progress_bar = widgets.IntProgress(\n",
        "        value=0,\n",
        "        min=0,\n",
        "        max=100,\n",
        "        description='Overall:',\n",
        "        bar_style='info',\n",
        "        style={'bar_color': '#007bff'},\n",
        "        orientation='horizontal',\n",
        "        layout=widgets.Layout(width='100%', margin=\"10px 0px\")\n",
        "    )\n",
        "\n",
        "    step_progress = widgets.HTML(\n",
        "        value=\"<p>Waiting for file upload...</p>\",\n",
        "        layout=widgets.Layout(margin=\"10px 0px\")\n",
        "    )\n",
        "\n",
        "    # Logs and output area\n",
        "    log_output = widgets.Output(\n",
        "        layout=widgets.Layout(\n",
        "            height='300px',\n",
        "            border='1px solid #ddd',\n",
        "            overflow_y='auto',\n",
        "            padding='10px',\n",
        "            margin=\"10px 0px\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    result_output = widgets.Output(\n",
        "        layout=widgets.Layout(\n",
        "            height='500px',\n",
        "            border='1px solid #ddd',\n",
        "            overflow_y='auto',\n",
        "            padding='10px',\n",
        "            margin=\"10px 0px\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add visualization area for results\n",
        "    visualization_area = widgets.Output(\n",
        "        layout=widgets.Layout(\n",
        "            height='auto',\n",
        "            padding='10px',\n",
        "            margin=\"20px 0px\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add report export options\n",
        "    export_options = widgets.HBox([\n",
        "        widgets.Button(\n",
        "            description='Download JSON',\n",
        "            icon='download',\n",
        "            button_style='success',\n",
        "            layout=widgets.Layout(width='150px')\n",
        "        ),\n",
        "        widgets.Button(\n",
        "            description='Download PDF',\n",
        "            icon='file-pdf',\n",
        "            button_style='info',\n",
        "            layout=widgets.Layout(width='150px')\n",
        "        ),\n",
        "        widgets.Button(\n",
        "            description='View Full Report',\n",
        "            icon='eye',\n",
        "            button_style='warning',\n",
        "            layout=widgets.Layout(width='150px')\n",
        "        )\n",
        "    ], layout=widgets.Layout(display='none'))\n",
        "\n",
        "    # Assemble tab contents\n",
        "    tab_analysis.children = [\n",
        "        widgets.HBox([upload_button, debug_checkbox]),\n",
        "        file_info,\n",
        "        status_label,\n",
        "        progress_bar,\n",
        "        step_progress,\n",
        "        log_output\n",
        "    ]\n",
        "\n",
        "    tab_results.children = [\n",
        "        visualization_area,\n",
        "        result_output,\n",
        "        export_options\n",
        "    ]\n",
        "\n",
        "    # Debug logs with custom styles\n",
        "    debug_controls = widgets.HBox([\n",
        "        widgets.Button(\n",
        "            description='Clear Logs',\n",
        "            icon='trash',\n",
        "            button_style='danger',\n",
        "            layout=widgets.Layout(width='120px')\n",
        "        ),\n",
        "        widgets.Checkbox(\n",
        "            value=True,\n",
        "            description='Auto-scroll',\n",
        "            disabled=False\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    debug_output = widgets.Output(\n",
        "        layout=widgets.Layout(\n",
        "            height='400px',\n",
        "            border='1px solid #ddd',\n",
        "            overflow_y='auto',\n",
        "            padding='10px',\n",
        "            font_family='monospace',\n",
        "            margin=\"10px 0px\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    tab_logs.children = [debug_controls, debug_output]\n",
        "\n",
        "    # Display the tabbed interface\n",
        "    display(tabs)\n",
        "\n",
        "    # Global variables to track progress\n",
        "    current_file_path = None\n",
        "    process_stages = {\n",
        "        \"preprocessing\": {\"started\": False, \"completed\": False, \"weight\": 10},\n",
        "        \"feature_extraction\": {\"started\": False, \"completed\": False, \"weight\": 20},\n",
        "        \"model_processing\": {\"started\": False, \"completed\": False, \"weight\": 40},\n",
        "        \"analysis\": {\"started\": False, \"completed\": False, \"weight\": 20},\n",
        "        \"report_generation\": {\"started\": False, \"completed\": False, \"weight\": 10}\n",
        "    }\n",
        "\n",
        "    # Create a function to update the progress\n",
        "    def update_progress(stage, status, message=None, substage=None):\n",
        "        \"\"\"Update the progress indicators for a specific stage.\"\"\"\n",
        "        if stage in process_stages:\n",
        "            if status == \"start\":\n",
        "                process_stages[stage][\"started\"] = True\n",
        "            elif status == \"complete\":\n",
        "                process_stages[stage][\"started\"] = True\n",
        "                process_stages[stage][\"completed\"] = True\n",
        "\n",
        "        # Calculate overall progress\n",
        "        total_weight = sum(stage_info[\"weight\"] for stage_info in process_stages.values())\n",
        "        completed_weight = sum(\n",
        "            stage_info[\"weight\"] for stage_info in process_stages.values()\n",
        "            if stage_info[\"completed\"]\n",
        "        )\n",
        "        in_progress_weight = sum(\n",
        "            stage_info[\"weight\"] * 0.5 for stage_info in process_stages.values()\n",
        "            if stage_info[\"started\"] and not stage_info[\"completed\"]\n",
        "        )\n",
        "\n",
        "        progress_value = int((completed_weight + in_progress_weight) / total_weight * 100)\n",
        "        progress_bar.value = progress_value\n",
        "\n",
        "        # Update status text\n",
        "        stage_display = stage.replace(\"_\", \" \").title()\n",
        "        if status == \"start\":\n",
        "            status_html = f\"<h3>Status: Processing - {stage_display}</h3>\"\n",
        "            status_label.value = status_html\n",
        "            substage_info = f\" - {substage}\" if substage else \"\"\n",
        "            step_progress.value = f\"<p><b>Current step:</b> {stage_display}{substage_info}</p>\"\n",
        "        elif status == \"complete\":\n",
        "            substage_info = f\" - {substage}\" if substage else \"\"\n",
        "            step_progress.value = f\"<p><b>Completed:</b> {stage_display}{substage_info}</p>\"\n",
        "        elif status == \"error\":\n",
        "            status_html = f\"<h3 style='color: #dc3545;'>Status: Error in {stage_display}</h3>\"\n",
        "            status_label.value = status_html\n",
        "            step_progress.value = f\"<p><b>Error:</b> {message}</p>\"\n",
        "\n",
        "        # Log the progress update\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        with debug_output:\n",
        "            print(f\"[{timestamp}] {stage_display}: {status.upper()} {message if message else ''}\")\n",
        "\n",
        "    # Create a custom debug print function that writes to the debug output\n",
        "    original_debug_print = debug_print\n",
        "\n",
        "    def ui_debug_print(msg: str):\n",
        "        \"\"\"Print debug messages to the debug output widget.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        with debug_output:\n",
        "            print(f\"[{timestamp}] [DEBUG] {msg}\")\n",
        "        original_debug_print(msg)\n",
        "\n",
        "    # Override debug print function\n",
        "    global debug_print\n",
        "    debug_print = ui_debug_print\n",
        "\n",
        "    # Create a function to visualize the results\n",
        "    def visualize_results(report):\n",
        "        \"\"\"Create visualizations for the analysis results.\"\"\"\n",
        "        with visualization_area:\n",
        "            clear_output()\n",
        "\n",
        "            # Create figure with subplots\n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "            # Add overall score at the top\n",
        "            ax_overall = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n",
        "            overall_score = report.multimodal_score\n",
        "            overall_color = cmap(overall_score)\n",
        "            ax_overall.barh(['Overall'], [overall_score], color=overall_color, height=0.5)\n",
        "            ax_overall.set_xlim(0, 1)\n",
        "            ax_overall.set_title(f'Overall Analysis: {report.verdict}', fontsize=14)\n",
        "            ax_overall.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "            # Add modality scores below\n",
        "            ax_modalities = plt.subplot2grid((3, 3), (1, 0), colspan=3)\n",
        "            modalities = []\n",
        "            scores = []\n",
        "            colors = []\n",
        "\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result:\n",
        "                    modalities.append(modality.capitalize())\n",
        "                    scores.append(result.score)\n",
        "                    colors.append(cmap(result.score))\n",
        "\n",
        "            ax_modalities.barh(modalities, scores, color=colors, height=0.5)\n",
        "            ax_modalities.set_xlim(0, 1)\n",
        "            ax_modalities.set_title('Analysis by Modality', fontsize=12)\n",
        "            ax_modalities.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "            for i, score in enumerate(scores):\n",
        "                ax_modalities.text(min(max(score + 0.05, 0.1), 0.95), i, f\"{score:.2f}\",\n",
        "                                 va='center', ha='center', fontweight='bold')\n",
        "\n",
        "            # Add anomalies summary\n",
        "            ax_anomalies = plt.subplot2grid((3, 3), (2, 0), colspan=3, rowspan=1)\n",
        "            ax_anomalies.axis('off')\n",
        "            anomaly_text = \"Key Anomalies Detected:\\n\"\n",
        "            anomaly_count = 0\n",
        "\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result and result.anomalies:\n",
        "                    anomaly_text += f\"\\n{modality.capitalize()}: {', '.join(result.anomalies[:3])}\"\n",
        "                    if len(result.anomalies) > 3:\n",
        "                        anomaly_text += f\" (+ {len(result.anomalies) - 3} more)\"\n",
        "                    anomaly_count += len(result.anomalies)\n",
        "\n",
        "            if anomaly_count == 0:\n",
        "                anomaly_text += \"\\nNo significant anomalies detected.\"\n",
        "\n",
        "            ax_anomalies.text(0.5, 0.5, anomaly_text,\n",
        "                             ha='center', va='center', fontsize=10,\n",
        "                             bbox=dict(boxstyle='round', facecolor='#f8f9fa', alpha=0.8))\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Function to generate and display the report\n",
        "    def display_report(report):\n",
        "        \"\"\"Generate and display a formatted report.\"\"\"\n",
        "        with result_output:\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"padding: 20px; background-color: #f8f9fa; border-radius: 10px;\">\n",
        "                <h1 style=\"text-align: center; color: #343a40;\">Deepfake Analysis Report</h1>\n",
        "                <h2 style=\"text-align: center; color: #495057;\">Case ID: {report.case_id}</h2>\n",
        "\n",
        "                <div style=\"margin: 20px 0; padding: 15px; background-color: {'#d4edda' if report.multimodal_score > 0.6 else '#f8d7da' if report.multimodal_score < 0.4 else '#fff3cd'}; border-radius: 5px;\">\n",
        "                    <h3 style=\"text-align: center; margin-bottom: 10px;\">VERDICT: {report.verdict}</h3>\n",
        "                    <p style=\"text-align: center; font-size: 18px;\">Overall Score: {report.multimodal_score:.2f} (0=Fake, 1=Real)</p>\n",
        "                </div>\n",
        "\n",
        "                <h3 style=\"margin-top: 20px;\">File Information</h3>\n",
        "                <table style=\"width: 100%; border-collapse: collapse;\">\n",
        "                    <tr>\n",
        "                        <th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\">Property</th>\n",
        "                        <th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\">Value</th>\n",
        "                    </tr>\n",
        "                    <tr>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">File Type</td>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{report.file_info['type'].upper()}</td>\n",
        "                    </tr>\n",
        "                    <tr>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">File Size</td>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{report.file_info['size'] / (1024*1024):.2f} MB</td>\n",
        "                    </tr>\n",
        "                    <tr>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">Extension</td>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{report.file_info['extension']}</td>\n",
        "                    </tr>\n",
        "                </table>\n",
        "\n",
        "                <h3 style=\"margin-top: 20px;\">Analysis Results</h3>\n",
        "            \"\"\"))\n",
        "\n",
        "            # Display modality results\n",
        "            for modality in [\"video\", \"audio\", \"image\", \"text\"]:\n",
        "                result = getattr(report, f\"{modality}_analysis\")\n",
        "                if result:\n",
        "                    score = result.score\n",
        "                    color = '#d4edda' if score > 0.7 else '#f8d7da' if score < 0.3 else '#fff3cd'\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <div style=\"margin: 10px 0; padding: 15px; background-color: {color}; border-radius: 5px;\">\n",
        "                        <h4 style=\"margin-top: 0;\">{modality.upper()} Analysis: {score:.2f} ({result.label})</h4>\n",
        "                        <p><strong>Confidence:</strong> {result.confidence:.2f}</p>\n",
        "                        <p><strong>Method:</strong> {result.method}</p>\n",
        "\n",
        "                        <h5>Detected Anomalies:</h5>\n",
        "                        <ul>\n",
        "                            {\"\".join(f\"<li>{anomaly}</li>\" for anomaly in result.anomalies) if result.anomalies else \"<li>No anomalies detected</li>\"}\n",
        "                        </ul>\n",
        "                    </div>\n",
        "                    \"\"\"))\n",
        "\n",
        "            # Display evidence summary\n",
        "            display(HTML(f\"\"\"\n",
        "                <h3 style=\"margin-top: 20px;\">Evidence Summary</h3>\n",
        "                <p>Total evidence items: {len(report.evidence)}</p>\n",
        "                <table style=\"width: 100%; border-collapse: collapse;\">\n",
        "                    <tr>\n",
        "                        <th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\">Type</th>\n",
        "                        <th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\">Description</th>\n",
        "                        <th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\">Confidence</th>\n",
        "                    </tr>\n",
        "                    {\"\".join(f'''\n",
        "                    <tr>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{evidence.type}</td>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{evidence.description}</td>\n",
        "                        <td style=\"padding: 8px; border-bottom: 1px solid #ddd;\">{evidence.confidence:.2f}</td>\n",
        "                    </tr>\n",
        "                    ''' for evidence in report.evidence[:10])}\n",
        "                </table>\n",
        "                {f\"<p>And {len(report.evidence) - 10} more evidence items...</p>\" if len(report.evidence) > 10 else \"\"}\n",
        "\n",
        "                <h3 style=\"margin-top: 20px;\">Recommendations</h3>\n",
        "                <ul>\n",
        "                    {\"\".join(f\"<li>{rec}</li>\" for rec in report.recommendations)}\n",
        "                </ul>\n",
        "\n",
        "                <div style=\"margin-top: 20px; text-align: center; font-size: 12px; color: #6c757d;\">\n",
        "                    <p>Processing time: {report.processing_time:.2f} seconds</p>\n",
        "                    <p>Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "    # Create PDF report generation function\n",
        "    def generate_pdf_report(report):\n",
        "        \"\"\"Generate a PDF report from the analysis results.\"\"\"\n",
        "        try:\n",
        "            # This is simplified - in a real implementation you'd use a PDF library\n",
        "            return f\"deepfake_report_{int(time.time())}.pdf\"\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error generating PDF: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Create function to download the report\n",
        "    def download_json_report(report, b):\n",
        "        \"\"\"Generate and download JSON report.\"\"\"\n",
        "        try:\n",
        "            report_json = report.json(indent=2)\n",
        "            report_filename = f\"deepfake_report_{int(time.time())}.json\"\n",
        "            with open(report_filename, 'w') as f:\n",
        "                f.write(report_json)\n",
        "            files.download(report_filename)\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error downloading JSON report: {e}\")\n",
        "\n",
        "    def download_pdf_report(report, b):\n",
        "        \"\"\"Generate and download PDF report.\"\"\"\n",
        "        try:\n",
        "            # This would be implemented with a proper PDF library\n",
        "            with log_output:\n",
        "                print(\"PDF export functionality is a placeholder - would be implemented with a PDF library\")\n",
        "        except Exception as e:\n",
        "            with log_output:\n",
        "                print(f\"Error downloading PDF report: {e}\")\n",
        "\n",
        "    def show_full_report(report, b):\n",
        "        \"\"\"Show the full detailed report in a modal.\"\"\"\n",
        "        with result_output:\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"border: 1px solid #ddd; padding: 20px; background-color: white; border-radius: 5px; margin-top: 20px;\">\n",
        "                <h2>Full Technical Report</h2>\n",
        "                <pre style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; max-height: 400px;\">\n",
        "{json.dumps(json.loads(report.json()), indent=2)}\n",
        "                </pre>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "    # Connect button handlers\n",
        "    def on_upload_clicked(b):\n",
        "        with log_output:\n",
        "            clear_output()\n",
        "            print(\"Please select a file to upload...\")\n",
        "\n",
        "        global DEBUG, current_file_path\n",
        "        DEBUG = debug_checkbox.value\n",
        "\n",
        "        # Reset progress tracking\n",
        "        for stage in process_stages:\n",
        "            process_stages[stage][\"started\"] = False\n",
        "            process_stages[stage][\"completed\"] = False\n",
        "\n",
        "        progress_bar.value = 0\n",
        "\n",
        "        try:\n",
        "            # Reset UI elements\n",
        "            status_label.value = \"<h3>Status: Uploading File</h3>\"\n",
        "            export_options.layout.display = 'none'\n",
        "\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                with log_output:\n",
        "                    print(\"No file was uploaded.\")\n",
        "                status_label.value = \"<h3>Status: Ready</h3>\"\n",
        "                step_progress.value = \"<p>Waiting for file upload...</p>\"\n",
        "                return\n",
        "\n",
        "            file_path = list(uploaded.keys())[0]\n",
        "            file_size_mb = uploaded[file_path].size / (1024*1024)\n",
        "            current_file_path = file_path\n",
        "\n",
        "            file_info.value = f\"\"\"\n",
        "            <div style=\"padding: 10px; background-color: #e9ecef; border-radius: 5px;\">\n",
        "                <p><strong>File:</strong> {file_path}</p>\n",
        "                <p><strong>Size:</strong> {file_size_mb:.2f} MB</p>\n",
        "                <p><strong>Type:</strong> {file_path.split('.')[-1].upper()}</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            with log_output:\n",
        "                print(f\"File uploaded: {file_path} ({file_size_mb:.2f} MB)\")\n",
        "                print(\"Preparing for analysis...\")\n",
        "\n",
        "            # Create temp file\n",
        "            temp_path = f\"/tmp/{file_path}\"\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                f.write(uploaded[file_path])\n",
        "\n",
        "            # Start analysis process\n",
        "            update_progress(\"preprocessing\", \"start\", f\"Initializing for {file_path}\")\n",
        "\n",
        "            # Create and run workflow\n",
        "            workflow = create_pipeline_graph()\n",
        "            state = PipelineState(file_path=temp_path)\n",
        "\n",
        "            # Display status\n",
        "            with log_output:\n",
        "                print(\"Starting analysis workflow...\")\n",
        "                print(\"This process may take several minutes depending on file size and complexity.\")\n",
        "\n",
        "            # Run analysis in background\n",
        "            asyncio.create_task(run_analysis(workflow, state))\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            with log_output:\n",
        "                print(f\"Error starting analysis: {e}\")\n",
        "                traceback.print_exc()\n",
        "            status_label.value = f\"<h3 style='color: #dc3545;'>Status: Error - {str(e)[:50]}...</h3>\"\n",
        "            step_progress.value = \"<p>Analysis failed</p>\"\n",
        "\n",
        "    # Create a function to handle the analysis process\n",
        "    async def run_analysis(workflow, state):\n",
        "        try:\n",
        "            # Preprocessing phase\n",
        "            await asyncio.sleep(1)  # Simulate some processing time\n",
        "            update_progress(\"preprocessing\", \"complete\")\n",
        "\n",
        "            # Feature extraction phase\n",
        "            update_progress(\"feature_extraction\", \"start\")\n",
        "\n",
        "            # Override pipeline methods to report progress\n",
        "            original_extract_video_features = extract_video_features\n",
        "            original_extract_audio_features = extract_audio_features\n",
        "            original_extract_image_features = extract_image_features\n",
        "\n",
        "            def wrapped_extract_video_features(frames):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Video Features\")\n",
        "                result = original_extract_video_features(frames)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Video Features\")\n",
        "                return result\n",
        "\n",
        "            def wrapped_extract_audio_features(audio_data, sr):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Audio Features\")\n",
        "                result = original_extract_audio_features(audio_data, sr)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Audio Features\")\n",
        "                return result\n",
        "\n",
        "            def wrapped_extract_image_features(image):\n",
        "                update_progress(\"feature_extraction\", \"start\", substage=\"Image Features\")\n",
        "                result = original_extract_image_features(image)\n",
        "                update_progress(\"feature_extraction\", \"complete\", substage=\"Image Features\")\n",
        "                return result\n",
        "\n",
        "            # Replace original functions with wrapped versions\n",
        "            global extract_video_features, extract_audio_features, extract_image_features\n",
        "            extract_video_features = wrapped_extract_video_features\n",
        "            extract_audio_features = wrapped_extract_audio_features\n",
        "            extract_image_features = wrapped_extract_image_features\n",
        "\n",
        "            # Processing phase - run the actual pipeline\n",
        "            update_progress(\"model_processing\", \"start\")\n",
        "            final_state = await workflow.ainvoke(state)\n",
        "\n",
        "            # Restore original functions\n",
        "            extract_video_features = original_extract_video_features\n",
        "            extract_audio_features = original_extract_audio_features\n",
        "            extract_image_features = original_extract_image_features\n",
        "\n",
        "            # Check for errors\n",
        "            if final_state.error:\n",
        "                update_progress(\"model_processing\", \"error\", final_state.error)\n",
        "                with log_output:\n",
        "                    print(f\"Error in analysis: {final_state.error}\")\n",
        "                status_label.value = f\"<h3 style='color: #dc3545;'>Status: Analysis Failed</h3>\"\n",
        "                return\n",
        "\n",
        "            update_progress(\"model_processing\", \"complete\")\n",
        "\n",
        "            # Analysis phase\n",
        "            update_progress(\"analysis\", \"start\")\n",
        "            await asyncio.sleep(1)  # Simulate processing time\n",
        "            update_progress(\"analysis\", \"complete\")\n",
        "\n",
        "            # Report generation phase\n",
        "            update_progress(\"report_generation\", \"start\")\n",
        "            report = final_state.report\n",
        "\n",
        "            # Switch to results tab to show the report\n",
        "            tabs.selected_index = 1\n",
        "\n",
        "            # Visualize results\n",
        "            visualize_results(report)\n",
        "\n",
        "            # Display detailed report\n",
        "            display_report(report)\n",
        "\n",
        "            # Make export options visible\n",
        "            export_options.layout.display = 'flex'\n",
        "\n",
        "            # Connect export buttons to the report\n",
        "            export_options.children[0].on_click(lambda b: download_json_report(report, b))\n",
        "            export_options.children[1].on_click(lambda b: download_pdf_report(report, b))\n",
        "            export_options.children[2].on_click(lambda b: show_full_report(report, b))\n",
        "\n",
        "            update_progress(\"report_generation\", \"complete\")\n",
        "\n",
        "            # Update final status\n",
        "            status_label.value = \"<h3 style='color: #28a745;'>Status: Analysis Complete</h3>\"\n",
        "            step_progress.value = f\"<p>Analysis completed. See Results tab for details.</p>\"\n",
        "\n",
        "            with log_output:\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(\"Analysis completed successfully!\")\n",
        "                print(f\"Overall score: {report.multimodal_score:.2f} - Verdict: {report.verdict}\")\n",
        "                print(\"=\"*50)\n",
        "                print(\"View the Results tab for detailed report and visualizations.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            with log_output:\n",
        "                print(f\"Error in analysis process: {e}\")\n",
        "                traceback.print_exc()\n",
        "            status_label.value = f\"<h3 style='color: #dc3545;'>Status: Error</h3>\"\n",
        "            step_progress.value = f\"<p>Analysis failed: {str(e)[:100]}</p>\"\n",
        "\n",
        "    # Connect clear logs button\n",
        "    tab_logs.children[0].children[0].on_click(lambda b: debug_output.clear_output())\n",
        "\n",
        "    # Connect upload button\n",
        "    upload_button.on_click(on_upload_clicked)\n",
        "\n",
        "    # Print initial message\n",
        "    with log_output:\n",
        "        print(\" Deepfake Detection Pipeline initialized\")\n",
        "        print(\"Click 'Upload File' to begin analysis\")\n",
        "\n",
        "    with debug_output:\n",
        "        print(\"[INFO] Debug logs will appear here\")\n",
        "        print(\"[INFO] Pipeline ready for file upload\")\n",
        "\n",
        "# Execute the UI\n",
        "run_deepfake_enhanced_ui()"
      ],
      "metadata": {
        "id": "HIi4g7QuAI0T",
        "outputId": "a11f911e-33d8-46af-9195-577654e90da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "name 'debug_print' is used prior to global declaration (<ipython-input-15-1f4583f1cbff>, line 251)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1f4583f1cbff>\"\u001b[0;36m, line \u001b[0;32m251\u001b[0m\n\u001b[0;31m    global debug_print\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'debug_print' is used prior to global declaration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2fH5hEvAJmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}