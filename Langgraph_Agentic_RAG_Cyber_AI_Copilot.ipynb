{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "cc0c5950-af36-41af-f88a-f61478ecc078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 6.56MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 12.9MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 5.21MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 7.89MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.12MB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 7.71MB/s]\n",
            "2024-12-02 05:09:29.907143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-02 05:09:29.933886: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-02 05:09:29.941406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-02 05:09:29.960677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-02 05:09:31.574213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:06<00:00, 75.4MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "\u001b[0mDownloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 10.6s\u001b[0K\u001b[1G161.3 MiB [] 0% 29.4s\u001b[0K\u001b[1G161.3 MiB [] 0% 18.5s\u001b[0K\u001b[1G161.3 MiB [] 0% 10.9s\u001b[0K\u001b[1G161.3 MiB [] 1% 5.2s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 3% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 5% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 5% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 6% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 7% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 8% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 9% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 10% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 11% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 12% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 13% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 14% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 15% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 17% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 18% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 19% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 20% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 22% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 23% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 24% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 25% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 26% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 27% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 30% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 32% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 35% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 38% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 40% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 41% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 44% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 45% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 46% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 47% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 49% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 50% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 52% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 55% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 57% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 58% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 61% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 62% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 67% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 73% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 76% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 24.7s\u001b[0K\u001b[1G100.9 MiB [] 0% 20.6s\u001b[0K\u001b[1G100.9 MiB [] 0% 9.6s\u001b[0K\u001b[1G100.9 MiB [] 1% 5.7s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.7s\u001b[0K\u001b[1G100.9 MiB [] 3% 2.8s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 6% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 8% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 11% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 12% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 14% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 15% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 20% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 21% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 23% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 28% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 32% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 33% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 35% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 36% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 37% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 39% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 43% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 45% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 47% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 50% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 52% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 57% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 64% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 66% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 68% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 83% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 87% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 89% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 24.8s\u001b[0K\u001b[1G87.6 MiB [] 0% 12.7s\u001b[0K\u001b[1G87.6 MiB [] 0% 8.3s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.4s\u001b[0K\u001b[1G87.6 MiB [] 2% 4.1s\u001b[0K\u001b[1G87.6 MiB [] 3% 3.3s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.9s\u001b[0K\u001b[1G87.6 MiB [] 5% 2.7s\u001b[0K\u001b[1G87.6 MiB [] 5% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 6% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 7% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 9% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 10% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 12% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 13% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 15% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 17% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 18% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 21% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 22% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 24% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 27% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 28% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 29% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 31% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 32% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 33% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 35% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 36% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 38% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 42% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 43% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 44% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 46% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 47% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 49% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 50% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 51% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 53% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 53% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 55% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 57% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 59% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 61% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 63% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 65% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 67% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 69% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 70% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 73% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 75% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 79% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 80% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 81% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 85% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 86% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 90% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 92% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 6.3s\u001b[0K\u001b[1G95.5 MiB [] 0% 35.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 18.4s\u001b[0K\u001b[1G95.5 MiB [] 0% 11.5s\u001b[0K\u001b[1G95.5 MiB [] 1% 6.4s\u001b[0K\u001b[1G95.5 MiB [] 2% 3.4s\u001b[0K\u001b[1G95.5 MiB [] 3% 2.6s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.2s\u001b[0K\u001b[1G95.5 MiB [] 6% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 8% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 9% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 13% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 15% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 20% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 30% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 31% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 34% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 36% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 38% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 40% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 42% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 44% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 46% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 50% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 52% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 56% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 58% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 61% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 65% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 67% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 69% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 71% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 73% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 76% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 78% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 80% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 82% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 86% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 91% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 12% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 37% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 84% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "TAVILY_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d645bded-3ef7-41be-81b1-d4dd0ecd2e99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    # Query expansion with related terms and synonyms\n",
        "    related_terms = get_related_terms(query)\n",
        "    if related_terms:\n",
        "        enhanced_query += f\" related_terms:{', '.join(related_terms)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def get_related_terms(query: str) -> List[str]:\n",
        "    # Use an ontology or knowledge graph to identify related concepts and terms\n",
        "    related_terms = {\n",
        "        \"cyber attack\": [\"hacking\", \"data breach\", \"malware\", \"ransomware\"],\n",
        "        \"threat actor\": [\"cyber gang\", \"hacker group\", \"APT\"],\n",
        "        \"vulnerability\": [\"exploit\", \"CVE\", \"security flaw\"],\n",
        "        \"phishing\": [\"spear phishing\", \"email scam\", \"social engineering\"],\n",
        "        # Add more related terms as needed\n",
        "    }\n",
        "\n",
        "    # Find related terms for the query\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = []\n",
        "    for term in query_terms:\n",
        "        if term in related_terms:\n",
        "            found_terms.extend(related_terms[term])\n",
        "\n",
        "    return found_terms\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            logging.error(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "       - **Actionable Recommendations**:\n",
        "         - Offer precise, immediate actions and mitigation strategies.\n",
        "         - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "         - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    5. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor capabilities, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "    - **Malware**\n",
        "    - **Ransomware**\n",
        "    - **Vulnerability**\n",
        "    - **Exploit**\n",
        "    - **Phishing**\n",
        "    - **Data Breach**\n",
        "    - **Cyber Attack**\n",
        "    - **Incident Response**\n",
        "    - **MITRE ATT&CK**\n",
        "    - **Indicators of Compromise (IOCs)**\n",
        "    - **Command and Control (C2)**\n",
        "    - **Dates**\n",
        "    - **Times**\n",
        "    - **Trojans**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific', 'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing', 'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK', 'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates', 'Times', 'Trojans'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Highlight important information\n",
        "    important_keywords = [\n",
        "        'Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific',\n",
        "        'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing',\n",
        "        'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK',\n",
        "        'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates',\n",
        "        'Times', 'Trojans'\n",
        "    ]\n",
        "    highlighted_response = highlight_keywords(processed_response, important_keywords)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": highlighted_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {highlighted_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "409af5d0-fea7-4e38-b6eb-58b54b97ea80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor? 2024 OR 2024 recent threat actor groups gangs companies locations\n",
            "ERROR in Google Programmable Image Search: IncompleteRead(5 bytes read)ERROR in Google Programmable Search: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2578)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-afc98cd362de>:40: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = search_and_contents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw results from Exa Search: Title: Our response to a recent security incident\n",
            "URL: https://www.goto.com/blog/our-response-to-a-recent-security-incident\n",
            "ID: https://www.goto.com/blog/our-response-to-a-recent-security-incident\n",
            "Score: 0.16677381098270416\n",
            "Published Date: 2023-04-20T00:00:00.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Update as of Thursday, April 20, 2023  \n",
            "To All GoTo Customers,\n",
            "We recently concluded our investigation into the security incident first shared with customers in November of 2022 regarding unauthorized activity in a third-party cloud storage environment. We eliminated the threat actor’s access to the environment and found no evidence of additional compromise or threat actor activity beyond what we previously disclosed as impacted in January. In addition, we concluded that GoTo Resolve, GoTo Connect, GoTo Meeting, GoTo Webinar, GoTo Contact Center, GoTo Assist, GoTo Training, and Grasshopper had no impact at all.\n",
            "We are constantly enhancing our security measures and monitoring capabilities to protect our customers, including:\n",
            "Accelerated the migration of customer accounts onto our enhanced Identity Management Platform.\n",
            "Full analysis of existing controls and configurations, and necessary changes to further harden existing environments.\n",
            "Comprehensive reviews of and, where appropriate, enhancements to our encryption practices within our applications and backup infrastructure.\n",
            "Thank you for your continued patience and understanding while we completed the investigation. We take our commitment to protect our customers very seriously and will continue to undertake efforts to ensure our services and infrastructure remain secure and are designed to detect and prevent future threats.\n",
            "Paddy Srinivasan\n",
            "CEO, GoTo\n",
            "  Update as of Monday, January 23, 2023  \n",
            "To All GoTo Customers,\n",
            "I am writing to update you on our ongoing investigation about the security incident we told you about in November 2022. \n",
            "Our investigation to date has determined that a threat actor exfiltrated encrypted backups from a third-party cloud storage service related to the following products: Central, Pro, join.me, Hamachi, and RemotelyAnywhere. We also have evidence that a threat actor exfiltrated an encryption key for a portion of the encrypted backups. The affected information, which varies by product, may include account usernames, salted and hashed passwords, a portion of Multi-Factor Authentication (MFA) settings, as well as some product settings and licensing information. In addition, while Rescue and GoToMyPC encrypted databases were not exfiltrated, MFA settings of a small subset of their customers were impacted. \n",
            "At this time, we have no evidence of exfiltration affecting any other GoTo products other than those referenced above or any of GoTo’s production systems.\n",
            " We are contacting affected customers directly to provide additional information and recommend actionable steps for them to take to further secure their accounts. Even though all account passwords were salted and hashed in accordance with best practices, out of an abundance of caution, we will also reset the passwords of affected users and/or reauthorize MFA settings where applicable. In addition, we are migrating their accounts onto an enhanced Identity Management Platform, which will provide additional security with more robust authentication and login-based security options. \n",
            " As a reminder, GoTo does not store full credit card or bank details. In addition, GoTo does not collect or use end user personal information, such as date of birth, home address, or Social Security numbers. \n",
            "We appreciate your understanding while we continue to work expeditiously to complete our investigation.\n",
            "Paddy Srinivasan\n",
            "CEO, GoTo\n",
            "   .   \n",
            "        Original Post from November 30, 2022  \n",
            "To All GoTo Customers,\n",
            "I am writing to inform you that GoTo is investigating a security incident. While we are currently working to better understand the scope of the issue, we wanted to let you know about the situation and how we are responding.\n",
            "Upon learning of the incident, we immediately launched an investigation, engaged Mandiant, a leading security firm, and alerted law enforcement. Based on the investigation to date, we have detected unusual activity within our development environment and third-party cloud storage service. The third-party cloud storage service is currently shared by both GoTo and  its affiliate, LastPass .\n",
            "GoTo’s products and services remain fully functional, and we are committed to our customers. As part of our efforts, we also continue to deploy enhanced security measures and monitoring capabilities across our infrastructure to help detect and prevent threat actor activity.\n",
            "Thank you for your patience as we work expeditiously to complete our investigation. We will continue to update you.\n",
            "Paddy Srinivasan\n",
            "CEO\n",
            "Highlights: ['While we are currently working to better understand the scope of the issue, we wanted to let you know about the situation and how we are responding. Upon learning of the incident, we immediately launched an investigation, engaged Mandiant, a leading security firm, and alerted law enforcement. Based on the investigation to date, we have detected unusual activity within our development environment and third-party cloud storage service. The third-party cloud storage service is currently shared by both GoTo and  its affiliate, LastPass . GoTo’s products and services remain fully functional, and we are committed to our customers.']\n",
            "Highlight Scores: [0.5169363021850586]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Security Incident December 2022 Update - LastPass - The LastPass Blog\n",
            "URL: https://blog.lastpass.com/posts/2022/12/notice-of-recent-security-incident\n",
            "ID: https://blog.lastpass.com/posts/2022/12/notice-of-recent-security-incident\n",
            "Score: 0.16499251127243042\n",
            "Published Date: 2022-08-25T00:00:00.000Z\n",
            "Author: Karim Toubba\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Please refer to the latest article for updated information.  \n",
            "    \n",
            "   \n",
            "   \n",
            "   \n",
            " \n",
            "   \n",
            "   \n",
            " \n",
            " \n",
            "   \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "   \n",
            " Update as of Wednesday, November 30, 2022\n",
            "Highlights: ['Please refer to the latest article for updated information. Update as of Wednesday, November 30, 2022']\n",
            "Highlight Scores: [0.22966259717941284]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Luna Moth Gang Invests in Call Centers to Target Businesses with Callback Phishing Campaigns\n",
            "URL: https://thehackernews.com/2022/11/luna-moth-gang-invests-in-call-centers.html\n",
            "ID: https://thehackernews.com/2022/11/luna-moth-gang-invests-in-call-centers.html\n",
            "Score: 0.16087521612644196\n",
            "Published Date: 2022-11-22T12:05:40.000Z\n",
            "Author: Nov 22, 2022Ravie Lakshmanan\n",
            "Image: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh09Ia1eCE0X1Yh1yk9kgsTxSKKbD8Z9rItd8pM6oeG41V__6xPr1UQJl66wkSEf6JhEBHCL-RtVww0V1zo-5wPCzJujQs2UrWhJ2x82hLZLUHsCBkvqFTD87BpAGIhcP7U22WHLs1hJhvSuNN8RnGjW1SrJNrQkKrZYmYpxTXVn2nYgsCEV9yb_kfU/s728-rw-e365/callback.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The Luna Moth campaign has extorted hundreds of thousands of dollars from several victims in the legal and retail sectors.\n",
            "The attacks are notable for employing a technique called callback phishing or telephone-oriented attack delivery (TOAD), wherein the victims are social engineered into making a phone call through phishing emails containing invoices and subscription-themed lures.\n",
            "Palo Alto Networks Unit 42 said the attacks are the \"product of a single highly organized campaign,\" adding, \"this threat actor has significantly invested in call centers and infrastructure that's unique to each victim.\"\n",
            "The cybersecurity firm described the activity as a \"pervasive multi-month campaign that is actively evolving.\"\n",
            " \n",
            "What's notable about callback phishing is that the email messages are completely devoid of any malicious attachment or booby-trapped link, allowing them to evade detection and slip past email protection solutions.\n",
            "These messages typically come with an invoice that includes a phone number that the users can call to cancel the supposed subscription. In reality, however, the victims are routed to an actor-controlled call center and connected to a live agent on the other end, who ends up installing a remote access tool for persistence.\n",
            "\"The attacker will then seek to identify valuable information on the victim's computer and connected file shares, and they will quietly exfiltrate it to a server they control using a file transfer tool,\" Unit 42 researcher Kristopher Russo said.\n",
            "   \n",
            "The campaign may be resource intensive, but is also technically less sophisticated and likely to have a much higher success rate than other phishing attacks.\n",
            "On top of that, it enables extortion without encryption, permitting malicious actors to plunder sensitive data sans the need to deploy ransomware to lock the files after exfiltration.\n",
            "The Luna Moth actor, also known as Silent Ransom, has become an expert of sorts when it comes to pulling off such schemes. According to AdvIntel, the cybercrime group is believed to be the mastermind behind the BazarCall attacks last year.\n",
            " \n",
            "To give these attacks a veneer of legitimacy, the adversaries, instead of dropping a malware like BazarLoader, take advantage of legitimate tools like Zoho Assist to remotely interact with a victim's computer, abusing the access to deploy other trusted software such as Rclone or WinSCP for harvesting data.\n",
            "Extortion demands range from two to 78 Bitcoin based on the organization targeted, with the threat actor creating unique cryptocurrency wallets for each payment. The adversary is also said to offer discounts of nearly 25% for prompt payment, although there's no guarantee that the data is deleted.\n",
            "\"The threat actors behind this campaign have taken great pains to avoid all non-essential tools and malware, to minimize the potential for detection,\" Russo said. \"Since there are very few early indicators that a victim is under attack, employee cybersecurity awareness training is the first line of defense.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: [\"The Luna Moth campaign has extorted hundreds of thousands of dollars from several victims in the legal and retail sectors. The attacks are notable for employing a technique called callback phishing or telephone-oriented attack delivery (TOAD), wherein the victims are social engineered into making a phone call through phishing emails containing invoices and subscription-themed lures. What's notable about callback phishing is that the email messages are completely devoid of any malicious attachment or booby-trapped link, allowing them to evade detection and slip past email protection solutions. These messages typically come with an invoice that includes a phone number that the users can call to cancel the supposed subscription. In reality, however, the victims are routed to an actor-controlled call center and connected to a live agent on the other end, who ends up installing a remote access tool for persistence.\"]\n",
            "Highlight Scores: [0.457763671875]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: 12-2-2022 Security Incident\n",
            "URL: https://support.8x8.com/support-services/support/12-2-2022_Security_Incident\n",
            "ID: https://support.8x8.com/support-services/support/12-2-2022_Security_Incident\n",
            "Score: 0.15967534482479095\n",
            "Published Date: 2023-04-13T16:42:28.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Last updated\n",
            " \n",
            "Save as PDF\n",
            " \n",
            " Loading...   \n",
            " Issue\n",
            "On December 2, 2022 we received a report of an internet post that was published mentioning 8x8. We take any potential compromise seriously and have initiated our Incident Response plans. The teams are actively working through the processes.\n",
            "  Status\n",
            "12-2-2022 10:44 AM Pacific : As we investigate this report, we determined no 8x8 UCaaS or CCaaS production systems, or customer content, was impacted. \n",
            "12-5-2022 5:24 PM Pacific: We are actively investigating with internal and third party forensic teams. We believe several IT servers may have been accessed by an unauthorized third-party. We do not believe any customer content was compromised. The teams are actively working through the processes.\n",
            "1-5-2023 7:00 AM Pacific: Our investigation with internal and third party forensic teams concluded that there was no live attack on 12/2/2022, but that the post was instead related to a September 2022 incident. \n",
            "In September 2022, we identified and mitigated an attempted ransomware attack in real-time, and removed the attacker from an 8x8 internal IT network segment. Our investigation identified a limited number of IT servers at issue, none of which involved our 8x8 UCaas and CCaaS production systems or customer content (customer information that is transmitted through our SaaS service such as voicemails, phone numbers, call records, etc.). We provided notice of the incident to relevant data protection authorities and believed that the incident had been contained.\n",
            "Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps.\n",
            " ||||I||||Skip to main content\n",
            "Visit the 8x8 Community!\n",
            "1. Hello, Anonymous User!\n",
            "1. My contributions\n",
            "2. My preferences\n",
            "3. My subscriptions\n",
            "4. Sign out\n",
            "Need Help? Get Support\n",
            "1. Search site\n",
            "Search Search\n",
            "Go back to previous article\n",
            "2.\n",
            "1. Sign in\n",
            "Expand/collapse global hierarchy\n",
            "1. Home\n",
            "2. Support & Services\n",
            "3. Technical Support\n",
            "4. 12-2-2022 Security Incident\n",
            "Expand/collapse global location\n",
            "12-2-2022 Security Incident\n",
            "1. Last updated\n",
            "2. Save as PDF\n",
            "3. Share\n",
            "1. Share\n",
            "2. Tweet\n",
            "3. Share\n",
            "Loading...\n",
            "Check out our upcoming webinars and get the most out of your 8x8 services!For faster support help, start a chat!Looking for user guides and product manuals? Click here!\n",
            "PrevNext\n",
            "Table of Contents\n",
            "1. Issue\n",
            "2. Status\n",
            "1. Issue\n",
            "2. Status\n",
            "Issue\n",
            "On December 2, 2022 we received a report of an internet post that was published mentioning 8x8. We take any potential compromise seriously and have initiated our Incident Response plans. The teams are actively working through the processes.\n",
            "Status\n",
            "12-2-2022 10:44 AM Pacific : As we investigate this report, we determined no 8x8 UCaaS or CCaaS production systems, or customer content, was impacted.\n",
            "12-5-2022 5:24 PM Pacific: We are actively investigating with internal and third party forensic teams. We believe several IT servers may have been accessed by an unauthorized third-party. We do not believe any customer content was compromised. The teams are actively working through the processes.\n",
            "1-5-2023 7:00 AM Pacific: Our investigation with internal and third party forensic teams concluded that there was no live attack on 12/2/2022, but that the post was instead related to a September 2022 incident.\n",
            "In September 2022, we identified and mitigated an attempted ransomware attack in real-time, and removed the attacker from an 8x8 internal IT network segment. Our investigation identified a limited number of IT servers at issue, none of which involved our 8x8 UCaas and CCaaS production systems or customer content (customer information that is transmitted through our SaaS service such as voicemails, phone numbers, call records, etc.). We provided notice of the incident to relevant data protection authorities and believed that the incident had been contained.\n",
            "Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps.\n",
            "1. Back to top\n",
            "2.\n",
            "+ Technical Support\n",
            "+ 8x8 Implementation Packages Overview\n",
            "* Was this article helpful?\n",
            "* Yes\n",
            "* No\n",
            "*\n",
            "Recommended articles\n",
            "1. Article type How-to Confidence Validated Flagged Not Flagged Governance Experience KCS Enabled Yes Visibility Public\n",
            "2. Tags\n",
            "1. dark web\n",
            "2. lockbit\n",
            "3. Ransomware\n",
            "1. © Copyright 2023 8x8 Support\n",
            "2. Powered by CXone Expert ®\n",
            "User Quick Links\n",
            "Log in to your 8x8 Application Panel\n",
            "Log in to 8x8 Work for Web\n",
            "Log in to ContactNow Support\n",
            "See Current System Status\n",
            "Co-Browse Help | sgs-ocx-css-team\n",
            "2023 8x8, Inc. All rights reserved.\n",
            "Privacy Policy | Terms & Conditions\n",
            "Need Help? Get Support\n",
            "Highlights: ['Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps. Check out our upcoming webinars and get the most out of your 8x8 services!For faster support help, start a chat!Looking for user guides and product manuals? Click here! On December 2, 2022 we received a report of an internet post that was published mentioning 8x8.']\n",
            "Highlight Scores: [0.4585976302623749]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Hackers use new, fake crypto app to breach networks, steal cryptocurrency\n",
            "URL: https://www.bleepingcomputer.com/news/security/hackers-use-new-fake-crypto-app-to-breach-networks-steal-cryptocurrency/\n",
            "ID: https://www.bleepingcomputer.com/news/security/hackers-use-new-fake-crypto-app-to-breach-networks-steal-cryptocurrency/\n",
            "Score: 0.15747074782848358\n",
            "Published Date: 2022-12-03T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The North Korean 'Lazarus' hacking group is linked to a new attack spreading fake cryptocurrency apps under the made-up brand, \"BloxHolder,\" to install the AppleJeus malware for initial access to networks and steal crypto assets.\n",
            "According to a joint FBI and CISA report from February 2021, AppleJeus has been in circulation since at least 2018, used by Lazarus in cryptocurrency hijacking and digital asset theft operations.\n",
            "A new report by Volexity has identified new, fake crypto programs and AppleJeus activity, with signs of evolution in the malware's infection chain and abilities.\n",
            "New BloxHolder campaign\n",
            "The new campaign attributed to Lazarus started in June 2022 and was active until at least October 2022.\n",
            "In this campaign, the threat actors used the \"bloxholder[.]com\" domain, a clone of the HaasOnline automated cryptocurrency trading platform.\n",
            "  Legitimate (left) and clone website (right) (Volexity)   \n",
            "This website distributed a 12.7MB Windows MSI installer that pretended to be the BloxHolder app. However, in reality, it was the AppleJeus malware bundled with the QTBitcoinTrader app.\n",
            "In October 2022, the hacking group evolved their campaign to use Microsoft Office documents instead of the MSI installer to distribute the malware.\n",
            "The 214KB document was named 'OKX Binance &amp; Huobi VIP fee comparision.xls' and contained a macro that creates three files on a target's computer.\n",
            "Volexity couldn't retrieve the final payload from this later infection chain, but they noticed similarities in the DLL sideloading mechanism found in the previously used MSI installer attacks, so they're confident it's the same campaign.\n",
            "Upon installation through the MSI infection chain, AppleJeus will create a scheduled task and drop additional files in the folder \"%APPDATA%\\Roaming\\Bloxholder\\\".\n",
            "Next, the malware will collect the MAC address, computer name, and OS version and send it to the C2 via a POST request, likely to identify if it's running on a virtual machine or sandbox.\n",
            "One novel element in recent campaigns is chained DLL sideloading to load the malware from within a trusted process, evading AV detection.\n",
            "\"Specifically, \"CameraSettingsUIHost.exe\" loads the \"dui70.dll\" file from the \"System32\" directory, which then causes the loading of the malicious \"DUser.dll\" file from the application's directory into the \"CameraSettingsUIHost.exe\" process,\" explains Volexity.\n",
            "\"The \"dui70.dll\" file is the \"Windows DirectUI Engine\" and is normally installed as part of the operating system.\"\n",
            "  Chained DLL sideloading (Volexity)   \n",
            "Volexity says the reason Lazarus opted for chained DLL sideloading is unclear but might be to impede malware analysis.\n",
            "Another new characteristic in recent AppleJeus samples is that all its strings and API calls are now obfuscated using a custom algorithm, making them stealthier against security products.\n",
            "Although Lazarus' focus on cryptocurrency assets is well documented, the North Korean hackers remain fixed on their goal to steal digital money, constantly refreshing themes and improving tools to stay as stealthy as possible.\n",
            "Who is the Lazarus Group\n",
            "The Lazarus Group (also tracked as ZINC) is a North Korean hacking group that has been active since at least 2009.\n",
            "The group gained notoriety after hacking Sony Films in Operation Blockbuster and the 2017 global WannaCry ransomware campaign that encrypted businesses worldwide.\n",
            "Google discovered in January 2021 that Lazarus was creating fake online personas to target security researchers in social engineering attacks that installed backdoors on their devices. A second attack using this tactic was discovered in March 2021.\n",
            "The U.S. government sanctioned the Lazarus hacking group in September 2019 and now offers a reward of up to $5 million for information that can disrupt their activities.\n",
            "More recent attacks have turned to the spreading of trojanized cryptocurrency wallets and trading apps that steal people's private keys and drain their crypto assets.\n",
            "In April, the U.S. government linked the Lazarus group to a cyberattack on Axie Infinity that allowed them to steal over $617 million worth of Ethereum and USDC tokens.\n",
            "It was later revealed that the Axie Infinity hack was made possible due to a phishing attack containing a malicious PDF file pretending to be a job offer sent to one of the company's engineers.\n",
            "Highlights: [\"In April, the U.S. government linked the Lazarus group to a cyberattack on Axie Infinity that allowed them to steal over $617 million worth of Ethereum and USDC tokens. It was later revealed that the Axie Infinity hack was made possible due to a phishing attack containing a malicious PDF file pretending to be a job offer sent to one of the company's engineers.\"]\n",
            "Highlight Scores: [0.4428800642490387]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is a link to the latest cyber incident involving the LunarsGo threat actor in 2024:\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[INIT].... → Crawl4AI 0.3.746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-afc98cd362de>:288: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
            "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
            "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
            "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
            "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
            "Pass warning=False to suppress this warning.\n",
            "  result = await crawler.arun(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[ERROR]... × No URL... | Error: \n",
            "┌───────────────────────────────────────────────────────────────────────────────┐\n",
            "│ × URL must start with 'http://', 'https://', 'file://', or 'raw:'             │\n",
            "└───────────────────────────────────────────────────────────────────────────────┘\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ERROR: Failed to crawl the page No URL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[FETCH]... ↓ https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Time: 38.61s\n",
            "[SCRAPE].. ◆ Processed https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 2211ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 1.5302056999998968s\n",
            "[COMPLETE] ● https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Total: 42.71s\n",
            "[FETCH]... ↓ https://cybermagazine.com/articles/the-rapidly-evo... | Status: True | Time: 54.89s\n",
            "[SCRAPE].. ◆ Processed https://cybermagazine.com/articles/the-rapidly-evo... | Time: 1822ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://cybermagazine.com/articles/the-rapidly-evo... | Time: 1.910637907000364s\n",
            "[COMPLETE] ● https://cybermagazine.com/articles/the-rapidly-evo... | Status: True | Total: 58.94s\n",
            "[FETCH]... ↓ https://www.picussecurity.com/resource/blog/may-10... | Status: True | Time: 67.46s\n",
            "[SCRAPE].. ◆ Processed https://www.picussecurity.com/resource/blog/may-10... | Time: 2633ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.picussecurity.com/resource/blog/may-10... | Time: 2.0990614930001357s\n",
            "[COMPLETE] ● https://www.picussecurity.com/resource/blog/may-10... | Status: True | Total: 72.61s\n",
            "[FETCH]... ↓ https://www.techradar.com/pro/top-data-breaches-an... | Status: True | Time: 81.73s\n",
            "[SCRAPE].. ◆ Processed https://www.techradar.com/pro/top-data-breaches-an... | Time: 871ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.techradar.com/pro/top-data-breaches-an... | Time: 0.7626144199998635s\n",
            "[COMPLETE] ● https://www.techradar.com/pro/top-data-breaches-an... | Status: True | Total: 83.58s\n",
            "Crawled Results: [[{'links': '#', 'images': 'https://socradar.io/wp-content/themes/socradar/assets/image/static/shadow-5.png.webp'}], [{'links': '#main', 'images': 'https://vanilla.futurecdn.net/techradar/media/shared/img/flags/nosize/US.svg'}], [{'links': '/', 'images': 'https://assets.bizclikmedia.net/original/97bfaa8c78a4dd97068c2d3be9d46795:ff3cfa83fbfaefdd36f731bb4f20c158/cyber-magazine-light-logo.png'}], [{'links': 'https://www.picussecurity.com/privacy', 'images': 'https://www.picussecurity.com/hubfs/light_logo-original-SVG.svg'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#', 'images': 'https://socradar.io/wp-content/themes/socradar/assets/image/static/shadow-5.png.webp'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#main', 'images': 'https://vanilla.futurecdn.net/techradar/media/shared/img/flags/nosize/US.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://assets.bizclikmedia.net/original/97bfaa8c78a4dd97068c2d3be9d46795:ff3cfa83fbfaefdd36f731bb4f20c158/cyber-magazine-light-logo.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.picussecurity.com/privacy', 'images': 'https://www.picussecurity.com/hubfs/light_logo-original-SVG.svg'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://istari-global.com/assets/Uploads/Ensign-threat-examples.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/qilin-ransomware-threat-actor-card.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://img.securityinfowatch.com/files/base/cygnus/siw/image/2024/01/65a04d811a5d24001e668b70-bigstock136552454.png?auto=format,compress&fit=fill&fill=blur&w=1200&h=630\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/cyber-army-of-russia-reborn-threat-actor-card.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://website.cdn.group-ib.com/wp-content/uploads/iab-by-country-and-industry.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://gbsits.com/wp-content/uploads/2024/06/LinkedIn-Post-2024-17.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "This analysis provides an overview of the current threat landscape, focusing on the top threat actors, ransomware groups, and vulnerabilities. The data is sourced from reputable cybersecurity and intelligence platforms, including SOCRadar, Securelist, and Trellix. The analysis highlights the growing threat of ransomware, with over 30 new groups emerging in 2024, and the increasing sophistication of attacks. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "### Top Threat Actors [Google Serper](https://socradar.io/top-10-threat-actors-of-2024-beyond-the-numbers/)\n",
            "\n",
            "The top threat actors in 2024 include: [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "1. **RansomHub**: A ransomware group that has been active since 2018, known for its sophisticated attacks and high ransom demands.\n",
            "2. **Qilin**: A ransomware group that has been linked to several high-profile attacks, including the 2024 ransomware campaign against the UAE government.\n",
            "3. **Dark Angels**: A threat actor group that has been linked to several cyber attacks, including phishing campaigns and data breaches.\n",
            "4. **LockBit**: A ransomware group that has been active since 2020, known for its high ransom demands and sophisticated attacks.\n",
            "5. **Whitewarlock**: A threat actor group that has been linked to several cyber attacks, including phishing campaigns and data breaches. [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "### **Ransomware** Groups [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "The number of ransomware groups has increased significantly in 2024, with over 30 new groups emerging. These groups are becoming increasingly sophisticated, using advanced techniques such as AI-powered attacks and zero-day exploits. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "### Vulnerabilities and **Exploit**s [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "\n",
            "Several vulnerabilities and exploits have been identified in 2024, including: [Vector Search](No URL)\n",
            "\n",
            "1. **CVE-2024-1234**: A vulnerability in a popular software application that has been exploited by several threat actors.\n",
            "2. ****Exploit** Kit**: A tool used by threat actors to exploit vulnerabilities in software applications.\n",
            "3. **Zero-Day **Exploit****: A type of exploit that takes advantage of previously unknown vulnerabilities in software applications. [Vector Search](No URL)\n",
            "\n",
            "### **Phishing** Campaigns [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n",
            "\n",
            "**Phishing** campaigns have become increasingly sophisticated in 2024, with threat actors using advanced techniques such as AI-powered attacks and social engineering tactics. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "### **Data Breach**es [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "Several data breaches have been reported in 2024, including: [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "1. **Integris Health**: A data breach that affected over 2.4 million patients, exposing sensitive personal and medical information.\n",
            "2. **HealthEC**: A data breach that affected over 4.5 million individuals, exposing sensitive personal and medical information. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "### **Incident Response** [Vector Search](No URL)\n",
            "\n",
            "Incident response plans should be reviewed and updated to address the growing threat of ransomware and other cyber attacks. This includes: [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "1. **Backup and Recovery**: Regular backups and recovery plans should be in place to minimize the impact of a ransomware attack.\n",
            "2. ****Incident Response** Team**: An incident response team should be established to respond to cyber attacks and minimize the impact.\n",
            "3. **Security Awareness**: Security awareness training should be provided to employees to educate them on the risks of phishing and other cyber attacks. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "### Recommendations [Vector Search](No URL)\n",
            "\n",
            "Based on the analysis, the following recommendations are made: [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "1. **Implement robust security measures**: Implement robust security measures, including backup and recovery plans, incident response teams, and security awareness training.\n",
            "2. **Stay up-to-date with the latest threat intelligence**: Stay up-to-date with the latest threat intelligence to stay ahead of emerging threats.\n",
            "3. **Conduct regular security audits**: Conduct regular security audits to identify vulnerabilities and weaknesses in systems and applications. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "**Most Recent Relevant Activities**\n",
            "\n",
            "The most recent relevant activities include: [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "1. ****Ransomware** campaign against the UAE government**: A ransomware campaign was launched against the UAE government, resulting in significant disruptions to government services.\n",
            "2. ****Phishing** campaign against a major financial institution**: A phishing campaign was launched against a major financial institution, resulting in the theft of sensitive financial information.\n",
            "3. **Data breach at a major healthcare organization**: A data breach was reported at a major healthcare organization, resulting in the exposure of sensitive patient information. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "**Source Citations and Evidence**\n",
            "\n",
            "The following sources were used to support the analysis: [Vector Search](No URL)\n",
            "\n",
            "1. **SOCRadar**: A threat intelligence platform that provides real-time threat intelligence on ransomware and other cyber threats.\n",
            "2. **Securelist**: A cybersecurity platform that provides threat intelligence and security research on ransomware and other cyber threats.\n",
            "3. **Trellix**: A cybersecurity platform that provides threat intelligence and security research on ransomware and other cyber threats. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "**Long-Term Forecast and Monitoring**\n",
            "\n",
            "The long-term forecast and monitoring include: [Vector Search](No URL)\n",
            "\n",
            "1. **Increased sophistication of ransomware attacks**: **Ransomware** attacks are expected to become increasingly sophisticated, using advanced techniques such as AI-powered attacks and zero-day exploits.\n",
            "2. **Growing threat of phishing campaigns**: **Phishing** campaigns are expected to become increasingly sophisticated, using advanced techniques such as AI-powered attacks and social engineering tactics.\n",
            "3. **Increased focus on incident response**: Incident response plans should be reviewed and updated to address the growing threat of ransomware and other cyber attacks. [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "\n",
            "**Specialized Query Handling**\n",
            "\n",
            "The following specialized query handling was performed: [Vector Search](No URL)\n",
            "\n",
            "1. **Threat actor group analysis**: An analysis of the top threat actors in 2024 was performed, including RansomHub, Qilin, Dark Angels, LockBit, and Whitewarlock.\n",
            "2. ****Ransomware** group analysis**: An analysis of the number of ransomware groups and their sophistication was performed.\n",
            "3. ****Vulnerability** and exploit analysis**: An analysis of vulnerabilities and exploits was performed, including CVE-2024-1234 and **Exploit** Kit.\n",
            "4. ****Phishing** campaign analysis**: An analysis of phishing campaigns was performed, including the use of AI-powered attacks and social engineering tactics.\n",
            "5. **Data breach analysis**: An analysis of data breaches was performed, including the Integris Health and HealthEC breaches. [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "- [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "- [Google Serper](https://unit42.paloaltonetworks.com/threat-assessment-north-korean-threat-groups-2024/)\n",
            "- [Google Serper](https://securelist.com/malware-report-q3-2024/114678/)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/cyber-army-of-russia-reborn-threat-actor-card.png.webp)\n",
            "- [Google Serper Image Search](https://website.cdn.group-ib.com/wp-content/uploads/iab-by-country-and-industry.png)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/september-2024-major-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Tavily Search](https://www.techradar.com/pro/top-data-breaches-and-cyber-attacks-in-2024)\n",
            "- [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/october-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper Image Search](https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg)\n",
            "- [Google Serper Image Search](https://img.securityinfowatch.com/files/base/cygnus/siw/image/2024/01/65a04d811a5d24001e668b70-bigstock136552454.png?auto=format,compress&fit=fill&fill=blur&w=1200&h=630)\n",
            "- [Google Serper](https://homeland.house.gov/wp-content/uploads/2024/11/11.12.24-Cyber-Threat-Snapshot.pdf)\n",
            "- [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n",
            "- [Google Serper](https://techinformed.com/ransomware-threat-groups-2024-aim-attacks-ai-cybercrime/)\n",
            "- [Google Serper Image Search](https://istari-global.com/assets/Uploads/Ensign-threat-examples.png)\n",
            "- [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "- [Tavily Search](https://cybermagazine.com/articles/the-rapidly-evolving-threat-landscape-of-2024)\n",
            "- [Google Serper Image Search](https://gbsits.com/wp-content/uploads/2024/06/LinkedIn-Post-2024-17.png)\n",
            "- [Google Serper](https://www.infosecurity-magazine.com/news/new-ransomware-groups-emerge-2024/)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/qilin-ransomware-threat-actor-card.png.webp)\n",
            "- [Google Serper](https://socradar.io/top-10-threat-actors-of-2024-beyond-the-numbers/)\n",
            "- [Google Serper Image Search](https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}