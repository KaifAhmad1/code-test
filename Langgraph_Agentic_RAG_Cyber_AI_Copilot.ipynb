{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "6dc351d9-bc90-4f5e-c979-fc738c2041ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3128bc19-c1df-4fdf-9ee0-e5c16e75fa7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced reranking function with semantic similarity and metadata scoring\n",
        "def enhanced_rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = (datetime.now() - date).days if date else 0\n",
        "        final_score = similarity + metadata_score - date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    vector_results: List[SearchResult],\n",
        "    serper_results: List[SearchResult],\n",
        "    exa_results: List[SearchResult],\n",
        "    tavily_results: List[SearchResult],\n",
        "    google_results: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    all_results.extend([(result, 'vector', 1.0, result.source_weight or 0, parse_date(result.date)) for result in vector_results])\n",
        "    all_results.extend([(result, 'serper', 0.9, result.source_weight or 0, parse_date(result.date)) for result in serper_results])\n",
        "    all_results.extend([(result, 'exa', 0.85, result.source_weight or 0, parse_date(result.date)) for result in exa_results])\n",
        "    all_results.extend([(result, 'tavily', 0.8, result.source_weight or 0, parse_date(result.date)) for result in tavily_results])\n",
        "    all_results.extend([(result, 'google', 0.75, result.source_weight or 0, parse_date(result.date)) for result in google_results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = (datetime.now() - date).days if date else 0\n",
        "            final_score = weight + source_weight - date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            print(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Enhanced reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = enhanced_rerank_results(query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines, APIs, and data sources to provide comprehensive, up-to-date, query-specific responses.\n",
        "\n",
        "SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "1. Primary Source Evaluation:\n",
        "    - Assess the credibility of each source domain.\n",
        "    - Verify publication dates to ensure the most recent information.\n",
        "    - Cross-reference information across multiple sources.\n",
        "    - Identify and flag potential misinformation or conflicting data.\n",
        "\n",
        "2. Content Processing Guidelines:\n",
        "    - Extract and normalize key data points.\n",
        "    - Identify patterns and correlations across sources.\n",
        "    - Prioritize information based on:\n",
        "        * Temporal relevance (newest to oldest).\n",
        "        * Source reliability.\n",
        "        * Direct query relevance.\n",
        "        * Technical depth.\n",
        "        * Actionable insights.\n",
        "\n",
        "3. Media Content Analysis:\n",
        "    - Evaluate included images, diagrams, or screenshots.\n",
        "    - Extract relevant technical indicators from visual data.\n",
        "    - Correlate visual evidence with textual information.\n",
        "    - Note any visual proof of concepts or attack demonstrations.\n",
        "\n",
        "RESPONSE STRUCTURE:\n",
        "1. Executive Summary (8-10 sentences):\n",
        "    - Core findings.\n",
        "    - Critical alerts or time-sensitive information.\n",
        "\n",
        "2. Detailed Analysis:\n",
        "    a) Key Findings:\n",
        "        - Bullet points of critical discoveries.\n",
        "        - Emerging threats or developments.\n",
        "        - Statistical data or metrics.\n",
        "\n",
        "    b) Technical Details:\n",
        "        - Specific vulnerabilities or exploits.\n",
        "        - Attack vectors and techniques.\n",
        "        - System impacts and affected components.\n",
        "\n",
        "    c) Contextual Analysis:\n",
        "        - Industry impact.\n",
        "        - Threat actor attribution (if applicable).\n",
        "        - Historical context or similar incidents.\n",
        "\n",
        "3. Evidence and Citations:\n",
        "    - Link every major claim to source material.\n",
        "    - Include relevant quote snippets.\n",
        "    - Provide context for technical indicators.\n",
        "    - Reference related media content.\n",
        "\n",
        "4. Actionable Intelligence:\n",
        "    - Immediate response recommendations.\n",
        "    - Mitigation strategies.\n",
        "    - Detection methods.\n",
        "    - Prevention measures.\n",
        "\n",
        "5. Future Implications\n",
        "    - Projected developments.\n",
        "    - Potential cascade effects.\n",
        "    - Areas requiring monitoring.\n",
        "\n",
        "SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "1. For Threat Intelligence:\n",
        "    - Extract and validate Indicators of Compromise (IOCs).\n",
        "    - Identify Tactics, Techniques, and Procedures (TTPs) and map to MITRE ATT&CK.\n",
        "    - Analyze malware behaviors.\n",
        "    - Document Command and Control (C2) infrastructure.\n",
        "\n",
        "2. For Vulnerability Analysis:\n",
        "    - Verify Common Vulnerabilities and Exposures (CVE) details.\n",
        "    - Document exploit requirements.\n",
        "    - Assess patch availability.\n",
        "    - Evaluate real-world exploitation scenarios.\n",
        "\n",
        "3. For Incident Response:\n",
        "    - Reconstruct attack timelines.\n",
        "    - Analyze attack paths.\n",
        "    - Perform impact assessments.\n",
        "    - Provide recovery and remediation recommendations.\n",
        "\n",
        "4. For Trend Analysis:\n",
        "    - Identify pattern changes.\n",
        "    - Map threat evolution.\n",
        "    - Project future developments.\n",
        "    - Compare against historical data.\n",
        "\n",
        "PREVIOUS CONVERSATION CONTEXT: {chat_history}\n",
        "CURRENT QUERY: {input}\n",
        "AVAILABLE SEARCH RESULTS: {search_results}\n",
        "CRAWLED RESULTS: {crawled_results}\n",
        "CURRENT TIMESTAMP: {current_date}\n",
        "\n",
        "RESPONSE REQUIREMENTS:\n",
        "1. Maintain clinical precision and technical accuracy.\n",
        "2. Prioritize actionable intelligence over general information.\n",
        "3. Include explicit confidence levels for all assessments.\n",
        "4. Cite ALL sources using [Source Name](URL) format.\n",
        "5. Highlight time-sensitive information.\n",
        "6. Address any information gaps or uncertainties.\n",
        "7. Format output for maximum readability.\n",
        "8. Include relevant media references.\n",
        "9. Provide specific, implementable recommendations.\n",
        "10. Maintain proper technical context throughout.\n",
        "\n",
        "Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the latest search results.\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Enhanced Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3143c4-7d04-45fa-cdb9-84fa9a6eb9e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\n",
            "DEBUG: Raw results from Exa Search: Title: India's Star Health says it received $68,000 ransom demand after data leak\n",
            "URL: https://www.reuters.com/world/india/indias-star-health-says-it-received-68k-ransom-demand-after-data-leak-2024-10-12/\n",
            "ID: https://www.reuters.com/world/india/indias-star-health-says-it-received-68k-ransom-demand-after-data-leak-2024-10-12/\n",
            "Score: 0.14450567960739136\n",
            "Published Date: 2024-10-12T00:00:00.000Z\n",
            "Author: Munsif Vengattil; Aditya Kalra\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Companies    NEW DELHI, Oct 12 (Reuters) - Star Health  (STAU.NS) , India's biggest health insurer, on Saturday said it had received a ransom demand of $68,000 from a cyberhacker in connection with a leak of customer data and medical records.  Star, which has a roughly $4 billion market cap, is battling a reputational and business crisis since Reuters reported on Sept. 20 that a hacker had used Telegram chatbots and a website to leak customers' sensitive data, including tax details and medical claim papers. The company, whose shares have declined 11%, has launched internal investigations and has taken legal action against Telegram and the hacker, whose website continues to share samples of Star customers' data. Star, which has previously said it is a \"victim of a targeted malicious cyberattack\", on Saturday revealed for the first time that in August \"the threat actor demanded a ransom of $68,000 in an email\" addressed to the company's managing director and its chief executive. The statement came after Indian stocks exchanges sought clarifications from Star on a Friday over a Reuters report that the company was investigating allegations that its chief security officer was involved in the data leak. Star reiterated on Saturday it has found no wrongdoing by the official, Amarjeet Khanuja, though the internal investigation is ongoing. Telegram has declined to share the account details or permanently ban accounts linked to the hacker - an individual dubbed xenZen - \"despite multiple notices issued in this regard,\" Star said on Saturday. Star said it has \"sought the assistance\" of Indian cyber security authorities to \"help us identify\" the hacker. Telegram did not respond to a request for comment. The Dubai-based messenger app has previously said it removed the chatbots when Reuters flagged them to the platform. Get the latest news from India and how it matters to the world with the Reuters India File newsletter. Sign up here.  Reporting by Aditya Kalra and Munsif Vengattil Our Standards: The Thomson Reuters Trust Principles.    Munsif Vengattil is a Reuters' India technology correspondent, based in New Delhi. He tracks how policymaking is influencing the business of tech in India, and how the country is now vying more aggressively to be a powerhouse in the global electronics supply chain. He also regularly reports on big tech giants, including Facebook and Google, and their strategies and challenges in the key Indian market.   Aditya Kalra is the Company News Editor for Reuters in India, overseeing business coverage and reporting stories on some of the world's biggest companies. He joined Reuters in 2008 and has in recent years written stories on challenges and strategies of a wide array of companies -- from Amazon, Google and Walmart to Xiaomi, Starbucks and Reliance. He also extensively works on deeply-reported and investigative business stories.\n",
            "Highlights: ['Star, which has a roughly $4 billion market cap, is battling a reputational and business crisis since Reuters reported on Sept. 20 that a hacker had used Telegram chatbots and a website to leak customers\\' sensitive data, including tax details and medical claim papers. The company, whose shares have declined 11%, has launched internal investigations and has taken legal action against Telegram and the hacker, whose website continues to share samples of Star customers\\' data. Star, which has previously said it is a \"victim of a targeted malicious cyberattack\", on Saturday revealed for the first time that in August \"the threat actor demanded a ransom of $68,000 in an email\" addressed to the company\\'s managing director and its chief executive. The statement came after Indian stocks exchanges sought clarifications from Star on a Friday over a Reuters report that the company was investigating allegations that its chief security officer was involved in the data leak. Star reiterated on Saturday it has found no wrongdoing by the official, Amarjeet Khanuja, though the internal investigation is ongoing.']\n",
            "Highlight Scores: [0.569767951965332]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: India’s wave of cyberattacks prompts calls for tougher rules and countermeasures\n",
            "URL: https://www.scmp.com/week-asia/economics/article/3282324/indias-wave-cyberattacks-prompts-calls-tougher-rules-and-countermeasures?utm_source=rss_feed\n",
            "ID: https://www.scmp.com/week-asia/economics/article/3282324/indias-wave-cyberattacks-prompts-calls-tougher-rules-and-countermeasures?utm_source=rss_feed\n",
            "Score: 0.14193105697631836\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: Vasudevan Sridharan\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: An alarming wave of cyberattacks is sweeping across  , compromising millions of personal records as tech analysts say stronger regulations and other countermeasures are needed to plug the critical weaknesses in the nation’s digital infrastructure. “Various sectors, including healthcare, education, government, and critical infrastructure, have been particularly hard hit,” said Diwakal Dayal, India-based vice president of SentinelOne, a global cybersecurity firm. “These sectors often struggle with underfunded   measures, making them attractive targets for malicious actors.” In recent weeks, major entities including Star Health Insurance, the Supreme Court’s YouTube channel, and the popular podcast BeerBiceps have fallen victim to cyberattacks, leading to significant data breaches and temporary shutdowns. A hacker infiltrated Star Health Insurance, accessing 7.24 terabytes of data and compromising the personal data of up to 3.1 million people, and demanded a ransom from the company in August. Details taken from the system of India’s largest health insurer, including names, birthdays, tax identities and confidential medical records, have been uploaded for sale online for US$150,000 and caused a sharp decline in its share price.\n",
            "Highlights: ['A hacker infiltrated Star Health Insurance, accessing 7.24 terabytes of data and compromising the personal data of up to 3.1 million people, and demanded a ransom from the company in August. Details taken from the system of India’s largest health insurer, including names, birthdays, tax identities and confidential medical records, have been uploaded for sale online for US$150,000 and caused a sharp decline in its share price.']\n",
            "Highlight Scores: [0.6020882725715637]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: India’s biggest health insurer gets ransomware following data breach\n",
            "URL: https://www.techradar.com/pro/star-health-say-it-received-a-usd68-000-ransom-after-data-breach\n",
            "ID: https://www.techradar.com/pro/star-health-say-it-received-a-usd68-000-ransom-after-data-breach\n",
            "Score: 0.1418752819299698\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: Ellen Jennings-Trace\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Indian health insurer Star Health says it has received an email containing a ransom of $68,000 following a \"targeted malicious cyberattack\". The attack allegedly resulted in the leak of personal data of up to 31 million Star Health policy holders and over 5.8 million insurance claims. The health organization confirmed that the cyberattack resulted in \"unauthorized and illegal access to certain data\" including full names, postal addresses, phone numbers, medical reports, and insurance claims. Since the attack, the company has suffered serious reputational damage and an 11% drop in shares, and has also launched legal action against Telegram, since Telegram chatbots were used to leak the information. A mountain of information  The stolen data was reported to total a staggering 7.24 terabytes, although it is not yet confirmed exactly what information was taken. Health Star is said to have sought the assistance of Indian cybersecurity authorities in its investigation. Although the hacker alleged that Chief Information Security Officer Amarjeet Khanuja was involved in the breach, the organization is yet to identify any wrongdoing - but the internal investigation is ongoing. “We also want to categorically mention that our CISO has been duly co-operating in the investigation, and we have not arrived at any finding of wrongdoing by him till date. We request that his privacy be respected as we know that the threat actor is trying to create panic” the insurer commented. Telegram have declined to comment on account details or permanently ban accounts linked to the hacker, an individual dubbed ‘xenZen’, despite \"multiple notices issued in this regard\", the Star has revealed.   Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed!   As is always the case with compromised data, this leaves customers exposed to malicious actors, specifically in regards to the risk of identity theft. We've listed the best identity theft protections for anyone concerned about their data.  Via   Reuters                Take a look at some of the best malware removal software         Healthcare organizations are being hit hard by cyberattacks         Check out our pick for best antivirus software                              \n",
            " Ellen has been writing for almost four years, with a focus on post-COVID policy whilst studying for BA Politics and International Relations at the University of Cardiff, followed by an MA in Political Communication. Before joining TechRadar Pro as a Junior Writer, she worked for Future Publishing’s MVC content team, working with merchants and retailers to upload content. \n",
            "  Most Popular\n",
            "Highlights: ['The attack allegedly resulted in the leak of personal data of up to 31 million Star Health policy holders and over 5.8 million insurance claims. The health organization confirmed that the cyberattack resulted in \"unauthorized and illegal access to certain data\" including full names, postal addresses, phone numbers, medical reports, and insurance claims. Since the attack, the company has suffered serious reputational damage and an 11% drop in shares, and has also launched legal action against Telegram, since Telegram chatbots were used to leak the information. A mountain of information  The stolen data was reported to total a staggering 7.24 terabytes, although it is not yet confirmed exactly what information was taken. Health Star is said to have sought the assistance of Indian cybersecurity authorities in its investigation.']\n",
            "Highlight Scores: [0.5790166258811951]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Tech giant Nidec confirms data breach following ransomware attack\n",
            "URL: https://www.bleepingcomputer.com/news/security/tech-giant-nidec-confirms-data-breach-following-ransomware-attack/\n",
            "ID: https://www.bleepingcomputer.com/news/security/tech-giant-nidec-confirms-data-breach-following-ransomware-attack/\n",
            "Score: 0.13776879012584686\n",
            "Published Date: 2024-10-18T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Nidec Corporation is informing that hackers behind a ransomware attack is suffered earlier this year stole data and leaked it on the dark web.\n",
            "The Japanese tech giant says the threat actors tried to extort the company and decided to leak the information after their demands were not met.\n",
            "The attack did not encrypt files and the incident is considered fully remediated at this time. However, Nidec employees, contractors, and associates, should be aware that the leaked data could be used in more targeted phishing attacks.\n",
            "Nidec Corporation is a global leader in the manufacturing of precision motors, automotive components, industrial parts, home appliance parts, and robotic systems.\n",
            "It operates in 40 countries, employs 120,000 people, and generates an annual revenue of more than $11 billion.\n",
            "Nidec Precision breach\n",
            "The cyberattack breached Nidec Precision division based in Vietnam, which specializes in manufacturing optical, electronic, and mechanical equipment for the photography industry.\n",
            "As per the results of the internal investigation, which is still ongoing, the hackers obtained valid VPN account credentials of a Nidec employee and accessed a server that contained confidential information.\n",
            "The company closed the entry point and implemented additional security measures, as per recommendations from external cybersecurity experts. Nidec employees are undergoing training on how to minimize such risks.\n",
            "The investigation also revealed that the attackers stole 50,694 files, including the following:\n",
            " Internal documents\n",
            "Letters from business partners\n",
            "Documents related to green procurement\n",
            "Labor safety and health policies (business and supply chain, etc.)\n",
            "Business documents (purchase orders, invoices, receipts)\n",
            "Contracts\n",
            " Nidec said that it would notify directly its business partners affected by the incident.\n",
            "8BASE and Everest gangs claim attacks\n",
            "The 8BASE ransomware gang claimed an attack on Nidec on June 18, alleging that the data had been stolen from the systems of the Japanese firm on June 3, 2024.\n",
            "8BASE claimed to be holding much of what Nidec confirmed via its investigation, plus personal data and “a huge mount of confidential information.”\n",
            "Nidec in July aknowledged a ransomware attack without naming the perpetrators, stating that it was the impacted division was Nidec Instruments.\n",
            "On August 8, the Everest ransomware group, known for receiving stolen data from other cybercriminals to perform new extortion attempts on victims, published data allegedly stolen from Nidec.\n",
            "The company states in the latest announcement that the threat actors first made contact on August 5, suggesting that the communication came from the Everest ransomware gang.\n",
            "Nidec has acknowledged that the data that leaked on the dark web comes from its systems but did not offer any clarification about the threat actors' claims.\n",
            "In any case, the company says it does not believe that any of the leaked data could be used to cause direct financial damage to it or its contractors and has not observed unauthorized use of the information.\n",
            " H/T: @H4ckManac\n",
            "Highlights: [\"The company states in the latest announcement that the threat actors first made contact on August 5, suggesting that the communication came from the Everest ransomware gang. Nidec has acknowledged that the data that leaked on the dark web comes from its systems but did not offer any clarification about the threat actors' claims. In any case, the company says it does not believe that any of the leaked data could be used to cause direct financial damage to it or its contractors and has not observed unauthorized use of the information.\"]\n",
            "Highlight Scores: [0.6048170924186707]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: IcePeony and Transparent Tribe Target Indian Entities with Cloud-Based Tools\n",
            "URL: https://thehackernews.com/2024/11/icepeony-and-transparent-tribe-target.html\n",
            "ID: https://thehackernews.com/2024/11/icepeony-and-transparent-tribe-target.html\n",
            "Score: 0.1365129053592682\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: High-profile entities in India have become the target of malicious campaigns orchestrated by the Pakistan-based Transparent Tribe threat actor and a previously unknown China-nexus cyber espionage group dubbed IcePeony.\n",
            "The intrusions linked to Transparent Tribe involve the use of a malware called ElizaRAT and a new stealer payload dubbed ApoloStealer on specific victims of interest, Check Point said in a technical write-up published this week.\n",
            "\"ElizaRAT samples indicate a systematic abuse of cloud-based services, including Telegram, Google Drive, and Slack, to facilitate command-and-control communications,\" the Israeli company said.\n",
            "ElizaRAT is a Windows remote access tool (RAT) that Transparent Tribe was first observed using in July 2023 as part of cyber attacks targeting Indian government sectors. Active since at least 2013, the adversary is also tracked under the names APT36, Datebug, Earth Karkaddan, Mythic Leopard, Operation C-Major, and PROJECTM.\n",
            " \n",
            "Its malware arsenal includes tools for compromising Windows, Android, and Linux devices. The increased targeting of Linux machines is motivated by the Indian government's use of a custom Ubuntu fork called Maya OS since last year.\n",
            "Infection chains are initiated by Control Panel (CPL) files likely distributed via spear-phishing techniques. As many as three distinct campaigns employing the RAT have been observed between December 2023 and August 2024, each using Slack, Google Drive, and a virtual private server (VPS) for command-and-control (C2).\n",
            "ApoloStealer is designed to gather files matching several extensions (e.g., DOC, XLS, PPT, TXT, RTF, ZIP, RAR, JPG, and PNG) from the compromised host and exfiltrate them to a remote server.\n",
            "In January 2024, the threat actor is said to have tweaked the modus operandi to include a dropper component that ensures the smooth functioning of ElizaRAT. Also observed in recent attacks is an additional stealer module codenamed ConnectX that's engineered to search for files from external drives, such as USBs.\n",
            "   \n",
            "The abuse of legitimate services widely used in enterprise environments heightens the threat as it complicates detection efforts and allows threat actors to blend into legitimate activities on the system.\n",
            "\"The progression of ElizaRAT reflects APT36's deliberate efforts to enhance their malware to better evade detection and effectively target Indian entities,\" Check Point said. \"Introducing new payloads such as ApolloStealer marks a significant expansion of APT36's malware arsenal and suggests the group is adopting a more flexible, modular approach to payload deployment.\"\n",
            "IcePeony Goes After India, Mauritius, and Vietnam\n",
            "The disclosure comes weeks after the nao_sec research team revealed an advanced persistent threat (APT) group it calls IcePeony has targeted government agencies, academic institutions, and political organizations in countries such as India, Mauritius, and Vietnam since at least 2023.\n",
            "\"Their attacks typically start with SQL Injection, followed by compromise via web shells and backdoors,\" security researchers Rintaro Koike and Shota Nakajima said. \"Ultimately, they aim to steal credentials.\"\n",
            " \n",
            "One of the most noteworthy tools in its malware portfolio is IceCache, which is designed to target Microsoft Internet Information Services (IIS) instances. An ELF binary written in the Go programming language, it's a custom version of the reGeorg web shell with added file transmission and command execution features.\n",
            "   \n",
            "The attacks are also characterized by the use of a unique passive-mode backdoor referred to as IceEvent that comes with capabilities to upload/download files and execute commands.\n",
            "\"It seems that the attackers work six days a week,\" the researchers noted. \"While they are less active on Fridays and Saturdays, their only full day off appears to be Sunday. This investigation suggests that the attackers are not conducting these attacks as personal activities, but are instead engaging in them as part of organized, professional operations.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['\"It seems that the attackers work six days a week,\" the researchers noted. \"While they are less active on Fridays and Saturdays, their only full day off appears to be Sunday. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.47160276770591736]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres a recent cyber incident involving Blackbasta ransomware in India from the past three months:\n",
            "Resolved Search Type: 2024-10-12T09:20:43.406Z\n",
            "ERROR in Exa Search: name 'SearchResponse' is not defined\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024 using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, success: True, time taken: 2.74 seconds\n",
            "[LOG] 🚀 Content extracted for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, success: True, time taken: 0.24 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, time taken: 0.30 seconds.\n",
            "[LOG] ✅ Crawled https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics successfully!\n",
            "[LOG] 🚀 Crawling done for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, success: True, time taken: 3.55 seconds\n",
            "[LOG] 🚀 Content extracted for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, success: True, time taken: 0.23 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, time taken: 0.27 seconds.\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 3.63 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.08 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.10 seconds.\n",
            "[LOG] ✅ Crawled https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024 successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024, success: True, time taken: 4.71 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024, success: True, time taken: 0.25 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024, time taken: 0.32 seconds.\n",
            "[LOG] ✅ Crawled https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, success: True, time taken: 5.71 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, success: True, time taken: 0.49 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, time taken: 0.62 seconds.\n",
            "Crawled Results: [[{'links': '/', 'images': '/themes/modern/images/logo-dark.svg'}], [{'links': 'https://www.barracuda.com', 'images': '/content/dam/barracuda-corp/images/site/header/logo_barracuda_primary_reversed.svg'}], [{'links': 'javascript:jumpScroll($(this).scrollTop());', 'images': '/content/dam/trendmicro/global/en/core/images/logos/tm-logo-red-white-t.svg'}], [{'links': 'https://twitter.com/thehackersnews', 'images': 'data:image/png;base64,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'}], [{'links': 'https://www.blackberry.com/', 'images': '/etc.clientlibs/bbcom/clientlibs/clientlib-etc-legacy/resources/cylance-web/icons/reports-icon-dark.svg'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': '/themes/modern/images/logo-dark.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.barracuda.com', 'images': '/content/dam/barracuda-corp/images/site/header/logo_barracuda_primary_reversed.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'javascript:jumpScroll($(this).scrollTop());', 'images': '/content/dam/trendmicro/global/en/core/images/logos/tm-logo-red-white-t.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://twitter.com/thehackersnews', 'images': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1IAAABdCAMAAACYRqd9AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAACWUExURUdwTP///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////7KwB0MAAAAxdFJOUwBAwHCwIA+QYNCgAfKA+AUK/FHgOu4YSByq18UveCm76RNZ5Wlkhsql3Jy2jDWUlyR/G9pxAAASqUlEQVR42u2d6WKyvBKAowKqAQFx39e6t+X+b+7U9m2LyWQlgqdf5mcrSibzJJPJZIKQFStWrFixYsWKFStWrFixYsWKFStWrFh5gEw7N2n8ilWJFSt5pJUS8v/1+iu1j+Oh+0+m3xJaG/iSnUNIZHXyX0RqsVb7vE82N3X/Vn92a6ryTU6d1Iz3BM2pUK9bgT84pD64tkhpSTUNsEUqO8akipKgZ0aKeqm0zmCP+mDTIqWJVNoKLVI/ErVVkbpYpCxSBFJpdWWR+paBKlFp1yJlkSKRSsfvFimmCYr8PmyRskhRSKW9rkXqS8aqSNWRRcoiRSOVttcWKYQQHur7fRYpi1QWqVQ28PfHZ6mdKlEbbJGySMFISQb+/jhSI1WkXpFFyiLFQCqdrP7zSOGevt9nkbJIkUilx/f/OlJdVaKW2CJlkWIjJRP4+9tIBapIvSGLlEWKg1Qar//bSE1UkapYpCxSXKTEgb8/jZQX5/D7LFIWKRCp9BD+d5HqqE5SJ2SRskiJkBIE/v40UidVpAYWKYuUGCl+4O9PI5UoEtULLVIWKQmkuIG/v4xU2KGEtsh95r9dJIPUsHGaJL7fSyaHWUX2IE3YnbVuT/nL8eJt2w2fByk878zezs1k6ft+0jy/1Soqp5dXnaBVPW5u6tiPXjrDgpDKrc6wMjudx0lvGWkhlcYNQ0jJtSRyM9L3PgT82KUGhhQ+pJ/9AqiXos/3WPr+cj+aqSi0QTXXkbfeWzOi3f7ub/5BIkM5XDvEsS3faXhPgFQ0uFRpC4gntaHMu+DuhcpKPtYr+NFI5Vfn++t3DkBVb5biBf4UkJJuyTv5lT5k9P32kv4zpo8Mnik7IN+jd6jgYpAKt0tat5OrYCB/AXM4/Ld5yUj1z+zzmU5FOD/NGH51EqweiZSaOue3EgHBp9Q/5LOrwstvJHimjRQz8CeNlEpLzuSnOsAXvqTpjv7rmh8yQMgLoPc4bsMCkHrfMw4Cc4iOAp9pt6P3UpHiH890uC+3qrOblbbr/UchpapOosNv7pqX3a2c6iPFCvxJIqXWEqqvRkCf+Gl6pE3RoWC5+wzuLFlHB9f40Uh1mDpYMD3yATdEEl+iEpGq8YM1/o7t8m0FyZP+Fisg1al+yvm7FFWr1TqtzKiTRiraszYiVZFKj3N9pBRbgo/ksEX7h5+5QlRyB+333S24Ql5YvPr+YKQ4G8dVGI1QeKg4qZSH1JvwOCbD4FyJrJS9K43UFfA/t4bUSSN19zYtlAeptFfRREq9JdT4R8VHPB/qZUz5fe3saBXxyx21t49FSnqT+Gexshc/GKtVtDKJVFXhiPPdpCKV3/9rbgKkhkt+rmUudVJI3W//r/MhBQb+JJDSaInni2IMM3ChRPt9WVPFZ9FrCM+IPQwpqHPe5fbFlEpumkRqKX43yPerSW7y9aWQwlETcKShftRSJ4lUeB+jXOVECgr8iZHSagnpU8QrIrDXA2OYHuUCZJaP+EX8FgtcFlIbCozpUvLRiVcKUp7Eq/m0zzKTbFVHzvHDDv3oGFKInjpJpO7doD3KjRQd+BMipdeSucA3rgFH/iCLn3CNhJ4sO6XNUlQ4driRfnTvlYGU1GGykeYclXUvuEhdgPkNWhRrqpNE6n5KDAwgRQX+REjpGsaCwwZC0RK2aIfjTuGmmCiha/xApDb3o1WkUptpEeojdXRAmQiRaki9GhHUGkgm948jOaSA4iAxtObXVSfRSEJ9XRNIpclcBSltw6CSwO/247dwl1F+35L3jRpEPRIpYvNNrerFqz5S0kIidZF66uV+hJWsPBBf5falKrHc+k1bnUSH98QpCOpIEYE/AVLahoETjl8UJnAYkzL4gLd/LBd4LQ6pkfoUwN8MfzBSjtRTyd3Cpyr5WzO5rd55TzLKqK1O7oMOMoPUfeCPj1QOw5hxOjT7tfGQTU32f6vYAFEPRcrPhEaGvurD/cKROso9pmMPVSyF1Ap4hTMUYNJXZ0M1nqmFVJq+YDmk8hjGqs10ysMjPCpR1GTH/Y4Joh6KVJpxdg7QeP9S6Ufe8LodtWXiAI9GKvzS9tIJ1lfXCyPvvbs7+NzdAU8yVOUPpXL8wqpksC+HOrlIDc0hlY4iKaRyGcaJ6cStGZu51FK1QmZbZGew08D1wn63do7BgNuDkBrPum5/3jn5XKiBYNotYQr/5KsCT1eKRQrfgrKTGhF9iOiNiuAuLROQ46Uz7a/c7u60hPfoWEgBqTBLMB0uhzp5SI2RQaR+A388pHIZBp6Smv/5z5jRZ2dek1vMsNCwHssSlRep9u67+X1gJfLGjHemqePdpchBAdOCZ6nOJnARotwsalw7/PIGxSaqmaEADxxyecxBCtjgiuEQXA51MpFqJ82ZLlJHbuCPh1ROw5gw/CLShetFDL9vy1lML7L/m+8DVARSGY4xpqfwn33rK01b1nSHUF7VZlAwUn1GuIscK868LSmfTMjpNtPEk0KqAyyO4XN9edQJI3VuuPJpYDRSjYAX+OMgldcw1nAoh95fqsHjox9xGnY/EkrrJx9Sd8NaRGWVHJkO8yH7gg3IaXyT3O01GJ5gSJc1UAC3oCR0Knb4Rs00IFJTXxSwRybUCSG1VzuoBiCFGmCwLN4JkMprGCGxS/yvgv+AtUm64EZTiYaNlZJNzSDVi/guUpu1vdbMTAhDyF0+SuejPxwpTBa9/vE7pnTHyZ33gpDqAykEDtynudQJIHVWPFADIYUqPjPwx0Yqv2EE4Dprzwhm9mPuvn2VN2MUgxSRbU5XCYzgH4kzLVkD6xGVU1MPQAoPp93Ktc/SdJPVnWn7inSRigAraEZyfaakTrrDlytkACk03bACf2yk8hsGCcmJlan3ecyQXJktBNvnLbdopEhPn/JgPXg18rsDDgU10uZVoQGGkQor9f0/E/AXsznknfw8tudtxKshhYEUgiUrpJ1LnXSH15ARpNAQTpCb9NlIGTAMonM+a3qBIck14Pd1hMGC6uyKi0RqKvLxv4wiJHQa/7APHUluB0rlfYwi5V6IobG5jaif+H4sivl+sApSQDSeOeXlUyfV4W3PEFLIg0/vJW0WUiYMo0tT0mX1NjmlJQQtWzjI4mznhSE1FH3ABXcPvqfbFZTdNVGs6WIQqVUdWGMvtyEDqYpkMEECqUbK3VG+d0vzqbPBDMvmRop/xpxGyoxh7KmNYEam3oBCZiaMpP4skluNfiFIkeNbAKtuB7cEGon8muo0a3BfipEBu2espWqiSVseKQBlthOZT51Uh1/MISV9M4yLzBlGgwyLTxk/WiV3sdrkIpJ/+9qk5j0NUnUoKrOCMlEW6stBU0jJnOe8f4z8ZR9rIwWs6rF0i9XU2Uh5m515kWIE0xlIGTGMkABwzcx+Jvd/6TobglnWn0VPgpQD/HWwgbZJNTYCDCGF35Qfc7SyPeSQGnP6Lp86G7IOph5SrGA6iJQZwyDGQvYpQp+RavErU9GLH6/PgdSeesyDjMrpI2QCKa1TvYE6iXtmVkV+pNocJzKfOhv0EsMoUqxgOoSUGcMYxnojKjQECs9utRtPgRSZVeFBJduWHYTKQ2qgMbk1pRWn4fhxpql86qQ6vGIYKVYwHUDKkGE4ekhBcPTF48H2GZGCnOXWCpWIlLfRQCrh7xvmQ4pTUC+fOh+PFCuYLkZK0zA4NVg4Xih8n8JV6LfGgydECti6GCBUJlJKVxizkNobRSptyCKlps4CkBIH011k1DDY0+KMPYMxtjyuwiNwm6h8pITlOl4jVCpSHjA0jU8vQX3RVkDK10cKqnrhszbo8qmzCKSEY5SLTBoG3jEnKY9Z9SpmxRCHwsrBs/KRErzjuItQuUjRXTL6Fx7waj3mY2OGoWggFUIqYqX45VNnMUgJgukuMmoYEWs/KWBXaGGbOa4JSvQkuHSkuH5O/BKhspEi1d7OBJbp0sLfj1Erhp02UnChpTcNt1GozoKQ4gfTXWTWMC6sSYpdnJHnUXoz/vzZLR0pXjnW5hSh0pHq8bZqvCPjsZO85oRIwZVE4FhXPnUWhRTmBdNdZNYw3jnLJfhYv+gs1DSYxJqeXxFIsSMy7VmIykfK5W8wdRiPUQYRD/WRAresfVctwCWjzqKQ4gbTXWTYMMAohL9i/4gw/R4jr/M6VnUai0IqYt0tOJkj9ARIDfj5BGS+NDNtlvXLUkiF0N0Ve8iw8qmzOKQ4wXQXGTaMASemN2G4hDIybByACOC4dKQQrNq7W8x+uuElLByptSDMAKfN4ojyDOK5PlLIhZZTYE5rLnUWiBQ7mO4iw4aBgfnkOyt2IL9MBb+6QnWXXz5SYAmRM+QmTY/S5/jMIUXm/Ef8LJUm27T2kT5S8supXOosEinmaslFpg0DqKzzYwfA/K82Cw7aUH+VipTXltvJDIM4TeNp0UiROujzZ4UmpxsP0PjqySGFXgH76gH2lUudxSLFCKa7yLRh0FuL7T5rMSw4IzYUG5lXOlJQ0vyJNr7O1+zdDAtGas2Pr+INA6k+YC4jap6aj0eSSIXQen4CaCOPOgtGCg6mu8i4YVDj0e+he7oEGS9tsNGjY+Tb50MKCnKO7p/Gg6ogVeRRSGHS2Saq1HSZeUxQnGl8n/4f1tp0eIlVbfbdl0ucyaPOopHC84SDlDnDmHPCr+Q0teGwOffT9k5kZLh8pMCk+WXtNyNyHmTWl/G12Flqzo0yUJdzNJmw/TOJ7o/GV7Uj1CDmNQNryU3JHOosGimE+k02UgYNY8EOQDDLOQPh1M+XPd8XjyM34pfoCZBywXBpexGsO51GMCJcq3FUKFIh6cAdMyMcfTaxKYq9pUmr9tGu7eVntzBZySEF3mm/pI8L5VBn8Ugh78xGypxhdDh7hPdDVdwX+o9xq/I7E1XIzf7JMyCFpI+hs2PHj8ueoCKwvZ9jpPNqykHqKtugs9xlOAi6+hq6b1lfnSUgRQfTXWTeMO4vcCOKL99BcZDicjmarQeVzq5Od8nrUyAVqtweyaqu/yikgMyF5KVznVZqUNJl9jCw9PF6ySvbbq68TAKMvjrLQArhGRMpc4aR/Y34nWPm7O+QvNOy8xRIoWlbRXXHqEikOkojZRYpL5G1hYocUuByCjAkbXWWghTC98F0Fz3AMLIXuJEHOLM3uLHrduOJ1Dv40XMghXZKdlsvEqlwqYsU6saST236ckiBVXo29NFWXXWWgxQRTHfRIwzjxNnKzYTB2QcGXozYZnFISd4uLZ6dzSOl5tA39Uw7syLiIgVesA5cOKCpzrKQQtlguoseYRi/9Y3oGzR/b8Nm1wSuyA2P7eHTIIUdQzO8caTwsK2NlPwh+0AOKTSHXobOnNZUZ2lI4Uww3UUPMYwJJ99oK4pw4JVkARLRfR4FIoVCFdU5qwJnKaWBsok0mRrIIQVmvgFbMnrqLA2pbDDdRQ8xjDXHhKNv5551XxE+y/3+JHwipLB8/cleAxXp+MHeliRSeCu5nkpCOaTA065HKglGT50lIvUbTHfRQwzj+wI3MBnwX0Yms9piJMf1RlhtskikbisPOQ/rIF8l01BpzCnnXLff5CKFUEUqvOFXJGcpGPAR0CYNdZaJ1E+g20WPMYyAY8H/pin2AXlck3iHpTiHvWCk0LQpMbUWnJD09W+2PtctAVLwbUiEjOdIFik4sAzVZFRXZ7lI/ctMd9FjDOMrkfnKwfnIS8+b74WdKHHNZdFI4XAmGAsmajX9TCGFuj3mclSIFEIdwQaVn70cSYgUGEeEijqrq7NkpL6C6S56kGEcOK7dZx0l/gF5vOXv9Y5kDgMXjdRtKHllrz3aI9UKZMaQwkOw8MctLVkCKRTWOAGj3mUlmT2RtQ1qiIwMqLNspD4z0+HybAYMo8uLsgfA/TfUfvGFDfZYrsZ4CUghNAxg85vU1Ms4G0MKIbymp5rFNINAr+m8ztbM2wnXcDGetLojWLgkzea+Wl04zqjVeqvXgUML3j65ybj5K9Xq1oA6TSPl+/4yUapn32+mzLKUuQ1jz6mj7fnpSfwNK9Y7rOXO8eFOkmx83/dlkArGn5ZQdRzn8GkK5KhZu/vAqV5nKAJjPHgl0nuTQ0Pr5o4gSX6tbvLx4yzD62YN9PaWVVq/YcfJDlJ+62vEe1mcgt1gLt4pG26dpZlmqYiKOnEnGX8TepOFo15S533quu7Q8zzN8lbe2UWPMowGb4gI5G7VwxUyVTbeB+9aTY08z+u7D7eAz7e+2d+g9tpyPvg7vWwrhfyqVKD32ri0nIVzuGy7GGlceOUOah9fcF44rUuj6xXz0nnUiYvXMcYPM4yQlyW+Oslj323M6q3DzRDq24qnqyaMrFgpaASwYsWKFStWrFixYsWKFStWrFixYsWKFStWrPxZ+R/nlAFgD+MsoQAAAABJRU5ErkJggg=='}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.blackberry.com/', 'images': '/etc.clientlibs/bbcom/clientlibs/clientlib-etc-legacy/resources/cylance-web/icons/reports-icon-dark.svg'}\n",
            "Enhanced Cyber AI Copilot Response:\n",
            "ENHANCED SEARCH RESULTS ANALYSIS: RANSOMWARE ATTACKS AND BLACK BASTA THREAT [Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics)\n",
            "\n",
            "Executive Summary:\n",
            "- Ransomware attacks against the energy sector are increasing, with nuclear and oil/gas being major targets (Resecurity).\n",
            "- Black Basta, a notorious ransomware cluster, has claimed nearly 100 victims in seven months and is known for sophisticated tactics across multiple sectors (Barracuda, Resecurity).\n",
            "- Black Basta has impacted over 500 organizations worldwide since its emergence in April 2022 (Fortinet, The Hacker News).\n",
            "- Black Basta ransomware now poses as IT support on Microsoft Teams to breach networks (ReliaQuest). [Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics)\n",
            "\n",
            "Detailed Analysis: [Google Programmable Search](https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024)\n",
            "\n",
            "Key Findings:\n",
            "- Black Basta ransomware attacks are on the rise, targeting various sectors including the energy sector (Resecurity, Fortinet, The Hacker News).\n",
            "- The group has claimed nearly 100 victims in seven months and impacted over 500 organizations worldwide (Barracuda, Fortinet, The Hacker News).\n",
            "- Black Basta has shifted its social engineering attacks to Microsoft Teams, posing as corporate help desks (ReliaQuest). [Tavily Search](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "\n",
            "Technical Details:\n",
            "- Black Basta first emerged in April 2022 and operates as a Ransomware-as-a-Service (RaaS) (Fortinet, Blackberry).\n",
            "- The group advertises its intent to buy and monetize corporate network access credentials on underground forums (Trend Micro).\n",
            "- Black Basta ransomware poses as IT support on Microsoft Teams to breach networks (ReliaQuest). [Tavily Search](https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html)\n",
            "\n",
            "Contextual Analysis:\n",
            "- Ransomware attacks against the energy sector are increasing, with nuclear and oil/gas being major targets (Resecurity).\n",
            "- The rise of Black Basta ransomware indicates a growing trend in sophisticated ransomware attacks (Barracuda, Resecurity, Fortinet, The Hacker News). [Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics)\n",
            "\n",
            "Evidence and Citations:\n",
            "- Ransomware attacks against the energy sector: [Resecurity](https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024)\n",
            "- Black Basta ransomware: [Barracuda](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics), [Resecurity](https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024), [Fortinet](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware), [The Hacker News](https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html), [Blackberry](https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta)\n",
            "- Black Basta ransomware on Microsoft Teams: [ReliaQuest](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "\n",
            "Actionable Intelligence:\n",
            "- Implement robust network security measures and multi-factor authentication.\n",
            "- Regularly update and patch systems.\n",
            "- Train employees to recognize and report suspicious activities, especially on Microsoft Teams.\n",
            "- Consider using a reputable Endpoint Detection and Response (EDR) solution. [Tavily Search](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "\n",
            "Future Implications:\n",
            "- The energy sector should anticipate and prepare for increased ransomware attacks.\n",
            "- Organizations must stay vigilant and adapt to evolving ransomware tactics, such as those employed by Black Basta.\n",
            "- Continuous monitoring and proactive threat hunting are crucial for early detection and mitigation of ransomware attacks. [Tavily Search](https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html)\n",
            "\n",
            "**Sources**\n",
            "- [Tavily Search](https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html)\n",
            "- [Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics)\n",
            "- [Google Serper](https://spin.ai/resources/ransomware-tracker/)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/biggest-cyber-attacks-data-breaches-ransomware-attacks-february-2024)\n",
            "- [Google Serper](https://www.blackberry.com/us/en/solutions/threat-intelligence/threat-report)\n",
            "- [Google Serper](https://konbriefing.com/en-topics/cyber-attacks.html)\n",
            "- [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "- [Google Programmable Search](https://cyberint.com/blog/research/ransomware-trends-2024-report/)\n",
            "- [Tavily Search](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "- [Google Programmable Search](https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024)\n",
            "- [Google Programmable Search](https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta)\n",
            "- [Google Serper](https://www.infosecurity-magazine.com/news/ransomware-demands-staggering-5m/)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Serper](https://www.cyfirma.com/research/tracking-ransomware-october-2024/)\n",
            "- [Tavily Search](https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html)\n",
            "- [Google Serper](https://www.picussecurity.com/resource/blog/august-2024-latest-malware-vulnerabilities-and-exploits)\n",
            "- [Google Serper](https://cybersecurityventures.com/ransomware-report/)\n",
            "- [Google Serper](https://tech.co/news/data-breaches-updated-list)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}