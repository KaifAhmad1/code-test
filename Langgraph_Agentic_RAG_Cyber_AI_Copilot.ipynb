{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "7689dd01-b199-45cc-f48f-be767e937d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.6/249.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 4.86MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 13.1MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 25.4MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 26.7MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.23MB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 10.7MB/s]\n",
            "2024-12-04 09:33:21.766704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-04 09:33:21.790816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-04 09:33:21.798111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-04 09:33:21.816197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-04 09:33:23.397811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 168MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "\u001b[0mDownloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 45.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 28.2s\u001b[0K\u001b[1G161.3 MiB [] 0% 14.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 7.2s\u001b[0K\u001b[1G161.3 MiB [] 1% 4.7s\u001b[0K\u001b[1G161.3 MiB [] 2% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 3% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 4% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 5% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 6% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 7% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 8% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 9% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 10% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 11% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 13% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 14% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 14% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 15% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 16% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 17% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 18% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 20% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 21% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 22% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 23% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 24% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 25% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 27% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 30% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 31% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 34% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 41% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 43% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 45% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 46% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 47% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 48% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 49% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 50% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 51% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 52% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 53% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 56% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 57% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 58% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 59% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 61% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 18.4s\u001b[0K\u001b[1G100.9 MiB [] 0% 14.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 9.6s\u001b[0K\u001b[1G100.9 MiB [] 1% 4.2s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.8s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.3s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 7% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 8% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 12% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 13% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 15% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 16% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 20% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 22% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 23% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 27% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 29% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 31% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 33% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 35% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 36% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 37% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 38% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 47% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 52% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 56% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 24.8s\u001b[0K\u001b[1G87.6 MiB [] 0% 14.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 8.9s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.1s\u001b[0K\u001b[1G87.6 MiB [] 3% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 6% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 8% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 9% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 10% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 13% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 16% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 17% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 19% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 22% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 24% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 28% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 29% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 30% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 31% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 33% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 34% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 35% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 37% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 38% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 39% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.7s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.8s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.9s\u001b[0K\u001b[1G87.6 MiB [] 44% 3.0s\u001b[0K\u001b[1G87.6 MiB [] 45% 3.0s\u001b[0K\u001b[1G87.6 MiB [] 45% 3.1s\u001b[0K\u001b[1G87.6 MiB [] 47% 2.9s\u001b[0K\u001b[1G87.6 MiB [] 49% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 51% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 52% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 53% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 54% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 56% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 57% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 60% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 62% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 64% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 66% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 68% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 70% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 71% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 73% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 75% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 79% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 81% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 85% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 90% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 92% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 98% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 28.3s\u001b[0K\u001b[1G95.5 MiB [] 0% 14.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 8.6s\u001b[0K\u001b[1G95.5 MiB [] 1% 4.5s\u001b[0K\u001b[1G95.5 MiB [] 3% 2.8s\u001b[0K\u001b[1G95.5 MiB [] 4% 2.2s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.0s\u001b[0K\u001b[1G95.5 MiB [] 6% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 8% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 9% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.9s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 14% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 15% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 17% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 20% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 21% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 27% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 30% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 32% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 33% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 35% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 36% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 37% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 38% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 39% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 40% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 42% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 43% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 45% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 47% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 48% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 50% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 51% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 53% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 54% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 55% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 56% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 57% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 59% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 62% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 67% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 69% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 72% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 73% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 78% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 79% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 80% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 86% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 94% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 27% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 57% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 96% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "TAVILY_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ac3488-21c2-4d55-b70a-fd70c3aeeb59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "cohere_reranker = Cohere(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    # Query expansion with related terms and synonyms\n",
        "    related_terms = get_related_terms(query)\n",
        "    if related_terms:\n",
        "        enhanced_query += f\" related_terms:{', '.join(related_terms)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def get_related_terms(query: str) -> List[str]:\n",
        "    # Use an ontology or knowledge graph to identify related concepts and terms\n",
        "    related_terms = {\n",
        "        \"cyber attack\": [\"hacking\", \"data breach\", \"malware\", \"ransomware\"],\n",
        "        \"threat actor\": [\"cyber gang\", \"hacker group\", \"APT\"],\n",
        "        \"vulnerability\": [\"exploit\", \"CVE\", \"security flaw\"],\n",
        "        \"phishing\": [\"spear phishing\", \"email scam\", \"social engineering\"],\n",
        "        # Add more related terms as needed\n",
        "    }\n",
        "\n",
        "    # Find related terms for the query\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = []\n",
        "    for term in query_terms:\n",
        "        if term in related_terms:\n",
        "            found_terms.extend(related_terms[term])\n",
        "\n",
        "    return found_terms\n",
        "\n",
        "# Entity-Aware Query Expansion\n",
        "def advanced_query_expansion(query: str, entities: Dict[str, List[str]]) -> str:\n",
        "    # Contextual query expansion with advanced techniques\n",
        "    expanded_terms = []\n",
        "\n",
        "    # Threat Actor Specific Expansion\n",
        "    if entities['threat_actors']:\n",
        "        expanded_terms.extend([\n",
        "            f\"\\\"{'|'.join(entities['threat_actors'])}\\\"\",\n",
        "            \"cyber threat group\",\n",
        "            \"threat intelligence\",\n",
        "            \"cyber gang\",\n",
        "            \"APT group\"\n",
        "        ])\n",
        "\n",
        "    # Location-Based Context\n",
        "    if entities['locations']:\n",
        "        expanded_terms.extend([\n",
        "            f\"\\\"{'|'.join(entities['locations'])}\\\"\",\n",
        "            \"geopolitical cyber threat\",\n",
        "            \"regional cyber incidents\"\n",
        "        ])\n",
        "\n",
        "    # Time-Based Filtering\n",
        "    current_year = datetime.now().year\n",
        "    time_filters = [\n",
        "        f\"after:{current_year-1}\",\n",
        "        \"recent developments\",\n",
        "        \"latest cyber incidents\"\n",
        "    ]\n",
        "\n",
        "    # Combine Advanced Techniques\n",
        "    advanced_query = (\n",
        "        f\"{query} \" +\n",
        "        \" \".join(expanded_terms) +\n",
        "        \" \" +\n",
        "        \" \".join(time_filters)\n",
        "    )\n",
        "\n",
        "    return advanced_query\n",
        "\n",
        "# Enhanced Semantic Similarity and Contextual Reranking\n",
        "def advanced_semantic_reranking(query: str, results: List[SearchResult], embeddings_model) -> List[SearchResult]:\n",
        "    # Use more advanced embedding model\n",
        "    query_embedding = embeddings_model.embed_query(query)\n",
        "\n",
        "    scored_results = []\n",
        "    for result in results:\n",
        "        # Combine multiple text sources for richer embedding\n",
        "        full_text = f\"{result.title} {result.snippet} {result.url}\"\n",
        "        result_embedding = embeddings_model.embed_query(full_text)\n",
        "\n",
        "        # Advanced similarity calculation\n",
        "        semantic_score = util.pytorch_cos_sim(query_embedding, result_embedding).item()\n",
        "\n",
        "        # Combine multiple scoring factors\n",
        "        final_score = (\n",
        "            semantic_score * 0.4 +  # Semantic relevance\n",
        "            (result.source_weight or 0.5) * 0.3 +  # Source credibility\n",
        "            calculate_recency_score(parse_date(result.date)) * 0.3  # Temporal relevance\n",
        "        )\n",
        "\n",
        "        scored_results.append((final_score, result))\n",
        "\n",
        "    # Sort by comprehensive score\n",
        "    scored_results.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [result for _, result in scored_results]\n",
        "\n",
        "# Cohere Reranking for Most Relevant and Up-to-Date Information\n",
        "def cohere_rerank(query: str, results: List[SearchResult]) -> List[SearchResult]:\n",
        "    # Prepare the inputs for Cohere reranking\n",
        "    inputs = [(query, result.title + \" \" + result.snippet) for result in results]\n",
        "    reranked_results = cohere_reranker.rerank(inputs)\n",
        "\n",
        "    # Sort results based on Cohere's reranking scores\n",
        "    sorted_results = sorted(reranked_results, key=lambda x: x[1], reverse=True)\n",
        "    return [result for result, _ in sorted_results]\n",
        "\n",
        "# Specialized Threat Actor Intelligence Extraction\n",
        "def extract_threat_actor_intelligence(results: List[SearchResult]) -> Dict[str, Any]:\n",
        "    threat_intel = {\n",
        "        \"name\": None,\n",
        "        \"aliases\": [],\n",
        "        \"confirmed_attacks\": [],\n",
        "        \"techniques\": [],\n",
        "        \"targeted_sectors\": [],\n",
        "        \"geographical_focus\": [],\n",
        "        \"recent_activities\": []\n",
        "    }\n",
        "\n",
        "    for result in results:\n",
        "        # Use NER and pattern matching to extract threat actor details\n",
        "        text = f\"{result.title} {result.snippet}\"\n",
        "\n",
        "        # Pattern matching for specific threat actor characteristics\n",
        "        if re.search(r'LunarsGo|Lunar(s)?', text, re.IGNORECASE):\n",
        "            threat_intel['name'] = 'LunarsGo'\n",
        "\n",
        "            # Extract potential aliases\n",
        "            aliases = re.findall(r'\\b([A-Z][a-z]+ (Group|Gang|Threat))\\b', text)\n",
        "            threat_intel['aliases'].extend([a[0] for a in aliases])\n",
        "\n",
        "            # Extract attack techniques\n",
        "            techniques = re.findall(r'\\b(malware|ransomware|phishing|exploit)\\b', text, re.IGNORECASE)\n",
        "            threat_intel['techniques'].extend(techniques)\n",
        "\n",
        "    return threat_intel\n",
        "\n",
        "# Multi-Source Confidence Scoring\n",
        "def calculate_source_confidence(results: List[SearchResult]) -> float:\n",
        "    # Define source reliability hierarchy\n",
        "    SOURCE_RELIABILITY = {\n",
        "        'threat_intelligence_platform': 1.0,\n",
        "        'government_advisory': 0.9,\n",
        "        'cybersecurity_firm_report': 0.85,\n",
        "        'news_media': 0.7,\n",
        "        'blog_post': 0.5\n",
        "    }\n",
        "\n",
        "    # Calculate weighted confidence\n",
        "    total_confidence = sum(\n",
        "        SOURCE_RELIABILITY.get(result.source.lower(), 0.5)\n",
        "        for result in results\n",
        "    ) / len(results)\n",
        "\n",
        "    return total_confidence\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            logging.error(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Advanced entity extraction\n",
        "    entities = extract_entities(query)\n",
        "\n",
        "    # Advanced query expansion\n",
        "    enhanced_query = advanced_query_expansion(query, entities)\n",
        "\n",
        "    # Execute searches with improved parallelism and error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Advanced semantic reranking\n",
        "    reranked_results = advanced_semantic_reranking(\n",
        "        enhanced_query,\n",
        "        combined_results,\n",
        "        embeddings\n",
        "    )\n",
        "\n",
        "    # Cohere reranking for most relevant and up-to-date information\n",
        "    final_reranked_results = cohere_rerank(enhanced_query, reranked_results)\n",
        "\n",
        "    # Extract threat actor intelligence\n",
        "    threat_actor_intel = extract_threat_actor_intelligence(final_reranked_results)\n",
        "\n",
        "    # Calculate source confidence\n",
        "    source_confidence = calculate_source_confidence(final_reranked_results)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in final_reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Threat Intelligence\",\n",
        "        \"results\": final_reranked_results,\n",
        "        \"threat_actor_intel\": threat_actor_intel,\n",
        "        \"source_confidence\": source_confidence,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "       - **Actionable Recommendations**:\n",
        "         - Offer precise, immediate actions and mitigation strategies.\n",
        "         - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "         - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    5. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor capabilities, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "    - **Malware**\n",
        "    - **Ransomware**\n",
        "    - **Vulnerability**\n",
        "    - **Exploit**\n",
        "    - **Phishing**\n",
        "    - **Data Breach**\n",
        "    - **Cyber Attack**\n",
        "    - **Incident Response**\n",
        "    - **MITRE ATT&CK**\n",
        "    - **Indicators of Compromise (IOCs)**\n",
        "    - **Command and Control (C2)**\n",
        "    - **Dates**\n",
        "    - **Times**\n",
        "    - **Trojans**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific', 'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing', 'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK', 'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates', 'Times', 'Trojans'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Highlight important information\n",
        "    important_keywords = [\n",
        "        'Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific',\n",
        "        'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing',\n",
        "        'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK',\n",
        "        'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates',\n",
        "        'Times', 'Trojans'\n",
        "    ]\n",
        "    highlighted_response = highlight_keywords(processed_response, important_keywords)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": highlighted_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {highlighted_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result\n",
        "\n",
        "# Named Entity Recognition (NER) for entity extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "def extract_entities(query: str) -> Dict[str, List[str]]:\n",
        "    ner_results = ner_pipeline(query)\n",
        "    entities = {\n",
        "        \"threat_actors\": [],\n",
        "        \"locations\": [],\n",
        "        \"organizations\": [],\n",
        "        \"dates\": []\n",
        "    }\n",
        "\n",
        "    for entity in ner_results:\n",
        "        entity_text = entity['word'].lower()\n",
        "        entity_label = entity['entity'].lower()\n",
        "\n",
        "        if \"threat\" in entity_label or \"actor\" in entity_label:\n",
        "            entities[\"threat_actors\"].append(entity_text)\n",
        "        elif \"location\" in entity_label or \"geo\" in entity_label:\n",
        "            entities[\"locations\"].append(entity_text)\n",
        "        elif \"organization\" in entity_label:\n",
        "            entities[\"organizations\"].append(entity_text)\n",
        "        elif \"date\" in entity_label:\n",
        "            entities[\"dates\"].append(entity_text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Enhanced query rewriting with entity extraction\n",
        "def enhance_search_query_with_entities(query: str) -> str:\n",
        "    entities = extract_entities(query)\n",
        "    enhanced_query = query\n",
        "\n",
        "    if entities[\"threat_actors\"]:\n",
        "        enhanced_query += f\" threat_actors:{', '.join(entities['threat_actors'])}\"\n",
        "    if entities[\"locations\"]:\n",
        "        enhanced_query += f\" locations:{', '.join(entities['locations'])}\"\n",
        "    if entities[\"organizations\"]:\n",
        "        enhanced_query += f\" organizations:{', '.join(entities['organizations'])}\"\n",
        "    if entities[\"dates\"]:\n",
        "        enhanced_query += f\" dates:{', '.join(entities['dates'])}\"\n",
        "\n",
        "    return enhanced_query"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL",
        "outputId": "bb5fd0fa-32d0-44b3-a26f-74d9964e20de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    enhanced_query = enhance_search_query_with_entities(query)\n",
        "    print(f\"Enhanced Query: {enhanced_query}\")\n",
        "    result = asyncio.run(run_agent(enhanced_query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f43bea3-efbc-4c34-f8a6-5323682821c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Query: Latest Cyber Incidents on LunarsGo Threat Actor?\n",
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor?  after:2023 recent developments latest cyber incidents\n",
            "DEBUG: Raw results from Exa Search: Title: Cyberattack at French hospital exposes health data of 750,000 patients\n",
            "URL: https://www.bleepingcomputer.com/news/security/cyberattack-at-french-hospital-exposes-health-data-of-750-000-patients/\n",
            "ID: https://www.bleepingcomputer.com/news/security/cyberattack-at-french-hospital-exposes-health-data-of-750-000-patients/\n",
            "Score: 0.15583229064941406\n",
            "Published Date: 2024-11-20T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A data breach at an unnamed French hospital exposed the medical records of 750,000 patients after a threat actor gained access to its electronic patient record system.\n",
            "A threat actor using the nickname 'nears' (previously near2tlg) claimed to have attacked multiple healthcare facilities in France, alleging that they have access to the patient records of over 1,500,000 people.\n",
            "The hacker claims they breached MediBoard by Software Medical Group, a company offering Electronic Patient Record (EPR) solutions across Europe.\n",
            "Softway Medical Group has confirmed that hackers have compromised a MediBoard account. However, it noted that this was not the result of a software vulnerability or misconfiguration on their part, but rather through the use of stolen credentials used by the hospital.\n",
            "In a letter sent to French media and shared with BleepingComputer by LeMagIT's editor-in-chief, Valéry Rieß-Marchive, Softway Medical Group says the exposed data was not directly managed by them, but rather hosted by the hospital.\n",
            "\"On November 19, 2024, a cyberattack was detected within a healthcare facility using the Mediboard software,\" reads the machine-translated email.\n",
            "\"We want to emphasize that the affected health data were not hosted by Softway Medical Group.\"\n",
            " \n",
            "BleepingComputer contacted Softway Medical Group for clarifications on which account and at what level was compromised, and a spokesperson shared the following statement:\n",
            "\"We can confirm that our software is not responsible, but rather, a privileged account within the client's infrastructure was compromised by an individual who exploited the standard functions of the solution,\" the Softway Medical Group told BleepingComputer.\n",
            "\"This hypothesis has been substantiated. It is therefore neither due to improper implementation of the software nor human error.\"\n",
            "Selling access to hospitals\n",
            "This all unfolded after the threat actor began selling what they claimed was access to the MediBoard platform for multiple French hospitals, including Centre Luxembourg, Clinique Alleray-Labrouste, Clinique Jean d'Arc, Clinique Saint-Isabelle, and Hôpital Privé de Thiais.\n",
            "This access allegedly would let the buyer view the hospitals' sensitive healthcare and billing information, patient records, and the ability to schedule and modify appointments or medical records.\n",
            "  Source: BleepingComputer   \n",
            "To prove that they gained access to the MediBoard accounts, the hacker also put the records of 758,912 patients from an unnamed French hospital up for sale.\n",
            "  Source: BleepingComputer   \n",
            "These records allegedly contain the following information:\n",
            " Full name\n",
            "Date of birth\n",
            "Gender\n",
            "Home address\n",
            "Phone number\n",
            "Email address\n",
            "Physician\n",
            "Prescriptions\n",
            "Health card history\n",
            " The data was offered for purchase to three users, and currently, no buyers have been declared on the sale listing.\n",
            "Even if the data isn't sold, there's always a risk of being leaked online for free, making it available to the broader cybercrime community.\n",
            "The type of data exposed in this incident raises the risk of phishing, scamming, and social engineering for impacted people.\n",
            "Highlights: [\"A threat actor using the nickname 'nears' (previously near2tlg) claimed to have attacked multiple healthcare facilities in France, alleging that they have access to the patient records of over 1,500,000 people. The hacker claims they breached MediBoard by Software Medical Group, a company offering Electronic Patient Record (EPR) solutions across Europe. Softway Medical Group has confirmed that hackers have compromised a MediBoard account. However, it noted that this was not the result of a software vulnerability or misconfiguration on their part, but rather through the use of stolen credentials used by the hospital. In a letter sent to French media and shared with BleepingComputer by LeMagIT's editor-in-chief, Valéry Rieß-Marchive, Softway Medical Group says the exposed data was not directly managed by them, but rather hosted by the hospital.\"]\n",
            "Highlight Scores: [0.4921216666698456]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: New Phishing Tool GoIssue Targets GitHub Developers in Bulk Email Campaigns\n",
            "URL: https://thehackernews.com/2024/11/new-phishing-tool-goissue-targets.html\n",
            "ID: https://thehackernews.com/2024/11/new-phishing-tool-goissue-targets.html\n",
            "Score: 0.15503954887390137\n",
            "Published Date: 2024-11-12T14:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJJIxgVlEBGE-FlStO-bFz1JV3E4V7h4cZAfs7UbS0KLXRPhdwFkO4TrBWw5Tr3lg_sDuP6OJWMfof8tSEY-Z7w613eTCL89XW8FrQE9qq04BK-MQfVf-O9jet9culEVzAau9riEHc6AeaeROdSsi9rlDwsLyGZq-LzvekDIWiGZTBhPL-JAmGN02LdDTA/s728-rw-e365/phishing.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Email Security / Threat Intelligence       \n",
            "Cybersecurity researchers are calling attention to a new sophisticated tool called GoIssue that can be used to send phishing messages at scale targeting GitHub users.\n",
            "The program, first marketed by a threat actor named cyberdluffy (aka Cyber D' Luffy) on the Runion forum earlier this August, is advertised as a tool that allows criminal actors to extract email addresses from public GitHub profiles and send bulk emails directly to user inboxes.\n",
            "\"Whether you're aiming to reach a specific audience or expand your outreach, GoIssue offers the precision and power you need,\" the threat actor claimed in their post. \"GoIssue can send bulk emails to GitHub users, directly to their inboxes, targeting any recipient.\"\n",
            " \n",
            "SlashNext said the tool marks a \"dangerous shift in targeted phishing\" that could act as a gateway to source code theft, supply chain attacks, and corporate network breaches via compromised developer credentials.\n",
            "\"Armed with this information, attackers can launch customized mass email campaigns designed to bypass spam filters and target specific developer communities,\" the company said.\n",
            "A custom build of GoIssue is available for $700. Alternatively, purchasers can gain complete access to its source code for $3,000. As of October 11, 2024, the prices have been slashed to $150 and $1,000 for the custom build and the full source code for \"the first 5 customers.\"\n",
            "In a hypothetical attack scenario, a threat actor could use this method to redirect victims to bogus pages that aim to capture their login credentials, download malware, or authorize a rogue OAuth app that requests for access to their private repositories and data.\n",
            "Another facet of cyberdluffy that bears notice is their Telegram profile, where they claim to be a \"member of Gitloker Team.\" Gitloker was previously attributed to a GitHub-focused extortion campaign that involved tricking users into clicking on a booby-trapped link by impersonating GitHub's security and recruitment teams.\n",
            "   \n",
            "The links are sent within email messages that are triggered automatically by GitHub after the developer accounts are tagged in spam comments on random open issues or pull requests using already compromised accounts. The fraudulent pages instruct them to sign in to their GitHub accounts and authorize a new OAuth application to apply for new jobs.\n",
            "Should the inattentive developer grant all the requested permissions to the malicious OAuth app, the threat actors proceed to purge all the repository contents and replace them with a ransom note that urges the victim to contact a persona named Gitloker on Telegram.\n",
            "\"GoIssue's ability to send these targeted emails in bulk allows attackers to scale up their campaigns, impacting thousands of developers at once,\" SlashNext said. \"This increases the risk of successful breaches, data theft, and compromised projects.\"\n",
            "The development comes as Perception Point outlined a new two-step phishing attack that employs Microsoft Visio (.vdsx) files and SharePoint to siphon credentials. The email messages masquerade as a business proposal and are sent from previously breached email accounts to bypass authentication checks.\n",
            " \n",
            "\"Clicking the provided URL in the email body or within the attached .eml file leads the victim to a Microsoft SharePoint page hosting a Visio (.vsdx) file,\" the company said. \"The SharePoint account used to upload and host the .vdsx files is often compromised as well.\"\n",
            "Present within the Visio file is another clickable link that ultimately leads the victim to a fake Microsoft 365 login page with the ultimate goal of harvesting their credentials.\n",
            "\"Two-step phishing attacks leveraging trusted platforms and file formats like SharePoint and Visio are becoming increasingly common,\" Perception Point added. \"These multi-layered evasion tactics exploit user trust in familiar tools while evading detection by standard email security platforms.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['\"Whether you\\'re aiming to reach a specific audience or expand your outreach, GoIssue offers the precision and power you need,\" the threat actor claimed in their post. SlashNext said the tool marks a \"dangerous shift in targeted phishing\" that could act as a gateway to source code theft, supply chain attacks, and corporate network breaches via compromised developer credentials. \"Armed with this information, attackers can launch customized mass email campaigns designed to bypass spam filters and target specific developer communities,\" the company said. A custom build of GoIssue is available for $700. Alternatively, purchasers can gain complete access to its source code for $3,000.']\n",
            "Highlight Scores: [0.2935466468334198]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Researchers Uncover Backdoor in Solana's Popular Web3.js npm Library\n",
            "URL: https://thehackernews.com/2024/12/researchers-uncover-backdoor-in-solanas.html\n",
            "ID: https://thehackernews.com/2024/12/researchers-uncover-backdoor-in-solanas.html\n",
            "Score: 0.14887693524360657\n",
            "Published Date: 2024-12-04T09:48:34.000Z\n",
            "Author: The Hacker News\n",
            "Image: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOtKLyHiBDWbyvczrNBcDGq8vWK2Koy0CS6t04qF3Z0UIK9UfvpJiAfPuP2-WK7lxjOq-EFlzJIWrZoB6biIhADrjutp84GtW0Mv6t4aGx2eylCg_NCaxDGTF5ANnGEvFL_N7fv6GkPA8PwXqlzSvuhck64LSBs9QsJPSm5Ok5tCJhyphenhyphenODrx50Xf415-GMa/s728-rw-e365/bacdoor.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Cybersecurity researchers are alerting to a software supply chain attack targeting the popular @solana/web3.js npm library that involved pushing two malicious versions capable of harvesting users' private keys with an aim to drain their cryptocurrency wallets.\n",
            "The attack has been detected in versions 1.95.6 and 1.95.7. Both these versions are no longer available for download from the npm registry. The package is widely used, attracting over 400,000 weekly downloads.\n",
            "\"These compromised versions contain injected malicious code that is designed to steal private keys from unsuspecting developers and users, potentially enabling attackers to drain cryptocurrency wallets,\" Socket said in a report.\n",
            "@solana/web3.js is an npm package that can be used to interact with the Solana JavaScript software development kit (SDK) for building Node.js and web apps.\n",
            " \n",
            "According to Datadog security researcher Christophe Tafani-Dereeper, \"the backdoor inserted in v1.95.7 adds an 'addToQueue' function which exfiltrates the private key through seemingly-legitimate CloudFlare headers\" and that \"calls to this function are then inserted in various places that (legitimately) access the private key.\"\n",
            "The command-and-control (C2) server to which the keys are exfiltrated to (\"sol-rpc[.]xyz\") is currently down. It was registered on November 22, 2024, on domain registrar NameSilo.\n",
            "It's suspected that the maintainers of the npm package fell victim to a phishing attack that allowed the threat actors to seize control of the accounts and publish the rogue versions.\n",
            "\"A publish-access account was compromised for @solana/web3.js, a JavaScript library that is commonly used by Solana dApps,\" Steven Luscher, one of the library maintainers, said in the release notes for version 1.95.8.\n",
            "\"This allowed an attacker to publish unauthorized and malicious packages that were modified, allowing them to steal private key material and drain funds from dApps, like bots, that handle private keys directly. This issue should not affect non-custodial wallets, as they generally do not expose private keys during transactions.\"\n",
            "Luscher also noted that the incident only impacts projects that directly handle private keys and that were updated within the window of 3:20 p.m. UTC and 8:25 p.m. UTC on December 2, 2024.\n",
            "Users who are relying on @solana/web3.js as a dependency are advised to update to the latest version as soon as possible, and optionally rotate their authority keys if they suspect they are compromised.\n",
            "The disclosure comes days after Socket warned of a bogus Solana-themed npm package named solana-systemprogram-utils that's designed to sneakily reroute a user's funds to an attacker-controlled hard-coded wallet address in 2% of transactions.\n",
            " \n",
            "\"The code cleverly masks its intent by functioning normally 98% of the time,\" the Socket Research Team said. \"This design minimizes suspicion while still allowing the attacker to siphon funds.\"\n",
            "It also follows the discovery of npm packages such as crypto-keccak, crypto-jsonwebtoken, and crypto-bignumber that masquerade as legitimate libraries but contain code to siphon credentials and cryptocurrency wallet data, once again highlighting how threat actors are continuing to abuse the trust developers place in the open-source ecosystem.\n",
            "\"The malware threatens individual developers by stealing their credentials and wallet data, which can lead to direct financial losses,\" security researcher Kirill Boychenko noted. \"For organizations, compromised systems create vulnerabilities that can spread throughout enterprise environments, enabling widespread exploitation.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['@solana/web3.js is an npm package that can be used to interact with the Solana JavaScript software development kit (SDK) for building Node.js and web apps. The command-and-control (C2) server to which the keys are exfiltrated to (\"sol-rpc[. ]xyz\") is currently down. It was registered on November 22, 2024, on domain registrar NameSilo. It\\'s suspected that the maintainers of the npm package fell victim to a phishing attack that allowed the threat actors to seize control of the accounts and publish the rogue versions.']\n",
            "Highlight Scores: [0.41128721833229065]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Operation Lunar Peek: More Than 2,000 Palo Alto Network Firewalls Hacked\n",
            "URL: https://hackread.com/operation-lunar-peek-palo-alto-firewalls-hacked/\n",
            "ID: https://hackread.com/operation-lunar-peek-palo-alto-firewalls-hacked/\n",
            "Score: 0.14676660299301147\n",
            "Published Date: 2024-11-22T18:31:27.000Z\n",
            "Author: Waqas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The Shadowserver Foundation reports over 2,000 Palo Alto Networks firewalls have been hacked via two zero-day vulnerabilities: CVE-2024-0012 &amp; CVE-2024-9474, enabling admin bypass and root access. Top targets: US &amp; India.\n",
            "Cybersecurity researchers at Shadowserver have revealed that approximately 2,000  Palo Alto Networks  firewalls have been compromised. The breaches leverage two recently identified zero-day vulnerabilities in the company’s PAN-OS software. These vulnerabilities have been labeled as CVE-2024-0012 and CVE-2024-9474.\n",
            " The Vulnerabilities \n",
            "   CVE-2024-0012  : This vulnerability is an authentication bypass in the PAN-OS management web interface. It allows remote attackers to gain administrator privileges without authentication. This means attackers can tamper with firewall settings, making them more susceptible to further exploitation.\n",
            "   CVE-2024-9474  : This flaw is a privilege escalation issue. Once exploited, it lets attackers execute commands with root privileges, giving them full control over the compromised firewall.\n",
            "  Heads-up! Thanks to collaboration with the Saudi NCA\n",
            "we are now scanning &amp; reporting Palo Alto Networks devices COMPROMISED as a result of a CVE-2024-0012/CVE-2024-9474 campaign.\n",
            "Found ~2000 instances compromised on 2024-11-20:\n",
            "dashboard.shadowserver.org/statistics/c…\n",
            "Top affected: US &amp; India — The Shadowserver Foundation (@shadowserver.bsky.social) November 21, 2024 at 9:45 AM \n",
            " Operation Lunar Peek – Ongoing Threat Activity \n",
            "Palo Alto Networks has  named  the initial exploitation of these vulnerabilities “Operation Lunar Peek.” Palo Alto Networks initially warned customers on November 8 about restricting access to their next-generation firewalls due to an unspecified remote code execution flaw. \n",
            "Since then, the company has observed a notable increase in threat activity following the public release of technical insights by third-party researchers on November 19, 2024. \n",
            "Unit 42, Palo Alto Networks threat intelligence team, assesses with moderate to high confidence that a functional exploit chaining CVE-2024-0012 and CVE-2024-9474 is publicly available, which could lead to broader threat activity.\n",
            "The company is currently investigating the ongoing attacks, which involve chaining these two vulnerabilities to target a limited number of device management web interfaces. The company has observed threat actors dropping malware and executing commands on compromised firewalls, indicating that a chain exploit is likely already in use.\n",
            " Recommendations for Users \n",
            "Palo Alto Networks has provided several recommendations to mitigate the risk:\n",
            " Monitor and Review: Users should monitor for any suspicious or abnormal activity on devices with a management web interface exposed to the internet. After applying the patch, it is crucial to review firewall configurations and audit logs for any signs of unauthorized administrator activity.\n",
            " Patch Immediately: Customers are advised to update their systems to receive the latest patches that fix CVE-2024-0012 and CVE-2024-9474. Detailed information about affected products and versions can be found in the Palo Alto Networks Security Advisories.\n",
            " Restrict Access: To reduce the risk, Palo Alto Networks recommends restricting access to the management web interface to only trusted internal IP addresses. This is in line with their recommended best practice deployment guidelines.\n",
            " Expert Insights \n",
            "  Elad Luz , Head of Research at Oasis Security, emphasizes the importance of immediate action even before patching. He advises affected customers to restrict access to the web management interface, preferably allowing only internal IPs. Luz also stresses the need to ensure that devices are free from any potential malware or malicious configurations after patching.\n",
            " Palo Alto Patches 0-Day Exploited by Python Backdoor \n",
            " Authentication bypass Flaw found in NATO approved firewall \n",
            " Critical RCE Vulnerability Puts 330,000 Fortinet Firewalls at Risk \n",
            " CISA Urges Patching of Palo Alto Networks’ Expedition Tool Flaw \n",
            " Backdoor account found in 100K+ Zyxel Firewalls, VPN Gateways\n",
            "Highlights: ['Unit 42, Palo Alto Networks threat intelligence team, assesses with moderate to high confidence that a functional exploit chaining CVE-2024-0012 and CVE-2024-9474 is publicly available, which could lead to broader threat activity. The company is currently investigating the ongoing attacks, which involve chaining these two vulnerabilities to target a limited number of device management web interfaces. The company has observed threat actors dropping malware and executing commands on compromised firewalls, indicating that a chain exploit is likely already in use. Palo Alto Networks has provided several recommendations to mitigate the risk:  Monitor and Review: Users should monitor for any suspicious or abnormal activity on devices with a management web interface exposed to the internet.']\n",
            "Highlight Scores: [0.39236992597579956]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: PyPI Python Library \"aiocpa\" Found Exfiltrating Crypto Keys via Telegram Bot\n",
            "URL: https://thehackernews.com/2024/11/pypi-python-library-aiocpa-found.html\n",
            "ID: https://thehackernews.com/2024/11/pypi-python-library-aiocpa-found.html\n",
            "Score: 0.14453084766864777\n",
            "Published Date: 2024-11-25T13:54:46.000Z\n",
            "Author: The Hacker News\n",
            "Image: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeJ5oQ86l0j222iIVUITZZT3yAHUBhpToQdOJiQxsxzwZ-nBzNBqMXJUa6V2yb8F-xUtbC0WecIrMT2LmXq6HDz96ALmv2IEfWZER1LdENzPIAhn8PJPR1gxRInjrD62wX2i-GMNTroA71nbAmcWC7LrAwxvwcnxbAjoBWKGAohZ29uVMCkNqWNZkofrH5/s728-rw-e365/python.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Software Supply Chain / Malware       \n",
            "The administrators of the Python Package Index (PyPI) repository have quarantined the package \"aiocpa\" following a new update that included malicious code to exfiltrate private keys via Telegram.\n",
            "The package in question is described as a synchronous and asynchronous Crypto Pay API client. The package, originally released in September 2024, has been downloaded 12,100 times to date.\n",
            "By putting the Python library in quarantine, it prevents further installation by clients and cannot be modified by its maintainers. \n",
            "Cybersecurity outfit Phylum, which shared details of the software supply chain attack last week, said the author of the package published the malicious update to PyPI, while keeping the library's GitHub repository clean in an attempt to evade detection.\n",
            " \n",
            "It's currently not clear if the original developer was behind the rogue update or if their credentials were compromised by a different threat actor.\n",
            "Signs of malicious activity were first spotted in version 0.1.13 of the library, which included a change to the Python script \"sync.py\" that's designed to decode and run an obfuscated blob of code immediately after the package is installed.\n",
            "   \n",
            "\"This particular blob is recursively encoded and compressed 50 times,\" Phylum said, adding it's used to capture and transmit the victim's Crypto Pay API token using a Telegram bot.\n",
            "It's worth noting that Crypto Pay is advertised as a payment system based on Crypto Bot (@CryptoBot) that allows users to accept payments in crypto and transfer coins to users using the API.\n",
            "The incident is significant, not least because it highlights the importance of scanning the package's source code prior to downloading them, as opposed to just checking their associated repositories.\n",
            "\"As evidenced here, attackers can deliberately maintain clean source repos while distributing malicious packages to the ecosystems,\" the company said, adding the attack \"serves as a reminder that a package's previous safety record doesn't guarantee its continued security.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['Cybersecurity outfit Phylum, which shared details of the software supply chain attack last week, said the author of the package published the malicious update to PyPI, while keeping the library\\'s GitHub repository clean in an attempt to evade detection. It\\'s currently not clear if the original developer was behind the rogue update or if their credentials were compromised by a different threat actor. Signs of malicious activity were first spotted in version 0.1.13 of the library, which included a change to the Python script \"sync.py\" that\\'s designed to decode and run an obfuscated blob of code immediately after the package is installed. \"This particular blob is recursively encoded and compressed 50 times,\" Phylum said, adding it\\'s used to capture and transmit the victim\\'s Crypto Pay API token using a Telegram bot. It\\'s worth noting that Crypto Pay is advertised as a payment system based on Crypto Bot (@CryptoBot) that allows users to accept payments in crypto and transfer coins to users using the API.']\n",
            "Highlight Scores: [0.41240769624710083]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres a recent cyber incident involving the LunarsGo threat actor after 2023:\n",
            "Resolved Search Type: 2024-11-05T11:12:23.404Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Cohere' object has no attribute 'rerank'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8d926496f76c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menhanced_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhance_search_query_with_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enhanced Query: {enhanced_query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhanced_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-aaf120829891>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query, memory)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m         async for chunk in self.astream(\n\u001b[0m\u001b[1;32m   1983\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mastream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m                     async for _ in runner.atick(\n\u001b[0m\u001b[1;32m   1868\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36matick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 await arun_with_retry(\n\u001b[0m\u001b[1;32m    223\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_astream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, writer)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-fc1ad7de0239>\u001b[0m in \u001b[0;36mexecute_searches\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Cohere reranking for most relevant and up-to-date information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mfinal_reranked_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere_rerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhanced_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreranked_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Extract threat actor intelligence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c1e5bb08231a>\u001b[0m in \u001b[0;36mcohere_rerank\u001b[0;34m(query, results)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Prepare the inputs for Cohere reranking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnippet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mreranked_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere_reranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;31m# Sort results based on Cohere's reranking scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    890\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Cohere' object has no attribute 'rerank'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}