{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "7689dd01-b199-45cc-f48f-be767e937d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.6/249.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 4.86MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 13.1MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 25.4MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 26.7MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.23MB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 10.7MB/s]\n",
            "2024-12-04 09:33:21.766704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-04 09:33:21.790816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-04 09:33:21.798111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-04 09:33:21.816197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-04 09:33:23.397811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 168MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "\u001b[0mDownloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 45.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 28.2s\u001b[0K\u001b[1G161.3 MiB [] 0% 14.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 7.2s\u001b[0K\u001b[1G161.3 MiB [] 1% 4.7s\u001b[0K\u001b[1G161.3 MiB [] 2% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 3% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 4% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 5% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 6% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 7% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 8% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 9% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 10% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 11% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 13% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 14% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 14% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 15% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 16% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 17% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 18% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 20% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 21% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 22% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 23% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 24% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 25% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 27% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 30% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 31% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 34% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 41% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 43% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 45% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 46% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 47% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 48% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 49% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 50% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 51% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 52% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 53% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 56% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 57% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 58% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 59% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 61% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 18.4s\u001b[0K\u001b[1G100.9 MiB [] 0% 14.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 9.6s\u001b[0K\u001b[1G100.9 MiB [] 1% 4.2s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.8s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.3s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 7% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 8% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 12% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 13% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 15% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 16% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 20% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 22% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 23% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 27% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 29% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 31% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 33% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 35% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 36% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 37% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 38% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 47% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 52% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 56% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 24.8s\u001b[0K\u001b[1G87.6 MiB [] 0% 14.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 8.9s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.1s\u001b[0K\u001b[1G87.6 MiB [] 3% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 6% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 8% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 9% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 10% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 13% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 16% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 17% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 19% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 22% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 24% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 28% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 29% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 30% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 31% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 33% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 34% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 35% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 37% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 38% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 39% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 40% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 42% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 43% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.7s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.8s\u001b[0K\u001b[1G87.6 MiB [] 44% 2.9s\u001b[0K\u001b[1G87.6 MiB [] 44% 3.0s\u001b[0K\u001b[1G87.6 MiB [] 45% 3.0s\u001b[0K\u001b[1G87.6 MiB [] 45% 3.1s\u001b[0K\u001b[1G87.6 MiB [] 47% 2.9s\u001b[0K\u001b[1G87.6 MiB [] 49% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 51% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 52% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 53% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 54% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 56% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 57% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 60% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 62% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 64% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 66% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 68% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 70% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 71% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 73% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 75% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 79% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 81% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 85% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 90% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 92% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 98% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 28.3s\u001b[0K\u001b[1G95.5 MiB [] 0% 14.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 8.6s\u001b[0K\u001b[1G95.5 MiB [] 1% 4.5s\u001b[0K\u001b[1G95.5 MiB [] 3% 2.8s\u001b[0K\u001b[1G95.5 MiB [] 4% 2.2s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.0s\u001b[0K\u001b[1G95.5 MiB [] 6% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 8% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 9% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.9s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 14% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 15% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 17% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 20% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 21% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 27% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 30% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 32% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 33% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 35% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 36% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 37% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 38% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 39% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 40% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 42% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 43% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 45% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 47% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 48% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 50% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 51% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 53% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 54% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 55% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 56% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 57% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 59% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 62% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 67% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 69% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 72% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 73% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 78% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 79% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 80% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 86% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 94% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 27% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 57% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 96% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "TAVILY_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6693830-2b72-445f-d1f5-fc3e314a2d56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "cohere_reranker = Cohere(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    # Query expansion with related terms and synonyms\n",
        "    related_terms = get_related_terms(query)\n",
        "    if related_terms:\n",
        "        enhanced_query += f\" related_terms:{', '.join(related_terms)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def get_related_terms(query: str) -> List[str]:\n",
        "    # Use an ontology or knowledge graph to identify related concepts and terms\n",
        "    related_terms = {\n",
        "        \"cyber attack\": [\"hacking\", \"data breach\", \"malware\", \"ransomware\"],\n",
        "        \"threat actor\": [\"cyber gang\", \"hacker group\", \"APT\"],\n",
        "        \"vulnerability\": [\"exploit\", \"CVE\", \"security flaw\"],\n",
        "        \"phishing\": [\"spear phishing\", \"email scam\", \"social engineering\"],\n",
        "        # Add more related terms as needed\n",
        "    }\n",
        "\n",
        "    # Find related terms for the query\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = []\n",
        "    for term in query_terms:\n",
        "        if term in related_terms:\n",
        "            found_terms.extend(related_terms[term])\n",
        "\n",
        "    return found_terms\n",
        "\n",
        "# Entity-Aware Query Expansion\n",
        "def advanced_query_expansion(query: str, entities: Dict[str, List[str]]) -> str:\n",
        "    # Contextual query expansion with advanced techniques\n",
        "    expanded_terms = []\n",
        "\n",
        "    # Threat Actor Specific Expansion\n",
        "    if entities['threat_actors']:\n",
        "        expanded_terms.extend([\n",
        "            f\"\\\"{'|'.join(entities['threat_actors'])}\\\"\",\n",
        "            \"cyber threat group\",\n",
        "            \"threat intelligence\",\n",
        "            \"cyber gang\",\n",
        "            \"APT group\"\n",
        "        ])\n",
        "\n",
        "    # Location-Based Context\n",
        "    if entities['locations']:\n",
        "        expanded_terms.extend([\n",
        "            f\"\\\"{'|'.join(entities['locations'])}\\\"\",\n",
        "            \"geopolitical cyber threat\",\n",
        "            \"regional cyber incidents\"\n",
        "        ])\n",
        "\n",
        "    # Time-Based Filtering\n",
        "    current_year = datetime.now().year\n",
        "    time_filters = [\n",
        "        f\"after:{current_year-1}\",\n",
        "        \"recent developments\",\n",
        "        \"latest cyber incidents\"\n",
        "    ]\n",
        "\n",
        "    # Combine Advanced Techniques\n",
        "    advanced_query = (\n",
        "        f\"{query} \" +\n",
        "        \" \".join(expanded_terms) +\n",
        "        \" \" +\n",
        "        \" \".join(time_filters)\n",
        "    )\n",
        "\n",
        "    return advanced_query\n",
        "\n",
        "# Enhanced Semantic Similarity and Contextual Reranking\n",
        "def advanced_semantic_reranking(query: str, results: List[SearchResult], embeddings_model) -> List[SearchResult]:\n",
        "    # Use more advanced embedding model\n",
        "    query_embedding = embeddings_model.embed_query(query)\n",
        "\n",
        "    scored_results = []\n",
        "    for result in results:\n",
        "        # Combine multiple text sources for richer embedding\n",
        "        full_text = f\"{result.title} {result.snippet} {result.url}\"\n",
        "        result_embedding = embeddings_model.embed_query(full_text)\n",
        "\n",
        "        # Advanced similarity calculation\n",
        "        semantic_score = util.pytorch_cos_sim(query_embedding, result_embedding).item()\n",
        "\n",
        "        # Combine multiple scoring factors\n",
        "        final_score = (\n",
        "            semantic_score * 0.4 +  # Semantic relevance\n",
        "            (result.source_weight or 0.5) * 0.3 +  # Source credibility\n",
        "            calculate_recency_score(parse_date(result.date)) * 0.3  # Temporal relevance\n",
        "        )\n",
        "\n",
        "        scored_results.append((final_score, result))\n",
        "\n",
        "    # Sort by comprehensive score\n",
        "    scored_results.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [result for _, result in scored_results]\n",
        "\n",
        "# Cohere Reranking for Most Relevant and Up-to-Date Information\n",
        "def cohere_rerank(query: str, results: List[SearchResult]) -> List[SearchResult]:\n",
        "    # Prepare the inputs for Cohere reranking\n",
        "    inputs = [(query, result.title + \" \" + result.snippet) for result in results]\n",
        "    reranked_results = cohere_reranker.rerank(inputs)\n",
        "\n",
        "    # Sort results based on Cohere's reranking scores\n",
        "    sorted_results = sorted(reranked_results, key=lambda x: x[1], reverse=True)\n",
        "    return [result for result, _ in sorted_results]\n",
        "\n",
        "# Specialized Threat Actor Intelligence Extraction\n",
        "def extract_threat_actor_intelligence(results: List[SearchResult]) -> Dict[str, Any]:\n",
        "    threat_intel = {\n",
        "        \"name\": None,\n",
        "        \"aliases\": [],\n",
        "        \"confirmed_attacks\": [],\n",
        "        \"techniques\": [],\n",
        "        \"targeted_sectors\": [],\n",
        "        \"geographical_focus\": [],\n",
        "        \"recent_activities\": []\n",
        "    }\n",
        "\n",
        "    for result in results:\n",
        "        # Use NER and pattern matching to extract threat actor details\n",
        "        text = f\"{result.title} {result.snippet}\"\n",
        "\n",
        "        # Pattern matching for specific threat actor characteristics\n",
        "        if re.search(r'LunarsGo|Lunar(s)?', text, re.IGNORECASE):\n",
        "            threat_intel['name'] = 'LunarsGo'\n",
        "\n",
        "            # Extract potential aliases\n",
        "            aliases = re.findall(r'\\b([A-Z][a-z]+ (Group|Gang|Threat))\\b', text)\n",
        "            threat_intel['aliases'].extend([a[0] for a in aliases])\n",
        "\n",
        "            # Extract attack techniques\n",
        "            techniques = re.findall(r'\\b(malware|ransomware|phishing|exploit)\\b', text, re.IGNORECASE)\n",
        "            threat_intel['techniques'].extend(techniques)\n",
        "\n",
        "    return threat_intel\n",
        "\n",
        "# Multi-Source Confidence Scoring\n",
        "def calculate_source_confidence(results: List[SearchResult]) -> float:\n",
        "    # Define source reliability hierarchy\n",
        "    SOURCE_RELIABILITY = {\n",
        "        'threat_intelligence_platform': 1.0,\n",
        "        'government_advisory': 0.9,\n",
        "        'cybersecurity_firm_report': 0.85,\n",
        "        'news_media': 0.7,\n",
        "        'blog_post': 0.5\n",
        "    }\n",
        "\n",
        "    # Calculate weighted confidence\n",
        "    total_confidence = sum(\n",
        "        SOURCE_RELIABILITY.get(result.source.lower(), 0.5)\n",
        "        for result in results\n",
        "    ) / len(results)\n",
        "\n",
        "    return total_confidence\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            logging.error(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Advanced entity extraction\n",
        "    entities = extract_entities(query)\n",
        "\n",
        "    # Advanced query expansion\n",
        "    enhanced_query = advanced_query_expansion(query, entities)\n",
        "\n",
        "    # Execute searches with improved parallelism and error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Advanced semantic reranking\n",
        "    reranked_results = advanced_semantic_reranking(\n",
        "        enhanced_query,\n",
        "        combined_results,\n",
        "        embeddings\n",
        "    )\n",
        "\n",
        "    # Cohere reranking for most relevant and up-to-date information\n",
        "    final_reranked_results = cohere_rerank(enhanced_query, reranked_results)\n",
        "\n",
        "    # Extract threat actor intelligence\n",
        "    threat_actor_intel = extract_threat_actor_intelligence(final_reranked_results)\n",
        "\n",
        "    # Calculate source confidence\n",
        "    source_confidence = calculate_source_confidence(final_reranked_results)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in final_reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Threat Intelligence\",\n",
        "        \"results\": final_reranked_results,\n",
        "        \"threat_actor_intel\": threat_actor_intel,\n",
        "        \"source_confidence\": source_confidence,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "       - **Actionable Recommendations**:\n",
        "         - Offer precise, immediate actions and mitigation strategies.\n",
        "         - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "         - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    5. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor capabilities, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "    - **Malware**\n",
        "    - **Ransomware**\n",
        "    - **Vulnerability**\n",
        "    - **Exploit**\n",
        "    - **Phishing**\n",
        "    - **Data Breach**\n",
        "    - **Cyber Attack**\n",
        "    - **Incident Response**\n",
        "    - **MITRE ATT&CK**\n",
        "    - **Indicators of Compromise (IOCs)**\n",
        "    - **Command and Control (C2)**\n",
        "    - **Dates**\n",
        "    - **Times**\n",
        "    - **Trojans**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific', 'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing', 'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK', 'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates', 'Times', 'Trojans'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Highlight important information\n",
        "    important_keywords = [\n",
        "        'Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific',\n",
        "        'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing',\n",
        "        'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK',\n",
        "        'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates',\n",
        "        'Times', 'Trojans'\n",
        "    ]\n",
        "    highlighted_response = highlight_keywords(processed_response, important_keywords)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": highlighted_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {highlighted_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result\n",
        "\n",
        "# Named Entity Recognition (NER) for entity extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "def extract_entities(query: str) -> Dict[str, List[str]]:\n",
        "    ner_results = ner_pipeline(query)\n",
        "    entities = {\n",
        "        \"threat_actors\": [],\n",
        "        \"locations\": [],\n",
        "        \"organizations\": [],\n",
        "        \"dates\": []\n",
        "    }\n",
        "\n",
        "    for entity in ner_results:\n",
        "        entity_text = entity['word'].lower()\n",
        "        entity_label = entity['entity'].lower()\n",
        "\n",
        "        if \"threat\" in entity_label or \"actor\" in entity_label:\n",
        "            entities[\"threat_actors\"].append(entity_text)\n",
        "        elif \"location\" in entity_label or \"geo\" in entity_label:\n",
        "            entities[\"locations\"].append(entity_text)\n",
        "        elif \"organization\" in entity_label:\n",
        "            entities[\"organizations\"].append(entity_text)\n",
        "        elif \"date\" in entity_label:\n",
        "            entities[\"dates\"].append(entity_text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Enhanced query rewriting with entity extraction\n",
        "def enhance_search_query_with_entities(query: str) -> str:\n",
        "    entities = extract_entities(query)\n",
        "    enhanced_query = query\n",
        "\n",
        "    if entities[\"threat_actors\"]:\n",
        "        enhanced_query += f\" threat_actors:{', '.join(entities['threat_actors'])}\"\n",
        "    if entities[\"locations\"]:\n",
        "        enhanced_query += f\" locations:{', '.join(entities['locations'])}\"\n",
        "    if entities[\"organizations\"]:\n",
        "        enhanced_query += f\" organizations:{', '.join(entities['organizations'])}\"\n",
        "    if entities[\"dates\"]:\n",
        "        enhanced_query += f\" dates:{', '.join(entities['dates'])}\"\n",
        "\n",
        "    return enhanced_query"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL",
        "outputId": "40aa8d31-df54-4a7b-b4be-95a41ddc1654",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    enhanced_query = enhance_search_query_with_entities(query)\n",
        "    print(f\"Enhanced Query: {enhanced_query}\")\n",
        "    result = asyncio.run(run_agent(enhanced_query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15ed07e6-56e9-4ec2-c688-4fa617d29f3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Query: Latest Cyber Incidents on LunarsGo Threat Actor?\n",
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor?  after:2023 recent developments latest cyber incidents\n",
            "DEBUG: Raw results from Exa Search: Title: Cyber Threat Intelligence Report: December 13th to December 20th, 2023\n",
            "URL: https://krypt3ia.wordpress.com/2023/12/20/cyber-threat-intelligence-report-december-13th-to-december-20th-2023/\n",
            "ID: https://krypt3ia.wordpress.com/2023/12/20/cyber-threat-intelligence-report-december-13th-to-december-20th-2023/\n",
            "Score: 0.16912543773651123\n",
            "Published Date: 2023-12-20T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://krypt3ia.wordpress.com/wp-content/uploads/2023/12/screenshot-2023-12-20-at-3.27.48e280afpm.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This threat intelligence report was created in tandem with ChatGPT4 by Scot Terban using the Icebreaker Threat Intelligence Analyst created by Scot Terban \n",
            "    \n",
            " Threat Actors and Activities: \n",
            " Ransomware Attacks: Ransomware remains a primary threat, with about 2,000 ransomware breach events reported in the first half of 2023. LockBit 3.0 was particularly impactful, accounting for over 500 breaches​ ​.\n",
            " Pro-Russian Hacktivism: Due to the Russia-Ukraine conflict, pro-Russian hacktivism has been prominent, although its activity declined in the second quarter of 2023​ ​.\n",
            " Access Sales: Over 2,000 instances were observed where access vendors offered to sell compromised credentials and unauthorized network or system access​ ​.\n",
            " AI and Law Enforcement Operations: An increase in discussions and activities related to artificial intelligence and law enforcement operations was noted, alongside a decrease in activities related to dump shops, ATM malware, and PoS malware​ ​.\n",
            " Recent Incidents: \n",
            " MongoDB Security Breach: MongoDB disclosed a security incident on December 13, 2023, involving unauthorized access to its corporate systems. This breach resulted in the exposure of customer account metadata and contact information. The attack was attributed to a phishing attack, with the malicious actor using Mullvad VPN to conceal their origins​ ​​ ​.\n",
            " Vulnerabilities: \n",
            " Microsoft Patch Tuesday Updates: Microsoft addressed 33 vulnerabilities in its final Patch Tuesday update for 2023. Four were rated critical, and 29 were important. Notable vulnerabilities included:\n",
            "Windows MSHTML Platform Remote Code Execution Vulnerability (CVE-2023-35628)\n",
            "Internet Connection Sharing Remote Code Execution Vulnerabilities (CVE-2023-35630, CVE-2023-35641)\n",
            "Microsoft Outlook Information Disclosure Vulnerability (CVE-2023-35636)\n",
            "Microsoft ODBC Driver Remote Code Execution Vulnerability (CVE-2023-35639)\n",
            "Microsoft Power Platform Connector Spoofing Vulnerability (CVE-2023-36019)​ ​.\n",
            " DHCP Server Vulnerabilities: Microsoft also addressed vulnerabilities in the Dynamic Host Configuration Protocol (DHCP) server service that could lead to denial-of-service or information disclosure, highlighted by CVE-2023-35638, CVE-2023-35643, and CVE-2023-36012. Akamai’s discovery of new attacks against Active Directory domains using Microsoft DHCP servers accentuated the risks associated with these vulnerabilities​ ​.\n",
            "This report consolidates a range of cyber threat intelligence, highlighting the ongoing risks posed by ransomware, hacktivism, and vulnerabilities in widely-used software like Microsoft’s products. The MongoDB breach serves as a recent example of the consequences of phishing attacks, underlining the need for continued vigilance and robust security measures across all organizations.\n",
            "Links\n",
            "Intel471 Cyber Threat Report 2023: Intel471.com \n",
            "MongoDB Security Breach: TheHackerNews – MongoDB Suffers Security Breach, Exposing Customer Data \n",
            "Microsoft’s Final 2023 Patch Tuesday: TheHackerNews – Microsoft’s Final 2023 Patch Tuesday: 33 Flaws Fixed, Including 4 Critical\n",
            "Highlights: ['This threat intelligence report was created in tandem with ChatGPT4 by Scot Terban using the Icebreaker Threat Intelligence Analyst created by Scot Terban   Ransomware Attacks: Ransomware remains a primary threat, with about 2,000 ransomware breach events reported in the first half of 2023. LockBit 3.0 was particularly impactful, accounting for over 500 breaches\\u200b \\u200b.  Pro-Russian Hacktivism: Due to the Russia-Ukraine conflict, pro-Russian hacktivism has been prominent, although its activity declined in the second quarter of 2023\\u200b \\u200b.  Access Sales: Over 2,000 instances were observed where access vendors offered to sell compromised credentials and unauthorized network or system access\\u200b \\u200b.']\n",
            "Highlight Scores: [0.31629040837287903]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Threat Intelligence Report – November 29, 2023\n",
            "URL: https://krypt3ia.wordpress.com/2023/11/29/threat-intelligence-report-november-29-2023/\n",
            "ID: https://krypt3ia.wordpress.com/2023/11/29/threat-intelligence-report-november-29-2023/\n",
            "Score: 0.16847673058509827\n",
            "Published Date: 2023-11-29T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://krypt3ia.wordpress.com/wp-content/uploads/2023/11/screenshot-2023-11-29-at-10.08.16e280afam.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: I have been working with ChatGPT4 on creating a threat intelligence agent analyst that has been trained, and is still being trained, on a multitude of sources on the net to synthesize threat intelligence reports, generate threat cards on actors, and generally be a Swiss Army knife of sorts in intel collection and dissemination. It is still a work in progress, but, just putting it out there….\n",
            "Assume many others are doing the same, and that is also the case for internal security teams at companies around the globe. Since this is all open source, there may be glitches in the data, and there could be hallucinations or perhaps disinfo/misinfo that ends up in the feeds, but, with weeding and oversight, I think this is a good timesaver for those looking for general threatscape reporting from the net….\n",
            "Your mileage may vary…\n",
            "    \n",
            "Overview\n",
            "This report provides a comprehensive overview of the latest cyber threats and vulnerabilities identified on November 29, 2023. Key highlights include an uptick in ransomware attacks targeting the healthcare sector, a significant rise in Black Friday phishing emails, a novel anti-sandbox technique used by LummaC2 malware, and a zero-day vulnerability in Microsoft Exchange Server.\n",
            "Ransomware Attacks on Healthcare Sector\n",
            " Ardent Health Service Incident: Ardent Health Service, operating 30 hospitals across various states, suffered a ransomware attack leading to the shutdown of numerous IT systems, including healthcare records​ ​.\n",
            " Henry Schein Inc. Targeted: The medical product distributor faced its second ransomware attack within two months by the BlackCat/AlphV gang. The attack may have involved re-encryption of files during stalled ransom negotiations​ ​.\n",
            "Cyber Vulnerability Insights\n",
            " National Vulnerability Database Report: 52% of new software vulnerabilities had a severity score of 7 or more out of 10, with 15% scoring above 9, indicating a high risk of exploitation​ ​.\n",
            " Email Security Analysis: Over 45 billion emails analyzed revealed that more than one-third were unwanted, with 3.6% containing malicious phishing or web links​ ​.\n",
            " Rise in Deepfake Incidents: A tenfold increase in deepfake videos, audio recordings, or documents detected over the past year, posing new challenges in verifying digital authenticity​ ​.\n",
            " iPhone NameDrop Feature Risks: A new feature in iOS 17, NameDrop, poses a privacy risk due to its proximity-based data-sharing capabilities​ ​.\n",
            " Google Chrome Update: An emergency update for Google Chrome (version 119.0.6045.200) has been released to address undisclosed vulnerabilities​ ​.\n",
            "Emerging Malware Techniques\n",
            " LummaC2 Malware: This infostealer malware now utilizes trigonometry to detect automated malware sandboxes, complicating detection and analysis efforts​ ​.\n",
            "Phishing Trends and Precautions\n",
            " Black Friday Phishing Surge: A 237% increase in phishing emails related to Black Friday sales, often mimicking well-known brands to deceive users​ ​.\n",
            " Precautionary Measures:\n",
            "Prefer credit cards over debit cards for online purchases.\n",
            "Implement multi-factor authentication.\n",
            "Use secure networks, avoiding public Wi-Fi.\n",
            "Exercise skepticism towards too-good-to-be-true offers.\n",
            "Trust instincts and avoid suspicious links or sellers​ ​.\n",
            "Critical Zero-Day Vulnerability\n",
            " Microsoft Exchange Server SSRF Vulnerability: An unpatched Server-Side Request Forgery (SSRF) vulnerability in Microsoft Exchange Server allows attackers to manipulate web applications and make unintended requests, posing a significant risk to sensitive information​ ​.\n",
            "Recommendations\n",
            " For Healthcare Organizations: Enhance ransomware defenses and emergency response protocols.\n",
            " IT Departments: Prioritize patching critical vulnerabilities and strengthen email security measures.\n",
            " Organizations Using Microsoft Exchange: Monitor advisories and apply patches once available. Increase vigilance against potential exploitation.\n",
            "Conclusion\n",
            "The cybersecurity landscape on November 29, 2023, is marked by sophisticated attacks and evolving threats. Organizations are advised to stay informed and proactive in implementing robust cybersecurity measures.\n",
            "Highlights: ['I have been working with ChatGPT4 on creating a threat intelligence agent analyst that has been trained, and is still being trained, on a multitude of sources on the net to synthesize threat intelligence reports, generate threat cards on actors, and generally be a Swiss Army knife of sorts in intel collection and dissemination. It is still a work in progress, but, just putting it out there…. Assume many others are doing the same, and that is also the case for internal security teams at companies around the globe. Since this is all open source, there may be glitches in the data, and there could be hallucinations or perhaps disinfo/misinfo that ends up in the feeds, but, with weeding and oversight, I think this is a good timesaver for those looking for general threatscape reporting from the net…. This report provides a comprehensive overview of the latest cyber threats and vulnerabilities identified on November 29, 2023.']\n",
            "Highlight Scores: [0.47507601976394653]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Active Threat Alerts | SpinSafe\n",
            "URL: https://spinsafe.com/active-threat-alerts/\n",
            "ID: https://spinsafe.com/active-threat-alerts/\n",
            "Score: 0.16742441058158875\n",
            "Published Date: 2024-05-11T00:00:00.000Z\n",
            "Author: /\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: #StopRansomware: Black Basta | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  May 11, 2024  by SecureTech           #StopRansomware: Akira Ransomware | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  April 18, 2024 /  by SecureTech           Threat Actors Exploit Multiple Vulnerabilities in Ivanti Connect Secure and Policy Secure Gateways SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  March 2, 2024 /  by SecureTech           #StopRansomware: Phobos Ransomware | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  March 1, 2024 /  by SecureTech           SVR Cyber Actors Adapt Tactics for Initial Cloud Access How SVR-Attributed Actors are Adapting to the Move…  February 27, 2024 /  by SecureTech           Threat Actor Leverages Compromised Account of Former Employee to Access State Government Organization SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  February 15, 2024 /  by SecureTech           PRC State-Sponsored Actors Compromise and Maintain Persistent Access to U.S. Critical Infrastructure SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  February 7, 2024 /  by SecureTech           Known Indicators of Compromise Associated with Androxgh0st Malware SUMMARY\n",
            "The Federal Bureau of Investigation (FBI)…  January 16, 2024 /  by SecureTech           #StopRansomware: Play Ransomware | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  December 21, 2023 /  by SecureTech           #StopRansomware: ALPHV Blackcat | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  December 20, 2023 /  by SecureTech           Enhancing Cyber Resilience: Insights from the CISA Healthcare and Public Health Sector Risk and Vulnerability Assessment SUMMARY\n",
            "In January 2023, the Cybersecurity and Infrastructure…  December 15, 2023 /  by SecureTech           Russian Foreign Intelligence Service (SVR) Exploiting JetBrains TeamCity CVE Globally SUMMARY\n",
            "The U.S. Federal Bureau of Investigation…  December 13, 2023 /  by SecureTech           Russian FSB Cyber Actor Star Blizzard Continues Worldwide Spear-phishing Campaigns The Russia-based actor is targeting organizations…  December 7, 2023 /  by SecureTech           Threat Actors Exploit Adobe ColdFusion CVE-2023-26360 for Initial Access to Government Servers SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  December 5, 2023 /  by SecureTech           IRGC-Affiliated Cyber Actors Exploit PLCs in Multiple Sectors, Including U.S. Water and Wastewater Systems Facilities SUMMARY\n",
            "The Federal Bureau of Investigation (FBI),…  December 2, 2023 /  by SecureTech           #StopRansomware: LockBit 3.0 Ransomware Affiliates Exploit CVE 2023-4966 Citrix Bleed Vulnerability SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  November 22, 2023 /  by SecureTech           Scattered Spider | CISA SUMMARY\n",
            "The Federal Bureau of Investigation (FBI)…  November 17, 2023 /  by SecureTech           #StopRansomware: Rhysida Ransomware | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  November 16, 2023 /  by SecureTech           Threat Actors Exploit Atlassian Confluence CVE-2023-22515 for Initial Access to Networks SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  October 17, 2023 /  by SecureTech           #StopRansomware: AvosLocker Ransomware (Update) | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  October 12, 2023 /  by SecureTech           People’s Republic of China-Linked Cyber Actors Hide in Router Firmware Executive Summary\n",
            "The United States National Security…  September 27, 2023 /  by SecureTech           #StopRansomware: Snatch Ransomware | CISA SUMMARY\n",
            "Note: This joint Cybersecurity Advisory…  September 21, 2023 /  by SecureTech           Multiple Nation-State Threat Actors Exploit CVE-2022-47966 and CVE-2022-42475 SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  September 7, 2023 /  by SecureTech           Identification and Disruption of QakBot Infrastructure SUMMARY\n",
            "The Cybersecurity and Infrastructure Security…  August 30, 2023 /  by SecureTech           2022 Top Routinely Exploited Vulnerabilities SUMMARY\n",
            "The following cybersecurity agencies coauthored…  August 3, 2023 /  by SecureTech\n",
            "Highlights: ['In January 2023, the Cybersecurity and Infrastructure…  December 15, 2023 /  by SecureTech           Russian Foreign Intelligence Service (SVR) Exploiting JetBrains TeamCity CVE Globally SUMMARY The U.S. Federal Bureau of Investigation…  December 13, 2023 /  by SecureTech           Russian FSB Cyber Actor Star Blizzard Continues Worldwide Spear-phishing Campaigns The Russia-based actor is targeting organizations…  December 7, 2023 /  by SecureTech           Threat Actors Exploit Adobe ColdFusion CVE-2023-26360 for Initial Access to Government Servers SUMMARY The Cybersecurity and Infrastructure Security…  December 5, 2023 /  by SecureTech           IRGC-Affiliated Cyber Actors Exploit PLCs in Multiple Sectors, Including U.S. Water and Wastewater Systems Facilities SUMMARY The Federal Bureau of Investigation (FBI),…  December 2, 2023 /  by SecureTech           #StopRansomware: LockBit 3.0 Ransomware Affiliates Exploit CVE 2023-4966 Citrix Bleed Vulnerability SUMMARY Note: This joint Cybersecurity Advisory…  November 22, 2023 /  by SecureTech           Scattered Spider | CISA SUMMARY']\n",
            "Highlight Scores: [0.09246981143951416]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Threat Intelligence Report 12/4/23\n",
            "URL: https://krypt3ia.wordpress.com/2023/12/04/threat-intelligence-report-12-4-23/\n",
            "ID: https://krypt3ia.wordpress.com/2023/12/04/threat-intelligence-report-12-4-23/\n",
            "Score: 0.16656237840652466\n",
            "Published Date: 2023-12-04T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://krypt3ia.wordpress.com/wp-content/uploads/2023/12/screenshot-2023-12-04-at-8.06.11e280afam.png?w=392\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This threat intelligence report was generated by the Icebreaker Intel Analyst Agent in CHtGPT4 as created by Scot Terban \n",
            "Here’s a comprehensive threat intelligence report for December 4, 2023, covering a range of cyber incidents and vulnerabilities:\n",
            " Cyber Av3ngers Group Activities: This group has defaced workstations at Pennsylvania’s Aliquippa municipal water authority. Affiliated with the Iranian Revolutionary Guard Corps, they have targeted multiple U.S. water utility companies by exploiting Unitronics’ PLC devices​ ​.\n",
            " Credit Union Service Disruption: 60 U.S. credit unions were disrupted following a ransomware attack on Ongoing Operations, a cloud hosting provider. The attackers likely exploited the Citrix NetScaler ‘Citrix Bleed’ vulnerability (CVE-2023-4966)​ ​.\n",
            " JAXA Cyberattack: Japan’s space agency, JAXA, reported a cyber-attack. While critical rocket or satellite operations weren’t affected, the breach’s full extent is under investigation​ ​.\n",
            " Booking.com Customer Targeting: Cybercriminals ramped up campaigns against hotels using Booking.com, redirecting customer payments to their accounts through the official app​ ​.\n",
            " Attack on Ziv Hospital Network: Israel’s Ziv hospital in Safed suffered a cyber incident. The Malek Team hacktivist group claimed responsibility, alleging the exfiltration of 500GB of patient data​ ​.\n",
            " National Aerospace Laboratories Ransomware Attack: India’s National Aerospace Laboratories faced a ransomware attack by the LockBit group, with several documents purportedly exfiltrated​ ​.\n",
            " Cryptocurrency Theft from KyberSwap: Over $50 million in cryptocurrency was stolen in an attack on blockchain platform KyberSwap, exploiting a vulnerability to transfer customer funds​ ​.\n",
            " Notable Vulnerabilities:\n",
            "Google Chrome: Seven vulnerabilities, including a critical one (CVE-2023-6345) allowing sandbox escape​ ​.\n",
            "Apple Devices: Patched an information-disclosure vulnerability (CVE-2023-42916) actively exploited in previous iOS versions​ ​.\n",
            "OwnCloud: A large-scale exploitation of a critical information disclosure vulnerability (CVE-2023-49103)​ ​.\n",
            "Zyxel NAS Devices: Six security vulnerabilities, three of which are critical and allow remote code execution (CVE-2023-4473, CVE-2023-4474, CVE-2023-35138)​ ​.\n",
            " Targeted Cyber Espionage and Malware Campaigns:\n",
            "U.S. Aviation Company: Spear phishing campaign for industrial espionage​ ​.\n",
            "Ukrainian Citizens: Remcos RAT infections via malicious court summons emails​ ​.\n",
            "Uzbekistan’s Ministry of Foreign Affairs: Targeted by a modified Gh0st Remote Access Trojan, SugarGh0st​ ​.\n",
            "Lumma Information Stealer Malware: Distributed via a breached website and phishing emails​ ​.\n",
            "This report highlights the diversity and complexity of current cyber threats, ranging from state-affiliated hacktivism to sophisticated ransomware and targeted espionage. It underscores the need for vigilance and robust cybersecurity measures across various sectors and geographies.\n",
            "Highlights: ['It underscores the need for vigilance and robust cybersecurity measures across various sectors and geographies.']\n",
            "Highlight Scores: [0.3436279892921448]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Cybersecurity Incident Tracker\n",
            "URL: https://www.board-cybersecurity.com/incidents/tracker/\n",
            "ID: https://www.board-cybersecurity.com/incidents/tracker/\n",
            "Score: 0.1651308834552765\n",
            "Published Date: 2024-06-12T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://www.board-cybersecurity.com/img/social-share.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Skip to content\n",
            "Accessibility Statement     Page last updated on June 12, 2024 Tracker for cybersecurity incidents reported in an entity’s 8-K.  Total Incidents: 175      Last Update Disclosure Date Company     2024-06-12 2024-05-28  KULICKE &amp; SOFFA INDUSTRIES INC    2024-05-31 2024-05-31  Live Nation Entertainment, Inc.    2024-05-28 2023-12-22  First American Financial Corp    2024-05-28 2024-05-07  BRANDYWINE OPERATING PARTNERSHIP, L.P.    2024-05-09 2024-05-09  KEY TRONIC CORP    2024-05-07 2024-05-07  DocGo Inc.    2024-05-01 2024-05-01  DROPBOX, INC.    2024-04-26 2023-12-18  V F CORP    2024-04-24 2024-02-22  UNITEDHEALTH GROUP INC    2024-04-18 2024-04-18  Frontier Communications Parent, Inc.    2024-04-12 2024-04-12  ORASURE TECHNOLOGIES INC    2024-04-08 2024-04-08  B. Riley Financial, Inc.    2024-04-01 2024-03-12  MARINEMAX INC    2024-03-29 2024-02-09  SouthState Corp    2024-03-20 2024-03-20  RADIANT LOGISTICS, INC    2024-03-08 2024-01-19  MICROSOFT CORP    2024-03-01 2024-03-01  Federal Home Loan Bank of New York    2024-02-27 2024-02-27  Cencora, Inc.    2024-02-26 2024-01-08  loanDepot, Inc.    2024-02-21 2024-02-13  PRUDENTIAL FINANCIAL INC    2024-02-09 2024-02-09  WILLIS LEASE FINANCE CORP    2024-02-02 2020-09-29  BLACKBAUD INC    2024-01-24 2024-01-24  Hewlett Packard Enterprise Co    2024-01-09 2023-11-21  Fidelity National Financial, Inc.    2023-12-21 2023-04-21  MIDDLEFIELD BANC CORP    2023-12-15 2023-11-02  Mr. Cooper Group Inc.    2023-12-13 2023-11-20  LivaNova PLC    2023-12-01 2023-10-10  23andMe Holding Co.    2023-11-29 2023-10-30  Mueller Water Products, Inc.    2023-11-29 2023-11-29  Okta, Inc.    2023-11-21 2023-11-21  Inspired Entertainment, Inc.    2023-11-13 2023-09-27  Johnson Controls International plc    2023-10-30 2020-12-14  SolarWinds Corp    2023-10-26 2023-10-26  REINSURANCE GROUP OF AMERICA INC    2023-10-24 2023-10-24  HENRY SCHEIN INC    2023-10-19 2023-10-11  Simpson Manufacturing Co., Inc.    2023-10-16 2023-10-16  HENRY SCHEIN INC    2023-10-13 2023-02-14  TRICO BANCSHARES /    2023-10-05 2023-10-05  MGM Resorts International    2023-09-21 2023-09-21  PROG Holdings, Inc.    2023-09-18 2023-08-14  CLOROX CO /DE/    2023-09-14 2023-09-14  Caesars Entertainment, Inc.    2023-09-13 2023-09-13  MGM Resorts International    2023-08-11 2023-08-11  FREEPORT-MCMORAN INC    2023-08-11 2023-08-11  COMMUNITY TRUST BANCORP INC /KY/    2023-08-07 2023-08-07  Topgolf Callaway Brands Corp.    2023-08-02 2023-08-02  Garrett Motion Inc.    2023-07-31 2023-07-31  TEMPUR SEALY INTERNATIONAL, INC.    2023-07-26 2023-07-26  MAXIMUS, INC.    2023-07-25 2023-07-25  PACIFIC PREMIER BANCORP INC    2023-07-20 2023-07-20  Paycom Software, Inc.    2023-07-19 2023-06-13  HAYNES INTERNATIONAL INC    2023-07-19 2023-07-19  ESTEE LAUDER COMPANIES INC    2023-07-14 2023-07-14  FIRSTSUN CAPITAL BANCORP    2023-07-14 2023-07-14  Sound Financial Bancorp, Inc.    2023-07-10 2023-07-10  HCA Healthcare, Inc.    2023-07-06 2023-07-06  FIRST COMMONWEALTH FINANCIAL CORP /PA/    2023-07-05 2023-07-05  FIRST MERCHANTS CORP    2023-07-03 2023-07-03  Hilltop Holdings Inc.    2023-06-30 2023-06-30  OPTICAL CABLE CORP    2023-06-30 2023-06-30  F&amp;G Annuities &amp; Life, Inc.    2023-06-27 2023-06-27  COLUMBIA BANKING SYSTEM, INC.    2023-06-26 2023-06-26  Corebridge Financial, Inc.    2023-06-26 2023-06-26  Jackson Financial Inc.    2023-06-22 2023-06-22  GENWORTH FINANCIAL INC    2023-06-13 2023-06-13  BRUNSWICK CORP    2023-06-05 2023-06-05  PROGRESS SOFTWARE CORP /MA    2023-05-30 2023-04-13  ENZO BIOCHEM INC    2023-05-26 2023-05-26  Polished.com Inc.    2023-05-05 2023-04-03  WESTERN DIGITAL CORP    2023-05-01 2023-05-01  AMERICOLD REALTY TRUST    2023-04-17 2023-04-17  NCR VOYIX Corp    2023-04-12 2023-04-12  HEICO CORP    2023-04-11 2023-01-06  SIFCO INDUSTRIES INC    2023-04-06 2022-11-07  PGT Innovations, Inc.    2023-03-27 2023-03-27  Level 3 Parent, LLC    2023-03-14 2023-03-14  QUICKLOGIC Corp    2023-03-10 2023-03-10  ARGAN INC    2023-03-02 2023-03-02  ULTRALIFE CORP    2023-02-28 2023-02-28  DISH DBS CORP    2023-02-24 2023-02-24  NATURES SUNSHINE PRODUCTS INC    2023-02-23 2023-02-23  Citi Trends Inc    2023-02-14 2023-02-14  Gates Industrial Corp plc    2023-02-13 2023-02-13  COMMUNITY HEALTH SYSTEMS INC    2023-02-13 2023-02-06  MKS INSTRUMENTS INC    2023-02-07 2023-02-07  A10 Networks, Inc.    2023-01-19 2023-01-19  T-Mobile US, Inc.    2023-01-19 2023-01-19  YUM BRANDS INC    2022-12-19 2022-12-19  PROGRESS SOFTWARE CORP /MA    2022-12-16 2022-12-16  Bruker Cellular Analysis, Inc.    2022-12-15 2022-12-15  BRIGHT HORIZONS FAMILY SOLUTIONS INC.    2022-12-15 2022-12-15  JAKKS PACIFIC INC    2022-12-14 2022-11-16  TRANSACT TECHNOLOGIES INC    2022-12-12 2022-12-12  BOEING CO    2022-12-09 2022-12-06  Rackspace Technology, Inc.    2022-12-05 2022-11-23  INTERFACE INC    2022-11-23 2022-11-23  Sonder Holdings Inc.    2022-10-07 2022-10-07  NEW ENGLAND REALTY ASSOCIATES LIMITED PARTNERSHIP    2022-09-19 2022-09-19  Uber Technologies, Inc    2022-09-09 2022-09-09  U-Haul Holding Co /NV/    2022-08-04 2022-05-09  OMNICELL, INC.    2022-07-25 2022-07-25  WESTINGHOUSE AIR BRAKE TECHNOLOGIES CORP    2022-07-22 2021-08-16  T-Mobile US, Inc.    2022-06-29 2022-06-29  NEW PEOPLES BANKSHARES INC    2022-05-31 2022-05-31  Hanesbrands Inc.    2022-04-19 2022-03-25  Okta, Inc.    2022-04-05 2022-04-05  PARKER HANNIFIN CORP    2022-04-04 2022-04-04  Block, Inc.    2022-03-08 2022-03-08  LAKE SHORE BANCORP, INC.    2022-02-28 2022-02-28  Aon plc    2022-02-22 2022-02-22  EXPEDITORS INTERNATIONAL OF WASHINGTON INC    2022-02-16 2022-02-16  Camping World Holdings, Inc.    2022-01-18 2021-12-27  RR Donnelley &amp; Sons Co    2021-12-30 2021-12-30  Usio, Inc.    2021-12-13 2021-12-13  RADIANT LOGISTICS, INC    2021-11-22 2021-11-22  GoDaddy Inc.    2021-11-08 2021-11-08  Robinhood Markets, Inc.    2021-10-27 2021-10-27  MARTEN TRANSPORT LTD    2021-10-18 2021-10-18  Sinclair Broadcast Group, LLC    2021-10-08 2021-10-08  QUEST DIAGNOSTICS INC    2021-10-07 2021-09-14  PCB BANCORP    2021-09-28 2021-09-28  Fortress Biotech, Inc.    2021-09-24 2021-09-24  TELKONET INC    2021-09-24 2021-09-24  STAR GROUP, L.P.    2021-09-20 2021-09-20  Marcus &amp; Millichap, Inc.    2021-09-14 2021-09-14  CREDITRISKMONITOR COM INC    2021-08-19 2021-08-19  ALJ REGIONAL HOLDINGS INC    2021-08-04 2021-08-04  IDEANOMICS, INC.    2021-07-28 2021-07-28  ON24 INC.    2021-06-14 2019-05-28  First American Financial Corp    2021-06-09 2021-06-09  MANITOWOC CO INC    2021-06-07 2021-06-07  NAVISTAR INTERNATIONAL CORP    2021-06-04 2021-06-04  ALLIED HEALTHCARE PRODUCTS INC    2021-06-03 2021-06-03  i3 Verticals, Inc.    2021-05-03 2021-05-03  SmileDirectClub, Inc.    2021-04-28 2021-04-28  FIRST HORIZON CORP    2021-04-26 2021-04-14  AMTECH SYSTEMS INC    2021-03-29 2021-03-29  LUB LIQUIDATING TRUST    2021-03-26 2021-03-11  MOLSON COORS BEVERAGE CO    2021-02-10 2021-01-26  CARRIAGE SERVICES INC    2021-02-01 2021-02-01  Metromile, LLC    2021-01-11 2021-01-11  CARNIVAL CORP    2020-12-31 2020-12-31  KAMAN Corp    2020-12-21 2020-12-21  FORWARD AIR CORP    2020-12-14 2020-12-08  Mandiant, Inc.    2020-11-30 2020-11-30  Stride, Inc.    2020-11-30 2020-11-16  AMERICOLD REALTY TRUST    2020-11-24 2020-11-24  BELDEN INC.    2020-11-03 2020-11-03  GEO GROUP INC    2020-10-26 2020-10-26  MINERALS TECHNOLOGIES INC    2020-10-26 2020-10-26  FIRST BANCORP /PR/    2020-09-29 2020-09-29  TYLER TECHNOLOGIES INC    2020-09-29 2020-09-29  UNIVERSAL HEALTH SERVICES INC    2020-09-28 2020-09-28  Arthur J. Gallagher &amp; Co.    2020-09-25 2020-09-25  cbdMD, Inc.    2020-09-23 2020-09-23  CINTAS CORP    2020-09-16 2020-09-10  EQUINIX INC    2020-08-17 2020-08-17  CARNIVAL CORP    2020-08-13 2020-08-13  SALEM MEDIA GROUP, INC. /DE/    2020-08-11 2020-08-11  Cornerstone Building Brands, Inc.    2020-08-07 2020-08-07  IMMUNIC, INC.    2020-07-27 2020-07-27  SiteOne Landscape Supply, Inc.    2020-07-13 2020-07-13  TCW Direct Lending VII LLC    2020-07-13 2020-07-13  TCW Direct Lending LLC    2020-07-08 2020-07-08  FORMFACTOR INC    2020-07-08 2020-06-11  NanoVibronix, Inc.    2020-06-16 2020-06-16  MAXLINEAR, INC    2020-04-20 2020-04-20  COGNIZANT TECHNOLOGY SOLUTIONS CORP    2020-01-13 2020-01-13  AutoWeb, Inc.    2019-06-06 2019-06-06  OPKO HEALTH, INC.    2019-06-04 2019-06-04  LABORATORY CORP OF AMERICA HOLDINGS    2019-06-03 2019-06-03  QUEST DIAGNOSTICS INC    2019-05-10 2019-05-10  URBAN ONE, INC.    2019-04-30 2019-04-30  CHARLES RIVER LABORATORIES INTERNATIONAL, INC.    2019-04-05 2019-04-05  RAVE RESTAURANT GROUP, INC.\n",
            "Highlights: ['Accessibility Statement     Page last updated on June 12, 2024 Tracker for cybersecurity incidents reported in an entity’s 8-K. Total Incidents: 175      Last Update Disclosure Date Company     2024-06-12 2024-05-28  KULICKE &amp; SOFFA INDUSTRIES INC    2024-05-31 2024-05-31  Live Nation Entertainment, Inc.    2024-05-28 2023-12-22  First American Financial Corp    2024-05-28 2024-05-07  BRANDYWINE OPERATING PARTNERSHIP, L.P.    2024-05-09 2024-05-09  KEY TRONIC CORP    2024-05-07 2024-05-07  DocGo Inc.    2024-05-01 2024-05-01  DROPBOX, INC.    2024-04-26 2023-12-18  V F CORP    2024-04-24 2024-02-22  UNITEDHEALTH GROUP INC    2024-04-18 2024-04-18  Frontier Communications Parent, Inc.    2024-04-12 2024-04-12  ORASURE TECHNOLOGIES INC    2024-04-08 2024-04-08  B. Riley Financial, Inc.    2024-04-01 2024-03-12  MARINEMAX INC    2024-03-29 2024-02-09  SouthState Corp    2024-03-20 2024-03-20  RADIANT LOGISTICS, INC    2024-03-08 2024-01-19  MICROSOFT CORP    2024-03-01 2024-03-01  Federal Home Loan Bank of New York    2024-02-27 2024-02-27  Cencora, Inc.    2024-02-26 2024-01-08  loanDepot, Inc.    2024-02-21 2024-02-13  PRUDENTIAL FINANCIAL INC    2024-02-09 2024-02-09  WILLIS LEASE FINANCE CORP    2024-02-02 2020-09-29  BLACKBAUD INC    2024-01-24 2024-01-24  Hewlett Packard Enterprise Co    2024-01-09 2023-11-21  Fidelity National Financial, Inc.    2023-12-21 2023-04-21  MIDDLEFIELD BANC CORP    2023-12-15 2023-11-02  Mr. Cooper Group Inc.    2023-12-13 2023-11-20  LivaNova PLC    2023-12-01 2023-10-10  23andMe Holding Co.    2023-11-29 2023-10-30  Mueller Water Products, Inc.    2023-11-29 2023-11-29  Okta, Inc.    2023-11-21 2023-11-21  Inspired Entertainment, Inc.    2023-11-13 2023-09-27  Johnson Controls International plc    2023-10-30 2020-12-14  SolarWinds Corp    2023-10-26 2023-10-26  REINSURANCE GROUP OF AMERICA INC    2023-10-24 2023-10-24  HENRY SCHEIN INC    2023-10-19 2023-10-11  Simpson Manufacturing Co., Inc.    2023-10-16 2023-10-16  HENRY SCHEIN INC    2023-10-13 2023-02-14  TRICO BANCSHARES /    2023-10-05 2023-10-05  MGM Resorts International    2023-09-21 2023-09-21  PROG Holdings, Inc.    2023-09-18 2023-08-14  CLOROX CO /DE/    2023-09-14 2023-09-14  Caesars Entertainment, Inc.    2023-09-13 2023-09-13  MGM Resorts International    2023-08-11 2023-08-11  FREEPORT-MCMORAN INC    2023-08-11 2023-08-11  COMMUNITY TRUST BANCORP INC /KY/    2023-08-07 2023-08-07  Topgolf Callaway Brands Corp.    2023-08-02 2023-08-02  Garrett Motion Inc.    2023-07-31 2023-07-31  TEMPUR SEALY INTERNATIONAL, INC.    2023-07-26 2023-07-26  MAXIMUS, INC.    2023-07-25 2023-07-25  PACIFIC PREMIER BANCORP INC    2023-07-20 2023-07-20  Paycom Software, Inc.    2023-07-19 2023-06-13  HAYNES INTERNATIONAL INC    2023-07-19 2023-07-19  ESTEE LAUDER COMPANIES INC    2023-07-14 2023-07-14  FIRSTSUN CAPITAL BANCORP    2023-07-14 2023-07-14  Sound Financial Bancorp, Inc.    2023-07-10 2023-07-10  HCA Healthcare, Inc.    2023-07-06 2023-07-06  FIRST COMMONWEALTH FINANCIAL CORP /PA/    2023-07-05 2023-07-05  FIRST MERCHANTS CORP    2023-07-03 2023-07-03  Hilltop Holdings Inc.    2023-06-30 2023-06-30  OPTICAL CABLE CORP    2023-06-30 2023-06-30  F&amp;G Annuities &amp; Life, Inc.    2023-06-27 2023-06-27  COLUMBIA BANKING SYSTEM, INC.    2023-06-26 2023-06-26  Corebridge Financial, Inc.    2023-06-26 2023-06-26  Jackson Financial Inc.    2023-06-22 2023-06-22  GENWORTH FINANCIAL INC    2023-06-13 2023-06-13  BRUNSWICK CORP    2023-06-05 2023-06-05  PROGRESS SOFTWARE CORP /MA    2023-05-30 2023-04-13  ENZO BIOCHEM INC    2023-05-26 2023-05-26  Polished.com Inc.    2023-05-05 2023-04-03  WESTERN DIGITAL CORP    2023-05-01 2023-05-01  AMERICOLD REALTY TRUST    2023-04-17 2023-04-17  NCR VOYIX Corp    2023-04-12 2023-04-12  HEICO CORP    2023-04-11 2023-01-06  SIFCO INDUSTRIES INC    2023-04-06 2022-11-07  PGT Innovations, Inc.    2023-03-27 2023-03-27  Level 3 Parent, LLC    2023-03-14 2023-03-14  QUICKLOGIC Corp    2023-03-10 2023-03-10  ARGAN INC    2023-03-02 2023-03-02  ULTRALIFE CORP    2023-02-28 2023-02-28  DISH DBS CORP    2023-02-24 2023-02-24  NATURES SUNSHINE PRODUCTS INC    2023-02-23 2023-02-23  Citi Trends Inc    2023-02-14 2023-02-14  Gates Industrial Corp plc    2023-02-13 2023-02-13  COMMUNITY HEALTH SYSTEMS INC    2023-02-13 2023-02-06  MKS INSTRUMENTS INC    2023-02-07 2023-02-07  A10 Networks, Inc.    2023-01-19 2023-01-19  T-Mobile US, Inc.    2023-01-19 2023-01-19  YUM BRANDS INC    2022-12-19 2022-12-19  PROGRESS SOFTWARE CORP /MA    2022-12-16 2022-12-16  Bruker Cellular Analysis, Inc.    2022-12-15 2022-12-15  BRIGHT HORIZONS FAMILY SOLUTIONS INC.    2022-12-15 2022-12-15  JAKKS PACIFIC INC    2022-12-14 2022-11-16  TRANSACT TECHNOLOGIES INC    2022-12-12 2022-12-12  BOEING CO    2022-12-09 2022-12-06  Rackspace Technology, Inc.    2022-12-05 2022-11-23  INTERFACE INC    2022-11-23 2022-11-23  Sonder Holdings Inc.    2022-10-07 2022-10-07  NEW ENGLAND REALTY ASSOCIATES LIMITED PARTNERSHIP    2022-09-19 2022-09-19  Uber Technologies, Inc    2022-09-09 2022-09-09  U-Haul Holding Co /NV/    2022-08-04 2022-05-09  OMNICELL, INC.    2022-07-25 2022-07-25  WESTINGHOUSE AIR BRAKE TECHNOLOGIES CORP    2022-07-22 2021-08-16  T-Mobile US, Inc.    2022-06-29 2022-06-29  NEW PEOPLES BANKSHARES INC    2022-05-31 2022-05-31  Hanesbrands Inc.    2022-04-19 2022-03-25  Okta, Inc.    2022-04-05 2022-04-05  PARKER HANNIFIN CORP    2022-04-04 2022-04-04  Block, Inc.    2022-03-08 2022-03-08  LAKE SHORE BANCORP, INC.    2022-02-28 2022-02-28  Aon plc    2022-02-22 2022-02-22  EXPEDITORS INTERNATIONAL OF WASHINGTON INC    2022-02-16 2022-02-16  Camping World Holdings, Inc.    2022-01-18 2021-12-27  RR Donnelley &amp; Sons Co    2021-12-30 2021-12-30  Usio, Inc.    2021-12-13 2021-12-13  RADIANT LOGISTICS, INC    2021-11-22 2021-11-22  GoDaddy Inc.    2021-11-08 2021-11-08  Robinhood Markets, Inc.    2021-10-27 2021-10-27  MARTEN TRANSPORT LTD    2021-10-18 2021-10-18  Sinclair Broadcast Group, LLC    2021-10-08 2021-10-08  QUEST DIAGNOSTICS INC    2021-10-07 2021-09-14  PCB BANCORP    2021-09-28 2021-09-28  Fortress Biotech, Inc.    2021-09-24 2021-09-24  TELKONET INC    2021-09-24 2021-09-24  STAR GROUP, L.P.    2021-09-20 2021-09-20  Marcus &amp; Millichap, Inc.    2021-09-14 2021-09-14  CREDITRISKMONITOR COM INC    2021-08-19 2021-08-19  ALJ REGIONAL HOLDINGS INC    2021-08-04 2021-08-04  IDEANOMICS, INC.    2021-07-28 2021-07-28  ON24 INC.    2021-06-14 2019-05-28  First American Financial Corp    2021-06-09 2021-06-09  MANITOWOC CO INC    2021-06-07 2021-06-07  NAVISTAR INTERNATIONAL CORP    2021-06-04 2021-06-04  ALLIED HEALTHCARE PRODUCTS INC    2021-06-03 2021-06-03  i3 Verticals, Inc.    2021-05-03 2021-05-03  SmileDirectClub, Inc.    2021-04-28 2021-04-28  FIRST HORIZON CORP    2021-04-26 2021-04-14  AMTECH SYSTEMS INC    2021-03-29 2021-03-29  LUB LIQUIDATING TRUST    2021-03-26 2021-03-11  MOLSON COORS BEVERAGE CO    2021-02-10 2021-01-26  CARRIAGE SERVICES INC    2021-02-01 2021-02-01  Metromile, LLC    2021-01-11 2021-01-11  CARNIVAL CORP    2020-12-31 2020-12-31  KAMAN Corp    2020-12-21 2020-12-21  FORWARD AIR CORP    2020-12-14 2020-12-08  Mandiant, Inc.    2020-11-30 2020-11-30  Stride, Inc.    2020-11-30 2020-11-16  AMERICOLD REALTY TRUST    2020-11-24 2020-11-24  BELDEN INC.    2020-11-03 2020-11-03  GEO GROUP INC    2020-10-26 2020-10-26  MINERALS TECHNOLOGIES INC    2020-10-26 2020-10-26  FIRST BANCORP /PR/    2020-09-29 2020-09-29  TYLER TECHNOLOGIES INC    2020-09-29 2020-09-29  UNIVERSAL HEALTH SERVICES INC    2020-09-28 2020-09-28  Arthur J. Gallagher &amp; Co.    2020-09-25 2020-09-25  cbdMD, Inc.    2020-09-23 2020-09-23  CINTAS CORP    2020-09-16 2020-09-10  EQUINIX INC    2020-08-17 2020-08-17  CARNIVAL CORP    2020-08-13 2020-08-13  SALEM MEDIA GROUP, INC. /DE/    2020-08-11 2020-08-11  Cornerstone Building Brands, Inc.    2020-08-07 2020-08-07  IMMUNIC, INC.    2020-07-27 2020-07-27  SiteOne Landscape Supply, Inc.    2020-07-13 2020-07-13  TCW Direct Lending VII LLC    2020-07-13 2020-07-13  TCW Direct Lending LLC    2020-07-08 2020-07-08  FORMFACTOR INC    2020-07-08 2020-06-11  NanoVibronix, Inc.    2020-06-16 2020-06-16  MAXLINEAR, INC    2020-04-20 2020-04-20  COGNIZANT TECHNOLOGY SOLUTIONS CORP    2020-01-13 2020-01-13  AutoWeb, Inc.    2019-06-06 2019-06-06  OPKO HEALTH, INC.    2019-06-04 2019-06-04  LABORATORY CORP OF AMERICA HOLDINGS    2019-06-03 2019-06-03  QUEST DIAGNOSTICS INC    2019-05-10 2019-05-10  URBAN ONE, INC.    2019-04-30 2019-04-30  CHARLES RIVER LABORATORIES INTERNATIONAL, INC.    2019-04-05 2019-04-05  RAVE RESTAURANT GROUP, INC.']\n",
            "Highlight Scores: [0.07964782416820526]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is a link to the latest cyber incidents involving the LunarsGo threat actor after 2023 and recent developments:\n",
            "Resolved Search Type: 2023-01-01T00:00:00.000Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Cohere' object has no attribute 'rerank'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8d926496f76c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menhanced_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhance_search_query_with_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enhanced Query: {enhanced_query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhanced_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-aaf120829891>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query, memory)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m         async for chunk in self.astream(\n\u001b[0m\u001b[1;32m   1981\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mastream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m                     async for _ in runner.atick(\n\u001b[0m\u001b[1;32m   1866\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36matick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 await arun_with_retry(\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_astream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, writer)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-fc1ad7de0239>\u001b[0m in \u001b[0;36mexecute_searches\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Cohere reranking for most relevant and up-to-date information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mfinal_reranked_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere_rerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhanced_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreranked_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Extract threat actor intelligence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c1e5bb08231a>\u001b[0m in \u001b[0;36mcohere_rerank\u001b[0;34m(query, results)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Prepare the inputs for Cohere reranking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnippet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mreranked_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere_reranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;31m# Sort results based on Cohere's reranking scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    890\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Cohere' object has no attribute 'rerank'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}