{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "2c0060f3-2fe0-42ff-9b29-6d236c1114ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.7/248.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 5.21MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 13.6MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 28.7MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 27.3MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.21MB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 8.43MB/s]\n",
            "2024-11-14 06:20:55.943920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 06:20:55.967232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 06:20:55.974508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 06:20:55.992178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 06:20:57.311109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 177MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "model.safetensors: 100% 499M/499M [00:05<00:00, 91.6MB/s]\n",
            "Downloading Chromium 129.0.6668.29 (playwright build v1134)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1134/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164 MiB [] 0% 0.0s\u001b[0K\u001b[1G164 MiB [] 0% 40.1s\u001b[0K\u001b[1G164 MiB [] 0% 26.3s\u001b[0K\u001b[1G164 MiB [] 0% 15.1s\u001b[0K\u001b[1G164 MiB [] 0% 14.2s\u001b[0K\u001b[1G164 MiB [] 0% 12.2s\u001b[0K\u001b[1G164 MiB [] 1% 8.7s\u001b[0K\u001b[1G164 MiB [] 1% 7.6s\u001b[0K\u001b[1G164 MiB [] 1% 6.8s\u001b[0K\u001b[1G164 MiB [] 2% 6.2s\u001b[0K\u001b[1G164 MiB [] 3% 6.3s\u001b[0K\u001b[1G164 MiB [] 3% 5.8s\u001b[0K\u001b[1G164 MiB [] 3% 5.9s\u001b[0K\u001b[1G164 MiB [] 4% 5.5s\u001b[0K\u001b[1G164 MiB [] 4% 5.4s\u001b[0K\u001b[1G164 MiB [] 5% 5.2s\u001b[0K\u001b[1G164 MiB [] 5% 5.0s\u001b[0K\u001b[1G164 MiB [] 6% 4.9s\u001b[0K\u001b[1G164 MiB [] 6% 4.8s\u001b[0K\u001b[1G164 MiB [] 7% 4.6s\u001b[0K\u001b[1G164 MiB [] 7% 4.4s\u001b[0K\u001b[1G164 MiB [] 8% 4.3s\u001b[0K\u001b[1G164 MiB [] 9% 4.1s\u001b[0K\u001b[1G164 MiB [] 9% 4.0s\u001b[0K\u001b[1G164 MiB [] 10% 3.9s\u001b[0K\u001b[1G164 MiB [] 11% 3.8s\u001b[0K\u001b[1G164 MiB [] 12% 3.7s\u001b[0K\u001b[1G164 MiB [] 13% 3.8s\u001b[0K\u001b[1G164 MiB [] 14% 3.7s\u001b[0K\u001b[1G164 MiB [] 14% 3.6s\u001b[0K\u001b[1G164 MiB [] 15% 3.6s\u001b[0K\u001b[1G164 MiB [] 16% 3.6s\u001b[0K\u001b[1G164 MiB [] 16% 3.5s\u001b[0K\u001b[1G164 MiB [] 17% 3.4s\u001b[0K\u001b[1G164 MiB [] 18% 3.3s\u001b[0K\u001b[1G164 MiB [] 19% 3.2s\u001b[0K\u001b[1G164 MiB [] 20% 3.1s\u001b[0K\u001b[1G164 MiB [] 20% 3.0s\u001b[0K\u001b[1G164 MiB [] 21% 2.9s\u001b[0K\u001b[1G164 MiB [] 22% 2.9s\u001b[0K\u001b[1G164 MiB [] 23% 2.8s\u001b[0K\u001b[1G164 MiB [] 24% 2.7s\u001b[0K\u001b[1G164 MiB [] 25% 2.6s\u001b[0K\u001b[1G164 MiB [] 26% 2.5s\u001b[0K\u001b[1G164 MiB [] 27% 2.5s\u001b[0K\u001b[1G164 MiB [] 28% 2.5s\u001b[0K\u001b[1G164 MiB [] 29% 2.4s\u001b[0K\u001b[1G164 MiB [] 30% 2.4s\u001b[0K\u001b[1G164 MiB [] 31% 2.3s\u001b[0K\u001b[1G164 MiB [] 32% 2.3s\u001b[0K\u001b[1G164 MiB [] 33% 2.2s\u001b[0K\u001b[1G164 MiB [] 34% 2.2s\u001b[0K\u001b[1G164 MiB [] 34% 2.1s\u001b[0K\u001b[1G164 MiB [] 35% 2.1s\u001b[0K\u001b[1G164 MiB [] 36% 2.1s\u001b[0K\u001b[1G164 MiB [] 37% 2.0s\u001b[0K\u001b[1G164 MiB [] 38% 1.9s\u001b[0K\u001b[1G164 MiB [] 39% 1.9s\u001b[0K\u001b[1G164 MiB [] 40% 1.8s\u001b[0K\u001b[1G164 MiB [] 41% 1.8s\u001b[0K\u001b[1G164 MiB [] 42% 1.7s\u001b[0K\u001b[1G164 MiB [] 43% 1.7s\u001b[0K\u001b[1G164 MiB [] 44% 1.7s\u001b[0K\u001b[1G164 MiB [] 45% 1.6s\u001b[0K\u001b[1G164 MiB [] 46% 1.6s\u001b[0K\u001b[1G164 MiB [] 47% 1.5s\u001b[0K\u001b[1G164 MiB [] 48% 1.5s\u001b[0K\u001b[1G164 MiB [] 49% 1.5s\u001b[0K\u001b[1G164 MiB [] 49% 1.4s\u001b[0K\u001b[1G164 MiB [] 50% 1.4s\u001b[0K\u001b[1G164 MiB [] 51% 1.3s\u001b[0K\u001b[1G164 MiB [] 52% 1.3s\u001b[0K\u001b[1G164 MiB [] 53% 1.3s\u001b[0K\u001b[1G164 MiB [] 55% 1.2s\u001b[0K\u001b[1G164 MiB [] 56% 1.2s\u001b[0K\u001b[1G164 MiB [] 58% 1.1s\u001b[0K\u001b[1G164 MiB [] 59% 1.1s\u001b[0K\u001b[1G164 MiB [] 60% 1.0s\u001b[0K\u001b[1G164 MiB [] 62% 1.0s\u001b[0K\u001b[1G164 MiB [] 63% 0.9s\u001b[0K\u001b[1G164 MiB [] 64% 0.9s\u001b[0K\u001b[1G164 MiB [] 65% 0.8s\u001b[0K\u001b[1G164 MiB [] 66% 0.8s\u001b[0K\u001b[1G164 MiB [] 67% 0.8s\u001b[0K\u001b[1G164 MiB [] 68% 0.8s\u001b[0K\u001b[1G164 MiB [] 69% 0.8s\u001b[0K\u001b[1G164 MiB [] 69% 0.7s\u001b[0K\u001b[1G164 MiB [] 70% 0.7s\u001b[0K\u001b[1G164 MiB [] 71% 0.7s\u001b[0K\u001b[1G164 MiB [] 72% 0.7s\u001b[0K\u001b[1G164 MiB [] 73% 0.7s\u001b[0K\u001b[1G164 MiB [] 74% 0.6s\u001b[0K\u001b[1G164 MiB [] 75% 0.6s\u001b[0K\u001b[1G164 MiB [] 76% 0.5s\u001b[0K\u001b[1G164 MiB [] 78% 0.5s\u001b[0K\u001b[1G164 MiB [] 79% 0.5s\u001b[0K\u001b[1G164 MiB [] 80% 0.4s\u001b[0K\u001b[1G164 MiB [] 81% 0.4s\u001b[0K\u001b[1G164 MiB [] 83% 0.4s\u001b[0K\u001b[1G164 MiB [] 84% 0.4s\u001b[0K\u001b[1G164 MiB [] 85% 0.3s\u001b[0K\u001b[1G164 MiB [] 86% 0.3s\u001b[0K\u001b[1G164 MiB [] 87% 0.3s\u001b[0K\u001b[1G164 MiB [] 88% 0.3s\u001b[0K\u001b[1G164 MiB [] 89% 0.2s\u001b[0K\u001b[1G164 MiB [] 90% 0.2s\u001b[0K\u001b[1G164 MiB [] 91% 0.2s\u001b[0K\u001b[1G164 MiB [] 93% 0.1s\u001b[0K\u001b[1G164 MiB [] 94% 0.1s\u001b[0K\u001b[1G164 MiB [] 95% 0.1s\u001b[0K\u001b[1G164 MiB [] 97% 0.1s\u001b[0K\u001b[1G164 MiB [] 98% 0.0s\u001b[0K\u001b[1G164 MiB [] 99% 0.0s\u001b[0K\u001b[1G164 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 129.0.6668.29 (playwright build v1134) downloaded to /root/.cache/ms-playwright/chromium-1134\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 3% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 9% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 22% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 39% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 72% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 95% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Firefox 130.0 (playwright build v1463)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1463/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G86.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 0% 22.3s\u001b[0K\u001b[1G86.4 MiB [] 0% 13.6s\u001b[0K\u001b[1G86.4 MiB [] 0% 8.5s\u001b[0K\u001b[1G86.4 MiB [] 1% 5.7s\u001b[0K\u001b[1G86.4 MiB [] 1% 4.7s\u001b[0K\u001b[1G86.4 MiB [] 2% 3.7s\u001b[0K\u001b[1G86.4 MiB [] 3% 3.2s\u001b[0K\u001b[1G86.4 MiB [] 4% 3.0s\u001b[0K\u001b[1G86.4 MiB [] 5% 3.0s\u001b[0K\u001b[1G86.4 MiB [] 5% 2.9s\u001b[0K\u001b[1G86.4 MiB [] 6% 3.0s\u001b[0K\u001b[1G86.4 MiB [] 7% 2.7s\u001b[0K\u001b[1G86.4 MiB [] 7% 2.8s\u001b[0K\u001b[1G86.4 MiB [] 8% 2.8s\u001b[0K\u001b[1G86.4 MiB [] 8% 2.7s\u001b[0K\u001b[1G86.4 MiB [] 9% 2.7s\u001b[0K\u001b[1G86.4 MiB [] 10% 2.6s\u001b[0K\u001b[1G86.4 MiB [] 12% 2.3s\u001b[0K\u001b[1G86.4 MiB [] 13% 2.4s\u001b[0K\u001b[1G86.4 MiB [] 15% 2.2s\u001b[0K\u001b[1G86.4 MiB [] 16% 2.0s\u001b[0K\u001b[1G86.4 MiB [] 18% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 19% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 21% 1.7s\u001b[0K\u001b[1G86.4 MiB [] 21% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 21% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 21% 2.0s\u001b[0K\u001b[1G86.4 MiB [] 22% 2.1s\u001b[0K\u001b[1G86.4 MiB [] 22% 2.2s\u001b[0K\u001b[1G86.4 MiB [] 22% 2.3s\u001b[0K\u001b[1G86.4 MiB [] 23% 2.2s\u001b[0K\u001b[1G86.4 MiB [] 24% 2.1s\u001b[0K\u001b[1G86.4 MiB [] 26% 2.0s\u001b[0K\u001b[1G86.4 MiB [] 27% 2.0s\u001b[0K\u001b[1G86.4 MiB [] 28% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 29% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 30% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 31% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 32% 1.7s\u001b[0K\u001b[1G86.4 MiB [] 33% 1.7s\u001b[0K\u001b[1G86.4 MiB [] 34% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 35% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 36% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 37% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 38% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 39% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 40% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 41% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 43% 1.4s\u001b[0K\u001b[1G86.4 MiB [] 44% 1.3s\u001b[0K\u001b[1G86.4 MiB [] 45% 1.3s\u001b[0K\u001b[1G86.4 MiB [] 47% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 48% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 49% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 50% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 51% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 52% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 54% 1.0s\u001b[0K\u001b[1G86.4 MiB [] 55% 1.0s\u001b[0K\u001b[1G86.4 MiB [] 57% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 59% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 60% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 61% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 63% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 64% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 65% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 67% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 68% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 70% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 71% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 73% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 74% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 76% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 77% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 78% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 79% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 81% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 83% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 85% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 86% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 87% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 89% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 91% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 92% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 94% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 96% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 97% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 99% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 130.0 (playwright build v1463) downloaded to /root/.cache/ms-playwright/firefox-1463\n",
            "Downloading Webkit 18.0 (playwright build v2070)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2070/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G88.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 0% 17.0s\u001b[0K\u001b[1G88.2 MiB [] 0% 12.8s\u001b[0K\u001b[1G88.2 MiB [] 0% 7.0s\u001b[0K\u001b[1G88.2 MiB [] 1% 4.3s\u001b[0K\u001b[1G88.2 MiB [] 2% 3.0s\u001b[0K\u001b[1G88.2 MiB [] 3% 2.5s\u001b[0K\u001b[1G88.2 MiB [] 4% 2.2s\u001b[0K\u001b[1G88.2 MiB [] 5% 2.1s\u001b[0K\u001b[1G88.2 MiB [] 6% 2.2s\u001b[0K\u001b[1G88.2 MiB [] 7% 2.1s\u001b[0K\u001b[1G88.2 MiB [] 8% 1.9s\u001b[0K\u001b[1G88.2 MiB [] 10% 1.8s\u001b[0K\u001b[1G88.2 MiB [] 11% 1.7s\u001b[0K\u001b[1G88.2 MiB [] 12% 1.7s\u001b[0K\u001b[1G88.2 MiB [] 13% 1.6s\u001b[0K\u001b[1G88.2 MiB [] 15% 1.5s\u001b[0K\u001b[1G88.2 MiB [] 16% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 17% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 18% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 19% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 20% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 22% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 23% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 24% 1.2s\u001b[0K\u001b[1G88.2 MiB [] 26% 1.2s\u001b[0K\u001b[1G88.2 MiB [] 27% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 29% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 31% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 32% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 34% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 36% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 37% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 39% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 42% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 43% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 45% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 46% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 47% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 48% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 50% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 51% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 52% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 54% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 55% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 57% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 59% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 60% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 62% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 64% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 65% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 67% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 69% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 71% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 73% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 74% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 76% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 78% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 80% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 82% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 84% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 85% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 87% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 90% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 92% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 94% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 96% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 99% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.0 (playwright build v2070) downloaded to /root/.cache/ms-playwright/webkit-2070\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0989eb9f-648b-4b9e-b91b-fe3ef2208c0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = (datetime.now() - date).days if date else 0\n",
        "        final_score = similarity + metadata_score - date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = (datetime.now() - date).days if date else 0\n",
        "            final_score = weight + source_weight - date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are a highly advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse data sources, yielding a response that combines comprehensive detail with actionable intelligence, customized to the specific context of each query.\n",
        "\n",
        "---\n",
        "\n",
        "**ANALYSIS PROTOCOL** *(Structured in Phases)*:\n",
        "\n",
        "1. **Source and Credibility Verification**:\n",
        "   - **Evaluate Domain Reliability**: Prioritize reputable cybersecurity and intelligence sources.\n",
        "   - **Ensure Timeliness**: Confirm that information is recent and directly relevant to the query.\n",
        "   - **Cross-Reference Findings**: Validate key data points across multiple credible sources, flagging any discrepancies.\n",
        "   - **Misinformation Detection**: Identify any unsupported or exaggerated claims, ensuring accuracy throughout.\n",
        "\n",
        "2. **Content Extraction and Relevance Filtering**:\n",
        "   - **Core Data Identification**: Extract essential information, such as threat vectors, indicators, metrics, and statistics.\n",
        "   - **Pattern Recognition**: Look for and highlight recurring themes, correlations, and emerging trends across sources.\n",
        "   - **Contextual Prioritization**:\n",
        "     * **Temporal Relevance**: Focus on the most recent and high-impact data.\n",
        "     * **Technical Depth**: Prioritize technical details pertinent to the query’s context.\n",
        "     * **Query Alignment**: Rank findings by their direct applicability to the user’s stated question.\n",
        "\n",
        "3. **Visual and Media Analysis**:\n",
        "   - **Visual Validation**: Assess images, diagrams, and screenshots for relevance, especially for technical PoCs or attack evidence.\n",
        "   - **Extract Technical Indicators**: Capture relevant data from visual media, such as IPs, file hashes, or attack paths.\n",
        "   - **Correlate Visual and Textual Information**: Ensure any media content aligns with textual data, emphasizing its technical implications.\n",
        "\n",
        "---\n",
        "\n",
        "**ADAPTIVE RESPONSE STRUCTURE** *(Dynamic format based on query type)*:\n",
        "\n",
        "1. **Executive Summary** *(Concise overview, 8-10 sentences)*:\n",
        "   - Summarize core findings, noting any urgent or high-priority insights and recommendations.\n",
        "\n",
        "2. **In-Depth Analysis** *(Tailored to query specifics)*:\n",
        "   - **Key Findings**:\n",
        "     * Bullet-point list of critical discoveries, emerging threats, or significant events.\n",
        "     * Include metrics, trends, or any statistical data directly relevant to the query.\n",
        "   - **Technical Breakdown**:\n",
        "     * Detailed information on specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "     * Affected components and dependencies, along with remediation measures if applicable.\n",
        "   - **Contextual and Industry Impact**:\n",
        "     * Analyze industry or sector-specific implications.\n",
        "     * Attribute threat actors or TTPs (Tactics, Techniques, and Procedures) if identifiable.\n",
        "     * Link with historical incidents or precedents for added context.\n",
        "\n",
        "3. **Source Citations and Evidence**:\n",
        "   - Cite all findings in the format [Source Name](URL), explicitly linking sources to major claims.\n",
        "   - Provide quote snippets when referencing specific assertions, and ensure context.\n",
        "   - **Media Links**: Reference any visual media, linking to screenshots or diagrams if available, with a brief description of their relevance.\n",
        "\n",
        "4. **Actionable Recommendations**:\n",
        "   - Provide specific, immediate actions and mitigation steps.\n",
        "   - Outline detection and prevention strategies directly relevant to the identified threats.\n",
        "   - Highlight operational security measures, particularly for high-severity issues.\n",
        "\n",
        "5. **Long-Term Forecast and Monitoring**:\n",
        "   - Discuss projected threat evolution and potential impacts.\n",
        "   - Highlight trends for ongoing observation and provide recommendations for continuous monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "**SPECIALIZED QUERY HANDLING** *(Select based on context)*:\n",
        "\n",
        "1. **For Threat Intelligence Queries**:\n",
        "   - Extract IOCs (Indicators of Compromise) such as IPs, domains, and hashes.\n",
        "   - Map relevant data to MITRE ATT&CK TTPs and assess malware behavior patterns.\n",
        "   - Document Command and Control (C2) configurations if applicable.\n",
        "\n",
        "2. **For Vulnerability and Exploit Analysis**:\n",
        "   - Validate CVE details, including severity, affected systems, and patch status.\n",
        "   - Assess real-world exploitability, contextualizing with any reported attacks.\n",
        "\n",
        "3. **For Incident Response**:\n",
        "   - Construct attack timelines, reconstructing paths and points of entry.\n",
        "   - Provide recovery steps and guidance for immediate containment and response.\n",
        "\n",
        "4. **For Trend Analysis**:\n",
        "   - Identify changes in attack patterns, mapping against historical baselines.\n",
        "   - Forecast likely developments in tactics or threat actors’ capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "**PROMPT VARIABLES**:\n",
        "- **Previous Context**: {chat_history}\n",
        "- **Current Query**: {input}\n",
        "- **Search Results**: {search_results}\n",
        "- **Additional Crawled Data**: {crawled_results}\n",
        "- **Current Date**: {current_date}\n",
        "\n",
        "---\n",
        "\n",
        "**RESPONSE REQUIREMENTS**:\n",
        "1. **Precision and Depth**: Ensure technical accuracy and concise, detailed insights.\n",
        "2. **Confidence Levels**: State the certainty of all assessments, highlighting uncertainties.\n",
        "3. **Citation Accuracy**: Use the [Source Name](URL) format for each major source; include relevant media references where applicable.\n",
        "4. **Highlight Urgency**: Mark any urgent or time-sensitive findings.\n",
        "5. **Readable Structure**: Use clear headings, subheadings, and bullet points for readability.\n",
        "6. **Address Gaps and Uncertainties**: Note any information limitations.\n",
        "7. **Embedded Media Links**: Include links to any referenced media (screenshots, diagrams) with brief contextual descriptions.\n",
        "8. **Actionable and Context-Specific Recommendations**: Tailor guidance based on query context.\n",
        "9. **Technical Context Maintenance**: Retain technical rigor and avoid generalizations.\n",
        "\n",
        "---\n",
        "\n",
        "Generate a comprehensive, precise response that directly addresses the query by synthesizing and presenting the latest, most relevant intelligence, and ensuring actionable insights and credible, source-backed evidence.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomeware Gang?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b79c992-b431-42f6-835b-5da2fcd1a15a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomeware Gang?\n",
            "DEBUG: Raw results from Exa Search: Title: Ransomware fiends boast they've stolen 1.4TB from US pharmacy network\n",
            "URL: https://www.theregister.com/2024/11/13/embargo_ransomware_breach_aap/\n",
            "ID: https://www.theregister.com/2024/11/13/embargo_ransomware_breach_aap/\n",
            "Score: 0.14478732645511627\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: Connor Jones\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: American Associated Pharmacies (AAP) is the latest US healthcare organization to have had its data stolen and encrypted by cyber-crooks, it is feared.\n",
            "The criminals over at the Embargo ransomware operation claimed responsibility for the hit job, allegedly stealing 1.469 TB of AAP's data, scrambling its files, and demanding payment to restore the information.\n",
            "AAP, which oversees a few thousand independent pharmacies in the country, hasn't officially confirmed an attack, nor has it responded to The Register's request for input on the claims. At the time of writing, its website warns all user passwords were recently force-reset. It did not explain why the resets were forced nor mention a cyberattack.\n",
            "\"All user passwords associated with both APIRx.com and RxAAP.com have been reset, so existing credentials will no longer be valid to access the sites,\" a website notice reads. \"Please click 'forgot password' on the log in screen and follow the prompts accordingly to reset your password.\"\n",
            "     American Associated Pharmacies' website informing customers their passwords were reset amid suspected ransomware attack ... Click to enlarge\n",
            "The notice also states that API Warehouse, an AAP subsidiary devoted to helping partners save on branded and generic prescription drugs through wholesale buying plans, had some nondescript inventory issues, which are now resolved.\n",
            "As ever with these things, take claims by ransomware miscreants with a pinch of salt, but Embargo's own site claims AAP paid $1.3 million to have their systems decrypted and that it's demanding an additional $1.3 million to keep a lid on the pilfered documents.\n",
            "If true, the demands made by Embargo exceed the average, which the FBI said earlier this year is in the region of $1.5 million.\n",
            "It's not said what kind of data the ransomware group stole from AAP, if any at all. The pharmacy network was given a deadline of November 20 to pay the remaining \"balance\" before its data would be leaked online – a classic double extortion scenario.\n",
            "What isn't a classic move is Embargo's tendency to assign blame to specific individuals after deciding to leak a victim's data. In a number of cases where victims have let the countdown timer run down, Embargo has listed the names, email addresses, and phone numbers of key figures in the organization that it believes hindered the payment and negotiation process. Sometimes this also included the third-party incident responders drafted to help handle an attack.\n",
            " Microsoft says more ransomware stopped before reaching encryption \n",
            " Schneider Electric ransomware crew demands $125k paid in baguettes \n",
            " US healthcare org admits up to 400,000 people's personal info was snatched \n",
            " Healthcare giant to pay $65M settlement after crooks stole and leaked nude patient pics \n",
            "Embargo is a relatively new group on the ransomware scene. Researchers at ESET first noticed it as recently as June, and it is among a number of gangs using endpoint detection and response (EDR) killing tools to deploy its main payload.\n",
            "Despite only being around for mere months, it's garnered attention from established cybercriminals, with the likes of Storm-0501 also seen using its Rust-based ransomware kit.\n",
            "As for AAP, it was founded in 2009 through the merger of Phoenix-based United Drugs and Alabama-based Associated Pharmacies. According to its website, the co-operative oversees more than 2,000 independent pharmacies across the US.\n",
            "Other than the sparsely detailed notice slapped on its website, AAP hasn't publicly acknowledged anything about the alleged robbery. Some interested folks have queried the \"outage\" to its social media channels, and they haven't received a response. ®\n",
            "Highlights: [\"American Associated Pharmacies (AAP) is the latest US healthcare organization to have had its data stolen and encrypted by cyber-crooks, it is feared. The criminals over at the Embargo ransomware operation claimed responsibility for the hit job, allegedly stealing 1.469 TB of AAP's data, scrambling its files, and demanding payment to restore the information. AAP, which oversees a few thousand independent pharmacies in the country, hasn't officially confirmed an attack, nor has it responded to The Register's request for input on the claims. At the time of writing, its website warns all user passwords were recently force-reset. It did not explain why the resets were forced nor mention a cyberattack.\"]\n",
            "Highlight Scores: [0.5206378102302551]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Free Decryptor Released for BitLocker-Based ShrinkLocker Ransomware Victims\n",
            "URL: https://thehackernews.com/2024/11/free-decryptor-released-for-bitlocker.html\n",
            "ID: https://thehackernews.com/2024/11/free-decryptor-released-for-bitlocker.html\n",
            "Score: 0.14328323304653168\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Romanian cybersecurity company Bitdefender has released a free decryptor to help victims recover data encrypted using the ShrinkLocker ransomware.\n",
            "The decryptor is the result of a comprehensive analysis of ShrinkLocker's inner workings, allowing the researchers to discover a \"specific window of opportunity for data recovery immediately after the removal of protectors from BitLocker-encrypted disks.\"\n",
            "ShrinkLocker was first documented in May 2024 by Kaspersky, which found the malware's use of Microsoft's native BitLocker utility for encrypting files as part of extortion attacks targeting Mexico, Indonesia, and Jordan.\n",
            "Bitdefender, which investigated a ShrinkLocker incident targeting an unnamed healthcare company in the Middle East, said the attack likely originated from a machine belonging to a contractor, once again highlighting how threat actors are increasingly abusing trusted relationships to infiltrate the supply chain.\n",
            " \n",
            "In the next stage, the threat actor moved laterally to an Active Directory domain controller by making use of legitimate credentials for a compromised account, followed by creating two scheduled tasks for activating the ransomware process.\n",
            "While the first task executed a Visual Basic Script (\"Check.vbs\") that copied the ransomware program to every domain-joined machine, the second task – scheduled for two days later — executed the locally deployed ransomware (\"Audit.vbs\").\n",
            "The attack, Bitdefender said, successfully encrypted systems running Windows 10, Windows 11, Windows Server 2016, and Windows Server 2019. That said, the ShrinkLocker variant used is said to be a modified version of the original version.\n",
            "Described as simple yet effective, the ransomware stands out for the fact that it's written in VBScript, a scripting language that Microsoft said is being deprecated starting the second half of 2024. Plus, instead of implementing its own encryption algorithm, the malware weaponizes BitLocker to achieve its goals.\n",
            "The script is designed to gather information about the system configuration and operating system, after which it attempts to check if BitLocker is already installed on a Windows Server machine, and if not, installs it using a PowerShell command and then performs a \"forced reboot\" using Win32Shutdown.\n",
            "   \n",
            "But Bitdefender said it noted a bug that causes this request to fail with a \"Privilege Not Held\" error, causing the VBScript to be stuck in an infinite loop due to a failed reboot attempt.\n",
            "\"Even if the server is rebooted manually (e.g. by an unsuspecting administrator), the script does not have a mechanism to resume its execution after the reboot, meaning that the attack may be interrupted or prevented,\" Martin Zugec, technical solutions director at Bitdefender, said.\n",
            "The ransomware is designed to generate a random password that's derived from system-specific information, such as network traffic, system memory, and disk utilization, using it to encrypt the system's drives.\n",
            "The unique password is then uploaded to a server controlled by the attacker. Following the restart, the user is prompted to enter the password to unlock the encrypted drive. The BitLocker screen is also configured to display the threat actor's contact email address to initiate the payment in exchange for the password.\n",
            "That's not all. The script makes several Registry modifications to restrict access to the system by disabling remote RDP connections and turning off local password-based logins. As part of its cleanup efforts, it also disables Windows Firewall rules and deletes audit files.\n",
            " \n",
            "Bitdefender further pointed out that the name ShrinkLocker is misleading as the namesake functionality is limited to legacy Windows systems and that it doesn't actually shrink partitions on current operating systems.\n",
            "\"By using a combination of Group Policy Objects (GPOs) and scheduled tasks, it can encrypt multiple systems within a network in as little as 10 minutes per device,\" Zugec noted. \"As a result, a complete compromise of a domain can be achieved with very little effort.\"\n",
            "\"Proactive monitoring of specific Windows event logs can help organizations identify and respond to potential BitLocker attacks, even in their early stages, such as when attackers are testing their encryption capabilities.\"\n",
            "\"By configuring BitLocker to store recovery information in Active Directory Domain Services (AD DS) and enforcing the policy \"Do not enable BitLocker until recovery information is stored to AD DS for operating system drives,\" organizations can significantly reduce the risk of BitLocker-based attacks.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: [\"The ransomware is designed to generate a random password that's derived from system-specific information, such as network traffic, system memory, and disk utilization, using it to encrypt the system's drives. The unique password is then uploaded to a server controlled by the attacker. Following the restart, the user is prompted to enter the password to unlock the encrypted drive. The BitLocker screen is also configured to display the threat actor's contact email address to initiate the payment in exchange for the password. That's not all.\"]\n",
            "Highlight Scores: [0.46414199471473694]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Hot Topic data breach exposed personal data of 57 million customers\n",
            "URL: https://techcrunch.com/2024/11/13/hot-topic-data-breach-exposed-personal-data-of-57-million-customers/\n",
            "ID: https://techcrunch.com/2024/11/13/hot-topic-data-breach-exposed-personal-data-of-57-million-customers/\n",
            "Score: 0.1268121898174286\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: Carly Page\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Millions of customers of Hot Topic have been informed that their personal data was compromised during an October data breach at the American retailer. \n",
            "Have I Been Pwned (HIBP), the breach notification service, said this week that it alerted 57 million Hot Topic customers that their data had been compromised.\n",
            "The stolen data includes email addresses, physical addresses, phone numbers, purchases, genders, and dates of birth. Partial credit card data was also included in the breach, according to HIBP, including credit card type, expiry dates, and the last four digits of the card number. \n",
            "Hot Topic, which has more than 640 stores across the U.S., has not yet confirmed the breach and did not respond to TechCrunch’s multiple requests for comment.\n",
            "The breach occurred on October 19, according to HIBP, and was claimed by a threat actor operating under the alias “Satanic” on October 21. In a post on the cybercrime forum BreachForums, Satanic claimed to have stolen 350 million user records from Hot Topic and its affiliated brands, Box Lunch and Torrid.\n",
            "The hacker initially attempted to sell the database for $20,000 and demanded a $100,000 ransom from Hot Topic to take down the information, according to a report by cybersecurity firm Hudson Rock.\n",
            "In the post on BreachForums, seen by TechCrunch, Satanic is now offering the database for $3,500.\n",
            "The nature of the security incident that led to the breach is unknown. According to a report from Hudson Rock, the threat actor may have leveraged credentials stolen via infostealer malware to steal credentials for an analytics platform used by Hot Topic to access the retailer’s cloud environments. \n",
            "It doesn’t appear that Hot Topic has yet notified customers or state offices of attorneys general about the data breach. \n",
            "Most Popular\n",
            " \n",
            "Carly Page is a Senior Reporter at TechCrunch, where she covers the cybersecurity beat. She has spent more than a decade in the technology industry, writing for titles including Forbes, TechRadar and WIRED. \n",
            "You can contact Carly securely on Signal at +441536 853956 or via email at carly.page@techcrunch.com. \n",
            "View Bio \n",
            "Newsletters\n",
            "Subscribe for the industry’s biggest tech news\n",
            "Related\n",
            "Latest in Security\n",
            "Highlights: ['The stolen data includes email addresses, physical addresses, phone numbers, purchases, genders, and dates of birth. Partial credit card data was also included in the breach, according to HIBP, including credit card type, expiry dates, and the last four digits of the card number.  Hot Topic, which has more than 640 stores across the U.S., has not yet confirmed the breach and did not respond to TechCrunch’s multiple requests for comment. The breach occurred on October 19, according to HIBP, and was claimed by a threat actor operating under the alias “Satanic” on October 21. In a post on the cybercrime forum BreachForums, Satanic claimed to have stolen 350 million user records from Hot Topic and its affiliated brands, Box Lunch and Torrid.']\n",
            "Highlight Scores: [0.5618309378623962]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: DeltaPrime on X: \"A message to the attacker (respond by nov 14th, 8am cet)\n",
            "\n",
            "Yesterday we sent you an on-chain message on: https://t.co/HSyGGCxqOz. \n",
            "\n",
            "It seems like you either missed this message, or decided to postpone a response in order to farm more rewards on LFJ and Stargate. \n",
            "\n",
            "We are confident\" / X\n",
            "URL: https://x.com/DeltaPrimeDefi/status/1856594612632326198\n",
            "ID: https://x.com/DeltaPrimeDefi/status/1856594612632326198\n",
            "Score: 0.12677514553070068\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: It seems like you either missed this message, or decided to postpone a response in order to farm more rewards on LFJ and Stargate.\n",
            "We are confident\" / X\n",
            "2024-11-13\n",
            "None\n",
            "         Conversation\n",
            "Highlights: ['It seems like you either missed this message, or decided to postpone a response in order to farm more rewards on LFJ and Stargate. We are confident\" / X\\n2024-11-13\\nNone\\n         Conversation']\n",
            "Highlight Scores: [0.19987528026103973]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Man Set Up and Extorted of $500,000 Worth of USDT by Criminals in Hotel Room: Report\n",
            "URL: https://dailyhodl.com/2024/11/13/man-set-up-and-extorted-of-500000-worth-of-usdt-by-criminals-in-hotel-room-report/\n",
            "ID: https://dailyhodl.com/2024/11/13/man-set-up-and-extorted-of-500000-worth-of-usdt-by-criminals-in-hotel-room-report/\n",
            "Score: 0.1245373785495758\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: Daily Hodl Staff\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A Ukrainian man reportedly lost $500,000 worth of USDT after being set up for a robbery in a Thailand hotel room.\n",
            "The Bangkok Post reports that Viacheslav Leibov, 23, told Thailand police a friend of his invited him to visit a hotel room in Phuket when criminals wearing masks accosted him.\n",
            "The criminals tied him up with ropes and cable ties and demanded he transfer $500,000 worth of Tether’s stablecoin USDT or they would break his fingers. They were armed with a hammer and a “long” knife.\n",
            "After he made the transfer, they tied him to a bed and warned him not to report the crime, before fleeing. He managed to free himself and report the crime to a police station in Kamala.\n",
            "In an update, the Bangkok Post reports that law enforcement has arrested the four suspects involved in the crime: Arman Grigoryan, 21, an Armenian, Alfred Chernyshuk, 18, a Ukrainian, Ruslan Musaiev, 22, a Ukrainian, and Mraz Atoian, 21, a Russian.\n",
            "Police say they arrested the suspects in a hotel room in Phang Nga, a nearby province.\n",
            "During interrogation, the suspects allegedly told police they had tried to hire a driver to go to Malaysia, but the driver refused so they got hotel rooms instead.\n",
            "Police say the victim had known one of the suspects for a long time, having met through crypto trading. The friend and suspect allegedly decided to rob him with the help of the three other suspects after learning how many digital assets he had.\n",
            "The four suspects are being held in custody and face charges of armed robbery and illegally detaining and confining another person.\n",
            "    \n",
            "   \n",
            "   \n",
            "   \n",
            "Disclaimer: Opinions expressed at The Daily Hodl are not investment advice. Investors should do their due diligence before making any high-risk investments in Bitcoin, cryptocurrency or digital assets. Please be advised that your transfers and trades are at your own risk, and any losses you may incur are your responsibility. The Daily Hodl does not recommend the buying or selling of any cryptocurrencies or digital assets, nor is The Daily Hodl an investment advisor. Please note that The Daily Hodl participates in affiliate marketing.\n",
            "Highlights: ['Police say they arrested the suspects in a hotel room in Phang Nga, a nearby province. During interrogation, the suspects allegedly told police they had tried to hire a driver to go to Malaysia, but the driver refused so they got hotel rooms instead. Police say the victim had known one of the suspects for a long time, having met through crypto trading. The friend and suspect allegedly decided to rob him with the help of the three other suspects after learning how many digital assets he had. The four suspects are being held in custody and face charges of armed robbery and illegally detaining and confining another person.']\n",
            "Highlight Scores: [0.6392323970794678]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is the latest cyber incident from the Blackbasta ransomware gang:\n",
            "Resolved Search Type: 2024-11-13T06:53:41.674Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://therecord.media/blackbasta-ransom-payments using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X, success: True, time taken: 26.14 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X, success: True, time taken: 1.97 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X, time taken: 2.82 seconds.\n",
            "[LOG] ✅ Crawled https://therecord.media/blackbasta-ransom-payments successfully!\n",
            "[LOG] 🚀 Crawling done for https://therecord.media/blackbasta-ransom-payments, success: True, time taken: 23.63 seconds\n",
            "[LOG] 🚀 Content extracted for https://therecord.media/blackbasta-ransom-payments, success: True, time taken: 0.61 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://therecord.media/blackbasta-ransom-payments, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://therecord.media/blackbasta-ransom-payments, time taken: 0.73 seconds.\n",
            "[LOG] ✅ Crawled https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports, success: True, time taken: 39.23 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports, success: True, time taken: 1.36 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports, time taken: 1.78 seconds.\n",
            "[LOG] ✅ Crawled https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/, success: True, time taken: 49.16 seconds\n",
            "[LOG] 🚀 Content extracted for https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/, success: True, time taken: 1.44 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/, time taken: 2.15 seconds.\n",
            "[LOG] ✅ Crawled https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, success: True, time taken: 50.52 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, success: True, time taken: 1.47 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, time taken: 1.76 seconds.\n",
            "Crawled Results: [[{'links': '/', 'images': 'https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.cyberriskalliance.com/wp-content/uploads/2023/08/Phishing-Hook-GettyImages-1335959802.jpg'}], [{'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}], [{'links': 'https://cybernews.com/news/', 'images': 'https://media.cybernews.com/2023/07/Ernestas-Naprys.jpg'}], [{'links': '/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement', 'images': 'https://media.licdn.com/dms/image/v2/C4E03AQHhKw-yQqYytQ/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1623200735372?e=2147483647&v=beta&t=G80m5VBZJL7JPL6_1EqMLygGdT1cW_pLN_V0ZXZFImk'}], [{'links': '/', 'images': 'https://recordedfuture.matomo.cloud/matomo.php?idsite=2&rec=1'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.cyberriskalliance.com/wp-content/uploads/2023/08/Phishing-Hook-GettyImages-1335959802.jpg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://cybernews.com/news/', 'images': 'https://media.cybernews.com/2023/07/Ernestas-Naprys.jpg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement', 'images': 'https://media.licdn.com/dms/image/v2/C4E03AQHhKw-yQqYytQ/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1623200735372?e=2147483647&v=beta&t=G80m5VBZJL7JPL6_1EqMLygGdT1cW_pLN_V0ZXZFImk'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://recordedfuture.matomo.cloud/matomo.php?idsite=2&rec=1'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://therecord.media/blackbasta-ransom-payments\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.exponential-e.com/images/easyblog_articles/545/b2ap3_large_THE-UP1.JPG\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://images.squarespace-cdn.com/content/v1/60c8a6d5ec828772430629e7/09add595-41a8-46a2-a8af-2eef3c03a690/blackbasta.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://images.ctfassets.net/yewqr8zk7e5s/79nDXNCWuGWHSWOg7afYwR/d55a7761e65f0b77d6d08d8fa6d3f3fa/Black_basta_wallpaper.jpg?w=754&q=75&fm=webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.picussecurity.com/hubfs/5-1.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://res.cloudinary.com/momentum-media-group-pty-ltd/image/upload/v1709683631/Black_Basta_hvd_host_ecmmwb.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.reuters.com/resizer/v2/RXWNWIVM7NLNPN74WQ52TWZK3I.jpg?auth=84a63adeea274f04f757bacb994fce6ff42268aa220a403ef818b1416eb02bcd&width=5674&quality=80\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Black Basta Ransomware Gang: Emerging Threat and Recent Activities**\n",
            "\n",
            "**Executive Summary**\n",
            "\n",
            "The Black Basta ransomware gang has been actively targeting various sectors, including healthcare, with a significant increase in attacks reported in recent months. This response provides an overview of the gang's activities, tactics, techniques, and procedures (TTPs), and highlights the urgency of the situation. We will also discuss the impact of the attacks, potential vulnerabilities, and recommendations for mitigation. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "*   **TTPs and Tactics**: Black Basta's TTPs include using social engineering tactics to pose as IT support, exploiting zero-day vulnerabilities, and encrypting data using the ransomware-as-a-service (RaaS) model.\n",
            "*   **Recent Attacks**: The gang has targeted over 500 organizations worldwide, including healthcare providers, with ransom demands ranging from $100,000 to $2 million.\n",
            "*   **Vulnerabilities**: The gang's success can be attributed to the use of zero-day vulnerabilities, which have been exploited to gain unauthorized access to systems.\n",
            "*   **Impact**: The attacks have resulted in significant financial losses, compromised sensitive data, and disrupted critical infrastructure. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Technical Breakdown**\n",
            "\n",
            "*   **Exploited Vulnerabilities**: The gang has exploited several zero-day vulnerabilities, including those in Microsoft Windows and other software.\n",
            "*   **Ransomware-as-a-Service (RaaS) Model**: Black Basta uses the RaaS model, which allows attackers to rent or buy ransomware tools and distribute them to other attackers.\n",
            "*   **Data Encryption**: The gang uses advanced encryption techniques to protect the encrypted data, making it difficult for victims to recover their data without paying the ransom. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Contextual and Industry Impact**\n",
            "\n",
            "*   **Healthcare Sector**: The attacks on healthcare providers have raised concerns about the potential for data breaches and disruptions to critical services.\n",
            "*   **Industry-Wide Impact**: The attacks have highlighted the need for organizations to prioritize cybersecurity and implement robust measures to prevent and respond to ransomware attacks. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Actionable Recommendations**\n",
            "\n",
            "1.  **Implement Robust Security Measures**: Organizations should implement robust security measures, including multi-factor authentication, encryption, and regular software updates.\n",
            "2.  **Conduct Regular Security Audits**: Regular security audits can help identify vulnerabilities and prevent attacks.\n",
            "3.  **Develop Incident Response Plans**: Organizations should develop incident response plans to quickly respond to and contain ransomware attacks.\n",
            "4.  **Train Employees**: Employees should be trained to recognize and report suspicious activity, and to avoid engaging with suspicious emails or attachments. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Long-Term Forecast and Monitoring**\n",
            "\n",
            "*   **Ongoing Threat**: The Black Basta ransomware gang is an ongoing threat, and organizations should remain vigilant and proactive in their cybersecurity efforts.\n",
            "*   **Monitoring**: Organizations should monitor their systems and networks for signs of ransomware activity and respond quickly to any incidents. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Citation Accuracy**\n",
            "\n",
            "*   [1] Black Basta Ransomware Gang (2022). Retrieved from <https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf>\n",
            "*   [2] Black Basta Ransomware Gang (2022). Retrieved from <https://www.reuters.com/resizer/v2/RXWNWIVM7NLNPN74WQ52TWZK3I.jpg>\n",
            "*   [3] Black Basta Ransomware Gang (2022). Retrieved from <https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png>\n",
            "\n",
            "**Embedded Media Links**\n",
            "\n",
            "*   [Screenshot of Black Basta ransomware gang's logo](https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf#page=1)\n",
            "*   [Image of Black Basta ransomware gang's ransom demand](https://www.reuters.com/resizer/v2/RXWNWIVM7NLNPN74WQ52TWZK3I.jpg)\n",
            "\n",
            "**Actionable and Context-Specific Recommendations**\n",
            "\n",
            "*   Implement robust security measures, including multi-factor authentication, encryption, and regular software updates.\n",
            "*   Conduct regular security audits to identify vulnerabilities and prevent attacks.\n",
            "*   Develop incident response plans to quickly respond to and contain ransomware attacks.\n",
            "*   Train employees to recognize and report suspicious activity, and to avoid engaging with suspicious emails or attachments. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Technical Context Maintenance**\n",
            "\n",
            "*   The Black Basta ransomware gang is an emerging threat, and organizations should remain vigilant and proactive in their cybersecurity efforts.\n",
            "*   The gang's TTPs and tactics are constantly evolving, and organizations should stay informed about the latest developments. [Tavily Search](https://www.pcmag.com/news/us-warns-about-black-basta-ransomware-after-ascension-hospital-hack)\n",
            "\n",
            "By following these recommendations and staying informed about the latest developments, organizations can reduce the risk of ransomware attacks and protect their critical infrastructure. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Sources**\n",
            "- [Google Programmable Search](https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf)\n",
            "- [Google Programmable Image Search](https://www.linkedin.com/posts/john-riggi-aha-fbi_blackbasta-oneteamonefight-ransomware-activity-7195616725214593025-3E-X)\n",
            "- [Google Serper Image Search](https://images.squarespace-cdn.com/content/v1/60c8a6d5ec828772430629e7/09add595-41a8-46a2-a8af-2eef3c03a690/blackbasta.jpg)\n",
            "- [Google Serper](https://www.aha.org/news/headline/2024-05-10-agencies-warn-accelerating-attacks-health-care-black-basta-ransomware-group)\n",
            "- [Google Serper Image Search](https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png)\n",
            "- [Google Serper](https://www.computerweekly.com/news/366588814/Black-Basta-ransomware-crew-may-be-exploiting-Microsoft-zero-day)\n",
            "- [Tavily Search](https://cybernews.com/security/black-basta-alert-follows-ascension-hospitals-breach/)\n",
            "- [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "- [Google Programmable Image Search](https://therecord.media/blackbasta-ransom-payments)\n",
            "- [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "- [Google Serper](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a)\n",
            "- [Google Serper](https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs)\n",
            "- [Google Serper](https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta)\n",
            "- [Google Programmable Search](https://www.msspalert.com/news/new-ransomware-gang-hits-50-companies-cybereason-reports)\n",
            "- [Google Serper Image Search](https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg)\n",
            "- [Tavily Search](https://www.pcmag.com/news/us-warns-about-black-basta-ransomware-after-ascension-hospital-hack)\n",
            "- [Google Serper](https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/tag/black-basta/)\n",
            "- [Google Serper Image Search](https://images.ctfassets.net/yewqr8zk7e5s/79nDXNCWuGWHSWOg7afYwR/d55a7761e65f0b77d6d08d8fa6d3f3fa/Black_basta_wallpaper.jpg?w=754&q=75&fm=webp)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper Image Search](https://www.exponential-e.com/images/easyblog_articles/545/b2ap3_large_THE-UP1.JPG)\n",
            "- [Google Serper](https://therecord.media/tag/black-basta)\n",
            "- [Google Serper Image Search](https://res.cloudinary.com/momentum-media-group-pty-ltd/image/upload/v1709683631/Black_Basta_hvd_host_ecmmwb.jpg)\n",
            "- [Google Serper Image Search](https://www.picussecurity.com/hubfs/5-1.jpg)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "- [Google Serper Image Search](https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png)\n",
            "- [Google Serper Image Search](https://www.reuters.com/resizer/v2/RXWNWIVM7NLNPN74WQ52TWZK3I.jpg?auth=84a63adeea274f04f757bacb994fce6ff42268aa220a403ef818b1416eb02bcd&width=5674&quality=80)\n",
            "- [Google Serper](https://www.sangfor.com/blog/cybersecurity/black-basta-ransomware-attack-targets-ascension-healthcare)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}