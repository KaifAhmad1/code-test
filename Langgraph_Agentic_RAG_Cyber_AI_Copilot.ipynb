{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "2bf784bb-5c6b-48a1-8832-eac15b9ba0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 7.12MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 5.86MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 3.52MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 15.8MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.11MB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 9.05MB/s]\n",
            "2024-12-02 07:38:10.997550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-02 07:38:11.035949: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-02 07:38:11.052476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-02 07:38:11.080688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-02 07:38:12.648705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 225MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "\u001b[0mDownloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 18.6s\u001b[0K\u001b[1G161.3 MiB [] 0% 8.8s\u001b[0K\u001b[1G161.3 MiB [] 1% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 2% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 2% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 3% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 4% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 5% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 6% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 7% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 8% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 9% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 10% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 11% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 12% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 13% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 14% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 14% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 15% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 16% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 17% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 19% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 20% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 21% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 23% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 24% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 25% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 26% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 27% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 30% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 31% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 34% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 35% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 38% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 40% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 44% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 45% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 46% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 47% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 48% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 49% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 50% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 51% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 52% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 53% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 55% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 57% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 57% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 59% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 61% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 67% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 71% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 76% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 82% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 15.7s\u001b[0K\u001b[1G100.9 MiB [] 0% 9.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 7.8s\u001b[0K\u001b[1G100.9 MiB [] 1% 6.2s\u001b[0K\u001b[1G100.9 MiB [] 1% 4.7s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.9s\u001b[0K\u001b[1G100.9 MiB [] 2% 4.1s\u001b[0K\u001b[1G100.9 MiB [] 3% 3.9s\u001b[0K\u001b[1G100.9 MiB [] 4% 3.5s\u001b[0K\u001b[1G100.9 MiB [] 4% 3.2s\u001b[0K\u001b[1G100.9 MiB [] 5% 3.0s\u001b[0K\u001b[1G100.9 MiB [] 6% 2.8s\u001b[0K\u001b[1G100.9 MiB [] 7% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 8% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 9% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 10% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 10% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 11% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 12% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 13% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 14% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 16% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 20% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 21% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 23% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 25% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 27% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 28% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 29% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 32% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 33% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 35% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 36% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 37% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 38% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 40% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 41% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 43% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 44% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 45% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 46% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 47% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 49% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 51% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 52% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 56% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 57% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 59% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 61% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 63% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 64% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 66% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 68% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 90% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 11.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 5.6s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.6s\u001b[0K\u001b[1G87.6 MiB [] 1% 3.9s\u001b[0K\u001b[1G87.6 MiB [] 2% 3.9s\u001b[0K\u001b[1G87.6 MiB [] 2% 3.7s\u001b[0K\u001b[1G87.6 MiB [] 3% 3.6s\u001b[0K\u001b[1G87.6 MiB [] 3% 3.5s\u001b[0K\u001b[1G87.6 MiB [] 4% 3.3s\u001b[0K\u001b[1G87.6 MiB [] 4% 3.2s\u001b[0K\u001b[1G87.6 MiB [] 5% 3.1s\u001b[0K\u001b[1G87.6 MiB [] 6% 2.8s\u001b[0K\u001b[1G87.6 MiB [] 7% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 8% 2.5s\u001b[0K\u001b[1G87.6 MiB [] 9% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 10% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 11% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 12% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 12% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 13% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 14% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 15% 2.2s\u001b[0K\u001b[1G87.6 MiB [] 16% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 17% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 18% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 19% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 20% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 21% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 22% 2.1s\u001b[0K\u001b[1G87.6 MiB [] 23% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 24% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 27% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 28% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 30% 1.9s\u001b[0K\u001b[1G87.6 MiB [] 31% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 33% 1.7s\u001b[0K\u001b[1G87.6 MiB [] 34% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 35% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 36% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 38% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 39% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 41% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 42% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 43% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 45% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 47% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 48% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 49% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 50% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 51% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 52% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 53% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 54% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 55% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 56% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 57% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 59% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 60% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 61% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 62% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 63% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 64% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 65% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 67% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 68% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 69% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 70% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 71% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 72% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 73% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 73% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 74% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 75% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 76% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 78% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 79% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 80% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 81% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 83% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 85% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 86% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 88% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 92% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 17.4s\u001b[0K\u001b[1G95.5 MiB [] 0% 10.1s\u001b[0K\u001b[1G95.5 MiB [] 1% 4.6s\u001b[0K\u001b[1G95.5 MiB [] 2% 2.6s\u001b[0K\u001b[1G95.5 MiB [] 3% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 5% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 6% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 8% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 14% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 21% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 28% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 30% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 31% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 33% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 36% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 38% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 39% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 41% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 43% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 45% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 47% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 51% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 53% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 55% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 57% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 61% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 68% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 72% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 79% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 83% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 25% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 46% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 82% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "TAVILY_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605f59d4-7a54-47b3-8123-d6594db98869"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    # Query expansion with related terms and synonyms\n",
        "    related_terms = get_related_terms(query)\n",
        "    if related_terms:\n",
        "        enhanced_query += f\" related_terms:{', '.join(related_terms)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def get_related_terms(query: str) -> List[str]:\n",
        "    # Use an ontology or knowledge graph to identify related concepts and terms\n",
        "    related_terms = {\n",
        "        \"cyber attack\": [\"hacking\", \"data breach\", \"malware\", \"ransomware\"],\n",
        "        \"threat actor\": [\"cyber gang\", \"hacker group\", \"APT\"],\n",
        "        \"vulnerability\": [\"exploit\", \"CVE\", \"security flaw\"],\n",
        "        \"phishing\": [\"spear phishing\", \"email scam\", \"social engineering\"],\n",
        "        # Add more related terms as needed\n",
        "    }\n",
        "\n",
        "    # Find related terms for the query\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = []\n",
        "    for term in query_terms:\n",
        "        if term in related_terms:\n",
        "            found_terms.extend(related_terms[term])\n",
        "\n",
        "    return found_terms\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            logging.error(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "       - **Actionable Recommendations**:\n",
        "         - Offer precise, immediate actions and mitigation strategies.\n",
        "         - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "         - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    5. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor capabilities, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "    - **Malware**\n",
        "    - **Ransomware**\n",
        "    - **Vulnerability**\n",
        "    - **Exploit**\n",
        "    - **Phishing**\n",
        "    - **Data Breach**\n",
        "    - **Cyber Attack**\n",
        "    - **Incident Response**\n",
        "    - **MITRE ATT&CK**\n",
        "    - **Indicators of Compromise (IOCs)**\n",
        "    - **Command and Control (C2)**\n",
        "    - **Dates**\n",
        "    - **Times**\n",
        "    - **Trojans**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific', 'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing', 'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK', 'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates', 'Times', 'Trojans'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Highlight important information\n",
        "    important_keywords = [\n",
        "        'Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific',\n",
        "        'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing',\n",
        "        'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK',\n",
        "        'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates',\n",
        "        'Times', 'Trojans'\n",
        "    ]\n",
        "    highlighted_response = highlight_keywords(processed_response, important_keywords)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": highlighted_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {highlighted_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result\n",
        "\n",
        "# Advanced Query Expansion Technique\n",
        "def enhance_threat_actor_query(query: str, actor_specific_details: Dict[str, Any] = None) -> str:\n",
        "    # Extract key entities and context\n",
        "    entities = extract_key_entities(query)\n",
        "\n",
        "    # Dynamic query expansion based on detected entities\n",
        "    expanded_query_parts = []\n",
        "\n",
        "    if \"threat actor\" in query.lower() or any(actor in query.lower() for actor in [\"group\", \"gang\", \"cyber\"]):\n",
        "        # Threat Actor Specific Expansion\n",
        "        expanded_query_parts.extend([\n",
        "            f\"recent activities of {entities.get('actor_name', '')}\",\n",
        "            f\"cyber incidents associated with {entities.get('actor_name', '')}\",\n",
        "            \"latest threat intelligence\",\n",
        "            \"attack vectors\",\n",
        "            \"malware infrastructure\"\n",
        "        ])\n",
        "\n",
        "    # Temporal and Geographic Contextualization\n",
        "    expanded_query_parts.extend([\n",
        "        f\"2024 cyber threats\",\n",
        "        f\"recent cyber incidents in {entities.get('region', 'global')}\",\n",
        "        \"emerging cyber threat landscape\"\n",
        "    ])\n",
        "\n",
        "    # Combine with original query\n",
        "    enhanced_query = f\"{query} \" + \" \".join(expanded_query_parts)\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def extract_key_entities(query: str) -> Dict[str, str]:\n",
        "    # Use NER or rule-based extraction\n",
        "    entities = {\n",
        "        \"actor_name\": None,\n",
        "        \"region\": None,\n",
        "        \"target_industry\": None\n",
        "    }\n",
        "\n",
        "    # Predefined threat actor name detection\n",
        "    known_actors = [\n",
        "        \"LunarsGo\", \"Blackbasta\", \"Redline Infostealer\",\n",
        "        \"Lazarus Group\", \"Fancy Bear\", \"Cozy Bear\"\n",
        "    ]\n",
        "\n",
        "    for actor in known_actors:\n",
        "        if actor.lower() in query.lower():\n",
        "            entities[\"actor_name\"] = actor\n",
        "            break\n",
        "\n",
        "    # Add more sophisticated NER logic here\n",
        "    return entities\n",
        "\n",
        "# Threat Intelligence Ontology and Contextual Mapping\n",
        "THREAT_INTELLIGENCE_ONTOLOGY = {\n",
        "    \"threat_actors\": {\n",
        "        \"LunarsGo\": {\n",
        "            \"known_aliases\": [\"Luna Group\"],\n",
        "            \"typical_targets\": [\"financial institutions\", \"healthcare\"],\n",
        "            \"preferred_attack_vectors\": [\"phishing\", \"ransomware\"],\n",
        "            \"associated_malware\": [\"custom ransomware\", \"info-stealers\"]\n",
        "        },\n",
        "        # Add more threat actor profiles\n",
        "    },\n",
        "    \"attack_techniques\": {\n",
        "        \"ransomware\": [\"encryption\", \"data exfiltration\", \"double extortion\"],\n",
        "        \"info_stealing\": [\"credential harvesting\", \"system reconnaissance\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "def enrich_query_with_ontology(query: str, ontology: Dict) -> List[str]:\n",
        "    enriched_queries = []\n",
        "\n",
        "    for actor, details in ontology[\"threat_actors\"].items():\n",
        "        if actor.lower() in query.lower():\n",
        "            # Generate contextually rich query variations\n",
        "            enriched_queries.extend([\n",
        "                f\"{query} {alias}\" for alias in details.get(\"known_aliases\", [])\n",
        "            ])\n",
        "            enriched_queries.extend([\n",
        "                f\"{query} targeting {target}\" for target in details.get(\"typical_targets\", [])\n",
        "            ])\n",
        "            enriched_queries.extend([\n",
        "                f\"{query} {technique}\" for technique in details.get(\"preferred_attack_vectors\", [])\n",
        "            ])\n",
        "\n",
        "    return enriched_queries\n",
        "\n",
        "# Multi-Source Semantic Search Strategy\n",
        "def semantic_threat_search(query: str, embedding_model, vector_store):\n",
        "    # Generate multiple semantic embeddings\n",
        "    query_variations = [\n",
        "        query,  # Original query\n",
        "        f\"cyber threat {query}\",\n",
        "        f\"latest cyber incident {query}\",\n",
        "        f\"threat intelligence {query}\"\n",
        "    ]\n",
        "\n",
        "    semantic_results = []\n",
        "    for variation in query_variations:\n",
        "        semantic_embedding = embedding_model.embed_query(variation)\n",
        "        semantic_results.extend(\n",
        "            vector_store.similarity_search_by_vector(semantic_embedding, k=3)\n",
        "        )\n",
        "\n",
        "    return semantic_results\n",
        "\n",
        "# Adaptive Confidence Scoring\n",
        "def adaptive_confidence_scoring(search_results):\n",
        "    confidence_factors = {\n",
        "        \"source_reputation\": {\n",
        "            \"threat intelligence platform\": 0.9,\n",
        "            \"cybersecurity research\": 0.8,\n",
        "            \"news media\": 0.6,\n",
        "            \"blog\": 0.5\n",
        "        },\n",
        "        \"recency_weight\": lambda days: max(0, 1 - (days / 365)),\n",
        "        \"specificity_multiplier\": {\n",
        "            \"exact_match\": 1.2,\n",
        "            \"partial_match\": 1.0,\n",
        "            \"weak_match\": 0.7\n",
        "        }\n",
        "    }\n",
        "\n",
        "    scored_results = []\n",
        "    for result in search_results:\n",
        "        # Calculate confidence\n",
        "        source_confidence = confidence_factors[\"source_reputation\"].get(\n",
        "            result.source.lower(), 0.5\n",
        "        )\n",
        "\n",
        "        recency_score = confidence_factors[\"recency_weight\"](\n",
        "            (datetime.now() - parse_date(result.date)).days if result.date else 365\n",
        "        )\n",
        "\n",
        "        # Combine scoring factors\n",
        "        final_confidence = source_confidence * recency_score\n",
        "        result.confidence_score = final_confidence\n",
        "        scored_results.append(result)\n",
        "\n",
        "    return sorted(scored_results, key=lambda x: x.confidence_score, reverse=True)"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb976ace-fecf-4c9e-984c-51fa27a2eb6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor? 2024 OR 2024 recent threat actor groups gangs companies locations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-afc98cd362de>:40: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = search_and_contents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw results from Exa Search: Title: Our response to a recent security incident\n",
            "URL: https://www.goto.com/blog/our-response-to-a-recent-security-incident\n",
            "ID: https://www.goto.com/blog/our-response-to-a-recent-security-incident\n",
            "Score: 0.16677381098270416\n",
            "Published Date: 2023-04-20T00:00:00.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Update as of Thursday, April 20, 2023  \n",
            "To All GoTo Customers,\n",
            "We recently concluded our investigation into the security incident first shared with customers in November of 2022 regarding unauthorized activity in a third-party cloud storage environment. We eliminated the threat actor’s access to the environment and found no evidence of additional compromise or threat actor activity beyond what we previously disclosed as impacted in January. In addition, we concluded that GoTo Resolve, GoTo Connect, GoTo Meeting, GoTo Webinar, GoTo Contact Center, GoTo Assist, GoTo Training, and Grasshopper had no impact at all.\n",
            "We are constantly enhancing our security measures and monitoring capabilities to protect our customers, including:\n",
            "Accelerated the migration of customer accounts onto our enhanced Identity Management Platform.\n",
            "Full analysis of existing controls and configurations, and necessary changes to further harden existing environments.\n",
            "Comprehensive reviews of and, where appropriate, enhancements to our encryption practices within our applications and backup infrastructure.\n",
            "Thank you for your continued patience and understanding while we completed the investigation. We take our commitment to protect our customers very seriously and will continue to undertake efforts to ensure our services and infrastructure remain secure and are designed to detect and prevent future threats.\n",
            "Paddy Srinivasan\n",
            "CEO, GoTo\n",
            "  Update as of Monday, January 23, 2023  \n",
            "To All GoTo Customers,\n",
            "I am writing to update you on our ongoing investigation about the security incident we told you about in November 2022. \n",
            "Our investigation to date has determined that a threat actor exfiltrated encrypted backups from a third-party cloud storage service related to the following products: Central, Pro, join.me, Hamachi, and RemotelyAnywhere. We also have evidence that a threat actor exfiltrated an encryption key for a portion of the encrypted backups. The affected information, which varies by product, may include account usernames, salted and hashed passwords, a portion of Multi-Factor Authentication (MFA) settings, as well as some product settings and licensing information. In addition, while Rescue and GoToMyPC encrypted databases were not exfiltrated, MFA settings of a small subset of their customers were impacted. \n",
            "At this time, we have no evidence of exfiltration affecting any other GoTo products other than those referenced above or any of GoTo’s production systems.\n",
            " We are contacting affected customers directly to provide additional information and recommend actionable steps for them to take to further secure their accounts. Even though all account passwords were salted and hashed in accordance with best practices, out of an abundance of caution, we will also reset the passwords of affected users and/or reauthorize MFA settings where applicable. In addition, we are migrating their accounts onto an enhanced Identity Management Platform, which will provide additional security with more robust authentication and login-based security options. \n",
            " As a reminder, GoTo does not store full credit card or bank details. In addition, GoTo does not collect or use end user personal information, such as date of birth, home address, or Social Security numbers. \n",
            "We appreciate your understanding while we continue to work expeditiously to complete our investigation.\n",
            "Paddy Srinivasan\n",
            "CEO, GoTo\n",
            "   .   \n",
            "        Original Post from November 30, 2022  \n",
            "To All GoTo Customers,\n",
            "I am writing to inform you that GoTo is investigating a security incident. While we are currently working to better understand the scope of the issue, we wanted to let you know about the situation and how we are responding.\n",
            "Upon learning of the incident, we immediately launched an investigation, engaged Mandiant, a leading security firm, and alerted law enforcement. Based on the investigation to date, we have detected unusual activity within our development environment and third-party cloud storage service. The third-party cloud storage service is currently shared by both GoTo and  its affiliate, LastPass .\n",
            "GoTo’s products and services remain fully functional, and we are committed to our customers. As part of our efforts, we also continue to deploy enhanced security measures and monitoring capabilities across our infrastructure to help detect and prevent threat actor activity.\n",
            "Thank you for your patience as we work expeditiously to complete our investigation. We will continue to update you.\n",
            "Paddy Srinivasan\n",
            "CEO\n",
            "Highlights: ['While we are currently working to better understand the scope of the issue, we wanted to let you know about the situation and how we are responding. Upon learning of the incident, we immediately launched an investigation, engaged Mandiant, a leading security firm, and alerted law enforcement. Based on the investigation to date, we have detected unusual activity within our development environment and third-party cloud storage service. The third-party cloud storage service is currently shared by both GoTo and  its affiliate, LastPass . GoTo’s products and services remain fully functional, and we are committed to our customers.']\n",
            "Highlight Scores: [0.5169363021850586]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Security Incident December 2022 Update - LastPass - The LastPass Blog\n",
            "URL: https://blog.lastpass.com/posts/2022/12/notice-of-recent-security-incident\n",
            "ID: https://blog.lastpass.com/posts/2022/12/notice-of-recent-security-incident\n",
            "Score: 0.16499251127243042\n",
            "Published Date: 2022-08-25T00:00:00.000Z\n",
            "Author: Karim Toubba\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Please refer to the latest article for updated information.  \n",
            "    \n",
            "   \n",
            "   \n",
            "   \n",
            " \n",
            "   \n",
            "   \n",
            " \n",
            " \n",
            "   \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "   \n",
            " Update as of Wednesday, November 30, 2022\n",
            "Highlights: ['Please refer to the latest article for updated information. Update as of Wednesday, November 30, 2022']\n",
            "Highlight Scores: [0.22966259717941284]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Luna Moth Gang Invests in Call Centers to Target Businesses with Callback Phishing Campaigns\n",
            "URL: https://thehackernews.com/2022/11/luna-moth-gang-invests-in-call-centers.html\n",
            "ID: https://thehackernews.com/2022/11/luna-moth-gang-invests-in-call-centers.html\n",
            "Score: 0.16087521612644196\n",
            "Published Date: 2022-11-22T12:05:40.000Z\n",
            "Author: Nov 22, 2022Ravie Lakshmanan\n",
            "Image: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh09Ia1eCE0X1Yh1yk9kgsTxSKKbD8Z9rItd8pM6oeG41V__6xPr1UQJl66wkSEf6JhEBHCL-RtVww0V1zo-5wPCzJujQs2UrWhJ2x82hLZLUHsCBkvqFTD87BpAGIhcP7U22WHLs1hJhvSuNN8RnGjW1SrJNrQkKrZYmYpxTXVn2nYgsCEV9yb_kfU/s728-rw-e365/callback.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The Luna Moth campaign has extorted hundreds of thousands of dollars from several victims in the legal and retail sectors.\n",
            "The attacks are notable for employing a technique called callback phishing or telephone-oriented attack delivery (TOAD), wherein the victims are social engineered into making a phone call through phishing emails containing invoices and subscription-themed lures.\n",
            "Palo Alto Networks Unit 42 said the attacks are the \"product of a single highly organized campaign,\" adding, \"this threat actor has significantly invested in call centers and infrastructure that's unique to each victim.\"\n",
            "The cybersecurity firm described the activity as a \"pervasive multi-month campaign that is actively evolving.\"\n",
            " \n",
            "What's notable about callback phishing is that the email messages are completely devoid of any malicious attachment or booby-trapped link, allowing them to evade detection and slip past email protection solutions.\n",
            "These messages typically come with an invoice that includes a phone number that the users can call to cancel the supposed subscription. In reality, however, the victims are routed to an actor-controlled call center and connected to a live agent on the other end, who ends up installing a remote access tool for persistence.\n",
            "\"The attacker will then seek to identify valuable information on the victim's computer and connected file shares, and they will quietly exfiltrate it to a server they control using a file transfer tool,\" Unit 42 researcher Kristopher Russo said.\n",
            "   \n",
            "The campaign may be resource intensive, but is also technically less sophisticated and likely to have a much higher success rate than other phishing attacks.\n",
            "On top of that, it enables extortion without encryption, permitting malicious actors to plunder sensitive data sans the need to deploy ransomware to lock the files after exfiltration.\n",
            "The Luna Moth actor, also known as Silent Ransom, has become an expert of sorts when it comes to pulling off such schemes. According to AdvIntel, the cybercrime group is believed to be the mastermind behind the BazarCall attacks last year.\n",
            " \n",
            "To give these attacks a veneer of legitimacy, the adversaries, instead of dropping a malware like BazarLoader, take advantage of legitimate tools like Zoho Assist to remotely interact with a victim's computer, abusing the access to deploy other trusted software such as Rclone or WinSCP for harvesting data.\n",
            "Extortion demands range from two to 78 Bitcoin based on the organization targeted, with the threat actor creating unique cryptocurrency wallets for each payment. The adversary is also said to offer discounts of nearly 25% for prompt payment, although there's no guarantee that the data is deleted.\n",
            "\"The threat actors behind this campaign have taken great pains to avoid all non-essential tools and malware, to minimize the potential for detection,\" Russo said. \"Since there are very few early indicators that a victim is under attack, employee cybersecurity awareness training is the first line of defense.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: [\"The Luna Moth campaign has extorted hundreds of thousands of dollars from several victims in the legal and retail sectors. The attacks are notable for employing a technique called callback phishing or telephone-oriented attack delivery (TOAD), wherein the victims are social engineered into making a phone call through phishing emails containing invoices and subscription-themed lures. What's notable about callback phishing is that the email messages are completely devoid of any malicious attachment or booby-trapped link, allowing them to evade detection and slip past email protection solutions. These messages typically come with an invoice that includes a phone number that the users can call to cancel the supposed subscription. In reality, however, the victims are routed to an actor-controlled call center and connected to a live agent on the other end, who ends up installing a remote access tool for persistence.\"]\n",
            "Highlight Scores: [0.457763671875]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: 12-2-2022 Security Incident\n",
            "URL: https://support.8x8.com/support-services/support/12-2-2022_Security_Incident\n",
            "ID: https://support.8x8.com/support-services/support/12-2-2022_Security_Incident\n",
            "Score: 0.15967534482479095\n",
            "Published Date: 2023-04-13T16:42:28.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Last updated\n",
            " \n",
            "Save as PDF\n",
            " \n",
            " Loading...   \n",
            " Issue\n",
            "On December 2, 2022 we received a report of an internet post that was published mentioning 8x8. We take any potential compromise seriously and have initiated our Incident Response plans. The teams are actively working through the processes.\n",
            "  Status\n",
            "12-2-2022 10:44 AM Pacific : As we investigate this report, we determined no 8x8 UCaaS or CCaaS production systems, or customer content, was impacted. \n",
            "12-5-2022 5:24 PM Pacific: We are actively investigating with internal and third party forensic teams. We believe several IT servers may have been accessed by an unauthorized third-party. We do not believe any customer content was compromised. The teams are actively working through the processes.\n",
            "1-5-2023 7:00 AM Pacific: Our investigation with internal and third party forensic teams concluded that there was no live attack on 12/2/2022, but that the post was instead related to a September 2022 incident. \n",
            "In September 2022, we identified and mitigated an attempted ransomware attack in real-time, and removed the attacker from an 8x8 internal IT network segment. Our investigation identified a limited number of IT servers at issue, none of which involved our 8x8 UCaas and CCaaS production systems or customer content (customer information that is transmitted through our SaaS service such as voicemails, phone numbers, call records, etc.). We provided notice of the incident to relevant data protection authorities and believed that the incident had been contained.\n",
            "Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps.\n",
            " ||||I||||Skip to main content\n",
            "Visit the 8x8 Community!\n",
            "1. Hello, Anonymous User!\n",
            "1. My contributions\n",
            "2. My preferences\n",
            "3. My subscriptions\n",
            "4. Sign out\n",
            "Need Help? Get Support\n",
            "1. Search site\n",
            "Search Search\n",
            "Go back to previous article\n",
            "2.\n",
            "1. Sign in\n",
            "Expand/collapse global hierarchy\n",
            "1. Home\n",
            "2. Support & Services\n",
            "3. Technical Support\n",
            "4. 12-2-2022 Security Incident\n",
            "Expand/collapse global location\n",
            "12-2-2022 Security Incident\n",
            "1. Last updated\n",
            "2. Save as PDF\n",
            "3. Share\n",
            "1. Share\n",
            "2. Tweet\n",
            "3. Share\n",
            "Loading...\n",
            "Check out our upcoming webinars and get the most out of your 8x8 services!For faster support help, start a chat!Looking for user guides and product manuals? Click here!\n",
            "PrevNext\n",
            "Table of Contents\n",
            "1. Issue\n",
            "2. Status\n",
            "1. Issue\n",
            "2. Status\n",
            "Issue\n",
            "On December 2, 2022 we received a report of an internet post that was published mentioning 8x8. We take any potential compromise seriously and have initiated our Incident Response plans. The teams are actively working through the processes.\n",
            "Status\n",
            "12-2-2022 10:44 AM Pacific : As we investigate this report, we determined no 8x8 UCaaS or CCaaS production systems, or customer content, was impacted.\n",
            "12-5-2022 5:24 PM Pacific: We are actively investigating with internal and third party forensic teams. We believe several IT servers may have been accessed by an unauthorized third-party. We do not believe any customer content was compromised. The teams are actively working through the processes.\n",
            "1-5-2023 7:00 AM Pacific: Our investigation with internal and third party forensic teams concluded that there was no live attack on 12/2/2022, but that the post was instead related to a September 2022 incident.\n",
            "In September 2022, we identified and mitigated an attempted ransomware attack in real-time, and removed the attacker from an 8x8 internal IT network segment. Our investigation identified a limited number of IT servers at issue, none of which involved our 8x8 UCaas and CCaaS production systems or customer content (customer information that is transmitted through our SaaS service such as voicemails, phone numbers, call records, etc.). We provided notice of the incident to relevant data protection authorities and believed that the incident had been contained.\n",
            "Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps.\n",
            "1. Back to top\n",
            "2.\n",
            "+ Technical Support\n",
            "+ 8x8 Implementation Packages Overview\n",
            "* Was this article helpful?\n",
            "* Yes\n",
            "* No\n",
            "*\n",
            "Recommended articles\n",
            "1. Article type How-to Confidence Validated Flagged Not Flagged Governance Experience KCS Enabled Yes Visibility Public\n",
            "2. Tags\n",
            "1. dark web\n",
            "2. lockbit\n",
            "3. Ransomware\n",
            "1. © Copyright 2023 8x8 Support\n",
            "2. Powered by CXone Expert ®\n",
            "User Quick Links\n",
            "Log in to your 8x8 Application Panel\n",
            "Log in to 8x8 Work for Web\n",
            "Log in to ContactNow Support\n",
            "See Current System Status\n",
            "Co-Browse Help | sgs-ocx-css-team\n",
            "2023 8x8, Inc. All rights reserved.\n",
            "Privacy Policy | Terms & Conditions\n",
            "Need Help? Get Support\n",
            "Highlights: ['Based on contact with the threat actor after the 12/2/2022 post, we have determined that the unauthorized third party in the September attack had exfiltrated a limited amount of business records, including 2018 payment authorization forms for a small number of current EU region customers, and some employee payroll data in EMEA. We are in the process of reaching out to all impacted customers and employees with next steps. Check out our upcoming webinars and get the most out of your 8x8 services!For faster support help, start a chat!Looking for user guides and product manuals? Click here! On December 2, 2022 we received a report of an internet post that was published mentioning 8x8.']\n",
            "Highlight Scores: [0.4585976302623749]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Hackers use new, fake crypto app to breach networks, steal cryptocurrency\n",
            "URL: https://www.bleepingcomputer.com/news/security/hackers-use-new-fake-crypto-app-to-breach-networks-steal-cryptocurrency/\n",
            "ID: https://www.bleepingcomputer.com/news/security/hackers-use-new-fake-crypto-app-to-breach-networks-steal-cryptocurrency/\n",
            "Score: 0.15747074782848358\n",
            "Published Date: 2022-12-03T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The North Korean 'Lazarus' hacking group is linked to a new attack spreading fake cryptocurrency apps under the made-up brand, \"BloxHolder,\" to install the AppleJeus malware for initial access to networks and steal crypto assets.\n",
            "According to a joint FBI and CISA report from February 2021, AppleJeus has been in circulation since at least 2018, used by Lazarus in cryptocurrency hijacking and digital asset theft operations.\n",
            "A new report by Volexity has identified new, fake crypto programs and AppleJeus activity, with signs of evolution in the malware's infection chain and abilities.\n",
            "New BloxHolder campaign\n",
            "The new campaign attributed to Lazarus started in June 2022 and was active until at least October 2022.\n",
            "In this campaign, the threat actors used the \"bloxholder[.]com\" domain, a clone of the HaasOnline automated cryptocurrency trading platform.\n",
            "  Legitimate (left) and clone website (right) (Volexity)   \n",
            "This website distributed a 12.7MB Windows MSI installer that pretended to be the BloxHolder app. However, in reality, it was the AppleJeus malware bundled with the QTBitcoinTrader app.\n",
            "In October 2022, the hacking group evolved their campaign to use Microsoft Office documents instead of the MSI installer to distribute the malware.\n",
            "The 214KB document was named 'OKX Binance &amp; Huobi VIP fee comparision.xls' and contained a macro that creates three files on a target's computer.\n",
            "Volexity couldn't retrieve the final payload from this later infection chain, but they noticed similarities in the DLL sideloading mechanism found in the previously used MSI installer attacks, so they're confident it's the same campaign.\n",
            "Upon installation through the MSI infection chain, AppleJeus will create a scheduled task and drop additional files in the folder \"%APPDATA%\\Roaming\\Bloxholder\\\".\n",
            "Next, the malware will collect the MAC address, computer name, and OS version and send it to the C2 via a POST request, likely to identify if it's running on a virtual machine or sandbox.\n",
            "One novel element in recent campaigns is chained DLL sideloading to load the malware from within a trusted process, evading AV detection.\n",
            "\"Specifically, \"CameraSettingsUIHost.exe\" loads the \"dui70.dll\" file from the \"System32\" directory, which then causes the loading of the malicious \"DUser.dll\" file from the application's directory into the \"CameraSettingsUIHost.exe\" process,\" explains Volexity.\n",
            "\"The \"dui70.dll\" file is the \"Windows DirectUI Engine\" and is normally installed as part of the operating system.\"\n",
            "  Chained DLL sideloading (Volexity)   \n",
            "Volexity says the reason Lazarus opted for chained DLL sideloading is unclear but might be to impede malware analysis.\n",
            "Another new characteristic in recent AppleJeus samples is that all its strings and API calls are now obfuscated using a custom algorithm, making them stealthier against security products.\n",
            "Although Lazarus' focus on cryptocurrency assets is well documented, the North Korean hackers remain fixed on their goal to steal digital money, constantly refreshing themes and improving tools to stay as stealthy as possible.\n",
            "Who is the Lazarus Group\n",
            "The Lazarus Group (also tracked as ZINC) is a North Korean hacking group that has been active since at least 2009.\n",
            "The group gained notoriety after hacking Sony Films in Operation Blockbuster and the 2017 global WannaCry ransomware campaign that encrypted businesses worldwide.\n",
            "Google discovered in January 2021 that Lazarus was creating fake online personas to target security researchers in social engineering attacks that installed backdoors on their devices. A second attack using this tactic was discovered in March 2021.\n",
            "The U.S. government sanctioned the Lazarus hacking group in September 2019 and now offers a reward of up to $5 million for information that can disrupt their activities.\n",
            "More recent attacks have turned to the spreading of trojanized cryptocurrency wallets and trading apps that steal people's private keys and drain their crypto assets.\n",
            "In April, the U.S. government linked the Lazarus group to a cyberattack on Axie Infinity that allowed them to steal over $617 million worth of Ethereum and USDC tokens.\n",
            "It was later revealed that the Axie Infinity hack was made possible due to a phishing attack containing a malicious PDF file pretending to be a job offer sent to one of the company's engineers.\n",
            "Highlights: [\"In April, the U.S. government linked the Lazarus group to a cyberattack on Axie Infinity that allowed them to steal over $617 million worth of Ethereum and USDC tokens. It was later revealed that the Axie Infinity hack was made possible due to a phishing attack containing a malicious PDF file pretending to be a job offer sent to one of the company's engineers.\"]\n",
            "Highlight Scores: [0.4428800642490387]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is a link to the latest cyber incident involving the LunarsGo threat actor in 2024:\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[INIT].... → Crawl4AI 0.3.746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-afc98cd362de>:288: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
            "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
            "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
            "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
            "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
            "Pass warning=False to suppress this warning.\n",
            "  result = await crawler.arun(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[ERROR]... × No URL... | Error: \n",
            "┌───────────────────────────────────────────────────────────────────────────────┐\n",
            "│ × URL must start with 'http://', 'https://', 'file://', or 'raw:'             │\n",
            "└───────────────────────────────────────────────────────────────────────────────┘\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ERROR: Failed to crawl the page No URL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[INIT].... → Crawl4AI 0.3.746\n",
            "[FETCH]... ↓ https://www.recordedfuture.com/threat-intelligence... | Status: True | Time: 24.47s\n",
            "[SCRAPE].. ◆ Processed https://www.recordedfuture.com/threat-intelligence... | Time: 522ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.recordedfuture.com/threat-intelligence... | Time: 0.49914895900019474s\n",
            "[COMPLETE] ● https://www.recordedfuture.com/threat-intelligence... | Status: True | Total: 25.67s\n",
            "[FETCH]... ↓ https://www.picussecurity.com/resource/blog/may-10... | Status: True | Time: 30.77s\n",
            "[SCRAPE].. ◆ Processed https://www.picussecurity.com/resource/blog/may-10... | Time: 516ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.picussecurity.com/resource/blog/may-10... | Time: 0.36305660100060777s\n",
            "[COMPLETE] ● https://www.picussecurity.com/resource/blog/may-10... | Status: True | Total: 31.77s\n",
            "[FETCH]... ↓ https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Time: 31.97s\n",
            "[SCRAPE].. ◆ Processed https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 501ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 0.15895731199998409s\n",
            "[COMPLETE] ● https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Total: 32.84s\n",
            "[FETCH]... ↓ https://securelist.com/malware-report-q3-2024/1146... | Status: True | Time: 43.62s\n",
            "[SCRAPE].. ◆ Processed https://securelist.com/malware-report-q3-2024/1146... | Time: 490ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://securelist.com/malware-report-q3-2024/1146... | Time: 0.32705635499951313s\n",
            "[COMPLETE] ● https://securelist.com/malware-report-q3-2024/1146... | Status: True | Total: 44.49s\n",
            "Crawled Results: [[{'links': '#', 'images': 'https://socradar.io/wp-content/themes/socradar/assets/image/static/shadow-5.png.webp'}], [{'links': 'https://www.picussecurity.com/privacy', 'images': 'https://www.picussecurity.com/hubfs/light_logo-original-SVG.svg'}], [{'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=233546881.37c51b88c43feb7b4d2edea622bfe0d2.1644012949909.1644012949909.1644012949909.1&__hssc=233546881.9.1644012949909&__hsfp=3094949200', 'images': 'https://cms.recordedfuture.com/uploads/brand_logo_long_black_f2ead5b5c6.svg?w=3840'}], [{'links': 'https://www.cookiebot.com/en/what-is-behind-powered-by-cookiebot/', 'images': 'data:,'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#', 'images': 'https://socradar.io/wp-content/themes/socradar/assets/image/static/shadow-5.png.webp'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.picussecurity.com/privacy', 'images': 'https://www.picussecurity.com/hubfs/light_logo-original-SVG.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=233546881.37c51b88c43feb7b4d2edea622bfe0d2.1644012949909.1644012949909.1644012949909.1&__hssc=233546881.9.1644012949909&__hsfp=3094949200', 'images': 'https://cms.recordedfuture.com/uploads/brand_logo_long_black_f2ead5b5c6.svg?w=3840'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.cookiebot.com/en/what-is-behind-powered-by-cookiebot/', 'images': 'data:,'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://istari-global.com/assets/Uploads/Ensign-threat-examples.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/qilin-ransomware-threat-actor-card.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://img.securityinfowatch.com/files/base/cygnus/siw/image/2024/01/65a04d811a5d24001e668b70-bigstock136552454.png?auto=format,compress&fit=fill&fill=blur&w=1200&h=630\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/cyber-army-of-russia-reborn-threat-actor-card.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://website.cdn.group-ib.com/wp-content/uploads/iab-by-country-and-industry.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://gbsits.com/wp-content/uploads/2024/06/LinkedIn-Post-2024-17.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "This analysis provides an overview of the top threat actors, malware, vulnerabilities, and exploits in 2024. The threat landscape has evolved, with a significant increase in ransomware attacks, phishing schemes, and sophisticated malware. The analysis highlights the top threat actors, including RansomHub, Qilin, Dark Angels, LockBit, Whitewarlock, and IntelBroker. The report also discusses the emergence of new malware families, such as Nood RAT, Revenge RAT, and DarkMe RAT. [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "### **Top Threat Actors** [Vector Search](No URL)\n",
            "\n",
            "1. **RansomHub**: A well-known ransomware group that has been actively involved in various attacks throughout 2024.\n",
            "2. **Qilin**: A ransomware group that has been linked to several high-profile attacks, including the breach of a major financial institution.\n",
            "3. **Dark Angels**: A threat actor group that has been involved in various cybercrimes, including ransomware attacks and data breaches.\n",
            "4. **LockBit**: A ransomware group that has been responsible for several high-profile attacks, including the breach of a major healthcare organization.\n",
            "5. **Whitewarlock**: A threat actor group that has been involved in various cybercrimes, including ransomware attacks and phishing schemes.\n",
            "6. **IntelBroker**: A threat actor group that has been linked to several high-profile attacks, including the breach of a major financial institution. [Tavily Search](https://www.picussecurity.com/resource/blog/february-2024-latest-malware-vulnerabilities-and-exploits)\n",
            "\n",
            "### ****Malware**** [Vector Search](No URL)\n",
            "\n",
            "1. **Nood RAT**: A Linux-focused variant of the well-known Gh0st RAT that has been actively used in cyberattacks since its first detection in 2018.\n",
            "2. **Revenge RAT**: A malware family that has been linked to several high-profile attacks, including the breach of a major financial institution.\n",
            "3. **DarkMe RAT**: A sophisticated malware family that has been linked to several high-profile attacks, including the breach of a major healthcare organization. [Tavily Search](https://www.picussecurity.com/resource/blog/february-2024-latest-malware-vulnerabilities-and-exploits)\n",
            "\n",
            "### **Vulnerabilities and **Exploit**s** [Vector Search](No URL)\n",
            "\n",
            "1. **CVE-2024-29824**: A critical vulnerability in Ivanti's Endpoint Manager (EPM) appliances that can allow threat actors to gain remote code execution.\n",
            "2. **CVE-2024-45519**: A vulnerability in a third-party library used by several popular applications that can allow threat actors to execute arbitrary code. [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/october-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "### ****Phishing** Schemes** [Vector Search](No URL)\n",
            "\n",
            "1. ****Phishing** campaigns targeting financial institutions**: Threat actors have been using phishing campaigns to target financial institutions, including banks and credit unions.\n",
            "2. ****Phishing** campaigns targeting healthcare organizations**: Threat actors have been using phishing campaigns to target healthcare organizations, including hospitals and medical research institutions. [Tavily Search](https://www.picussecurity.com/resource/blog/february-2024-latest-malware-vulnerabilities-and-exploits)\n",
            "\n",
            "### ****Data Breach**es** [Vector Search](No URL)\n",
            "\n",
            "1. **Change Healthcare breach**: A major healthcare organization suffered a data breach, exposing sensitive patient information.\n",
            "2. **Integris Health breach**: A healthcare organization suffered a data breach, exposing sensitive patient information. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "### ****Incident Response**** [Vector Search](No URL)\n",
            "\n",
            "1. **Incident response plan**: Organizations should have an incident response plan in place to quickly respond to and contain cyber threats.\n",
            "2. **Regular security audits**: Regular security audits should be performed to identify vulnerabilities and weaknesses in the organization's security posture. [Vector Search](No URL)\n",
            "\n",
            "**Most Recent Relevant Activities**\n",
            "\n",
            "1. ****Ransomware** attacks**: **Ransomware** attacks have been on the rise throughout 2024, with several high-profile attacks reported.\n",
            "2. ****Phishing** campaigns**: **Phishing** campaigns have been used to target financial institutions and healthcare organizations.\n",
            "3. **Data breaches**: Data breaches have been reported at several organizations, including healthcare organizations and financial institutions. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "**Source Citations and Evidence**\n",
            "\n",
            "1. **SOCRadar**: SOCRadar is a threat intelligence platform that provides real-time threat intelligence and analytics.\n",
            "2. **Google Serper**: Google Serper is a search engine that provides access to a wide range of sources, including news articles, research papers, and academic journals.\n",
            "3. **Picus Security**: Picus Security is a cybersecurity firm that provides threat intelligence and incident response services. [Vector Search](No URL)\n",
            "\n",
            "**Actionable Recommendations**\n",
            "\n",
            "1. **Implement a robust security posture**: Organizations should implement a robust security posture, including regular security audits and incident response planning.\n",
            "2. **Use threat intelligence**: Organizations should use threat intelligence to stay ahead of emerging threats and vulnerabilities.\n",
            "3. **Implement security awareness training**: Organizations should implement security awareness training to educate employees on cybersecurity best practices. [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "\n",
            "**Long-Term Forecast and Monitoring**\n",
            "\n",
            "1. **Emerging threats**: Emerging threats, including ransomware and phishing campaigns, should be monitored closely.\n",
            "2. ****Vulnerability** management**: Organizations should prioritize vulnerability management to identify and address vulnerabilities before they can be exploited.\n",
            "3. **Incident response planning**: Organizations should have an incident response plan in place to quickly respond to and contain cyber threats. [Vector Search](No URL)\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper Image Search](https://istari-global.com/assets/Uploads/Ensign-threat-examples.png)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg)\n",
            "- [Google Serper Image Search](https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg)\n",
            "- [Tavily Search](https://www.crn.com/news/security/2024/10-major-cyberattacks-and-data-breaches-in-2024-so-far)\n",
            "- [Google Serper Image Search](https://website.cdn.group-ib.com/wp-content/uploads/iab-by-country-and-industry.png)\n",
            "- [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "- [Google Serper](https://homeland.house.gov/wp-content/uploads/2024/11/11.12.24-Cyber-Threat-Snapshot.pdf)\n",
            "- [Google Serper](https://techinformed.com/ransomware-threat-groups-2024-aim-attacks-ai-cybercrime/)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/qilin-ransomware-threat-actor-card.png.webp)\n",
            "- [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/october-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Serper](https://socradar.io/top-10-threat-actors-of-2024-beyond-the-numbers/)\n",
            "- [Tavily Search](https://intellizence.com/insights/business-signals-trends/major-cyber-attacks-data-breaches-leading-companies/)\n",
            "- [Google Serper Image Search](https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg)\n",
            "- [Google Serper Image Search](https://img.securityinfowatch.com/files/base/cygnus/siw/image/2024/01/65a04d811a5d24001e668b70-bigstock136552454.png?auto=format,compress&fit=fill&fill=blur&w=1200&h=630)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/cyber-army-of-russia-reborn-threat-actor-card.png.webp)\n",
            "- [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n",
            "- [Google Serper](https://securelist.com/malware-report-q3-2024/114678/)\n",
            "- [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/biggest-cyber-attacks-data-breaches-ransomware-attacks-february-2024)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp)\n",
            "- [Google Serper](https://unit42.paloaltonetworks.com/threat-assessment-north-korean-threat-groups-2024/)\n",
            "- [Google Serper Image Search](https://gbsits.com/wp-content/uploads/2024/06/LinkedIn-Post-2024-17.png)\n",
            "- [Google Programmable Search](https://www.techtarget.com/whatis/definition/threat-actor)\n",
            "- [Google Serper](https://www.infosecurity-magazine.com/news/new-ransomware-groups-emerge-2024/)\n",
            "- [Tavily Search](https://www.picussecurity.com/resource/blog/february-2024-latest-malware-vulnerabilities-and-exploits)\n",
            "- [Google Serper](https://www.recordedfuture.com/threat-intelligence-101/cyber-threats/ransomware-groups)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}