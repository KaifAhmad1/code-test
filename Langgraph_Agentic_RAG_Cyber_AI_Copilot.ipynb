{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "795952db-f8a5-4452-970c-f02e8337fb99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all] langchain-openai\n",
        "!pip install --upgrade --quiet faiss\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import JinaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_compressors import JinaRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "JINA_API_KEY =\"jina_72680535332e480f80ca0833953b3f3066AVc7Bh7cAoYB0uWt0CvMTgrJsq\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"JINA_API_KEY\"] = JINA_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b87338-3c78-4625-dcff-b426e18025f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "async def extract_content_from_url(url):\n",
        "    schema = {\n",
        "        \"name\": \"Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "        print(f\"DEBUG: Extracted content from {url}: {extracted_content}\")\n",
        "        return extracted_content\n",
        "\n",
        "async def scrape_links_and_content(urls):\n",
        "    tasks = [extract_content_from_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return [result for result in results if result is not None]"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search),\n",
        "        (\"Google Serper Image Search\", google_serper_image_search),\n",
        "        (\"Google Programmable Image Search\", google_programmable_image_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in {name}: {str(e)}\")\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        parsed_date = parse_date(x.date)\n",
        "        return (parsed_date is not None, parsed_date or datetime.min, x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant and recent results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    # Extract URLs for further crawling\n",
        "    urls_to_crawl = [result.url for result in top_results if result.url]\n",
        "\n",
        "    # Scrape links and content from the extracted URLs\n",
        "    crawled_results = asyncio.run(scrape_links_and_content(urls_to_crawl))\n",
        "\n",
        "    # Add crawled results to the state\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Crawled Results\", \"crawled_results\": crawled_results})\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state\n",
        "\n",
        "# Initialize Jina Reranker\n",
        "compressor = JinaRerank()\n",
        "\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "def rerank_results(query: str, results: List[SearchResult]) -> List[SearchResult]:\n",
        "    # Extract text content for reranking\n",
        "    texts = [result.snippet for result in results]\n",
        "    # Rerank the results based on the query\n",
        "    reranked_results = compression_retriever.get_relevant_documents(query)\n",
        "    # Map reranked Document objects to original results\n",
        "    reranked_indices = [doc.metadata.get(\"index\") for doc in reranked_results if \"index\" in doc.metadata]\n",
        "    reranked_results = [results[i] for i in reranked_indices if i < len(results)]\n",
        "    return reranked_results"
      ],
      "metadata": {
        "id": "cjMU8sqFJu18"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                         if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines and data sources to provide comprehensive, query-specific responses.\n",
        "\n",
        "SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "1. Primary Source Evaluation:\n",
        "   - Assess credibility of each source domain\n",
        "   - Verify publication dates for temporal relevance\n",
        "   - Cross-reference information across multiple sources\n",
        "   - Identify and flag potential misinformation or conflicting data\n",
        "\n",
        "2. Content Processing Guidelines:\n",
        "   - Extract and normalize key data points\n",
        "   - Identify patterns and correlations across sources\n",
        "   - Prioritize information based on:\n",
        "     * Temporal relevance (newest to oldest)\n",
        "     * Source reliability\n",
        "     * Direct query relevance\n",
        "     * Technical depth\n",
        "     * Actionable insights\n",
        "\n",
        "3. Media Content Analysis:\n",
        "   - Evaluate included images, diagrams, or screenshots\n",
        "   - Extract relevant technical indicators from visual data\n",
        "   - Correlate visual evidence with textual information\n",
        "   - Note any visual proof of concepts or attack demonstrations\n",
        "\n",
        "RESPONSE STRUCTURE:\n",
        "1. Executive Summary (2-3 sentences)\n",
        "   - Core findings\n",
        "   - Critical alerts or time-sensitive information\n",
        "   - Confidence level in findings\n",
        "\n",
        "2. Detailed Analysis:\n",
        "   a) Key Findings\n",
        "      - Bullet points of critical discoveries\n",
        "      - Emerging threats or developments\n",
        "      - Statistical data or metrics\n",
        "\n",
        "   b) Technical Details\n",
        "      - Specific vulnerabilities or exploits\n",
        "      - Attack vectors and techniques\n",
        "      - System impacts and affected components\n",
        "\n",
        "   c) Contextual Analysis\n",
        "      - Industry impact\n",
        "      - Threat actor attribution (if applicable)\n",
        "      - Historical context or similar incidents\n",
        "\n",
        "3. Evidence and Citations:\n",
        "   - Link every major claim to source material\n",
        "   - Include relevant quote snippets\n",
        "   - Provide context for technical indicators\n",
        "   - Reference related media content\n",
        "\n",
        "4. Actionable Intelligence:\n",
        "   - Immediate response recommendations\n",
        "   - Mitigation strategies\n",
        "   - Detection methods\n",
        "   - Prevention measures\n",
        "\n",
        "5. Future Implications:\n",
        "   - Projected developments\n",
        "   - Potential cascade effects\n",
        "   - Areas requiring monitoring\n",
        "\n",
        "SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "1. For Threat Intelligence:\n",
        "   - Extract and validate IOCs\n",
        "   - Identify TTPs and map to MITRE ATT&CK\n",
        "   - Analyze malware behaviors\n",
        "   - Document C2 infrastructure\n",
        "\n",
        "2. For Vulnerability Analysis:\n",
        "   - Verify CVE details\n",
        "   - Document exploit requirements\n",
        "   - Assess patch availability\n",
        "   - Evaluate real-world exploitation\n",
        "\n",
        "3. For Incident Response:\n",
        "   - Timeline reconstruction\n",
        "   - Attack path analysis\n",
        "   - Impact assessment\n",
        "   - Recovery recommendations\n",
        "\n",
        "4. For Trend Analysis:\n",
        "   - Identify pattern changes\n",
        "   - Map threat evolution\n",
        "   - Project future developments\n",
        "   - Compare against historical data\n",
        "\n",
        "Previous conversation context: {chat_history}\n",
        "Current query: {input}\n",
        "Available search results: {search_results}\n",
        "Crawled results: {crawled_results}\n",
        "Current timestamp: {current_date}\n",
        "\n",
        "RESPONSE REQUIREMENTS:\n",
        "1. Maintain clinical precision and technical accuracy\n",
        "2. Prioritize actionable intelligence over general information\n",
        "3. Include explicit confidence levels for all assessments\n",
        "4. Cite ALL sources using [Source Name](URL) format\n",
        "5. Highlight time-sensitive information\n",
        "6. Address any information gaps or uncertainties\n",
        "7. Format output for maximum readability\n",
        "8. Include relevant media references\n",
        "9. Provide specific, implementable recommendations\n",
        "10. Maintain proper technical context throughout\n",
        "\n",
        "Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the search results:\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Rerank search results\n",
        "    reranked_results = rerank_results(query, search_results)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, reranked_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in reranked_results:\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                if media.get(\"type\") == \"image\":\n",
        "                    display(Image(url=media.get(\"url\"), width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = graph.invoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomware?\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52efdcfc-386b-4918-9e16-bedb4e5f0b36"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomware?\n",
            "DEBUG: Raw results from Exa Search: Title: New CRON#TRAP Malware Infects Windows by Hiding in Linux VM to Evade Antivirus\n",
            "URL: https://thehackernews.com/2024/11/new-crontrap-malware-infects-windows-by.html\n",
            "ID: https://thehackernews.com/2024/11/new-crontrap-malware-infects-windows-by.html\n",
            "Score: 0.1414097100496292\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Cybersecurity researchers have flagged a new malware campaign that infects Windows systems with a Linux virtual instance containing a backdoor capable of establishing remote access to the compromised hosts.\n",
            "The \"intriguing\" campaign, codenamed CRON#TRAP, starts with a malicious Windows shortcut (LNK) file likely distributed in the form of a ZIP archive via a phishing email.\n",
            "\"What makes the CRON#TRAP campaign particularly concerning is that the emulated Linux instance comes pre-configured with a backdoor that automatically connects to an attacker-controlled command-and-control (C2) server,\" Securonix researchers Den Iuzvyk and Tim Peck said in an analysis.\n",
            " \n",
            "\"This setup allows the attacker to maintain a stealthy presence on the victim's machine, staging further malicious activity within a concealed environment, making detection challenging for traditional antivirus solutions.\"\n",
            "The phishing messages purport to be an \"OneAmerica survey\" that comes with a large 285MB ZIP archive that, when opened, triggers the infection process.\n",
            "As part of the as-yet-unattributed attack campaign, the LNK file serves as a conduit to extract and initiate a lightweight, custom Linux environment emulated through Quick Emulator (QEMU), a legitimate, open-source virtualization tool. The virtual machine runs on Tiny Core Linux.\n",
            "   \n",
            "The shortcut subsequently launches PowerShell commands responsible for re-extracting the ZIP file and executing a hidden \"start.bat\" script, which, in turn, displays a fake error message to the victim to give them the impression that the survey link is no longer working.\n",
            "But in the background, it sets up the QEMU virtual Linux environment referred to as PivotBox, which comes preloaded with the Chisel tunneling utility, granting remote access to the host immediately following the startup of the QEMU instance.\n",
            "\"The binary appears to be a pre-configured Chisel client designed to connect to a remote Command and Control (C2) server at 18.208.230[.]174 via websockets,\" the researchers said. \"The attackers' approach effectively transforms this Chisel client into a full backdoor, enabling remote command and control traffic to flow in and out of the Linux environment.\"\n",
            "   \n",
            "The development is one of the many constantly evolving tactics that threat actors are using to target organizations and conceal malicious activity -- case in point is a spear-phishing campaign that has been observed targeting electronic manufacturing, engineering, and industrial companies in European countries to deliver the evasive GuLoader malware.\n",
            "\"The emails typically include order inquiries and contain an archive file attachment,\" Cado Security researcher Tara Gould said. \"The emails are sent from various email addresses including from fake companies and compromised accounts. The emails typically hijack an existing email thread or request information about an order.\"\n",
            " \n",
            "The activity, which has mainly targeted countries like Romania, Poland, Germany, and Kazakhstan, starts with a batch file present within the archive file. The batch file embeds an obfuscated PowerShell script that subsequently downloads another PowerShell script from a remote server.\n",
            "The secondary PowerShell script includes functionality to allocate memory and ultimately execute the GuLoader shellcode to ultimately fetch the next-stage payload.\n",
            "\"Guloader malware continues to adapt its techniques to evade detection to deliver RATs,\" Gould said. \"Threat actors are continually targeting specific industries in certain countries. Its resilience highlights the need for proactive security measures.\" \n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: [']174 via websockets,\" the researchers said. The development is one of the many constantly evolving tactics that threat actors are using to target organizations and conceal malicious activity -- case in point is a spear-phishing campaign that has been observed targeting electronic manufacturing, engineering, and industrial companies in European countries to deliver the evasive GuLoader malware. \"The emails typically include order inquiries and contain an archive file attachment,\" Cado Security researcher Tara Gould said. \"The emails are sent from various email addresses including from fake companies and compromised accounts. The activity, which has mainly targeted countries like Romania, Poland, Germany, and Kazakhstan, starts with a batch file present within the archive file.']\n",
            "Highlight Scores: [0.4658278822898865]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: SteelFox and Rhadamanthys Malware Use Copyright Scams, Driver Exploits to Target Victims\n",
            "URL: https://thehackernews.com/2024/11/steelfox-and-rhadamanthys-malware-use.html\n",
            "ID: https://thehackernews.com/2024/11/steelfox-and-rhadamanthys-malware-use.html\n",
            "Score: 0.14100061357021332\n",
            "Published Date: 2024-11-07T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: An ongoing phishing campaign is employing copyright infringement-related themes to trick victims into downloading a newer version of the Rhadamanthys information stealer since July 2024.\n",
            "Cybersecurity firm Check Point is tracking the large-scale campaign under the name CopyRh(ight)adamantys. Targeted regions include the United States, Europe, East Asia, and South America.\n",
            "\"The campaign impersonates dozens of companies, while each email is sent to a specific targeted entity from a different Gmail account, adapting the impersonated company and the language per targeted entity,\" the company said in a technical analysis. \"Almost 70% of the impersonated companies are from the Entertainment /Media and Technology/Software sectors.\"\n",
            "The attacks are notable for the deployment of version 0.7 of the Rhadamanthys stealer, which, as detailed by Recorded Future's Insikt Group early last month, incorporates artificial intelligence (AI) for optical character recognition (OCR).\n",
            "The Israeli company said the activity overlaps with a campaign that Cisco Talos disclosed last week as targeting Facebook business and advertising account users in Taiwan to deliver Lumma or Rhadamanthys stealer malware.\n",
            " \n",
            "The attack chains are characterized by the use of spear-phishing tactics that entail sending email messages claiming purported copyright violations by masquerading as well-known companies.\n",
            "These emails are sent from Gmail accounts and claim to be from legal representatives of the impersonated companies. The contents of the message accuse the recipients of misusing their brand on social media platforms and request them to remove the concerned images and videos.\n",
            "\"The removal instructions are said to be in a password-protected file. However, the attached file is a download link to appspot.com, linked to the Gmail account, which redirects the user to Dropbox or Discord to download a password-protected archive (with the password provided in the email),\" Check Point said.\n",
            "   \n",
            "The RAR archive contains three components, a legitimate executable vulnerable to DLL side-loading, the malicious DLL containing the stealer payload, and a decoy document. Once the binary is run, it sideloads the DLL file, which then paves the way for the deployment of Rhadamanthys.\n",
            "Check Point, which attributed the campaign to a likely cybercrime group, said that it's possible the threat actors have utilized AI tools given the scale of the campaign and the variety of the lures and sender emails.\n",
            "\"The campaign's widespread and indiscriminate targeting of organizations across multiple regions suggests it was orchestrated by a financially motivated cybercrime group rather than a nation-state actor,\" it said. \"Its global reach, automated phishing tactics, and diverse lures demonstrate how attackers continuously evolve to improve their success rates.\"\n",
            "New SteelFox Malware Exploits Vulnerable Driver\n",
            "The findings come as Kaspersky shed light on a new \"full-featured crimeware bundle\" dubbed SteelFox that's propagated via forums posts, torrent trackers, and blogs, passing off as legitimate utilities like Foxit PDF Editor, JetBrains, and AutoCAD.\n",
            "The campaign, dating back to February 2023, has claimed victims across the world, particularly those located in Brazil, China, Russia, Mexico, UAE, Egypt, Algeria, Vietnam, India, and Sri Lanka. It has not been attributed to any known threat actor or group.\n",
            "\"Delivered via sophisticated execution chains including shellcoding, this threat abuses Windows services and drivers,\" security researcher Kirill Korchemny said. \"It also uses stealer malware to extract the victim's credit card data as well as details about the infected device.\"\n",
            "The starting point is a dropper app that impersonates cracked versions of popular software, which, when executed, asks for administrator access and drops a next-stage loader that, in turn, establishes persistence and launches the SteelFox DLL.\n",
            " \n",
            "The admin access is subsequently abused to create a service that runs an older version of WinRing0.sys, a hardware access library for Windows that's vulnerable to CVE-2020-14979 and CVE-2021-41285, thereby allowing the threat actor to obtain NT\\SYSTEM privileges.\n",
            "\"This driver is also a component of the XMRig miner, so it is utilized for mining purposes,\" Korchemny noted. \"After initializing the driver, the sample launches the miner. This represents a modified executable of XMRig with junk code fillers. It connects to a mining pool with hardcoded credentials.\"\n",
            "The miner, for its part, is downloaded from a GitHub repository, with the malware also initiating contact with a remote server over TLS version 1.3 to exfiltrate sensitive data from web browsers, such as cookies, credit card data, browsing history, and visited places, system metadata, installed software, and timezone, among others.\n",
            "\"Highly sophisticated usage of modern C++ combined with external libraries grant this malware formidable power,\" Kaspersky said. \"Usage of TLSv1.3 and SSL pinning ensures secure communication and harvesting of sensitive data.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['An ongoing phishing campaign is employing copyright infringement-related themes to trick victims into downloading a newer version of the Rhadamanthys information stealer since July 2024. Cybersecurity firm Check Point is tracking the large-scale campaign under the name CopyRh(ight)adamantys. Targeted regions include the United States, Europe, East Asia, and South America. \"The campaign impersonates dozens of companies, while each email is sent to a specific targeted entity from a different Gmail account, adapting the impersonated company and the language per targeted entity,\" the company said in a technical analysis. The attacks are notable for the deployment of version 0.7 of the Rhadamanthys stealer, which, as detailed by Recorded Future\\'s Insikt Group early last month, incorporates artificial intelligence (AI) for optical character recognition (OCR).']\n",
            "Highlight Scores: [0.4886050224304199]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Unwrapping the emerging Interlock ransomware attack\n",
            "URL: https://blog.talosintelligence.com/emerging-interlock-ransomware/\n",
            "ID: https://blog.talosintelligence.com/emerging-interlock-ransomware/\n",
            "Score: 0.1402113139629364\n",
            "Published Date: 2024-11-07T00:00:00.000Z\n",
            "Author: Elio Biasiotto\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Thursday, November 7, 2024 06:00\n",
            " \n",
            " Cisco Talos Incident Response (Talos IR) recently observed an attacker conducting big-game hunting and double extortion attacks using the relatively new Interlock ransomware.  Our analysis uncovered that the attacker used multiple components in the delivery chain including a Remote Access Tool (RAT) masquerading as a fake browser updater, PowerShell scripts, a credential stealer, and a keylogger before deploying and enabling the ransomware encryptor binary.  We also observed that the attacker primarily used remote desktop protocol (RDP) to move laterally within the victim’s network, as well as other tools such as AnyDesk and PuTTY.  The attacker used Azure Storage Explorer, which leverages the utility AZCopy, to exfiltrate the victim’s data to an attacker-controlled Azure storage blob.  The timeline of the attacker’s activity, from the initial compromise stage until the deployment of ransomware encryptor binary, indicates their dwelling time in the victim’s environment was about 17 days.  Talos assesses with low confidence that Interlock ransomware is likely a new diversified group that emerged from Rhysida ransomware operators or developers, based on some similarities in the operators’ tactics, techniques, and procedures (TTPs) and in the ransomware encryptor binaries.   Who is Interlock?  Interlock first appeared in  public reporting  in September 2024 and has been observed launching big-game hunting and double extortion attacks. The group has notably targeted businesses in a wide range of sectors, which at the time of reporting includes healthcare, technology, government in the U.S. and manufacturing in Europe, according to the data leak site disclosure, indicating their targeting is opportunistic.  Like other ransomware players in the big-game hunting space, Interlock also operates a data leak site called “Worldwide Secrets Blog,” providing links to victims’ leaked data, chat support for victims' communications, and the email address, “interlock@2mail[.]co”.    In their blog, Interlock claims to target organizations’ infrastructure by exploiting unaddressed vulnerabilities and claims their actions are in part motivated by a desire to hold companies’ accountable for poor cybersecurity, in addition to monetary gain.    Recent attack methodologies  Throughout the investigation into the Interlock ransomware attack, Talos observed several notable TTPs used by the attacker in each stage of the delivery chain. Talos assesses that the attacker was present in the victim’s environment for approximately 17 days, from the initial compromise until deployment and execution of the Interlock ransomware.    Initial access  The attacker gained access to the victim machine via a fake Google Chrome browser updater executable that the victim was prompted to download from a compromised legitimate news website. When clicked, the fake browser updater executable “upd_2327991.exe” was downloaded onto the victim machine from a second compromised URL of a legitimate retailer.  Execution  Talos IR discovered the fake browser updater executable is a Remote Access Tool (RAT) that automatically executes an embedded PowerShell script when downloaded and run. The script initially downloads a legitimate Chrome setup executable “ChromeSetup.exe” to the victim machine’s applications temporary folder and established persistence by dropping a Windows shortcut file in the Windows StartUp folder with the file name “fahhs.lnk” configured to run the RAT every time the victim logs in, establishing persistence.      The RAT executes the command “cmd.exe /c systeminfo\" and collects information from victim machine, listed below:\n",
            " \n",
            "Host Name\n",
            " \n",
            "Time Zone\n",
            "OS Name\n",
            " \n",
            "Total Physical Memory\n",
            "OS Version\n",
            " \n",
            "Available Physical Memory\n",
            "OS Manufacturer\n",
            " \n",
            "Virtual Memory\n",
            "OS Configuration\n",
            " \n",
            "Max Size\n",
            "OS Build Type\n",
            " \n",
            "Virtual Memory: Available\n",
            "Registered Owner\n",
            " \n",
            "Virtual Memory: In Use\n",
            "Registered Organization\n",
            " \n",
            "Page File Location(s)\n",
            "Product ID\n",
            " \n",
            "Domain\n",
            "Original Install Date\n",
            " \n",
            "Logon Server\n",
            "System Boot Time\n",
            " \n",
            "Hotfix(s)\n",
            "System Manufacturer\n",
            " \n",
            "Network Card(s)\n",
            "System Model\n",
            " \n",
            "Connection Name\n",
            "System Type\n",
            " \n",
            "Status\n",
            "Processor(s)\n",
            " \n",
            "DHCP Enabled\n",
            "BIOS Version\n",
            " \n",
            "DHCP Server\n",
            "Windows Directory\n",
            " \n",
            "IP address(es)\n",
            "System Directory\n",
            " \n",
            "Hyper-V Requirements\n",
            "Boot Device\n",
            " \n",
            "System Locale\n",
            " \n",
            "Then, the RAT encrypts the collected information in the memory stream. It establishes a secured socket to the command and control (C2) server hidden behind the attacker-controlled Cloudflare domain “apple-online[.]shop”, sends the encrypted data stream of victim machine information to the C2 server, and waits to receive the response.  The RAT also allowed the attacker to execute two other PowerShell commands on the victim machine, which downloads the encrypted data blobs of a credential stealer “cht.exe” and a keylogger binary “klg.dll”, decrypts them with the passwords “jgSkhg934@kjv#1vkfg2S” and runs them. We observed that the keylogger is a DLL file that is run using the LOLBin “rundll32.exe”.      Defense Evasion  Talos IR observed that EDR was disabled on some of the compromised servers in the victim environment during the investigation. According to the indicators seen, Talos IR believes that the attacker could have either leveraged an EDR uninstaller tool or instrumented a vulnerable device driver Sysmon.sys (TfSysMon.sys) to disable the EDR on the victim machine. We also observed the attacker’s attempts to delete contents of the Event logs on some of the compromised systems.  Credential Access  The credential stealer discovered in this campaign is compiled in Golang. It enumerates the installed browser profiles on the victim machine and copies the Login data, Login State, key4.db, browser history and bookmarks files to the victim’s application profile temporary folder. The stealer then processes the data and uses SQL queries to collect the login information of victims’ online accounts along with the associated account URLs. Finally, the data is written to a file “chrgetpdsi.txt” in the user profile temporary folder.  The keylogger DLL running on the victim machine is a tiny executable, which hooks to the victim machine keyboard and logs keystrokes in a file called “conhost.txt”, the same folder where the Keylogger was downloaded.  Discovery  The attacker ran PowerShell commands that are known indicators of pre-kerberoasting reconnaissance, a method used to obtain domain admin credentials. We assess with moderate confidence that a Kerberoasting attack was used to obtain accounts with higher privileges. \n",
            "(('AD_Computers: {0}' -f ([adsiSearcher]'(ObjectClass=computer)').FindAll().count)\n",
            "([adsisearcher]'(&amp;(objectCategory=user)(servicePrincipalName=*))').FindAll()\n",
            "Lateral Movement  Talos IR observed that the attacker primarily used Remote Desktop Protocol (RDP) and several compromised credentials to move between systems. Further analysis showed that the attacker has also used AnyDesk and possibly LogMeIn to allow remote connectivity. We also spotted the installation of PuTTY on the compromised machines, which was likely used to move laterally to Linux hosts. We are not clear how these tools were dropped and executed on the infected machines.  Sample RDP command executions observed during our analysis and with the redacted IP address details are shown below. \n",
            "mstsc /v 10.*.*.*\n",
            ".\\conhost.exe -d \\10.*.*.*\\e$\n",
            "Collection and Exfiltration  The attacker executed storage-explorer, a tool that allows users to manage and interact with Azure Storage, and AzCopy, which allows users to copy files to a remote Azure storage, in the victim’s machine. We believe that the attacker used storage-explorer to navigate and identify sensitive information in the victim network and executed AzCopy to upload the data to the Azure storage blob according to network artifacts analysis. We were not able to confirm how the storage-explorer and AzCopy were delivered to the victim machine.    Impact  The attacker deployed the Interlock ransomware encryptor binary with the file name “conhost.exe”, masquerading as a legitimate file, onto the victim machine and stored it in a folder named with a single digit number (example: “3” or “4”) in the user profile application data temporary folder. When run, the ransomware encrypts the targeted files on the victim machine with the file extension “.Interlock” and drops the ransom note “!__README__!.txt” file in every folder containing files that the encryptor has attempted to encrypt. Talos IR also observed that the attacker configured the ransom note to display during interactive login, was pushed using Group Policy Objects (GPO\n",
            "Highlights: ['The timeline of the attacker’s activity, from the initial compromise stage until the deployment of ransomware encryptor binary, indicates their dwelling time in the victim’s environment was about 17 days. Talos assesses with low confidence that Interlock ransomware is likely a new diversified group that emerged from Rhysida ransomware operators or developers, based on some similarities in the operators’ tactics, techniques, and procedures (TTPs) and in the ransomware encryptor binaries. Who is Interlock? Interlock first appeared in  public reporting  in September 2024 and has been observed launching big-game hunting and double extortion attacks. The group has notably targeted businesses in a wide range of sectors, which at the time of reporting includes healthcare, technology, government in the U.S. and manufacturing in Europe, according to the data leak site disclosure, indicating their targeting is opportunistic.']\n",
            "Highlight Scores: [0.4919053614139557]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: North Korean Hackers Target Crypto Firms with Hidden Risk Malware on macOS\n",
            "URL: https://thehackernews.com/2024/11/north-korean-hackers-target-crypto.html\n",
            "ID: https://thehackernews.com/2024/11/north-korean-hackers-target-crypto.html\n",
            "Score: 0.13194090127944946\n",
            "Published Date: 2024-11-07T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A threat actor with ties to the Democratic People's Republic of Korea (DPRK) has been observed targeting cryptocurrency-related businesses with a multi-stage malware capable of infecting Apple macOS devices.\n",
            "Cybersecurity company SentinelOne, which dubbed the campaign Hidden Risk, attributed it with high confidence to BlueNoroff, which has been previously linked to malware families such as RustBucket, KANDYKORN, ObjCShellz, RustDoor (aka Thiefbucket), and TodoSwift.\n",
            "The activity \"uses emails propagating fake news about cryptocurrency trends to infect targets via a malicious application disguised as a PDF file,\" researchers Raffaele Sabato, Phil Stokes, and Tom Hegel said in a report shared with The Hacker News.\n",
            "\"The campaign likely began as early as July 2024 and uses email and PDF lures with fake news headlines or stories about crypto-related topics.\"\n",
            " \n",
            "As revealed by the U.S. Federal Bureau of Investigation (FBI) in a September 2024 advisory, these campaigns are part of \"highly tailored, difficult-to-detect social engineering\" attacks aimed at employees working in the decentralized finance (DeFi) and cryptocurrency sectors.\n",
            "The attacks take the form of bogus job opportunities or corporate investment, engaging with their targets for extended periods of time to build trust before delivering malware.\n",
            "SentinelOne said it observed an email phishing attempt on a crypto-related industry in late October 2024 that delivered a dropper application mimicking a PDF file (\"Hidden Risk Behind New Surge of Bitcoin Price.app\") hosted on delphidigital[.]org.\n",
            "The application, written in the Swift programming language, has been found to be signed and notarized on October 19, 2024, with the Apple developer ID \"Avantis Regtech Private Limited (2S8XHJ7948).\" The signature has since been revoked by the iPhone maker.\n",
            "Upon launch, the application downloads and displays to the victim a decoy PDF file retrieved from Google Drive, while covertly retrieving a second-stage executable from a remote server and executing it. A Mach-O x86-64 executable, the C++-based unsigned binary acts as a backdoor to execute remote commands.\n",
            "The backdoor also incorporates a novel persistence mechanism that abuses the zshenv configuration file, marking the first time the technique has been abused in the wild by malware authors.\n",
            "\"It has particular value on modern versions of macOS since Apple introduced user notifications for background Login Items as of macOS 13 Ventura,\" the researchers said.\n",
            "\"Apple's notification aims to warn users when a persistence method is installed, particularly oft-abused LaunchAgents and LaunchDaemons. Abusing Zshenv, however, does not trigger such a notification in current versions of macOS.\"\n",
            "The threat actor has also been observed using domain registrar Namecheap to establish an infrastructure that's centered around themes related to cryptocurrency, Web3, and investments to give it a veneer of legitimacy. Quickpacket, Routerhosting, and Hostwinds are among the most commonly used hosting providers.\n",
            "It's worth noting that the attack chain shares some level of overlap with a previous campaign that Kandji highlighted in August 2024, which also employed a similarly named macOS dropper app \"Risk factors for Bitcoin's price decline are emerging(2024).app\" to deploy TodoSwift.\n",
            "It's not clear what prompted the threat actors to shift their tactics, and if it's in response to public reporting. \"North Korean actors are known for their creativity, adaptability, and awareness of reports on their activities, so it's entirely possible that we're simply seeing different successful methods emerge from their offensive cyber program,\" Stokes told The Hacker News.\n",
            "Another concerning aspect of the campaign is BlueNoroff's ability to acquire or hijack valid Apple developer accounts and use them to have their malware notarized by Apple.\n",
            "\"Over the last 12 months or so, North Korean cyber actors have engaged in a series of campaigns against crypto-related industries, many of which involved extensive 'grooming' of targets via social media,\" the researchers said.\n",
            "\"The Hidden Risk campaign diverts from this strategy taking a more traditional and cruder, though not necessarily any less effective, email phishing approach. Despite the bluntness of the initial infection method, other hallmarks of previous DPRK-backed campaigns are evident.\"\n",
            " \n",
            "The development also comes amid other campaigns orchestrated by North Korean hackers to seek employment at various companies in the West and deliver malware using booby-trapped codebases and conferencing tools to prospective job seekers under the guise of a hiring challenge or an assignment.\n",
            "The two intrusion sets, dubbed Wagemole (aka UNC5267) and Contagious Interview, have been attributed to a threat group tracked as Famous Chollima (aka CL-STA-0240 and Tenacious Pungsan).\n",
            "ESET, which has given Contagious Interview the moniker DeceptiveDevelopment, has classified it as a new Lazarus Group activity cluster that's focused on targeting freelance developers around the world with the aim of cryptocurrency theft.\n",
            "\"The Contagious Interview and Wagemole campaigns showcase the evolving tactics of North Korean threat actors as they continue to steal data, land remote jobs in Western countries, and bypass financial sanctions,\" Zscaler ThreatLabz researcher Seongsu Park said earlier this week.\n",
            "\"With refined obfuscation techniques, multi-platform compatibility, and widespread data theft, these campaigns represent a growing threat to businesses and individuals alike.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['It\\'s worth noting that the attack chain shares some level of overlap with a previous campaign that Kandji highlighted in August 2024, which also employed a similarly named macOS dropper app \"Risk factors for Bitcoin\\'s price decline are emerging(2024).app\" to deploy TodoSwift. It\\'s not clear what prompted the threat actors to shift their tactics, and if it\\'s in response to public reporting. \"North Korean actors are known for their creativity, adaptability, and awareness of reports on their activities, so it\\'s entirely possible that we\\'re simply seeing different successful methods emerge from their offensive cyber program,\" Stokes told The Hacker News. Another concerning aspect of the campaign is BlueNoroff\\'s ability to acquire or hijack valid Apple developer accounts and use them to have their malware notarized by Apple. \"Over the last 12 months or so, North Korean cyber actors have engaged in a series of campaigns against crypto-related industries, many of which involved extensive \\'grooming\\' of targets via social media,\" the researchers said.']\n",
            "Highlight Scores: [0.5306764841079712]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: CISA Alerts to Active Exploitation of Critical Palo Alto Networks Vulnerability\n",
            "URL: https://thehackernews.com/2024/11/cisa-alerts-to-active-exploitation-of.html\n",
            "ID: https://thehackernews.com/2024/11/cisa-alerts-to-active-exploitation-of.html\n",
            "Score: 0.13092637062072754\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Thursday added a now-patched critical security flaw impacting Palo Alto Networks Expedition to its Known Exploited Vulnerabilities (KEV) catalog, citing evidence of active exploitation.\n",
            "The vulnerability, tracked as CVE-2024-5910 (CVSS score: 9.3), concerns a case of missing authentication in the Expedition migration tool that could lead to an admin account takeover.\n",
            "\"Palo Alto Expedition contains a missing authentication vulnerability that allows an attacker with network access to takeover an Expedition admin account and potentially access configuration secrets, credentials, and other data,\" CISA said in an alert.\n",
            " \n",
            "The shortcoming impacts all versions of Expedition prior to version 1.2.92, which was released in July 2024 to plug the problem.\n",
            "There are currently no reports on how the vulnerability is being weaponized in real-world attacks, but Palo Alto Networks has since revised its original advisory to acknowledge that it's \"aware of reports from CISA that there is evidence of active exploitation.\"\n",
            "Also added to the KEV catalog are two other flaws, including a privilege escalation vulnerability in the Android Framework component (CVE-2024-43093) that Google disclosed this week as having come under \"limited, targeted exploitation.\"\n",
            "The other security defect is CVE-2024-51567 (CVSS score: 10.0), a critical flaw affecting CyberPanel that allows a remote, unauthenticated attacker to execute commands as root. The issue has been resolved in version 2.3.8.\n",
            " \n",
            "In late October 2023, it emerged that the vulnerability was being exploited en masse by malicious actors to deploy PSAUX ransomware on more than 22,000 internet-exposed CyberPanel instances, according to LeakIX and a security researcher who goes by the online alias Gi7w0rm.\n",
            "LeakIX also noted that three distinct ransomware groups have quickly capitalized on the vulnerability, with files encrypted multiple times in some cases.\n",
            "Federal Civilian Executive Branch (FCEB) agencies have been recommended to remediate the identified vulnerabilities by November 28, 2024, to secure their networks against active threats.\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['LeakIX also noted that three distinct ransomware groups have quickly capitalized on the vulnerability, with files encrypted multiple times in some cases. Federal Civilian Executive Branch (FCEB) agencies have been recommended to remediate the identified vulnerabilities by November 28, 2024, to secure their networks against active threats. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.29087668657302856]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is the latest cyber incident from Blackbasta Ransomware:\n",
            "Resolved Search Type: 2024-11-07T10:14:27.468Z\n",
            "ERROR in Exa Search: name 'SearchResponse' is not defined\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://therecord.media/black-basta-ransomware-zero-day-windows using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf, error: [ERROR] 🚫 crawl(): Failed to crawl https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf: Page.goto: net::ERR_ABORTED at https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\n",
            "Call log:\n",
            "navigating to \"https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\n",
            "[LOG] 🕸️ Crawling https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, success: True, time taken: 9.09 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, time taken: 0.00 seconds.\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://therecord.media/black-basta-ransomware-zero-day-windows successfully!\n",
            "[LOG] 🚀 Crawling done for https://therecord.media/black-basta-ransomware-zero-day-windows, success: True, time taken: 10.04 seconds\n",
            "[LOG] 🚀 Content extracted for https://therecord.media/black-basta-ransomware-zero-day-windows, success: True, time taken: 0.06 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://therecord.media/black-basta-ransomware-zero-day-windows, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://therecord.media/black-basta-ransomware-zero-day-windows, time taken: 0.08 seconds.\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "DEBUG: Extracted content from https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf: []\n",
            "DEBUG: Extracted content from https://therecord.media/black-basta-ransomware-zero-day-windows: [{'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=156209188.aa39884302b090147c6f347498a5726c.1731060882831.1731060882831.1731060882831.1&__hssc=156209188.1.1731060882832&__hsfp=702054290'}]\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] ✅ Crawled https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, success: True, time taken: 10.95 seconds\n",
            "[LOG] 🚀 Content extracted for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, time taken: 0.00 seconds.\n",
            "[LOG] ✅ Crawled https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, success: True, time taken: 10.59 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, success: True, time taken: 0.01 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, time taken: 0.01 seconds.\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 13.83 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.07 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.09 seconds.\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "DEBUG: Extracted content from https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png: []\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 14.09 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.09 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.11 seconds.\n",
            "DEBUG: Extracted content from https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png: []\n",
            "DEBUG: Extracted content from https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta: [{'links': 'https://www.blackberry.com/'}]\n",
            "DEBUG: Extracted content from https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta: [{'links': 'https://www.blackberry.com/'}]\n",
            "Crawled Results: [[], [{'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=156209188.aa39884302b090147c6f347498a5726c.1731060882831.1731060882831.1731060882831.1&__hssc=156209188.1.1731060882832&__hsfp=702054290'}], [{'links': 'https://www.blackberry.com/'}], [{'links': 'https://www.blackberry.com/'}], [], []]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=156209188.aa39884302b090147c6f347498a5726c.1731060882831.1731060882831.1731060882831.1&__hssc=156209188.1.1731060882832&__hsfp=702054290'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.blackberry.com/'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.blackberry.com/'}\n",
            "WARNING: No search results available for citation.\n",
            "Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\n",
            "Executive Summary:\n",
            "- The Black Basta ransomware group, active since early 2022, has been linked to a surge in recent attacks targeting the marine transportation industry (US Coast Guard Cyber Command Maritime Cyber Alert 02-23 June 2023).\n",
            "- There is evidence suggesting that Black Basta may have exploited a zero-day Windows vulnerability (Google Serper, \"Windows flaw may have been exploited with Black Basta...\").\n",
            "- Confidence: 85%\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Key Findings:\n",
            "- Black Basta is a ransomware operator and Ransomware-as-a-Service (RaaS) criminal enterprise (Google Serper, \"Who Is Black Basta?\").\n",
            "- A recent increase in Black Basta campaigns has been observed (US Coast Guard Cyber Command Maritime Cyber Alert 02-23 June 2023).\n",
            "- The group's attacks can be distinguished by examining the beginning of each encrypted file (Google Programmable Search, \"Who Is Black Basta?\").\n",
            "\n",
            "Technical Details:\n",
            "- A Windows vulnerability may have been exploited by Black Basta as a zero-day (Google Serper, \"Windows flaw may have been exploited with Black Basta...\").\n",
            "- The specific vulnerability has not been disclosed.\n",
            "\n",
            "Contextual Analysis:\n",
            "- The marine transportation industry appears to be a current focus for Black Basta (US Coast Guard Cyber Command Maritime Cyber Alert 02-23 June 2023).\n",
            "- Threat actor attribution is not specified in the available sources.\n",
            "\n",
            "Evidence and Citations:\n",
            "- [US Coast Guard Cyber Command Maritime Cyber Alert 02-23 June 2023](https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf)\n",
            "- [Google Serper, \"Windows flaw may have been exploited with Black Basta...\"](https://therecord.media/black-basta-ransomware-zero-day-windows)\n",
            "- [Google Programmable Search, \"Who Is Black Basta?\"](https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta)\n",
            "\n",
            "Actionable Intelligence:\n",
            "- Patch all systems and applications as soon as possible.\n",
            "- Implement a robust backup strategy and test restorations regularly.\n",
            "- Monitor network traffic for signs of ransomware activity.\n",
            "- Consider deploying a reputable Endpoint Detection and Response (EDR) solution.\n",
            "\n",
            "Future Implications:\n",
            "- Further exploitation of zero-day vulnerabilities is likely.\n",
            "- The marine transportation industry should remain vigilant for Black Basta attacks.\n",
            "- Regular threat intelligence updates are crucial for effective defense.\n",
            "\n",
            "Information Gaps and Uncertainties:\n",
            "- The specific Windows vulnerability exploited by Black Basta remains undisclosed.\n",
            "- The identity of the threat actors behind Black Basta is unknown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIp1L2bcInA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7mH99gGInD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VWtEsGIInHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}