{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "321e0233-1293-4204-98be-c421ca8b985a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import JinaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_compressors import JinaRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "JINA_API_KEY =\"jina_72680535332e480f80ca0833953b3f3066AVc7Bh7cAoYB0uWt0CvMTgrJsq\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"JINA_API_KEY\"] = JINA_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf96b28-8bef-42cd-a9f6-82e3e9f8dc12"
      },
      "execution_count": 54,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Jina Reranker\n",
        "compressor = JinaRerank()\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "async def extract_content_from_url(url):\n",
        "    schema = {\n",
        "        \"name\": \"Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "        print(f\"DEBUG: Extracted content from {url}: {extracted_content}\")\n",
        "        return extracted_content\n",
        "\n",
        "async def scrape_links_and_content(urls):\n",
        "    tasks = [extract_content_from_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return [result for result in results if result is not None]"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search),\n",
        "        (\"Google Serper Image Search\", google_serper_image_search),\n",
        "        (\"Google Programmable Image Search\", google_programmable_image_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in {name}: {str(e)}\")\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        parsed_date = parse_date(x.date)\n",
        "        return (parsed_date is not None, parsed_date or datetime.min, x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant and recent results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    # Extract URLs for further crawling\n",
        "    urls_to_crawl = [result.url for result in top_results if result.url]\n",
        "\n",
        "    # Scrape links and content from the extracted URLs\n",
        "    crawled_results = asyncio.run(scrape_links_and_content(urls_to_crawl))\n",
        "\n",
        "    # Add crawled results to the state\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Crawled Results\", \"crawled_results\": crawled_results})\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state\n",
        "\n",
        "def rerank_results(query: str, results: List[SearchResult]) -> List[SearchResult]:\n",
        "    # Extract text content for reranking\n",
        "    texts = [result.snippet for result in results]\n",
        "    # Rerank the results based on the query\n",
        "    reranked_results = compression_retriever.get_relevant_documents(query)\n",
        "    # Map reranked Document objects to original results\n",
        "    reranked_indices = [doc.metadata.get(\"index\") for doc in reranked_results if \"index\" in doc.metadata]\n",
        "    reranked_results = [results[i] for i in reranked_indices if i < len(results)]\n",
        "    return reranked_results\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)"
      ],
      "metadata": {
        "id": "cjMU8sqFJu18"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                         if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines, APIs, and data sources to provide comprehensive, up-to-date, query-specific responses.\n",
        "\n",
        "SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "1. Primary Source Evaluation:\n",
        "    - Assess the credibility of each source domain.\n",
        "    - Verify publication dates to ensure the most recent information.\n",
        "    - Cross-reference information across multiple sources.\n",
        "    - Identify and flag potential misinformation or conflicting data.\n",
        "\n",
        "2. Content Processing Guidelines:\n",
        "    - Extract and normalize key data points.\n",
        "    - Identify patterns and correlations across sources.\n",
        "    - Prioritize information based on:\n",
        "        * Temporal relevance (newest to oldest).\n",
        "        * Source reliability.\n",
        "        * Direct query relevance.\n",
        "        * Technical depth.\n",
        "        * Actionable insights.\n",
        "\n",
        "3. Media Content Analysis:\n",
        "    - Evaluate included images, diagrams, or screenshots.\n",
        "    - Extract relevant technical indicators from visual data.\n",
        "    - Correlate visual evidence with textual information.\n",
        "    - Note any visual proof of concepts or attack demonstrations.\n",
        "\n",
        "RESPONSE STRUCTURE:\n",
        "1. Executive Summary (8-10 sentences):\n",
        "    - Core findings.\n",
        "    - Critical alerts or time-sensitive information.\n",
        "\n",
        "2. Detailed Analysis:\n",
        "    a) Key Findings:\n",
        "        - Bullet points of critical discoveries.\n",
        "        - Emerging threats or developments.\n",
        "        - Statistical data or metrics.\n",
        "\n",
        "    b) Technical Details:\n",
        "        - Specific vulnerabilities or exploits.\n",
        "        - Attack vectors and techniques.\n",
        "        - System impacts and affected components.\n",
        "\n",
        "    c) Contextual Analysis:\n",
        "        - Industry impact.\n",
        "        - Threat actor attribution (if applicable).\n",
        "        - Historical context or similar incidents.\n",
        "\n",
        "3. Evidence and Citations:\n",
        "    - Link every major claim to source material.\n",
        "    - Include relevant quote snippets.\n",
        "    - Provide context for technical indicators.\n",
        "    - Reference related media content.\n",
        "\n",
        "4. Actionable Intelligence:\n",
        "    - Immediate response recommendations.\n",
        "    - Mitigation strategies.\n",
        "    - Detection methods.\n",
        "    - Prevention measures.\n",
        "\n",
        "5. Future Implications:\n",
        "    - Projected developments.\n",
        "    - Potential cascade effects.\n",
        "    - Areas requiring monitoring.\n",
        "\n",
        "SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "1. For Threat Intelligence:\n",
        "    - Extract and validate Indicators of Compromise (IOCs).\n",
        "    - Identify Tactics, Techniques, and Procedures (TTPs) and map to MITRE ATT&CK.\n",
        "    - Analyze malware behaviors.\n",
        "    - Document Command and Control (C2) infrastructure.\n",
        "\n",
        "2. For Vulnerability Analysis:\n",
        "    - Verify Common Vulnerabilities and Exposures (CVE) details.\n",
        "    - Document exploit requirements.\n",
        "    - Assess patch availability.\n",
        "    - Evaluate real-world exploitation scenarios.\n",
        "\n",
        "3. For Incident Response:\n",
        "    - Reconstruct attack timelines.\n",
        "    - Analyze attack paths.\n",
        "    - Perform impact assessments.\n",
        "    - Provide recovery and remediation recommendations.\n",
        "\n",
        "4. For Trend Analysis:\n",
        "    - Identify pattern changes.\n",
        "    - Map threat evolution.\n",
        "    - Project future developments.\n",
        "    - Compare against historical data.\n",
        "\n",
        "PREVIOUS CONVERSATION CONTEXT: {chat_history}\n",
        "CURRENT QUERY: {input}\n",
        "AVAILABLE SEARCH RESULTS: {search_results}\n",
        "CRAWLED RESULTS: {crawled_results}\n",
        "CURRENT TIMESTAMP: {current_date}\n",
        "\n",
        "RESPONSE REQUIREMENTS:\n",
        "1. Maintain clinical precision and technical accuracy.\n",
        "2. Prioritize actionable intelligence over general information.\n",
        "3. Include explicit confidence levels for all assessments.\n",
        "4. Cite ALL sources using [Source Name](URL) format.\n",
        "5. Highlight time-sensitive information.\n",
        "6. Address any information gaps or uncertainties.\n",
        "7. Format output for maximum readability.\n",
        "8. Include relevant media references.\n",
        "9. Provide specific, implementable recommendations.\n",
        "10. Maintain proper technical context throughout.\n",
        "\n",
        "Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the latest search results.\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Rerank search results\n",
        "    reranked_results = rerank_results(query, search_results)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, reranked_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in reranked_results:\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                if media.get(\"type\") == \"image\":\n",
        "                    display(Image(url=media.get(\"url\"), width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = graph.invoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700f12ab-58ee-45ca-86a8-d8dcfb84f8a1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\n",
            "DEBUG: Raw results from Exa Search: Title: India's Star Health says it received $68,000 ransom demand after data leak\n",
            "URL: https://www.reuters.com/world/india/indias-star-health-says-it-received-68k-ransom-demand-after-data-leak-2024-10-12/\n",
            "ID: https://www.reuters.com/world/india/indias-star-health-says-it-received-68k-ransom-demand-after-data-leak-2024-10-12/\n",
            "Score: 0.1438344269990921\n",
            "Published Date: 2024-10-12T00:00:00.000Z\n",
            "Author: Munsif Vengattil; Aditya Kalra\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Companies    NEW DELHI, Oct 12 (Reuters) - Star Health  (STAU.NS) , India's biggest health insurer, on Saturday said it had received a ransom demand of $68,000 from a cyberhacker in connection with a leak of customer data and medical records.  Star, which has a roughly $4 billion market cap, is battling a reputational and business crisis since Reuters reported on Sept. 20 that a hacker had used Telegram chatbots and a website to leak customers' sensitive data, including tax details and medical claim papers. The company, whose shares have declined 11%, has launched internal investigations and has taken legal action against Telegram and the hacker, whose website continues to share samples of Star customers' data. Star, which has previously said it is a \"victim of a targeted malicious cyberattack\", on Saturday revealed for the first time that in August \"the threat actor demanded a ransom of $68,000 in an email\" addressed to the company's managing director and its chief executive. The statement came after Indian stocks exchanges sought clarifications from Star on a Friday over a Reuters report that the company was investigating allegations that its chief security officer was involved in the data leak. Star reiterated on Saturday it has found no wrongdoing by the official, Amarjeet Khanuja, though the internal investigation is ongoing. Telegram has declined to share the account details or permanently ban accounts linked to the hacker - an individual dubbed xenZen - \"despite multiple notices issued in this regard,\" Star said on Saturday. Star said it has \"sought the assistance\" of Indian cyber security authorities to \"help us identify\" the hacker. Telegram did not respond to a request for comment. The Dubai-based messenger app has previously said it removed the chatbots when Reuters flagged them to the platform. Get the latest news from India and how it matters to the world with the Reuters India File newsletter. Sign up here.  Reporting by Aditya Kalra and Munsif Vengattil Our Standards: The Thomson Reuters Trust Principles.    Munsif Vengattil is a Reuters' India technology correspondent, based in New Delhi. He tracks how policymaking is influencing the business of tech in India, and how the country is now vying more aggressively to be a powerhouse in the global electronics supply chain. He also regularly reports on big tech giants, including Facebook and Google, and their strategies and challenges in the key Indian market.   Aditya Kalra is the Company News Editor for Reuters in India, overseeing business coverage and reporting stories on some of the world's biggest companies. He joined Reuters in 2008 and has in recent years written stories on challenges and strategies of a wide array of companies -- from Amazon, Google and Walmart to Xiaomi, Starbucks and Reliance. He also extensively works on deeply-reported and investigative business stories.\n",
            "Highlights: ['Star, which has a roughly $4 billion market cap, is battling a reputational and business crisis since Reuters reported on Sept. 20 that a hacker had used Telegram chatbots and a website to leak customers\\' sensitive data, including tax details and medical claim papers. The company, whose shares have declined 11%, has launched internal investigations and has taken legal action against Telegram and the hacker, whose website continues to share samples of Star customers\\' data. Star, which has previously said it is a \"victim of a targeted malicious cyberattack\", on Saturday revealed for the first time that in August \"the threat actor demanded a ransom of $68,000 in an email\" addressed to the company\\'s managing director and its chief executive. The statement came after Indian stocks exchanges sought clarifications from Star on a Friday over a Reuters report that the company was investigating allegations that its chief security officer was involved in the data leak. Star reiterated on Saturday it has found no wrongdoing by the official, Amarjeet Khanuja, though the internal investigation is ongoing.']\n",
            "Highlight Scores: [0.569767951965332]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: India’s wave of cyberattacks prompts calls for tougher rules and countermeasures\n",
            "URL: https://www.scmp.com/week-asia/economics/article/3282324/indias-wave-cyberattacks-prompts-calls-tougher-rules-and-countermeasures?utm_source=rss_feed\n",
            "ID: https://www.scmp.com/week-asia/economics/article/3282324/indias-wave-cyberattacks-prompts-calls-tougher-rules-and-countermeasures?utm_source=rss_feed\n",
            "Score: 0.14184261858463287\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: Vasudevan Sridharan\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: An alarming wave of cyberattacks is sweeping across  , compromising millions of personal records as tech analysts say stronger regulations and other countermeasures are needed to plug the critical weaknesses in the nation’s digital infrastructure. “Various sectors, including healthcare, education, government, and critical infrastructure, have been particularly hard hit,” said Diwakal Dayal, India-based vice president of SentinelOne, a global cybersecurity firm. “These sectors often struggle with underfunded   measures, making them attractive targets for malicious actors.” In recent weeks, major entities including Star Health Insurance, the Supreme Court’s YouTube channel, and the popular podcast BeerBiceps have fallen victim to cyberattacks, leading to significant data breaches and temporary shutdowns. A hacker infiltrated Star Health Insurance, accessing 7.24 terabytes of data and compromising the personal data of up to 3.1 million people, and demanded a ransom from the company in August. Details taken from the system of India’s largest health insurer, including names, birthdays, tax identities and confidential medical records, have been uploaded for sale online for US$150,000 and caused a sharp decline in its share price.\n",
            "Highlights: ['A hacker infiltrated Star Health Insurance, accessing 7.24 terabytes of data and compromising the personal data of up to 3.1 million people, and demanded a ransom from the company in August. Details taken from the system of India’s largest health insurer, including names, birthdays, tax identities and confidential medical records, have been uploaded for sale online for US$150,000 and caused a sharp decline in its share price.']\n",
            "Highlight Scores: [0.6020882725715637]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: India’s biggest health insurer gets ransomware following data breach\n",
            "URL: https://www.techradar.com/pro/star-health-say-it-received-a-usd68-000-ransom-after-data-breach\n",
            "ID: https://www.techradar.com/pro/star-health-say-it-received-a-usd68-000-ransom-after-data-breach\n",
            "Score: 0.1411535143852234\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: Ellen Jennings-Trace\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Indian health insurer Star Health says it has received an email containing a ransom of $68,000 following a \"targeted malicious cyberattack\". The attack allegedly resulted in the leak of personal data of up to 31 million Star Health policy holders and over 5.8 million insurance claims. The health organization confirmed that the cyberattack resulted in \"unauthorized and illegal access to certain data\" including full names, postal addresses, phone numbers, medical reports, and insurance claims. Since the attack, the company has suffered serious reputational damage and an 11% drop in shares, and has also launched legal action against Telegram, since Telegram chatbots were used to leak the information. A mountain of information  The stolen data was reported to total a staggering 7.24 terabytes, although it is not yet confirmed exactly what information was taken. Health Star is said to have sought the assistance of Indian cybersecurity authorities in its investigation. Although the hacker alleged that Chief Information Security Officer Amarjeet Khanuja was involved in the breach, the organization is yet to identify any wrongdoing - but the internal investigation is ongoing. “We also want to categorically mention that our CISO has been duly co-operating in the investigation, and we have not arrived at any finding of wrongdoing by him till date. We request that his privacy be respected as we know that the threat actor is trying to create panic” the insurer commented. Telegram have declined to comment on account details or permanently ban accounts linked to the hacker, an individual dubbed ‘xenZen’, despite \"multiple notices issued in this regard\", the Star has revealed.   Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed!   As is always the case with compromised data, this leaves customers exposed to malicious actors, specifically in regards to the risk of identity theft. We've listed the best identity theft protections for anyone concerned about their data.  Via   Reuters                Take a look at some of the best malware removal software         Healthcare organizations are being hit hard by cyberattacks         Check out our pick for best antivirus software                              \n",
            " Ellen has been writing for almost four years, with a focus on post-COVID policy whilst studying for BA Politics and International Relations at the University of Cardiff, followed by an MA in Political Communication. Before joining TechRadar Pro as a Junior Writer, she worked for Future Publishing’s MVC content team, working with merchants and retailers to upload content. \n",
            "  Most Popular\n",
            "Highlights: ['The attack allegedly resulted in the leak of personal data of up to 31 million Star Health policy holders and over 5.8 million insurance claims. The health organization confirmed that the cyberattack resulted in \"unauthorized and illegal access to certain data\" including full names, postal addresses, phone numbers, medical reports, and insurance claims. Since the attack, the company has suffered serious reputational damage and an 11% drop in shares, and has also launched legal action against Telegram, since Telegram chatbots were used to leak the information. A mountain of information  The stolen data was reported to total a staggering 7.24 terabytes, although it is not yet confirmed exactly what information was taken. Health Star is said to have sought the assistance of Indian cybersecurity authorities in its investigation.']\n",
            "Highlight Scores: [0.5790166258811951]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Tech giant Nidec confirms data breach following ransomware attack\n",
            "URL: https://www.bleepingcomputer.com/news/security/tech-giant-nidec-confirms-data-breach-following-ransomware-attack/\n",
            "ID: https://www.bleepingcomputer.com/news/security/tech-giant-nidec-confirms-data-breach-following-ransomware-attack/\n",
            "Score: 0.13733400404453278\n",
            "Published Date: 2024-10-18T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Nidec Corporation is informing that hackers behind a ransomware attack is suffered earlier this year stole data and leaked it on the dark web.\n",
            "The Japanese tech giant says the threat actors tried to extort the company and decided to leak the information after their demands were not met.\n",
            "The attack did not encrypt files and the incident is considered fully remediated at this time. However, Nidec employees, contractors, and associates, should be aware that the leaked data could be used in more targeted phishing attacks.\n",
            "Nidec Corporation is a global leader in the manufacturing of precision motors, automotive components, industrial parts, home appliance parts, and robotic systems.\n",
            "It operates in 40 countries, employs 120,000 people, and generates an annual revenue of more than $11 billion.\n",
            "Nidec Precision breach\n",
            "The cyberattack breached Nidec Precision division based in Vietnam, which specializes in manufacturing optical, electronic, and mechanical equipment for the photography industry.\n",
            "As per the results of the internal investigation, which is still ongoing, the hackers obtained valid VPN account credentials of a Nidec employee and accessed a server that contained confidential information.\n",
            "The company closed the entry point and implemented additional security measures, as per recommendations from external cybersecurity experts. Nidec employees are undergoing training on how to minimize such risks.\n",
            "The investigation also revealed that the attackers stole 50,694 files, including the following:\n",
            " Internal documents\n",
            "Letters from business partners\n",
            "Documents related to green procurement\n",
            "Labor safety and health policies (business and supply chain, etc.)\n",
            "Business documents (purchase orders, invoices, receipts)\n",
            "Contracts\n",
            " Nidec said that it would notify directly its business partners affected by the incident.\n",
            "8BASE and Everest gangs claim attacks\n",
            "The 8BASE ransomware gang claimed an attack on Nidec on June 18, alleging that the data had been stolen from the systems of the Japanese firm on June 3, 2024.\n",
            "8BASE claimed to be holding much of what Nidec confirmed via its investigation, plus personal data and “a huge mount of confidential information.”\n",
            "Nidec in July aknowledged a ransomware attack without naming the perpetrators, stating that it was the impacted division was Nidec Instruments.\n",
            "On August 8, the Everest ransomware group, known for receiving stolen data from other cybercriminals to perform new extortion attempts on victims, published data allegedly stolen from Nidec.\n",
            "The company states in the latest announcement that the threat actors first made contact on August 5, suggesting that the communication came from the Everest ransomware gang.\n",
            "Nidec has acknowledged that the data that leaked on the dark web comes from its systems but did not offer any clarification about the threat actors' claims.\n",
            "In any case, the company says it does not believe that any of the leaked data could be used to cause direct financial damage to it or its contractors and has not observed unauthorized use of the information.\n",
            " H/T: @H4ckManac\n",
            "Highlights: [\"The company states in the latest announcement that the threat actors first made contact on August 5, suggesting that the communication came from the Everest ransomware gang. Nidec has acknowledged that the data that leaked on the dark web comes from its systems but did not offer any clarification about the threat actors' claims. In any case, the company says it does not believe that any of the leaked data could be used to cause direct financial damage to it or its contractors and has not observed unauthorized use of the information.\"]\n",
            "Highlight Scores: [0.6048170924186707]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: IcePeony and Transparent Tribe Target Indian Entities with Cloud-Based Tools\n",
            "URL: https://thehackernews.com/2024/11/icepeony-and-transparent-tribe-target.html\n",
            "ID: https://thehackernews.com/2024/11/icepeony-and-transparent-tribe-target.html\n",
            "Score: 0.13634811341762543\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: High-profile entities in India have become the target of malicious campaigns orchestrated by the Pakistan-based Transparent Tribe threat actor and a previously unknown China-nexus cyber espionage group dubbed IcePeony.\n",
            "The intrusions linked to Transparent Tribe involve the use of a malware called ElizaRAT and a new stealer payload dubbed ApoloStealer on specific victims of interest, Check Point said in a technical write-up published this week.\n",
            "\"ElizaRAT samples indicate a systematic abuse of cloud-based services, including Telegram, Google Drive, and Slack, to facilitate command-and-control communications,\" the Israeli company said.\n",
            "ElizaRAT is a Windows remote access tool (RAT) that Transparent Tribe was first observed using in July 2023 as part of cyber attacks targeting Indian government sectors. Active since at least 2013, the adversary is also tracked under the names APT36, Datebug, Earth Karkaddan, Mythic Leopard, Operation C-Major, and PROJECTM.\n",
            " \n",
            "Its malware arsenal includes tools for compromising Windows, Android, and Linux devices. The increased targeting of Linux machines is motivated by the Indian government's use of a custom Ubuntu fork called Maya OS since last year.\n",
            "Infection chains are initiated by Control Panel (CPL) files likely distributed via spear-phishing techniques. As many as three distinct campaigns employing the RAT have been observed between December 2023 and August 2024, each using Slack, Google Drive, and a virtual private server (VPS) for command-and-control (C2).\n",
            "ApoloStealer is designed to gather files matching several extensions (e.g., DOC, XLS, PPT, TXT, RTF, ZIP, RAR, JPG, and PNG) from the compromised host and exfiltrate them to a remote server.\n",
            "In January 2024, the threat actor is said to have tweaked the modus operandi to include a dropper component that ensures the smooth functioning of ElizaRAT. Also observed in recent attacks is an additional stealer module codenamed ConnectX that's engineered to search for files from external drives, such as USBs.\n",
            "   \n",
            "The abuse of legitimate services widely used in enterprise environments heightens the threat as it complicates detection efforts and allows threat actors to blend into legitimate activities on the system.\n",
            "\"The progression of ElizaRAT reflects APT36's deliberate efforts to enhance their malware to better evade detection and effectively target Indian entities,\" Check Point said. \"Introducing new payloads such as ApolloStealer marks a significant expansion of APT36's malware arsenal and suggests the group is adopting a more flexible, modular approach to payload deployment.\"\n",
            "IcePeony Goes After India, Mauritius, and Vietnam\n",
            "The disclosure comes weeks after the nao_sec research team revealed an advanced persistent threat (APT) group it calls IcePeony has targeted government agencies, academic institutions, and political organizations in countries such as India, Mauritius, and Vietnam since at least 2023.\n",
            "\"Their attacks typically start with SQL Injection, followed by compromise via web shells and backdoors,\" security researchers Rintaro Koike and Shota Nakajima said. \"Ultimately, they aim to steal credentials.\"\n",
            " \n",
            "One of the most noteworthy tools in its malware portfolio is IceCache, which is designed to target Microsoft Internet Information Services (IIS) instances. An ELF binary written in the Go programming language, it's a custom version of the reGeorg web shell with added file transmission and command execution features.\n",
            "   \n",
            "The attacks are also characterized by the use of a unique passive-mode backdoor referred to as IceEvent that comes with capabilities to upload/download files and execute commands.\n",
            "\"It seems that the attackers work six days a week,\" the researchers noted. \"While they are less active on Fridays and Saturdays, their only full day off appears to be Sunday. This investigation suggests that the attackers are not conducting these attacks as personal activities, but are instead engaging in them as part of organized, professional operations.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['\"It seems that the attackers work six days a week,\" the researchers noted. \"While they are less active on Fridays and Saturdays, their only full day off appears to be Sunday. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.47160276770591736]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres a recent cyber incident involving Blackbasta ransomware in India from the past three months:\n",
            "Resolved Search Type: 2024-10-11T08:59:57.283Z\n",
            "ERROR in Exa Search: name 'SearchResponse' is not defined\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.fortinet.com/fortiguard/outbreak-alert using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🕸️ Crawling https://cyberint.com/wp-content/uploads/2023/04/282828.svg using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, success: True, time taken: 5.16 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, time taken: 0.00 seconds.\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "DEBUG: Extracted content from https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png: []\n",
            "[LOG] 🕸️ Crawling https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 6.48 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.06 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.08 seconds.\n",
            "[LOG] ✅ Crawled https://www.fortinet.com/fortiguard/outbreak-alert successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.fortinet.com/fortiguard/outbreak-alert, success: True, time taken: 6.70 seconds\n",
            "Error extracting metadata: 'content'\n",
            "[LOG] 🚀 Content extracted for https://www.fortinet.com/fortiguard/outbreak-alert, success: True, time taken: 1.80 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.fortinet.com/fortiguard/outbreak-alert, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.fortinet.com/fortiguard/outbreak-alert, time taken: 2.07 seconds.\n",
            "DEBUG: Extracted content from https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta: [{'links': 'https://www.blackberry.com/'}]\n",
            "DEBUG: Extracted content from https://www.fortinet.com/fortiguard/outbreak-alert: [{'links': '#CONTENT'}]\n",
            "[LOG] ✅ Crawled https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp successfully!\n",
            "[LOG] 🚀 Crawling done for https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp, success: True, time taken: 4.05 seconds\n",
            "[LOG] 🚀 Content extracted for https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp, time taken: 0.00 seconds.\n",
            "DEBUG: Extracted content from https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp: []\n",
            "[ERROR] 🚫 arun(): Failed to crawl https://cyberint.com/wp-content/uploads/2023/04/282828.svg, error: [ERROR] 🚫 crawl(): Failed to crawl https://cyberint.com/wp-content/uploads/2023/04/282828.svg: Page.evaluate: TypeError: Failed to execute 'getComputedStyle' on 'Window': parameter 1 is not of type 'Element'.\n",
            "    at eval (eval at evaluate (:234:30), <anonymous>:3:46)\n",
            "    at UtilityScript.evaluate (<anonymous>:241:19)\n",
            "    at UtilityScript.<anonymous> (<anonymous>:1:44)\n",
            "ERROR: Failed to crawl the page https://cyberint.com/wp-content/uploads/2023/04/282828.svg\n",
            "Crawled Results: [[{'links': 'https://www.blackberry.com/'}], [], [{'links': '#CONTENT'}], []]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.blackberry.com/'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#CONTENT'}\n",
            "WARNING: No search results available for citation.\n",
            "Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\n",
            "Executive Summary:\n",
            "\n",
            "- Black Basta is a ransomware operator and Ransomware-as-a-Service (RaaS) criminal enterprise that emerged in early 2022, targeting various industries with a triple-extortion tactic [Source 1].\n",
            "- A 35% increase in ransomware victims from the professional, scientific, and technical services (PSTS) sector, which includes software companies, has been noted in Q2 2024 [Sources 2, 4].\n",
            "- The Cyber Centre is monitoring BlackCat malware, associated with Black Basta, and providing related Tactics, Techniques, and Procedures (TTPs) and Indicators of Compromise (IOCs) [Source 3].\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Key Findings:\n",
            "- Black Basta emerged as a significant ransomware operator in early 2022 [Source 1].\n",
            "- A 35% increase in ransomware victims from the PSTS sector was observed in Q2 2024 [Sources 2, 4].\n",
            "- BlackCat malware, linked to Black Basta, has been targeting various industries with a triple-extortion tactic [Source 1].\n",
            "- The Cyber Centre is actively monitoring BlackCat malware and providing related TTPs and IOCs [Source 3].\n",
            "\n",
            "Technical Details:\n",
            "- Black Basta operates as a RaaS criminal enterprise [Source 1].\n",
            "- BlackCat malware, associated with Black Basta, has been targeting companies in the financial, manufacturing, legal, and professional services industries, among others [Source 1].\n",
            "- The campaign often employs a triple-extortion tactic: making individual ransom demands for decryption, not publishing stolen data, and not launching denial-of-service (DoS) attacks [Source 1].\n",
            "- The Cyber Centre has provided TTPs and IOCs related to BlackCat malware [Source 3].\n",
            "\n",
            "Contextual Analysis:\n",
            "- The increase in ransomware victims from the PSTS sector suggests a shifting RaaS landscape [Sources 2, 4].\n",
            "- The Black Basta group's targeting of various industries indicates a broad attack surface [Source 1].\n",
            "- The Cyber Centre's monitoring and provision of TTPs and IOCs demonstrate a proactive approach to threat intelligence [Source 3].\n",
            "\n",
            "Evidence and Citations:\n",
            "[Source 1] <https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta>\n",
            "[Source 2] <https://www.scmagazine.com/news/q2-2024-ransomware-stats-reflect-shifting-raas-landscape>\n",
            "[Source 3] <No URL>\n",
            "[Source 4] <https://www.scmagazine.com/news/q2-2024-ransomware-stats-reflect-shifting-raas-landscape>\n",
            "\n",
            "Actionable Intelligence:\n",
            "- Implement the provided TTPs and IOCs from the Cyber Centre to protect networks from BlackCat malware [Source 3].\n",
            "- Stay vigilant for ransomware attacks targeting the PSTS sector [Sources 2, 4].\n",
            "- Ensure that cybersecurity best practices are followed to protect environments [Source 3].\n",
            "\n",
            "Future Implications:\n",
            "- Continue monitoring the RaaS landscape for shifts in targeting and tactics [Sources 2, 4].\n",
            "- Regularly update and review TTPs and IOCs related to Black Basta and BlackCat malware [Source 3].\n",
            "- Prepare for potential future increases in ransomware attacks across various industries [Source 1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}