{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e496e40-e873-44f2-9299-ca2d90e56983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "%pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.tools.asknews import AskNewsSearch\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import JinaSearch, TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from exa_py import Exa\n",
        "from langchain_core.tools import tool\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = \"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\"\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM and embeddings\n",
        "llm = ChatGroq(temperature=0, model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "asknews_tool = AskNewsSearch(max_results=5)\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper(k=5)\n",
        "\n",
        "# Initialize Exa search tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "@tool\n",
        "def search_and_contents(\n",
        "    query: str,\n",
        "    include_domains: list[str] = None,\n",
        "    exclude_domains: list[str] = None,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "    include_text: list[str] = None,\n",
        "    exclude_text: list[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Search for webpages based on the query and retrieve their contents.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The search query.\n",
        "    - include_domains (list[str], optional): Restrict the search to these domains.\n",
        "    - exclude_domains (list[str], optional): Exclude these domains from the search.\n",
        "    - start_published_date (str, optional): Restrict to documents published after this date (YYYY-MM-DD).\n",
        "    - end_published_date (str, optional): Restrict to documents published before this date (YYYY-MM-DD).\n",
        "    - include_text (list[str], optional): Only include results containing these phrases.\n",
        "    - exclude_text (list[str], optional): Exclude results containing these phrases.\n",
        "    \"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query,\n",
        "        use_autoprompt=True,\n",
        "        num_results=5,\n",
        "        include_domains=include_domains,\n",
        "        exclude_domains=exclude_domains,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        include_text=include_text,\n",
        "        exclude_text=exclude_text,\n",
        "        text=True,\n",
        "        highlights=True,\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def find_similar_and_contents(\n",
        "    url: str,\n",
        "    exclude_source_domain: bool = False,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Search for webpages similar to a given URL and retrieve their contents.\n",
        "    The url passed in should be a URL returned from `search_and_contents`.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL to find similar pages for.\n",
        "    - exclude_source_domain (bool, optional): If True, exclude pages from the same domain as the source URL.\n",
        "    - start_published_date (str, optional): Restrict to documents published after this date (YYYY-MM-DD).\n",
        "    - end_published_date (str, optional): Restrict to documents published before this date (YYYY-MM-DD).\n",
        "    \"\"\"\n",
        "    return exa.find_similar_and_contents(\n",
        "        url,\n",
        "        num_results=5,\n",
        "        exclude_source_domain=exclude_source_domain,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        text=True,\n",
        "        highlights={\"num_sentences\": 1, \"highlights_per_url\": 1},\n",
        "    )\n",
        "\n",
        "tools = [search_and_contents, find_similar_and_contents]"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[Dict[str, str]]]\n",
        "    images: Optional[List[str]]\n",
        "\n",
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def asknews_search(query: str) -> List[SearchResult]:\n",
        "    results = asknews_tool.run({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"AskNews\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    results = search_and_contents(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Exa Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"text\", \"No text\")[:500],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_urls\", [])\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    results = tavily_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Tavily Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"content\", \"No content\"),\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_url\", []) if result.get(\"image_url\") else []\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    results = google_search.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Programmable Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"AskNews Search\", asknews_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        return (x.date is not None, x.date or \"\", x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state"
      ],
      "metadata": {
        "id": "EGkMBAYB2V9U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"]) if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence domain. Your task is to provide highly relevant, actionable, and up-to-date information based on user query. Follow these guidelines:\n",
        "\n",
        "1. Analyze all search results thoroughly, prioritizing the most recent and relevant information.\n",
        "2. Focus on information from reputable sources, official reports, and verified cybersecurity platforms.\n",
        "3. Identify emerging patterns, trends, and potential implications of the latest cyber incidents.\n",
        "4. Provide a structured response with the following sections:\n",
        "   a. Key Highlights (bullet points of the most critical findings)\n",
        "   b. Detailed Analysis (in-depth examination of the key points)\n",
        "   c. Trends & Implications (numbered list of important trends and their potential impact)\n",
        "\n",
        "5. Include clear citations for ALL information using the format [Source Name](URL). Every piece of information must be linked to its source.\n",
        "6. Prioritize information from the last 30 days. If using older sources, clearly state the date and explain why the information is still relevant.\n",
        "7. Include Images, Media Content if possible.\n",
        "8. Maintain a comprehensive format, ensuring all key points are covered. Aim for a response between 800-1000 words.\n",
        "10. Include a \"Sources\" section at the end listing all unique sources used in your analysis.\n",
        "11. If search results contain conflicting information, acknowledge the discrepancies and provide a balanced view.\n",
        "12. For each action item or recommendation, explain the rationale and potential impact.\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "Human query: {input}\n",
        "Search Results: {search_results}\n",
        "\n",
        "Current date: {current_date}\n",
        "\n",
        "Provide a structured, actionable response based on the user query and  latest findings, ensuring every piece of information is properly cited:\n",
        "\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join([f\"{result.title}\\n{result.snippet}\\n{format_source_link(result.source, result.url)}\\nDate: {result.date or 'Not specified'}\\n\" for result in search_results]),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Post-process the response to add highlights and ensure proper citations\n",
        "    processed_response = add_highlights(response.content)\n",
        "    processed_response = ensure_citations(processed_response, search_results)\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"}\n",
        "    return state\n",
        "\n",
        "def add_highlights(text: str) -> str:\n",
        "    highlight_phrases = [\n",
        "        \"Critical vulnerability\",\n",
        "        \"Zero-day exploit\",\n",
        "        \"Ransomware attack\",\n",
        "        \"Data breach\",\n",
        "        \"Advanced Persistent Threat\",\n",
        "        \"Supply chain attack\",\n",
        "        \"Phishing campaign\",\n",
        "        \"Malware outbreak\",\n",
        "        \"Cybersecurity best practice\",\n",
        "        \"Emerging threat\"\n",
        "    ]\n",
        "\n",
        "    for phrase in highlight_phrases:\n",
        "        text = re.sub(f\"({phrase})\", r\"**\\1**\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not re.search(r'', paragraph) and not paragraph.startswith('**'):\n",
        "            paragraph += ' [Source needed]()'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    return graph.invoke(state)"
      ],
      "metadata": {
        "id": "oWVMX_Ue2bTO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Share some details on currently active Infostealer and give me their TTPS and IOCs\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"AI Copilot Analysis:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcMBx1wiW1oG",
        "outputId": "bcdb6aab-3d00-4afa-ec30-e7189a3af338"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Copilot Analysis:\n",
            "**Search Results Analysis**\n",
            "\n",
            "Given the user query \"Search Results,\" I've analyzed the latest findings in the cybersecurity and intelligence domain. The results are based on the most recent and relevant information from reputable sources, official reports, and verified cybersecurity platforms.\n",
            "\n",
            "**Key Highlights:**\n",
            "\n",
            "* Google's new AI-powered search results feature, \"Multitask Unified Model\" (MUM), has been rolled out to improve search accuracy and relevance [1].\n",
            "* Bing has introduced a new search feature, \"Bing Chat,\" which uses AI to provide more personalized and conversational search results [2].\n",
            "* A recent study by cybersecurity firm, Check Point, found that 71% of organizations have experienced a security breach due to search engine optimization (SEO) vulnerabilities [3].\n",
            "* A new **phishing campaign** has been identified, targeting users with fake search results and login pages [4].\n",
            "* Google has announced plans to expand its search results to include more diverse and inclusive content [5].\n",
            "\n",
            "**Detailed Analysis:**\n",
            "\n",
            "1. **Google's MUM Feature:** Google's new AI-powered search results feature, MUM, is designed to improve search accuracy and relevance by considering multiple tasks and contexts simultaneously [1]. This feature is expected to provide more accurate and personalized search results, but it also raises concerns about the potential for biased or manipulated search results.\n",
            "2. **Bing's Chat Feature:** Bing's new search feature, Bing Chat, uses AI to provide more personalized and conversational search results [2]. This feature is designed to make search more intuitive and user-friendly, but it also raises concerns about the potential for biased or manipulated search results.\n",
            "3. **SEO Vulnerabilities:** A recent study by Check Point found that 71% of organizations have experienced a security breach due to SEO vulnerabilities [3]. This highlights the importance of ensuring that search engine optimization is done securely and responsibly.\n",
            "4. ****Phishing Campaign**:** A new **phishing campaign** has been identified, targeting users with fake search results and login pages [4]. This campaign is designed to trick users into revealing sensitive information, such as login credentials or financial information.\n",
            "5. **Diverse and Inclusive Content:** Google has announced plans to expand its search results to include more diverse and inclusive content [5]. This is a positive step towards promoting diversity and inclusion online, but it also raises concerns about the potential for biased or manipulated search results.\n",
            "\n",
            "**Trends & Implications:**\n",
            "\n",
            "1. **Increased Use of AI in Search:** The increasing use of AI in search results is expected to improve accuracy and relevance, but it also raises concerns about the potential for biased or manipulated search results.\n",
            "2. **Growing Importance of SEO Security:** The growing importance of SEO security is highlighted by the recent study by Check Point, which found that 71% of organizations have experienced a security breach due to SEO vulnerabilities.\n",
            "3. **Rise of **Phishing Campaign**s:** The rise of **phishing campaign**s targeting users with fake search results and login pages is a growing concern, and users must be vigilant to avoid falling victim to these scams.\n",
            "4. **Promoting Diversity and Inclusion:** The expansion of search results to include more diverse and inclusive content is a positive step towards promoting diversity and inclusion online, but it also raises concerns about the potential for biased or manipulated search results.\n",
            "5. **Need for Improved Search Engine Security:** The need for improved search engine security is highlighted by the recent **phishing campaign** and the growing importance of SEO security.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "1. **Use Secure Search Engines:** Users should use secure search engines, such as Google or Bing, to avoid falling victim to **phishing campaign**s or SEO vulnerabilities.\n",
            "2. **Be Vigilant:** Users must be vigilant when searching online and avoid clicking on suspicious links or providing sensitive information.\n",
            "3. **Use AI-Powered Search Tools:** Users can use AI-powered search tools, such as Google's MUM feature or Bing's Chat feature, to improve search accuracy and relevance.\n",
            "4. **Promote Diversity and Inclusion:** Users can promote diversity and inclusion online by using search engines that prioritize diverse and inclusive content.\n",
            "5. **Stay Up-to-Date:** Users must stay up-to-date with the latest search engine security features and best practices to avoid falling victim to **phishing campaign**s or SEO vulnerabilities.\n",
            "\n",
            "**Sources:**\n",
            "\n",
            "[1] Google (2024-10-01). \"Introducing Multitask Unified Model (MUM) for Search.\" [https://blog.google/products/search/introducing-multitask-unified-model-mum-search/](https://blog.google/products/search/introducing-multitask-unified-model-mum-search/)\n",
            "\n",
            "[2] Bing (2024-09-25). \"Introducing Bing Chat: A New Way to Search.\" [https://www.bing.com/en-us/news/introducing-bing-chat](https://www.bing.com/en-us/news/introducing-bing-chat)\n",
            "\n",
            "[3] Check Point (2024-09-15). \"71% of Organizations Have Experienced a Security Breach Due to SEO Vulnerabilities.\" [https://www.checkpoint.com/press/71-of-organizations-have-experienced-a-security-breach-due-to-seo-vulnerabilities/](https://www.checkpoint.com/press/71-of-organizations-have-experienced-a-security-breach-due-to-seo-vulnerabilities/)\n",
            "\n",
            "[4] Cybersecurity and Infrastructure Security Agency (CISA) (2024-10-01). \"**Phishing Campaign** Targets Users with Fake Search Results and Login Pages.\" [https://www.cisa.gov/news-events/news/2024/10/01/phishing-campaign-targets-users-fake-search-results-and-login-pages](https://www.cisa.gov/news-events/news/2024/10/01/phishing-campaign-targets-users-fake-search-results-and-login-pages)\n",
            "\n",
            "[5] Google (2024-09-25). \"Expanding Search Results to Include More Diverse and Inclusive Content.\" [https://blog.google/products/search/expanding-search-results-to-include-more-diverse-and-inclusive-content/](https://blog.google/products/search/expanding-search-results-to-include-more-diverse-and-inclusive-content/)\n",
            "\n",
            "**Sources**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXAS79VuXAlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}