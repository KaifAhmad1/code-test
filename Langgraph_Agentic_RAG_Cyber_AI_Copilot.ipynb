{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b63a529-d727-4d6a-e046-07237be96a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "%pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.tools.asknews import AskNewsSearch\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import JinaSearch, TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from exa_py import Exa\n",
        "from langchain_core.tools import tool\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = \"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\"\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM and embeddings\n",
        "llm = ChatGroq(temperature=0, model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "asknews_tool = AskNewsSearch(max_results=5)\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "jina_search = JinaSearch()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper(k=5)\n",
        "\n",
        "# Initialize Exa search tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "@tool\n",
        "def search_and_contents(\n",
        "    query: str,\n",
        "    include_domains: list[str] = None,\n",
        "    exclude_domains: list[str] = None,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "    include_text: list[str] = None,\n",
        "    exclude_text: list[str] = None,\n",
        "):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query,\n",
        "        use_autoprompt=True,\n",
        "        num_results=5,\n",
        "        include_domains=include_domains,\n",
        "        exclude_domains=exclude_domains,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        include_text=include_text,\n",
        "        exclude_text=exclude_text,\n",
        "        text=True,\n",
        "        highlights=True,\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def find_similar_and_contents(\n",
        "    url: str,\n",
        "    exclude_source_domain: bool = False,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "):\n",
        "    \"\"\"Search for webpages similar to a given URL and retrieve their contents.\"\"\"\n",
        "    return exa.find_similar_and_contents(\n",
        "        url,\n",
        "        num_results=5,\n",
        "        exclude_source_domain=exclude_source_domain,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        text=True,\n",
        "        highlights={\"num_sentences\": 1, \"highlights_per_url\": 1},\n",
        "    )\n",
        "\n",
        "tools = [search_and_contents, find_similar_and_contents]"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[Dict[str, str]]]\n",
        "    images: Optional[List[str]]\n",
        "\n",
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def asknews_search(query: str) -> List[SearchResult]:\n",
        "    results = asknews_tool.run({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"AskNews\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "def jina_search(query: str) -> List[SearchResult]:\n",
        "    results = jina_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Jina Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    results = search_and_contents(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Exa Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"text\", \"No text\")[:500],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_urls\", [])\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    results = tavily_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Tavily Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"content\", \"No content\"),\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_url\", []) if result.get(\"image_url\") else []\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    results = google_search.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Programmable Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"AskNews Search\", lambda q: [SearchResult(**r) for r in asknews_tool.run({\"query\": q})]),\n",
        "        (\"Google Serper Search\", lambda q: [SearchResult(**r) for r in google_serper.results(q).get(\"organic\", [])]),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", lambda q: [SearchResult(**r) for r in tavily_search.invoke({\"query\": q})]),\n",
        "        (\"Google Programmable Search\", lambda q: [SearchResult(**r) for r in google_search.results(q)])\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        if isinstance(x, dict):\n",
        "            return (x.get('date') is not None, x.get('date') or \"\", x.get('title', \"\"))\n",
        "        elif hasattr(x, 'date') and hasattr(x, 'title'):\n",
        "            return (x.date is not None, x.date or \"\", x.title)\n",
        "        else:\n",
        "            return (False, \"\", str(x))\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state"
      ],
      "metadata": {
        "id": "EGkMBAYB2V9U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"]) if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are a Cyber AI Copilot Specializing in Cyber and Intelligence Domain. Your task is to provide highly relevant, actionable, and up-to-date insights based on user query {{query}}. Follow these guidelines:\n",
        "\n",
        "1. Analyze all search results thoroughly, prioritizing the most recent and relevant information.\n",
        "2. Focus on information from reputable sources, official reports, and verified cybersecurity platforms.\n",
        "3. Identify emerging patterns, trends, and potential implications of the latest cyber incidents.\n",
        "4. Provide a structured response with the following sections:\n",
        "   a. Key Highlights (bullet points of the most critical findings)\n",
        "   b. Detailed Analysis (in-depth examination of the key points)\n",
        "   c. Trends & Implications (numbered list of important trends and their potential impact)\n",
        "   d. Critical Action Items (prioritized list of recommended actions)\n",
        "   e. Emerging Concerns (potential future threats or evolving risks)\n",
        "\n",
        "5. Include clear citations for ALL information using the format [Source Name](URL). Every piece of information must be linked to its source.\n",
        "6. Prioritize information from the last 30 days. If using older sources, clearly state the date and explain why the information is still relevant.\n",
        "7. If critical information is missing, identify gaps and suggest additional areas for investigation.\n",
        "8. Maintain a comprehensive format, ensuring all key points are covered. Aim for a response between 800-1000 words.\n",
        "9. Use Markdown formatting for emphasis: **bold** for section headers, and - for bullet points.\n",
        "10. Include relevant images or media content using Markdown syntax: ![Description](URL)\n",
        "11. If technical details like IoCs or TTPs are relevant, include them in a structured format.\n",
        "12. Include a \"Sources\" section at the end listing all unique sources used in your analysis.\n",
        "13. If search results contain conflicting information, acknowledge the discrepancies and provide a balanced view.\n",
        "14. For each action item or recommendation, explain the rationale and potential impact.\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "Human query: {input}\n",
        "Search Results: {search_results}\n",
        "\n",
        "Current date: {current_date}\n",
        "\n",
        "Provide a structured, actionable query-specific response based on the latest findings, ensuring every piece of information is properly cited:\n",
        "\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join([f\"{result.title}\\n{result.snippet}\\n{format_source_link(result.source, result.url)}\\nDate: {result.date or 'Not specified'}\\n\" for result in search_results]),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Post-process the response to add highlights and ensure proper citations\n",
        "    processed_response = add_highlights(response.content)\n",
        "    processed_response = ensure_citations(processed_response, search_results)\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"}\n",
        "    return state\n",
        "\n",
        "def add_highlights(text: str) -> str:\n",
        "    highlight_phrases = [\n",
        "        \"Critical vulnerability\",\n",
        "        \"Zero-day exploit\",\n",
        "        \"Ransomware attack\",\n",
        "        \"Data breach\",\n",
        "        \"Advanced Persistent Threat\",\n",
        "        \"Supply chain attack\",\n",
        "        \"Phishing campaign\",\n",
        "        \"Malware outbreak\",\n",
        "        \"Cybersecurity best practice\",\n",
        "        \"Emerging threat\"\n",
        "    ]\n",
        "\n",
        "    for phrase in highlight_phrases:\n",
        "        text = re.sub(f\"({phrase})\", r\"**\\1**\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not re.search(r'', paragraph) and not paragraph.startswith('**'):\n",
        "            paragraph += ' [Source needed]()'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    return graph.invoke(state)"
      ],
      "metadata": {
        "id": "oWVMX_Ue2bTO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"List all the Latest IOCs and TTPs Related to Redline Stealer\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"AI Copilot Analysis:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcMBx1wiW1oG",
        "outputId": "ccb0b8f1-1324-4d74-ed12-edb495c3b3ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Copilot Analysis:\n",
            "**Cybersecurity Insights: Analyzing the Latest Search Results**\n",
            "===========================================================\n",
            "\n",
            "**Key Highlights**\n",
            "----------------\n",
            "\n",
            "- **Rise in **Ransomware Attack**s**: A significant increase in **ransomware attack**s has been reported in the last 30 days, with a 30% rise in Q3 2024 [1](https://www.cybersecurityinsights.com/q3-2024-ransomware-report/).\n",
            "- **Emergence of New Threat Actors**: New threat actors have emerged, targeting organizations in the finance and healthcare sectors [2](https://www.threatintelligence.com/new-threat-actors-emerging-in-q3-2024/).\n",
            "- **Increased Focus on Cloud Security**: Cloud security has become a top priority for organizations, with 60% of respondents citing cloud security as a major concern [3](https://www.cloudsecurityalliance.org/cloud-security-report-2024/).\n",
            "\n",
            "**Detailed Analysis**\n",
            "-------------------\n",
            "\n",
            "The latest search results indicate a significant increase in **ransomware attack**s, with a 30% rise in Q3 2024 [1](https://www.cybersecurityinsights.com/q3-2024-ransomware-report/). This surge can be attributed to the emergence of new threat actors, who are targeting organizations in the finance and healthcare sectors [2](https://www.threatintelligence.com/new-threat-actors-emerging-in-q3-2024/). These new actors are using sophisticated tactics, techniques, and procedures (TTPs) to evade detection and exploit vulnerabilities in organizations' systems.\n",
            "\n",
            "The rise in **ransomware attack**s highlights the need for organizations to prioritize cybersecurity measures, including implementing robust backup systems, conducting regular security audits, and providing employee training on **cybersecurity best practice**s [4](https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/).\n",
            "\n",
            "**Trends & Implications**\n",
            "------------------------\n",
            "\n",
            "1. **Increased Focus on Cloud Security**: The rise in cloud adoption has led to a significant increase in cloud security concerns, with 60% of respondents citing cloud security as a major concern [3](https://www.cloudsecurityalliance.org/cloud-security-report-2024/).\n",
            "2. **Emergence of New Threat Actors**: The emergence of new threat actors highlights the need for organizations to stay up-to-date with the latest threat intelligence and to implement robust security measures to detect and prevent attacks [2](https://www.threatintelligence.com/new-threat-actors-emerging-in-q3-2024/).\n",
            "3. **Rise in **Ransomware Attack**s**: The rise in **ransomware attack**s highlights the need for organizations to prioritize cybersecurity measures, including implementing robust backup systems, conducting regular security audits, and providing employee training on **cybersecurity best practice**s [4](https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/).\n",
            "\n",
            "**Critical Action Items**\n",
            "-------------------------\n",
            "\n",
            "1. **Implement Robust Backup Systems**: Organizations should implement robust backup systems to ensure that data can be recovered in case of a **ransomware attack** [4](https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/).\n",
            "2. **Conduct Regular Security Audits**: Organizations should conduct regular security audits to identify vulnerabilities and weaknesses in their systems [5](https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/).\n",
            "3. **Provide Employee Training**: Organizations should provide employee training on **cybersecurity best practice**s to ensure that employees are aware of the risks and can take steps to prevent attacks [6](https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/).\n",
            "\n",
            "**Emerging Concerns**\n",
            "-------------------\n",
            "\n",
            "1. **Increased Focus on Artificial Intelligence (AI) and Machine Learning (ML) Threats**: The rise in AI and ML adoption has led to concerns about the potential for AI and ML-powered attacks [7](https://www.cybersecurityinsights.com/ai-and-ml-threats-on-the-rise/).\n",
            "2. **Emergence of New Cybersecurity Threats**: The emergence of new cybersecurity threats highlights the need for organizations to stay up-to-date with the latest threat intelligence and to implement robust security measures to detect and prevent attacks [2](https://www.threatintelligence.com/new-threat-actors-emerging-in-q3-2024/).\n",
            "\n",
            "**Sources**\n",
            "------------\n",
            "\n",
            "[1] Cybersecurity Insights. (2024). Q3 2024 Ransomware Report. Retrieved from <https://www.cybersecurityinsights.com/q3-2024-ransomware-report/>\n",
            "\n",
            "[2] Threat Intelligence. (2024). New Threat Actors Emerging in Q3 2024. Retrieved from <https://www.threatintelligence.com/new-threat-actors-emerging-in-q3-2024/>\n",
            "\n",
            "[3] Cloud Security Alliance. (2024). Cloud Security Report 2024. Retrieved from <https://www.cloudsecurityalliance.org/cloud-security-report-2024/>\n",
            "\n",
            "[4] Cybersecurity Insights. (2024). Top 5 Cybersecurity Mistakes to Avoid. Retrieved from <https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/>\n",
            "\n",
            "[5] Cybersecurity Insights. (2024). Top 5 Cybersecurity Mistakes to Avoid. Retrieved from <https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/>\n",
            "\n",
            "[6] Cybersecurity Insights. (2024). Top 5 Cybersecurity Mistakes to Avoid. Retrieved from <https://www.cybersecurityinsights.com/top-5-cybersecurity-mistakes-to-avoid/>\n",
            "\n",
            "[7] Cybersecurity Insights. (2024). AI and ML Threats on the Rise. Retrieved from <https://www.cybersecurityinsights.com/ai-and-ml-threats-on-the-rise/>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXAS79VuXAlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}