{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "2d70bb82-36d5-4d57-d7e3-78330d0f13d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 6.95MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 11.4MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 14.6MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 21.7MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.66MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 17.0MB/s]\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 225MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Downloading Chromium 129.0.6668.29 (playwright build v1134)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1134/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164 MiB [] 0% 0.0s\u001b[0K\u001b[1G164 MiB [] 0% 42.2s\u001b[0K\u001b[1G164 MiB [] 0% 68.5s\u001b[0K\u001b[1G164 MiB [] 0% 32.1s\u001b[0K\u001b[1G164 MiB [] 0% 18.6s\u001b[0K\u001b[1G164 MiB [] 0% 23.5s\u001b[0K\u001b[1G164 MiB [] 0% 18.9s\u001b[0K\u001b[1G164 MiB [] 1% 11.6s\u001b[0K\u001b[1G164 MiB [] 2% 8.4s\u001b[0K\u001b[1G164 MiB [] 3% 6.1s\u001b[0K\u001b[1G164 MiB [] 4% 4.9s\u001b[0K\u001b[1G164 MiB [] 5% 4.2s\u001b[0K\u001b[1G164 MiB [] 6% 3.8s\u001b[0K\u001b[1G164 MiB [] 7% 3.5s\u001b[0K\u001b[1G164 MiB [] 8% 3.2s\u001b[0K\u001b[1G164 MiB [] 9% 3.0s\u001b[0K\u001b[1G164 MiB [] 10% 2.8s\u001b[0K\u001b[1G164 MiB [] 11% 2.6s\u001b[0K\u001b[1G164 MiB [] 12% 2.5s\u001b[0K\u001b[1G164 MiB [] 13% 2.3s\u001b[0K\u001b[1G164 MiB [] 14% 2.2s\u001b[0K\u001b[1G164 MiB [] 16% 2.0s\u001b[0K\u001b[1G164 MiB [] 17% 1.9s\u001b[0K\u001b[1G164 MiB [] 19% 1.8s\u001b[0K\u001b[1G164 MiB [] 20% 1.7s\u001b[0K\u001b[1G164 MiB [] 22% 1.6s\u001b[0K\u001b[1G164 MiB [] 23% 1.6s\u001b[0K\u001b[1G164 MiB [] 24% 1.5s\u001b[0K\u001b[1G164 MiB [] 25% 1.5s\u001b[0K\u001b[1G164 MiB [] 26% 1.4s\u001b[0K\u001b[1G164 MiB [] 28% 1.4s\u001b[0K\u001b[1G164 MiB [] 29% 1.4s\u001b[0K\u001b[1G164 MiB [] 29% 1.3s\u001b[0K\u001b[1G164 MiB [] 30% 1.3s\u001b[0K\u001b[1G164 MiB [] 32% 1.3s\u001b[0K\u001b[1G164 MiB [] 33% 1.2s\u001b[0K\u001b[1G164 MiB [] 35% 1.2s\u001b[0K\u001b[1G164 MiB [] 36% 1.1s\u001b[0K\u001b[1G164 MiB [] 38% 1.1s\u001b[0K\u001b[1G164 MiB [] 39% 1.0s\u001b[0K\u001b[1G164 MiB [] 41% 1.0s\u001b[0K\u001b[1G164 MiB [] 42% 1.0s\u001b[0K\u001b[1G164 MiB [] 43% 1.0s\u001b[0K\u001b[1G164 MiB [] 45% 0.9s\u001b[0K\u001b[1G164 MiB [] 47% 0.9s\u001b[0K\u001b[1G164 MiB [] 48% 0.8s\u001b[0K\u001b[1G164 MiB [] 49% 0.8s\u001b[0K\u001b[1G164 MiB [] 50% 0.8s\u001b[0K\u001b[1G164 MiB [] 51% 0.8s\u001b[0K\u001b[1G164 MiB [] 53% 0.8s\u001b[0K\u001b[1G164 MiB [] 54% 0.7s\u001b[0K\u001b[1G164 MiB [] 55% 0.7s\u001b[0K\u001b[1G164 MiB [] 56% 0.7s\u001b[0K\u001b[1G164 MiB [] 57% 0.7s\u001b[0K\u001b[1G164 MiB [] 59% 0.6s\u001b[0K\u001b[1G164 MiB [] 61% 0.6s\u001b[0K\u001b[1G164 MiB [] 63% 0.6s\u001b[0K\u001b[1G164 MiB [] 64% 0.5s\u001b[0K\u001b[1G164 MiB [] 65% 0.5s\u001b[0K\u001b[1G164 MiB [] 66% 0.5s\u001b[0K\u001b[1G164 MiB [] 68% 0.5s\u001b[0K\u001b[1G164 MiB [] 69% 0.5s\u001b[0K\u001b[1G164 MiB [] 70% 0.4s\u001b[0K\u001b[1G164 MiB [] 71% 0.4s\u001b[0K\u001b[1G164 MiB [] 73% 0.4s\u001b[0K\u001b[1G164 MiB [] 75% 0.4s\u001b[0K\u001b[1G164 MiB [] 77% 0.3s\u001b[0K\u001b[1G164 MiB [] 79% 0.3s\u001b[0K\u001b[1G164 MiB [] 80% 0.3s\u001b[0K\u001b[1G164 MiB [] 81% 0.3s\u001b[0K\u001b[1G164 MiB [] 83% 0.2s\u001b[0K\u001b[1G164 MiB [] 84% 0.2s\u001b[0K\u001b[1G164 MiB [] 85% 0.2s\u001b[0K\u001b[1G164 MiB [] 86% 0.2s\u001b[0K\u001b[1G164 MiB [] 87% 0.2s\u001b[0K\u001b[1G164 MiB [] 89% 0.2s\u001b[0K\u001b[1G164 MiB [] 91% 0.1s\u001b[0K\u001b[1G164 MiB [] 92% 0.1s\u001b[0K\u001b[1G164 MiB [] 94% 0.1s\u001b[0K\u001b[1G164 MiB [] 95% 0.1s\u001b[0K\u001b[1G164 MiB [] 96% 0.0s\u001b[0K\u001b[1G164 MiB [] 98% 0.0s\u001b[0K\u001b[1G164 MiB [] 99% 0.0s\u001b[0K\u001b[1G164 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 129.0.6668.29 (playwright build v1134) downloaded to /root/.cache/ms-playwright/chromium-1134\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 12% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 31% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 90% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Firefox 130.0 (playwright build v1463)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1463/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G86.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 0% 25.1s\u001b[0K\u001b[1G86.4 MiB [] 0% 18.8s\u001b[0K\u001b[1G86.4 MiB [] 0% 10.6s\u001b[0K\u001b[1G86.4 MiB [] 1% 5.2s\u001b[0K\u001b[1G86.4 MiB [] 2% 2.7s\u001b[0K\u001b[1G86.4 MiB [] 4% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 6% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 8% 1.4s\u001b[0K\u001b[1G86.4 MiB [] 11% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 14% 1.0s\u001b[0K\u001b[1G86.4 MiB [] 16% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 19% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 20% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 21% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 22% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 24% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 26% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 28% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 29% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 32% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 35% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 37% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 38% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 39% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 42% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 45% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 47% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 48% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 49% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 52% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 55% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 56% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 59% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 62% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 65% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 66% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 68% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 71% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 74% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 75% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 78% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 81% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 83% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 84% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 85% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 88% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 91% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 93% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 95% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 99% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 130.0 (playwright build v1463) downloaded to /root/.cache/ms-playwright/firefox-1463\n",
            "Downloading Webkit 18.0 (playwright build v2070)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2070/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G88.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 0% 22.7s\u001b[0K\u001b[1G88.2 MiB [] 0% 19.2s\u001b[0K\u001b[1G88.2 MiB [] 0% 13.3s\u001b[0K\u001b[1G88.2 MiB [] 1% 5.7s\u001b[0K\u001b[1G88.2 MiB [] 2% 3.2s\u001b[0K\u001b[1G88.2 MiB [] 4% 2.1s\u001b[0K\u001b[1G88.2 MiB [] 6% 1.6s\u001b[0K\u001b[1G88.2 MiB [] 8% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 10% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 11% 1.2s\u001b[0K\u001b[1G88.2 MiB [] 14% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 16% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 18% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 19% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 21% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 23% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 26% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 28% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 29% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 31% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 34% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 36% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 37% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 38% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 40% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 43% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 45% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 46% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 47% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 48% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 49% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 51% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 54% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 55% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 56% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 59% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 63% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 64% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 65% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 68% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 71% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 72% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 73% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 74% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 77% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 80% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 82% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 83% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 86% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 90% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 91% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 92% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 94% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.0 (playwright build v2070) downloaded to /root/.cache/ms-playwright/webkit-2070\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import JinaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_compressors import JinaRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain import hub\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "JINA_API_KEY =\"jina_72680535332e480f80ca0833953b3f3066AVc7Bh7cAoYB0uWt0CvMTgrJsq\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"JINA_API_KEY\"] = JINA_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065d8df5-63e2-4116-8c48-bbea78bb6ea5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "class SearchResponse:\n",
        "    def __init__(self, results: List[dict]):\n",
        "        self.results = results\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "async def extract_content_from_url(url):\n",
        "    schema = {\n",
        "        \"name\": \"Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "        print(f\"DEBUG: Extracted content from {url}: {extracted_content}\")\n",
        "        return extracted_content\n",
        "\n",
        "async def scrape_links_and_content(urls):\n",
        "    tasks = [extract_content_from_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return [result for result in results if result is not None]"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search),\n",
        "        (\"Google Serper Image Search\", google_serper_image_search),\n",
        "        (\"Google Programmable Image Search\", google_programmable_image_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in {name}: {str(e)}\")\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        parsed_date = parse_date(x.date)\n",
        "        return (parsed_date is not None, parsed_date or datetime.min, x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant and recent results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    # Extract URLs for further crawling\n",
        "    urls_to_crawl = [result.url for result in top_results if result.url]\n",
        "\n",
        "    # Scrape links and content from the extracted URLs\n",
        "    crawled_results = asyncio.run(scrape_links_and_content(urls_to_crawl))\n",
        "\n",
        "    # Add crawled results to the state\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Crawled Results\", \"crawled_results\": crawled_results})\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "\n",
        "    # Add search results to the state for reranking\n",
        "    state[\"search_results\"] = top_results\n",
        "\n",
        "    # Debugging statement\n",
        "    print(\"Search Results Set:\", state[\"search_results\"])\n",
        "\n",
        "    return state\n",
        "\n",
        "# Initialize Jina Reranker\n",
        "compressor = JinaRerank()\n",
        "\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "def rerank_results(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Check if search_results key exists in the state\n",
        "    if \"search_results\" not in state:\n",
        "        print(\"ERROR: search_results key not found in state\")\n",
        "        return state\n",
        "\n",
        "    results = state[\"search_results\"]\n",
        "\n",
        "    # Extract text content for reranking\n",
        "    texts = [result.snippet for result in results]\n",
        "    # Rerank the results based on the query\n",
        "    reranked_results = compression_retriever.get_relevant_documents(query)\n",
        "    # Map reranked Document objects to original results\n",
        "    reranked_indices = [doc.metadata.get(\"index\") for doc in reranked_results if \"index\" in doc.metadata]\n",
        "    reranked_results = [results[i] for i in reranked_indices if i < len(results)]\n",
        "\n",
        "    # Add reranked results to the state\n",
        "    state[\"reranked_results\"] = reranked_results\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "cjMU8sqFJu18"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = state[\"reranked_results\"]\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines and data sources to provide comprehensive, query-specific responses.\n",
        "\n",
        "SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "1. Primary Source Evaluation:\n",
        "   - Assess credibility of each source domain\n",
        "   - Verify publication dates for temporal relevance\n",
        "   - Cross-reference information across multiple sources\n",
        "   - Identify and flag potential misinformation or conflicting data\n",
        "\n",
        "2. Content Processing Guidelines:\n",
        "   - Extract and normalize key data points\n",
        "   - Identify patterns and correlations across sources\n",
        "   - Prioritize information based on:\n",
        "     * Temporal relevance (newest to oldest)\n",
        "     * Source reliability\n",
        "     * Direct query relevance\n",
        "     * Technical depth\n",
        "     * Actionable insights\n",
        "\n",
        "3. Media Content Analysis:\n",
        "   - Evaluate included images, diagrams, or screenshots\n",
        "   - Extract relevant technical indicators from visual data\n",
        "   - Correlate visual evidence with textual information\n",
        "   - Note any visual proof of concepts or attack demonstrations\n",
        "\n",
        "RESPONSE STRUCTURE:\n",
        "1. Executive Summary (2-3 sentences)\n",
        "   - Core findings\n",
        "   - Critical alerts or time-sensitive information\n",
        "   - Confidence level in findings\n",
        "\n",
        "2. Detailed Analysis:\n",
        "   a) Key Findings\n",
        "      - Bullet points of critical discoveries\n",
        "      - Emerging threats or developments\n",
        "      - Statistical data or metrics\n",
        "\n",
        "   b) Technical Details\n",
        "      - Specific vulnerabilities or exploits\n",
        "      - Attack vectors and techniques\n",
        "      - System impacts and affected components\n",
        "\n",
        "   c) Contextual Analysis\n",
        "      - Industry impact\n",
        "      - Threat actor attribution (if applicable)\n",
        "      - Historical context or similar incidents\n",
        "\n",
        "3. Evidence and Citations:\n",
        "   - Link every major claim to source material\n",
        "   - Include relevant quote snippets\n",
        "   - Provide context for technical indicators\n",
        "   - Reference related media content\n",
        "\n",
        "4. Actionable Intelligence:\n",
        "   - Immediate response recommendations\n",
        "   - Mitigation strategies\n",
        "   - Detection methods\n",
        "   - Prevention measures\n",
        "\n",
        "5. Future Implications:\n",
        "   - Projected developments\n",
        "   - Potential cascade effects\n",
        "   - Areas requiring monitoring\n",
        "\n",
        "SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "1. For Threat Intelligence:\n",
        "   - Extract and validate IOCs\n",
        "   - Identify TTPs and map to MITRE ATT&CK\n",
        "   - Analyze malware behaviors\n",
        "   - Document C2 infrastructure\n",
        "\n",
        "2. For Vulnerability Analysis:\n",
        "   - Verify CVE details\n",
        "   - Document exploit requirements\n",
        "   - Assess patch availability\n",
        "   - Evaluate real-world exploitation\n",
        "\n",
        "3. For Incident Response:\n",
        "   - Timeline reconstruction\n",
        "   - Attack path analysis\n",
        "   - Impact assessment\n",
        "   - Recovery recommendations\n",
        "\n",
        "4. For Trend Analysis:\n",
        "   - Identify pattern changes\n",
        "   - Map threat evolution\n",
        "   - Project future developments\n",
        "   - Compare against historical data\n",
        "\n",
        "Previous conversation context: {chat_history}\n",
        "Current query: {input}\n",
        "Available search results: {search_results}\n",
        "Crawled results: {crawled_results}\n",
        "Current timestamp: {current_date}\n",
        "\n",
        "RESPONSE REQUIREMENTS:\n",
        "1. Maintain clinical precision and technical accuracy\n",
        "2. Prioritize actionable intelligence over general information\n",
        "3. Include explicit confidence levels for all assessments\n",
        "4. Cite ALL sources using [Source Name](URL) format\n",
        "5. Highlight time-sensitive information\n",
        "6. Address any information gaps or uncertainties\n",
        "7. Format output for maximum readability\n",
        "8. Include relevant media references\n",
        "9. Provide specific, implementable recommendations\n",
        "10. Maintain proper technical context throughout\n",
        "\n",
        "Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the search results:\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                if media.get(\"type\") == \"image\":\n",
        "                    display(Image(url=media.get(\"url\"), width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"rerank_results\", rerank_results)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"rerank_results\")\n",
        "workflow.add_edge(\"rerank_results\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = graph.invoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomware?\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4285e1b-8eda-40f3-9a12-33a58fa4f7d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomware?\n",
            "DEBUG: Raw results from Exa Search: Title: New CRON#TRAP Malware Infects Windows by Hiding in Linux VM to Evade Antivirus\n",
            "URL: https://thehackernews.com/2024/11/new-crontrap-malware-infects-windows-by.html\n",
            "ID: https://thehackernews.com/2024/11/new-crontrap-malware-infects-windows-by.html\n",
            "Score: 0.14465974271297455\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Cybersecurity researchers have flagged a new malware campaign that infects Windows systems with a Linux virtual instance containing a backdoor capable of establishing remote access to the compromised hosts.\n",
            "The \"intriguing\" campaign, codenamed CRON#TRAP, starts with a malicious Windows shortcut (LNK) file likely distributed in the form of a ZIP archive via a phishing email.\n",
            "\"What makes the CRON#TRAP campaign particularly concerning is that the emulated Linux instance comes pre-configured with a backdoor that automatically connects to an attacker-controlled command-and-control (C2) server,\" Securonix researchers Den Iuzvyk and Tim Peck said in an analysis.\n",
            " \n",
            "\"This setup allows the attacker to maintain a stealthy presence on the victim's machine, staging further malicious activity within a concealed environment, making detection challenging for traditional antivirus solutions.\"\n",
            "The phishing messages purport to be an \"OneAmerica survey\" that comes with a large 285MB ZIP archive that, when opened, triggers the infection process.\n",
            "As part of the as-yet-unattributed attack campaign, the LNK file serves as a conduit to extract and initiate a lightweight, custom Linux environment emulated through Quick Emulator (QEMU), a legitimate, open-source virtualization tool. The virtual machine runs on Tiny Core Linux.\n",
            "   \n",
            "The shortcut subsequently launches PowerShell commands responsible for re-extracting the ZIP file and executing a hidden \"start.bat\" script, which, in turn, displays a fake error message to the victim to give them the impression that the survey link is no longer working.\n",
            "But in the background, it sets up the QEMU virtual Linux environment referred to as PivotBox, which comes preloaded with the Chisel tunneling utility, granting remote access to the host immediately following the startup of the QEMU instance.\n",
            "\"The binary appears to be a pre-configured Chisel client designed to connect to a remote Command and Control (C2) server at 18.208.230[.]174 via websockets,\" the researchers said. \"The attackers' approach effectively transforms this Chisel client into a full backdoor, enabling remote command and control traffic to flow in and out of the Linux environment.\"\n",
            "   \n",
            "The development is one of the many constantly evolving tactics that threat actors are using to target organizations and conceal malicious activity -- case in point is a spear-phishing campaign that has been observed targeting electronic manufacturing, engineering, and industrial companies in European countries to deliver the evasive GuLoader malware.\n",
            "\"The emails typically include order inquiries and contain an archive file attachment,\" Cado Security researcher Tara Gould said. \"The emails are sent from various email addresses including from fake companies and compromised accounts. The emails typically hijack an existing email thread or request information about an order.\"\n",
            " \n",
            "The activity, which has mainly targeted countries like Romania, Poland, Germany, and Kazakhstan, starts with a batch file present within the archive file. The batch file embeds an obfuscated PowerShell script that subsequently downloads another PowerShell script from a remote server.\n",
            "The secondary PowerShell script includes functionality to allocate memory and ultimately execute the GuLoader shellcode to ultimately fetch the next-stage payload.\n",
            "\"Guloader malware continues to adapt its techniques to evade detection to deliver RATs,\" Gould said. \"Threat actors are continually targeting specific industries in certain countries. Its resilience highlights the need for proactive security measures.\" \n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: [']174 via websockets,\" the researchers said. The development is one of the many constantly evolving tactics that threat actors are using to target organizations and conceal malicious activity -- case in point is a spear-phishing campaign that has been observed targeting electronic manufacturing, engineering, and industrial companies in European countries to deliver the evasive GuLoader malware. \"The emails typically include order inquiries and contain an archive file attachment,\" Cado Security researcher Tara Gould said. \"The emails are sent from various email addresses including from fake companies and compromised accounts. The activity, which has mainly targeted countries like Romania, Poland, Germany, and Kazakhstan, starts with a batch file present within the archive file.']\n",
            "Highlight Scores: [0.4658278822898865]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: CISA Alerts to Active Exploitation of Critical Palo Alto Networks Vulnerability\n",
            "URL: https://thehackernews.com/2024/11/cisa-alerts-to-active-exploitation-of.html\n",
            "ID: https://thehackernews.com/2024/11/cisa-alerts-to-active-exploitation-of.html\n",
            "Score: 0.13351956009864807\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The U.S. Cybersecurity and Infrastructure Security Agency (CISA) on Thursday added a now-patched critical security flaw impacting Palo Alto Networks Expedition to its Known Exploited Vulnerabilities (KEV) catalog, citing evidence of active exploitation.\n",
            "The vulnerability, tracked as CVE-2024-5910 (CVSS score: 9.3), concerns a case of missing authentication in the Expedition migration tool that could lead to an admin account takeover.\n",
            "\"Palo Alto Expedition contains a missing authentication vulnerability that allows an attacker with network access to takeover an Expedition admin account and potentially access configuration secrets, credentials, and other data,\" CISA said in an alert.\n",
            " \n",
            "The shortcoming impacts all versions of Expedition prior to version 1.2.92, which was released in July 2024 to plug the problem.\n",
            "There are currently no reports on how the vulnerability is being weaponized in real-world attacks, but Palo Alto Networks has since revised its original advisory to acknowledge that it's \"aware of reports from CISA that there is evidence of active exploitation.\"\n",
            "Also added to the KEV catalog are two other flaws, including a privilege escalation vulnerability in the Android Framework component (CVE-2024-43093) that Google disclosed this week as having come under \"limited, targeted exploitation.\"\n",
            "The other security defect is CVE-2024-51567 (CVSS score: 10.0), a critical flaw affecting CyberPanel that allows a remote, unauthenticated attacker to execute commands as root. The issue has been resolved in version 2.3.8.\n",
            " \n",
            "In late October 2023, it emerged that the vulnerability was being exploited en masse by malicious actors to deploy PSAUX ransomware on more than 22,000 internet-exposed CyberPanel instances, according to LeakIX and a security researcher who goes by the online alias Gi7w0rm.\n",
            "LeakIX also noted that three distinct ransomware groups have quickly capitalized on the vulnerability, with files encrypted multiple times in some cases.\n",
            "Federal Civilian Executive Branch (FCEB) agencies have been recommended to remediate the identified vulnerabilities by November 28, 2024, to secure their networks against active threats.\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['LeakIX also noted that three distinct ransomware groups have quickly capitalized on the vulnerability, with files encrypted multiple times in some cases. Federal Civilian Executive Branch (FCEB) agencies have been recommended to remediate the identified vulnerabilities by November 28, 2024, to secure their networks against active threats. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.29087668657302856]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Malicious NPM Packages Target Roblox Users with Data-Stealing Malware\n",
            "URL: https://thehackernews.com/2024/11/malicious-npm-packages-target-roblox.html\n",
            "ID: https://thehackernews.com/2024/11/malicious-npm-packages-target-roblox.html\n",
            "Score: 0.13081510365009308\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A new campaign has targeted the npm package repository with malicious JavaScript libraries that are designed to infect Roblox users with open-source stealer malware such as Skuld and Blank-Grabber.\n",
            "\"This incident highlights the alarming ease with which threat actors can launch supply chain attacks by exploiting trust and human error within the open source ecosystem, and using readily available commodity malware, public platforms like GitHub for hosting malicious executables, and communication channels like Discord and Telegram for C2 operations to bypass traditional security measures,\" Socket security researcher Kirill Boychenko said in a report shared with The Hacker News.\n",
            " \n",
            "The list of malicious packages is as follows -\n",
            " node-dlls (77 downloads)\n",
            " ro.dll (74 downloads)\n",
            " autoadv (66 downloads)\n",
            " rolimons-api (107 downloads)\n",
            "It's worth pointing out that \"node-dlls\" is an attempt on part of the threat actor to masquerade as the legitimate node-dll package, which offers a doubly linked list implementation for JavaScript. Similarly, rolimons-api is a deceptive variant of Rolimon's API.\n",
            "   \n",
            "\"While there are unofficial wrappers and modules — such as the rolimons Python package (downloaded over 17,000 times) and the Rolimons Lua module on GitHub — the malicious rolimons-api packages sought to exploit developers' trust in familiar names,\" Boychenko noted.\n",
            "The rogue packages incorporate obfuscated code that downloads and executes Skuld and Blank Grabber, stealer malware families written in Golang and Python, respectively, that are capable of harvesting a wide range of information from infected systems. The captured data is then exfiltrated to the attacker via Discord webhook or Telegram.\n",
            " \n",
            "In a further attempt to bypass security protections, the malware binaries are retrieved from a GitHub repository (\"github[.]com/zvydev/code/\") controlled by the threat actor.\n",
            "Roblox's popularity in recent years has led to threat actors actively pushing bogus packages to target both developers and users. Earlier this year, several malicious packages like noblox.js-proxy-server, noblox-ts, and noblox.js-async were discovered impersonating the popular noblox.js library.\n",
            "With bad actors exploiting the trust with widely-used packages to push typosquatted packages, developers are advised to verify package names and scrutinize source code prior to downloading them.\n",
            "\"As open-source ecosystems grow and more developers rely on shared code, the attack surface expands, with threat actors looking for more opportunities to infiltrate malicious code,\" Boychenko said. \"This incident emphasizes the need for heightened awareness and robust security practices among developers.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['\"This incident highlights the alarming ease with which threat actors can launch supply chain attacks by exploiting trust and human error within the open source ecosystem, and using readily available commodity malware, public platforms like GitHub for hosting malicious executables, and communication channels like Discord and Telegram for C2 operations to bypass traditional security measures,\" Socket security researcher Kirill Boychenko said in a report shared with The Hacker News. It\\'s worth pointing out that \"node-dlls\" is an attempt on part of the threat actor to masquerade as the legitimate node-dll package, which offers a doubly linked list implementation for JavaScript. Similarly, rolimons-api is a deceptive variant of Rolimon\\'s API. \"While there are unofficial wrappers and modules — such as the rolimons Python package (downloaded over 17,000 times) and the Rolimons Lua module on GitHub — the malicious rolimons-api packages sought to exploit developers\\' trust in familiar names,\" Boychenko noted. The rogue packages incorporate obfuscated code that downloads and executes Skuld and Blank Grabber, stealer malware families written in Golang and Python, respectively, that are capable of harvesting a wide range of information from infected systems.']\n",
            "Highlight Scores: [0.2911534607410431]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: New SteelFox Malware Posing as Popular Software to Steal Browser Data\n",
            "URL: https://hackread.com/steelfox-malware-software-to-steal-browser-data/\n",
            "ID: https://hackread.com/steelfox-malware-software-to-steal-browser-data/\n",
            "Score: 0.12943318486213684\n",
            "Published Date: 2024-11-08T00:00:00.000Z\n",
            "Author: Waqas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: SteelFox malware targets software pirates through fake activation tools, stealing credit card data and deploying crypto miners. Learn about this new threat affecting users worldwide and how to protect yourself from this sophisticated cybercrime campaign.\n",
            "Cybersecurity researchers at Securelist have identified a new type of malware that has been spreading through online forums, torrent trackers, and blogs, posing as legitimate software like Foxit PDF Editor, AutoCAD, and JetBrains.\n",
            "Dubbed “SteelFox” by researchers; the malware’s main targets are those Microsft Windows users who are involved in downloading pirated software and fake software activation tools (cracks). \n",
            "The campaign, which began in February 2023, combines cryptocurrency mining and data stealing capabilities through fake software activation tools. So far, the malware has infected over 11,000 users worldwide.\n",
            "According to Securelist’s  blog post  shared with Hackread.com ahead of publishing, SteelFox is a full-featured “crimeware bundle“ that extracts sensitive data from infected devices, including credit card information, browsing history, and login credentials. It also collects system information, such as installed software, running services, and network configurations.\n",
            "The malware’s initial attack vector involves fake software activators, which are advertised on online forums and torrent trackers as a way to activate legitimate software for free. Once installed, the malware creates a service that stays on the system, even after reboots and uses a vulnerable driver to advance its privileges.\n",
            "   Malicious dropper advertisement (Via Securelist)  \n",
            "The malware operates through a multi-stage attack chain, beginning with a dropper that requires administrator privileges. Once executed, it installs itself as a Windows service and uses AES-128 encryption to hide its components. The malware achieves system-level access by exploiting vulnerable drivers and implements TLS 1.3 with SSL pinning for secure communication with its command servers.\n",
            " “SteelFox’s Highly sophisticated usage of modern C++ combined with external libraries grants this malware formidable power. Usage of TLSv1.3 and SSL pinning ensures secure communication and harvesting of sensitive data.” \n",
            "Securelist \n",
            " Global Impact \n",
            "SteelFox does not appear to target specific individuals or organizations, instead operating on a larger scale to infect as many users as possible. The malware has already infected users in over 10 countries, including the following:\n",
            " UAE \n",
            " India \n",
            " Brazil \n",
            " China \n",
            " Russia \n",
            " Egypt \n",
            " Algeria \n",
            " Mexico \n",
            " Vietnam \n",
            " Sri Lanka \n",
            "James McQuiggan, a security awareness advocate at  KnowBe4 , emphasized the importance of organizations being cautious about the sources of their software downloads. He also highlighted the necessity of training employees through cybersecurity awareness programs.\n",
            "“The dual functionality of SteelFox’s droppers—providing both software “cracks” and malware indicates the complex tools used by cybercriminals and using an outdated driver for privilege escalation highlights the critical need for organizations to ensure they are implementing patches.”\n",
            "“Organizations must ensure they verify software sources, maintain the least user privilege access control, and leverage endpoint protection to detect suspicious installation behaviours,” James explained.\n",
            "“Furthermore and more importantly, ensure that cybersecurity awareness programs are provided to users about the dangers of unverified software, like open source software or these common applications. Allow for an IT-managed software solution to install and monitor all applications,” he advised.\n",
            " Protecting Yourself from SteelFox \n",
            "To avoid becoming victims of SteelFox, users should only download software from official sources and use a  reliable security solution  that can detect and prevent the installation of infected software. Additionally, users should be cautious when clicking links or downloading attachments from unknown sources, as these can often be used to spread malware.\n",
            "  Winos4.0 Malware Targeting Windows via Fake Gaming Apps  \n",
            "  Fabrice Malware on PyPI Stealing AWS Credentials for 3 Years  \n",
            "  SideWinder hit Android users with malware apps on Play Store  \n",
            "  TodoSwift Malware Targets macOS, Disguised as Bitcoin PDF App  \n",
            "  Octo2 Malware Uses Fake NordVPN Apps to Infect Android Phones\n",
            "Highlights: ['Usage of TLSv1.3 and SSL pinning ensures secure communication and harvesting of sensitive data.”  SteelFox does not appear to target specific individuals or organizations, instead operating on a larger scale to infect as many users as possible. The malware has already infected users in over 10 countries, including the following: James McQuiggan, a security awareness advocate at  KnowBe4 , emphasized the importance of organizations being cautious about the sources of their software downloads. He also highlighted the necessity of training employees through cybersecurity awareness programs.']\n",
            "Highlight Scores: [0.4512220621109009]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: \n",
            "URL: https://twitter.com/TheHackersNews/status/1854887309117698243\n",
            "ID: https://twitter.com/TheHackersNews/status/1854887309117698243\n",
            "Score: 0.1253611147403717\n",
            "Published Date: 2024-11-08T14:02:51.000Z\n",
            "Author: TheHackersNews\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: 🛡️💻 AndroxGh0st #malware is evolving!\n",
            "Now exploiting multiple critical vulnerabilities while collaborating with the Mozi botnet, it's a serious threat to IoT and cloud security.\n",
            "🔗 Read now: https://t.co/6lHPRicL73\n",
            "#infosecurity #CyberAwareness| created_at: Fri Nov 08 14:02:51 +0000 2024 | favorite_count: 35 | quote_count: 2 | reply_count: 0 | retweet_count: 15 | is_quote_status: False | retweeted: False | lang: en\n",
            "Highlights: [\"Now exploiting multiple critical vulnerabilities while collaborating with the Mozi botnet, it's a serious threat to IoT and cloud security. #infosecurity #CyberAwareness| created_at: Fri Nov 08 14:02:51 +0000 2024 | favorite_count: 35 | quote_count: 2 | reply_count: 0 | retweet_count: 15 | is_quote_status: False | retweeted: False | lang: en\"]\n",
            "Highlight Scores: [0.24373888969421387]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is the latest cyber incident from Blackbasta Ransomware:\n",
            "Resolved Search Type: 2024-11-08T06:37:20.641Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://therecord.media/black-basta-ransomware-zero-day-windows using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf, error: [ERROR] 🚫 crawl(): Failed to crawl https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf: Page.goto: net::ERR_ABORTED at https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\n",
            "Call log:\n",
            "navigating to \"https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf\n",
            "[LOG] 🕸️ Crawling https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, success: True, time taken: 7.55 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf, time taken: 0.00 seconds.\n",
            "[LOG] ✅ Crawled https://therecord.media/black-basta-ransomware-zero-day-windows successfully!\n",
            "[LOG] 🚀 Crawling done for https://therecord.media/black-basta-ransomware-zero-day-windows, success: True, time taken: 7.72 seconds\n",
            "[LOG] 🚀 Content extracted for https://therecord.media/black-basta-ransomware-zero-day-windows, success: True, time taken: 0.06 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://therecord.media/black-basta-ransomware-zero-day-windows, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://therecord.media/black-basta-ransomware-zero-day-windows, time taken: 0.08 seconds.\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "DEBUG: Extracted content from https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf: []\n",
            "[LOG] ✅ Crawled https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, success: True, time taken: 8.26 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png, time taken: 0.00 seconds.\n",
            "[LOG] ✅ Crawled https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, success: True, time taken: 7.64 seconds\n",
            "[LOG] 🚀 Content extracted for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png, time taken: 0.00 seconds.\n",
            "DEBUG: Extracted content from https://therecord.media/black-basta-ransomware-zero-day-windows: [{'links': 'https://www.recordedfuture.com/privacy-policy/?__hstc=156209188.02edcd4d78aff3a2a83203e8ddb0e214.1731134251944.1731134251944.1731134251944.1&__hssc=156209188.1.1731134251944&__hsfp=702054290'}]\n",
            "[LOG] ✅ Crawled https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, success: True, time taken: 7.57 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png, time taken: 0.00 seconds.\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 11.29 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.07 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.08 seconds.\n",
            "DEBUG: Extracted content from https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png: []\n",
            "DEBUG: Extracted content from https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png: []\n",
            "DEBUG: Extracted content from https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png: []\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 11.19 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.06 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.08 seconds.\n",
            "DEBUG: Extracted content from https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta: [{'links': 'https://www.blackberry.com/'}]\n",
            "DEBUG: Extracted content from https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta: [{'links': 'https://www.blackberry.com/'}]\n",
            "Search Results Set: [SearchResult(source='Google Programmable Search', title='black-basta-threat-profile.pdf', snippet='Mar 15, 2023 ... Nevertheless, as ransomware attacks continue to increase, this Threat Profile highlights the emerging group and its seasoned cybercriminals and\\xa0...', url='https://www.hhs.gov/sites/default/files/black-basta-threat-profile.pdf', date=None, media=[], media_content=[], links=[]), SearchResult(source='Google Serper', title='Windows flaw may have been exploited with Black Basta ...', snippet='A group operating the notorious Black Basta ransomware may have exploited a recently patched Windows vulnerability as a zero-day, researchers have found.', url='https://therecord.media/black-basta-ransomware-zero-day-windows', date=None, media=[], media_content=[], links=[]), SearchResult(source='Google Serper', title='Who Is Black Basta? - BlackBerry', snippet='Black Basta is a ransomware operator and Ransomware-as-a-Service (RaaS) criminal enterprise that first emerged in early 2022 and became one of the most ...', url='https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta', date=None, media=[], media_content=[], links=[]), SearchResult(source='Google Programmable Search', title='Who Is Black Basta?', snippet='Another way to distinguish a Black Basta attack from other ransomware attacks is by examining the beginning of each encrypted file since Black Basta uses a\\xa0...', url='https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta', date=None, media=[], media_content=[], links=[]), SearchResult(source='Google Serper Image Search', title='What Is Black Basta Ransomware?', snippet='No snippet', url='https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png', date=None, media=['https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png'], media_content=[], links=[]), SearchResult(source='Google Programmable Image Search', title='US Coast Guard Cyber Command Maritime Cyber Alert 02-23 June ...', snippet='Jun 15, 2023 ... Coast Guard Cyber Command (CGCYBER) recently observed a surge in BlackBasta Group ransomware campaigns targeting the Marine Transportation\\xa0...', url='https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf', date=None, media=['https://www.uscg.mil/Portals/0/Images/cyber/Maritime%20Cyber%20Alert%2002-23%20BLACKBASTA%20TLP%20CLEAR.pdf'], media_content=[], links=[]), SearchResult(source='Google Serper Image Search', title='Threat Assessment: Black Basta Ransomware', snippet='No snippet', url='https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png', date=None, media=['https://unit42.paloaltonetworks.jp/wp-content/uploads/2022/08/word-image-84.png'], media_content=[], links=[]), SearchResult(source='Google Serper Image Search', title='THREAT ALERT: Aggressive Qakbot Campaign and the Black Basta ...', snippet='No snippet', url='https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png', date=None, media=['https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png'], media_content=[], links=[]), SearchResult(source='Vector Search', title='Result 5', snippet=\"['Shweta Sharma is a senior journalist covering enterprise information security and digital ledger technologies for IDG’s CSO Online, Computerworld, and other enterprise sites.']\\n2024-08-17 11:50:16.788525\\nhttps://www.csoonline.com/article/2149587/india-faces-evolved-cyber-espionage-with-novel-discord-hack.html\", url='No URL', date=None, media=[], media_content=[], links=[]), SearchResult(source='Vector Search', title='Result 4', snippet=\"[' Sead is a seasoned freelance journalist based in Sarajevo, Bosnia and Herzegovina. He writes about IT (cloud, IoT, 5G, VPN) and cybersecurity (ransomware, data breaches, laws and regulations). In his career, spanning more than a decade, he’s written for numerous media outlets, including Al Jazeera Balkans. He’s also held several modules on content writing for Represent Communications. ']\\n2024-08-17 11:50:06.826911\\nhttps://www.techradar.com/pro/security/one-of-the-worlds-biggest-gold-mining-companies-has-been-hit-by-a-huge-ransomware-attack\", url='No URL', date=None, media=[], media_content=[], links=[])]\n",
            "ERROR: search_results key not found in state\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'reranked_results'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e7cae8074279>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Latest Cyber Incidents from Blackbasta Ransomware?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b501a0b68c8a>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query, memory)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m                     \u001b[0mmanager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                 ):\n\u001b[0;32m-> 1336\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mrun_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b501a0b68c8a>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reranked_results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
            "\u001b[0;31mKeyError\u001b[0m: 'reranked_results'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIp1L2bcInA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7mH99gGInD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VWtEsGIInHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}