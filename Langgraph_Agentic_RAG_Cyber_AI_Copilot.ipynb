{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "0ebc531f-df25-48a2-c529-b85bad34b0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "2024-11-26 05:49:58.598365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-26 05:49:58.666815: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-26 05:49:58.685773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-26 05:49:58.725111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-26 05:50:01.017644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "\u001b[0mPlaywright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ca74c6-1a94-40ca-e2ef-9ad1539aa94c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d",
        "outputId": "f6ac2b66-940e-4314-fde1-8d5da7fceabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent\"\n",
        "    return enhanced_query\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "\n",
        "    5. **Actionable Recommendations**:\n",
        "       - Offer precise, immediate actions and mitigation strategies.\n",
        "       - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "       - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    6. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor tactics, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['LunarsGo Threat Actor', 'Blackbasta Ransomware Gang', 'City', 'Countries', 'Geo-specific'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79bccca5-4e9b-4fd5-b047-4ae85ee43a2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor? 2024 OR 2024 recent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6f1cf4047857>:35: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = search_and_contents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw results from Exa Search: Title: Scammer robs homebuyers of life savings in $20 million theft spree\n",
            "URL: https://www.malwarebytes.com/blog/news/2024/11/scammer-robs-homebuyers-of-life-savings-in-20-million-theft-spree\n",
            "ID: https://www.malwarebytes.com/blog/news/2024/11/scammer-robs-homebuyers-of-life-savings-in-20-million-theft-spree\n",
            "Score: 0.13473837077617645\n",
            "Published Date: 2024-11-14T00:00:00.000Z\n",
            "Author: Pieter Arntz\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A 33-year-old Nigerian man living in the UK and his co-conspirators defrauded over 400 would-be home buyers in the US.\n",
            "In the initial phase, Babatunde Francis Ayeni and his criminal gang targeted US title companies, real estate agents, and real estate attorneys. Employees of these companies were tricked into clicking malicious attachments and links and filling in their email account login information on fake sites. The entered information went straight to the phishers and allowed the criminals to monitor the emails of those employees.\n",
            "As soon as the scammers spotted an email where someone was asked to make a payment as part of a real estate transaction, they would change the wiring instructions and let the victims deposit their payments into bank accounts associated with the criminals instead of the legitimate real estate transaction.\n",
            "Some 400 people fell victim to this sophisticated business email compromise (BEC) scheme. 231 of these victims were unable to reverse the wire transactions in time and lost their entire transaction—often their life savings.\n",
            "The total losses amount to nearly $20 million. To cover their tracks, the gang would buy Bitcoin with the stolen funds and divide it over three different addresses.\n",
            "Last year, the FBI warned BEC focused on the real estate sector was on the rise.\n",
            "“From calendar years 2020 to 2022, there was a 27% increase in victim reports to the Internet Crime Complaint Center (IC3) of BECs with a real estate nexus. In this same time frame, there was a 72% increase in victim loss of BECs with a real estate nexus.”\n",
            "Ayeni was sentenced to ten years in federal prison for his role in the massive cyber fraud conspiracy.\n",
            "During the multi-day sentencing hearing, numerous victims provided victim impact statements about how the crime affected them. They noted that in addition to losing all of the money they saved for the purchase of a new home, they felt significant shame, despair, and depression due to being victimized the way they were.\n",
            "United States Attorney Sean P. Costello said:\n",
            "“Cyber-enabled crimes can cause substantial and lasting harm to victims in an instant. Criminals across the world may believe that they are causing no harm to their victims and that they are safe behind their keyboards, but this case proves otherwise. With our law enforcement partners, we will continue to aggressively investigate, pursue, and hold accountable the crooks who perpetrate frauds online, wherever they are.”\n",
            "Better to double-check\n",
            "When transferring large sums of money, it’s advisable to double check whether the account details mentioned in any email correspond with those of the expected receiver of the funds.\n",
            "Use trusted contact information: always verify account details using contact information from a trusted source, and check whether it matches the information provided in the suspicious email or invoice.\n",
            "Call the company directly: Use a known, verified phone number to call the company and confirm any changes to payment instructions or account details.\n",
            "Use secure verification methods: If available, use secure portals or platforms provided by legitimate vendors to verify account information.\n",
            "If possible, follow up whether the payment came through at the legitimate receiver’s end while you still have the option to reverse the transaction.\n",
            " We don’t just report on threats—we remove them \n",
            "Cybersecurity risks should never spread beyond a headline. Keep threats off your devices by downloading Malwarebytes today.\n",
            "Highlights: ['“Cyber-enabled crimes can cause substantial and lasting harm to victims in an instant. Criminals across the world may believe that they are causing no harm to their victims and that they are safe behind their keyboards, but this case proves otherwise. With our law enforcement partners, we will continue to aggressively investigate, pursue, and hold accountable the crooks who perpetrate frauds online, wherever they are.” When transferring large sums of money, it’s advisable to double check whether the account details mentioned in any email correspond with those of the expected receiver of the funds. Use trusted contact information: always verify account details using contact information from a trusted source, and check whether it matches the information provided in the suspicious email or invoice.']\n",
            "Highlight Scores: [0.37941232323646545]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Crypto phishing scam nets $129 million in USDT then funds mysteriously return\n",
            "URL: https://cryptoslate.com/crypto-phishing-scam-nets-129-million-in-usdt-then-funds-mysteriously-return/\n",
            "ID: https://cryptoslate.com/crypto-phishing-scam-nets-129-million-in-usdt-then-funds-mysteriously-return/\n",
            "Score: 0.1320408135652542\n",
            "Published Date: 2024-11-20T12:40:57.000Z\n",
            "Author: Oluwapelumi Adejumo\n",
            "Image: https://cryptoslate.com/wp-content/uploads/2024/11/phishing-hack-.jpg\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Crypto phishing scam nets $129 million in USDT then funds mysteriously return   9 seconds ago ·  2 min read         Blockchain firm Scam Sniffer sheds light on how subtle address differences can lead to massive crypto losses amid a recent phishing attempt.           2 min read   Updated: Nov. 20, 2024 at 12:40 pm UTC        Cover art/illustration via CryptoSlate. Image includes combined content which may include AI-generated content.     A crypto whale narrowly avoided a $129 million USDT loss after falling victim to a phishing scam on the TRON blockchain. Blockchain security firm Scam Sniffer reported the incident on Nov. 20, detailing how the stolen funds were unexpectedly returned within hours. What happened? According to the firm, the scammer used a fake wallet address, “THc…bu8,” crafted to closely resemble the intended recipient’s “TMS…bu8.” The fraudulent address mimicked the original by matching its starting and ending characters. Despite testing the waters with a test 100 USDT transaction, the victim could not spot the subtle differences and eventually transferred $129 million to the wrong address. Surprisingly, the fraudster returned 90% of the stolen funds—116.7 million USDT— within an hour of the incident and eventually returned the remaining balance of 12.96 million USDT after four hours. Following the fund return, the victim promptly redirected the funds to their original destination, “TMS…bu8,” where they have since remained. Rising threat of address poisoning attacks Scam Sniffer identified this incident as a classic example of an address-poisoning attack, a phishing tactic gaining widespread traction in the industry. This scam involves creating wallet addresses nearly identical to those used by victims, differing by just one or two characters. Fraudsters then send small token amounts to victims, embedding the fake address in their transaction history to exploit copy-and-paste errors during future transfers. CertiK, another blockchain security firm, noted that this phishing tactic, along with wallet drainers, has led to the loss of more than $800 million worth of crypto assets this year. Due to this, Yu Xian, founder of web3 firm Slowmist, cautioned crypto users about the risks of copying sensitive information. He advised clearing clipboard data after use to avoid falling prey to such scams. Xian emphasized that no connected device is entirely secure, reinforcing the need for vigilance in safeguarding digital assets. Observers stated that this case further emphasizes the evolving sophistication of crypto phishing scams and highlights the importance of double-checking wallet addresses before making transfers. Mentioned in this article  Latest TRON Stories    Latest Press Releases\n",
            "Highlights: ['Blockchain security firm Scam Sniffer reported the incident on Nov. 20, detailing how the stolen funds were unexpectedly returned within hours. What happened? According to the firm, the scammer used a fake wallet address, “THc…bu8,” crafted to closely resemble the intended recipient’s “TMS…bu8.” The fraudulent address mimicked the original by matching its starting and ending characters. Despite testing the waters with a test 100 USDT transaction, the victim could not spot the subtle differences and eventually transferred $129 million to the wrong address. Surprisingly, the fraudster returned 90% of the stolen funds—116.7 million USDT— within an hour of the incident and eventually returned the remaining balance of 12.96 million USDT after four hours.']\n",
            "Highlight Scores: [0.29955828189849854]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: BrazenBamboo Weaponizes FortiClient Vulnerability to Steal VPN Credentials via DEEPDATA\n",
            "URL: https://www.volexity.com/blog/2024/11/15/brazenbamboo-weaponizes-forticlient-vulnerability-to-steal-vpn-credentials-via-deepdata/\n",
            "ID: https://www.volexity.com/blog/2024/11/15/brazenbamboo-weaponizes-forticlient-vulnerability-to-steal-vpn-credentials-via-deepdata/\n",
            "Score: 0.13093391060829163\n",
            "Published Date: 2024-11-15T00:00:00.000Z\n",
            "Author: Volexity\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: November 15, 2024\n",
            " \n",
            " \n",
            " Volexity discovered and reported a vulnerability in Fortinet's Windows VPN client, FortiClient, where user credentials remain in process memory after a user authenticates to the VPN. \n",
            " This vulnerability was abused by BrazenBamboo in their DEEPDATA malware. \n",
            " BrazenBamboo is the threat actor behind development of the LIGHTSPY malware family. \n",
            " LIGHTSPY variants have been discovered for all major operating systems, including iOS, and Volexity has recently discovered a new Windows variant. \n",
            "In July 2024, Volexity identified exploitation of a zero-day credential disclosure vulnerability in Fortinet’s Windows VPN client that allowed credentials to be stolen from the memory of the client’s process. This vulnerability was discovered while analyzing a recent sample of the DEEPDATA malware family. DEEPDATA is a modular post-exploitation tool for the Windows operating system that is used to gather a wide range of information from target devices. Analysis of the sample revealed a plugin that was designed to extract credentials from FortiClient VPN client process memory. On July 18, 2024, Volexity notified Fortinet about this vulnerability. Since the time of Volexity's initial discovery and reporting to Fortinet, ThreatFabric and Blackberry have each published reports that cover different aspects of some of the content discussed in this post.   \n",
            "Volexity attributes the development of DEEPDATA to a Chinese state-affiliated threat actor that it tracks as BrazenBamboo. Volexity has observed links between BrazenBamboo and three distinct malware families: LIGHTSPY, DEEPDATA, and DEEPPOST. Volexity tracks BrazenBamboo as the developer of these malware families and not necessarily one of the operators using them (there may be many). Volexity has also identified a new Windows variant of LIGHTSPY that was not previously documented at the time of writing.\n",
            "This blog post details the use and functionality of DEEPDATA, with a key look at zero-day exploitation of the FortiClient vulnerability, and how DEEPPOST is used to exfiltrate files from compromised systems. This blog post also looks at the recently discovered Windows variant of LIGHTSPY, including notable changes, and the associated wider command-and-control (C2) infrastructure of the BrazenBamboo threat actor.\n",
            "Malware Analysis\n",
            "Volexity’s analysis began with discovery of an archive file named deepdata.zip (SHA256:666a4c569d435d0e6bf9fa4d337d1bf014952b42cc6d20e797db6c9df92dd724) that is tied to BrazenBamboo. This archive contains several files that are part of two Windows malware families, which Volexity refers to as DEEPDATA and DEEPPOST. Each malware family is analyzed in the sections that follow. Volexity also separately obtained and analyzed a new Windows variant of LIGHTSPY that is described further below.\n",
            "DEEPDATA\n",
            "As previously mentioned, DEEPDATA is a modular post-exploitation tool for Windows that facilitates collection of sensitive information from a compromised system. This tool must be run from the command line of a system by an attacker. The DEEPDATA malware elements include the following:\n",
            " Filename \n",
            " Description \n",
            " data.dll \n",
            "DEEPDATA Loader\n",
            " mod.dat \n",
            "DEEPDATA Virtual File System (VFS)\n",
            " readme.txt \n",
            "File containing DEEPDATA Execution Options\n",
            "The readme.txt file describes how to execute the DEEPDATA loader, along with available parameters and a decryption key.\n",
            " \n",
            "The key parameter is used by the DEEPDATA loader file to decrypt and load the “core” components of the DEEPDATA malware family stored in the local VFS file (mod.dat). These components will always execute and are not dependent on additional parameters passed on the command line.\n",
            "The core components of DEEPDATA include the following files:\n",
            "  Filename \n",
            " Purpose \n",
            " frame.dll \n",
            "Shellcode – core orchestrator for plugin execution\n",
            " ffmpeg.dll \n",
            "Contains Heaven’s Gate code to load 32-bit code in 64-bit processes\n",
            " vertdll.dll \n",
            "Collects event logs\n",
            " iumdll.dll \n",
            "Library used to collect locally stored WeChat data\n",
            " ucrtbase_enclave.dll \n",
            "Library used to collect locally stored Feishu data\n",
            " d3dcompiler_47.dll \n",
            "Checks the running instant messaging apps (Line, Feishu, WeChat)\n",
            "The architecture of DEEPDATA’s loader, core, and plugins is shown below.\n",
            " \n",
            "The core components are always included in the VFS files, but Volexity was only able to find frame.dll stored on the C2 servers. While DEEPDATA plugins are stored in the VFS files, they are also stored as their own dedicated files on the C2 servers; they can be loaded from either location. The DEEPDATA plugins in the VFS are decrypted using the same key as the other components in the VFS.\n",
            "The overall plugin logic is the same as that seen in LIGHTSPY malware samples, with the following exported functions used by the core orchestrator:\n",
            " ExecuteCommand \n",
            " GetPluginCommandID \n",
            " GetPluginName \n",
            " GetPluginVersion \n",
            "DEEPDATA maintains configuration data within the VFS file with the following files stored in an encrypted state:\n",
            " Filename \n",
            " Description \n",
            " config.json \n",
            "Contains DEEPDATA configuration information\n",
            " manifest.json \n",
            "Contains DEEPDATA plugin information\n",
            " manifest1.json \n",
            "Contains DEEPDATA plugin information\n",
            " date.ini \n",
            "Purpose unclear, contains a single byte of 0x30 \n",
            "The manifest.json file is also stored on the C2 server but in an unencrypted state.\n",
            "Volexity identified a total of 12 unique plugins for DEEPDATA, which are summarized below:\n",
            " Plugin Name \n",
            " Plugin Capabilities \n",
            " AccountInfo \n",
            "Steal credentials from 18 different sources on the compromised device.\n",
            " AppData \n",
            "Collect data from WeChat, WhatsApp and Signal on the compromised device.\n",
            " Audio \n",
            "Record audio on compromised devices.\n",
            " ChatIndexedDb \n",
            "Steal databases from WhatsApp and Zalo chat clients.\n",
            "  FortiClient  \n",
            " Extract credentials and server information from process memory of FortiClient VPN processes.  \n",
            " Outlook \n",
            "Collect contacts and emails from local Microsoft Outlook instances.\n",
            " SocialSoft \n",
            "Steal data from WeChat, Line, QQ, DingDing, Skype, Telegram, and Feishu applications.\n",
            " SoftwareList \n",
            "List installed software, folders, and files recursively from a base location.\n",
            " SystemInfo \n",
            "Gather basic enumeration information from the compromised device.\n",
            " TdMonitor \n",
            "Hook Telegram to retrieve messages from the application.\n",
            " WebBrowser \n",
            "Collect history, cookies, and passwords from Firefox, Chrome, Opera, and Edge web browsers.\n",
            " WifiList \n",
            "Collect details of stored WiFi keys and nearby hotspots.\n",
            "As shown above, DEEPDATA supports a wide range of functionality to extract data from victims’ systems. The observed functionality of several plugins is commonly seen and includes items typically stolen from victim systems. However, Volexity noted the  FortiClient  plugin was uncommon and investigated it further. Volexity found the FortiClient plugin was included through a library with the filename msenvico.dll. This plugin was found to exploit a zero-day vulnerability in the Fortinet VPN client on Windows that allows it to extract the credentials for the user from memory of the client’s process.\n",
            "As seen in the code snippet below, the FortiClient plugin looks for the username, password, remote gateway, and port from two different JSON objects in memory.\n",
            " \n",
            "This is similar to a previously documented vulnerability identified in 2016, where credentials could be discovered in memory based on hardcoded offsets in memory. The previous vulnerability does not have an associated CVE.\n",
            "Volexity verified the presence of these JSON objects in memory and confirmed this approach works against the latest version available at the time of discovery (v\n",
            "Highlights: ['This archive contains several files that are part of two Windows malware families, which Volexity refers to as DEEPDATA and DEEPPOST. Each malware family is analyzed in the sections that follow. Volexity also separately obtained and analyzed a new Windows variant of LIGHTSPY that is described further below. As previously mentioned, DEEPDATA is a modular post-exploitation tool for Windows that facilitates collection of sensitive information from a compromised system. This tool must be run from the command line of a system by an attacker.']\n",
            "Highlight Scores: [0.35498353838920593]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: SteelFox and Rhadamanthys Malware Use Copyright Scams, Driver Exploits to Target Victims\n",
            "URL: https://thehackernews.com/2024/11/steelfox-and-rhadamanthys-malware-use.html\n",
            "ID: https://thehackernews.com/2024/11/steelfox-and-rhadamanthys-malware-use.html\n",
            "Score: 0.13092276453971863\n",
            "Published Date: 2024-11-07T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: An ongoing phishing campaign is employing copyright infringement-related themes to trick victims into downloading a newer version of the Rhadamanthys information stealer since July 2024.\n",
            "Cybersecurity firm Check Point is tracking the large-scale campaign under the name CopyRh(ight)adamantys. Targeted regions include the United States, Europe, East Asia, and South America.\n",
            "\"The campaign impersonates dozens of companies, while each email is sent to a specific targeted entity from a different Gmail account, adapting the impersonated company and the language per targeted entity,\" the company said in a technical analysis. \"Almost 70% of the impersonated companies are from the Entertainment /Media and Technology/Software sectors.\"\n",
            "The attacks are notable for the deployment of version 0.7 of the Rhadamanthys stealer, which, as detailed by Recorded Future's Insikt Group early last month, incorporates artificial intelligence (AI) for optical character recognition (OCR).\n",
            "The Israeli company said the activity overlaps with a campaign that Cisco Talos disclosed last week as targeting Facebook business and advertising account users in Taiwan to deliver Lumma or Rhadamanthys stealer malware.\n",
            " \n",
            "The attack chains are characterized by the use of spear-phishing tactics that entail sending email messages claiming purported copyright violations by masquerading as well-known companies.\n",
            "These emails are sent from Gmail accounts and claim to be from legal representatives of the impersonated companies. The contents of the message accuse the recipients of misusing their brand on social media platforms and request them to remove the concerned images and videos.\n",
            "\"The removal instructions are said to be in a password-protected file. However, the attached file is a download link to appspot.com, linked to the Gmail account, which redirects the user to Dropbox or Discord to download a password-protected archive (with the password provided in the email),\" Check Point said.\n",
            "   \n",
            "The RAR archive contains three components, a legitimate executable vulnerable to DLL side-loading, the malicious DLL containing the stealer payload, and a decoy document. Once the binary is run, it sideloads the DLL file, which then paves the way for the deployment of Rhadamanthys.\n",
            "Check Point, which attributed the campaign to a likely cybercrime group, said that it's possible the threat actors have utilized AI tools given the scale of the campaign and the variety of the lures and sender emails.\n",
            "\"The campaign's widespread and indiscriminate targeting of organizations across multiple regions suggests it was orchestrated by a financially motivated cybercrime group rather than a nation-state actor,\" it said. \"Its global reach, automated phishing tactics, and diverse lures demonstrate how attackers continuously evolve to improve their success rates.\"\n",
            "New SteelFox Malware Exploits Vulnerable Driver\n",
            "The findings come as Kaspersky shed light on a new \"full-featured crimeware bundle\" dubbed SteelFox that's propagated via forums posts, torrent trackers, and blogs, passing off as legitimate utilities like Foxit PDF Editor, JetBrains, and AutoCAD.\n",
            "The campaign, dating back to February 2023, has claimed victims across the world, particularly those located in Brazil, China, Russia, Mexico, UAE, Egypt, Algeria, Vietnam, India, and Sri Lanka. It has not been attributed to any known threat actor or group.\n",
            "\"Delivered via sophisticated execution chains including shellcoding, this threat abuses Windows services and drivers,\" security researcher Kirill Korchemny said. \"It also uses stealer malware to extract the victim's credit card data as well as details about the infected device.\"\n",
            "The starting point is a dropper app that impersonates cracked versions of popular software, which, when executed, asks for administrator access and drops a next-stage loader that, in turn, establishes persistence and launches the SteelFox DLL.\n",
            " \n",
            "The admin access is subsequently abused to create a service that runs an older version of WinRing0.sys, a hardware access library for Windows that's vulnerable to CVE-2020-14979 and CVE-2021-41285, thereby allowing the threat actor to obtain NT\\SYSTEM privileges.\n",
            "\"This driver is also a component of the XMRig miner, so it is utilized for mining purposes,\" Korchemny noted. \"After initializing the driver, the sample launches the miner. This represents a modified executable of XMRig with junk code fillers. It connects to a mining pool with hardcoded credentials.\"\n",
            "The miner, for its part, is downloaded from a GitHub repository, with the malware also initiating contact with a remote server over TLS version 1.3 to exfiltrate sensitive data from web browsers, such as cookies, credit card data, browsing history, and visited places, system metadata, installed software, and timezone, among others.\n",
            "\"Highly sophisticated usage of modern C++ combined with external libraries grant this malware formidable power,\" Kaspersky said. \"Usage of TLSv1.3 and SSL pinning ensures secure communication and harvesting of sensitive data.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['An ongoing phishing campaign is employing copyright infringement-related themes to trick victims into downloading a newer version of the Rhadamanthys information stealer since July 2024. Cybersecurity firm Check Point is tracking the large-scale campaign under the name CopyRh(ight)adamantys. Targeted regions include the United States, Europe, East Asia, and South America. \"The campaign impersonates dozens of companies, while each email is sent to a specific targeted entity from a different Gmail account, adapting the impersonated company and the language per targeted entity,\" the company said in a technical analysis. The attacks are notable for the deployment of version 0.7 of the Rhadamanthys stealer, which, as detailed by Recorded Future\\'s Insikt Group early last month, incorporates artificial intelligence (AI) for optical character recognition (OCR).']\n",
            "Highlight Scores: [0.38518399000167847]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Jumpy Pisces Engages in Play Ransomware\n",
            "URL: https://unit42.paloaltonetworks.com/north-korean-threat-group-play-ransomware/\n",
            "ID: https://unit42.paloaltonetworks.com/north-korean-threat-group-play-ransomware/\n",
            "Score: 0.128228560090065\n",
            "Published Date: 2024-10-30T00:00:00.000Z\n",
            "Author: Unit\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Executive Summary\n",
            "Unit 42 has identified Jumpy Pisces, a North Korean state-sponsored threat group associated with the Reconnaissance General Bureau of the Korean People's Army, as a key player in a recent ransomware incident. Our investigation indicates a likely shift in the group’s tactics. We believe with moderate confidence that Jumpy Pisces, or a faction of the group, is now collaborating with the Play ransomware group (Fiddling Scorpius).\n",
            "This change marks the first observed instance of the group using existing ransomware infrastructure, potentially acting as an initial access broker (IAB) or an affiliate of the Play ransomware group. This shift in their tactics, techniques and procedures (TTPs) signals deeper involvement in the broader ransomware threat landscape.\n",
            " Jumpy Pisces, also known as Andariel and Onyx Sleet, was historically involved in cyberespionage, financial crime and ransomware attacks. The group was indicted by the U.S Justice Department for deploying custom-developed ransomware, Maui.\n",
            "We expect their attacks will increasingly target a wide range of victims globally. Network defenders should view Jumpy Pisces activity as a potential precursor to ransomware attacks, not just espionage, underscoring the need for heightened vigilance.\n",
            "Palo Alto Networks customers are better protected from the threats discussed in this article through the following products:\n",
            " Cortex XDR and XSIAM \n",
            " Advanced WildFire \n",
            " Advanced URL Filtering and Advanced DNS Security \n",
            "If you think you might have been compromised or have an urgent matter, contact the Unit 42 Incident Response team.\n",
            "  Jumpy Pisces’ Intrusion Leads to Play Ransomware\n",
            "In early September 2024, Unit 42 engaged in incident response services for a client impacted by Play ransomware. Play ransomware was first reported in mid-2022. A closed group we track as Fiddling Scorpius is believed to be operating this threat, for both developing and executing the attacks.\n",
            "Some suggest that Fiddling Scorpius has transitioned to a ransomware-as-a-service (RaaS) model. However, the group has announced on its Play ransomware leak site that it does not provide a RaaS ecosystem.\n",
            "During our investigation, we discovered with high confidence that the North Korean state-sponsored threat group Jumpy Pisces gained initial access via a compromised user account in May 2024. Jumpy Pisces carried out lateral movement and maintained persistence by spreading the open-source tool Sliver and their unique custom malware, DTrack, to other hosts via Server Message Block (SMB) protocol.\n",
            "These remote tools continued to communicate with their command-and-control (C2) server until early September. This ultimately led to the deployment of Play ransomware.\n",
            "Threat actors had access to the network between May-September 2024. Figure 1 shows an overview of the events from this time frame.\n",
            "  Attack Lifecycle – Timeline of Events\n",
            " Figure 1. High-level timeline of events. \n",
            "We observed the earliest signs of unauthorized activity at the end of May 2024. A compromised user account accessed a particular host through a firewall device. Partial registry dumps on the host indicate possible use of Impacket's credential harvesting module, .\n",
            "Attackers copied files associated with the Sliver and DTrack malware family to various hosts using the compromised account over SMB, with the following commands:\n",
            " \n",
            "  // /  \n",
            " \n",
            "DTrack execution was blocked by the endpoint detection and response (EDR) solution. However, we did observe Sliver beaconing activity spanning multiple days until early September 2024, with quiet periods in July and sporadically on other days.\n",
            "In early September, an unidentified threat actor entered the network through the same compromised user account used by Jumpy Pisces. They carried out pre-ransomware activities including credential harvesting, privilege escalation and the uninstallation of EDR sensors, which eventually led to the deployment of Play ransomware.\n",
            "  Threat Actor Tooling\n",
            "We observed the following tools and malware during the attack timeline up to the day before the attackers deployed the ransomware. Note that some of the suspicious files observed did not successfully execute, or were not recoverable at the time of investigation.\n",
            "Sliver: Attackers used a customized version of the open-source, red-teaming tool for C2 purposes. This tool is often seen as an alternative to Cobalt Strike. This customized version beacons to the IP address . This IP address has been flagged as a Sliver C2. Both the IP address and the corresponding domain have been linked to Jumpy Pisces.\n",
            "DTrack: This is an infostealer previously used in reported incidents attributed to North Korean threat groups. The data it collects is compressed and disguised as a GIF file.\n",
            "Attackers used a dedicated tool built to create a privileged user account on victim machines with Remote Desktop Protocol (RDP) enabled.\n",
            " Mimikatz: Attackers used a customized version of the publicly available credential dumping tool, with as its credential dump log.\n",
            "Attackers used a trojanized binary that steals browser history, autofills and credit card details for Chrome, Edge and Brave internet browsers. The scraped information is saved in a file in directory.\n",
            "All the above-mentioned files were signed using a couple of invalid certificates that we note in the Indicators of Compromise section of this article. These certificates, previously linked to Jumpy Pisces, enabled the files to impersonate ones created by legitimate entities.\n",
            "  Assessment of Jumpy Pisces – Play Ransomware Collaboration\n",
            "We assess with moderate confidence a degree of collaboration between Jumpy Pisces and Play Ransomware in this incident, based on the following factors:\n",
            "The compromised account that attackers used for initial access and subsequent spreading of the Jumpy Pisces-linked toolset (e.g., Sliver and DTrack), was the same one used prior to ransomware deployment. The ransomware actor leveraged the account to abuse Windows access tokens, move laterally and escalate to SYSTEM privileges via PsExec. This eventually led to the mass uninstallation of EDR sensors and the onset of Play ransomware activity.\n",
            "As highlighted previously, we observed Sliver C2 communication until the day before ransomware deployment. Furthermore, our research also suggests that the C2 IP address has been offline since the day attackers deployed Play ransomware in this incident.\n",
            " Adlumin’s report on Play ransomware suggests various commonalities in TTPs across multiple attacks they’ve tracked. One such TTP was the presence of its tools in the folder . We observed some tools used prior to ransomware deployment (i.e., TokenPlayer for Windows access token abuse, and PsExec) both located in .\n",
            "  Conclusion\n",
            "Highlights: ['They carried out pre-ransomware activities including credential harvesting, privilege escalation and the uninstallation of EDR sensors, which eventually led to the deployment of Play ransomware. We observed the following tools and malware during the attack timeline up to the day before the attackers deployed the ransomware. Note that some of the suspicious files observed did not successfully execute, or were not recoverable at the time of investigation. Sliver: Attackers used a customized version of the open-source, red-teaming tool for C2 purposes. This tool is often seen as an alternative to Cobalt Strike.']\n",
            "Highlight Scores: [0.44853857159614563]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres the latest cyber incident involving LunarsGo Threat Actor in 2024:\n",
            "Resolved Search Type: 2024-10-26T05:51:23.906Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[INIT].... → Crawl4AI 0.3.742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6f1cf4047857>:246: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
            "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
            "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
            "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
            "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
            "Pass warning=False to suppress this warning.\n",
            "  result = await crawler.arun(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR]... × No URL... | Error: URL must start with 'http://', 'https://', 'file://', or 'raw:'\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[INIT].... → Crawl4AI 0.3.742\n",
            "[INIT].... → Crawl4AI 0.3.742\n",
            "[INIT].... → Crawl4AI 0.3.742\n",
            "[INIT].... → Crawl4AI 0.3.742\n",
            "[FETCH]... ↓ https://www.cm-alliance.com/cybersecurity-blog/oct... | Status: True | Time: 23.31s\n",
            "[SCRAPE].. ◆ Processed https://www.cm-alliance.com/cybersecurity-blog/oct... | Time: 2367ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.cm-alliance.com/cybersecurity-blog/oct... | Time: 1.0244045379999989s\n",
            "[COMPLETE] ● https://www.cm-alliance.com/cybersecurity-blog/oct... | Status: True | Total: 28.02s\n",
            "[FETCH]... ↓ https://www.avg.com/en/signal/malware-statistics... | Status: True | Time: 38.30s\n",
            "[SCRAPE].. ◆ Processed https://www.avg.com/en/signal/malware-statistics... | Time: 4162ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.avg.com/en/signal/malware-statistics... | Time: 3.846630518999973s\n",
            "[COMPLETE] ● https://www.avg.com/en/signal/malware-statistics... | Status: True | Total: 48.02s\n",
            "[FETCH]... ↓ https://purplesec.us/breach-report/... | Status: True | Time: 49.90s\n",
            "[SCRAPE].. ◆ Processed https://purplesec.us/breach-report/... | Time: 1545ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://purplesec.us/breach-report/... | Time: 0.652790929000048s\n",
            "[COMPLETE] ● https://purplesec.us/breach-report/... | Status: True | Total: 52.97s\n",
            "[FETCH]... ↓ https://www.techradar.com/pro/top-data-breaches-an... | Status: True | Time: 56.27s\n",
            "[SCRAPE].. ◆ Processed https://www.techradar.com/pro/top-data-breaches-an... | Time: 141ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.techradar.com/pro/top-data-breaches-an... | Time: 0.10205477199997404s\n",
            "[COMPLETE] ● https://www.techradar.com/pro/top-data-breaches-an... | Status: True | Total: 56.61s\n",
            "Crawled Results: [[{'links': '/', 'images': 'https://purplesec.us/wp-content/uploads/2022/09/cropped-PurpleSec-Favicon.png'}], [{'links': 'https://www.avg.com/en-ww/cookies', 'images': 'https://cdn.cookielaw.org/logos/static/powered_by_logo.svg'}], [{'links': '#main', 'images': 'https://vanilla.futurecdn.net/techradar/media/shared/img/flags/nosize/US.svg'}], [{'links': 'javascript:void(0);', 'images': 'https://cdn-cookieyes.com/assets/images/revisit.svg'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://purplesec.us/wp-content/uploads/2022/09/cropped-PurpleSec-Favicon.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.avg.com/en-ww/cookies', 'images': 'https://cdn.cookielaw.org/logos/static/powered_by_logo.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#main', 'images': 'https://vanilla.futurecdn.net/techradar/media/shared/img/flags/nosize/US.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'javascript:void(0);', 'images': 'https://cdn-cookieyes.com/assets/images/revisit.svg'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://ciobusinessworld.com/wp-content/uploads/2023/12/The-3-Cyber-Security-Incidents-in-2024.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://quointelligence.eu/wp-content/uploads/2024/01/Analysis-of-The-Red-Cross-Rules-Of-Engagement-For-Hacktivists-1024x576.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://threatmon.io/storage/180/global-cyber-threat-report-mid-year-2024.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/10/major-cyber-attacks-in-review-september-2024.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://erepublic.brightspotcdn.com/dims4/default/9aef090/2147483647/strip/true/crop/466x229+0+0/resize/840x413!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2F05%2Fe5%2F87a4003047a9aedcd572b2a1d68f%2Fzerofox-2024.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.securitymagazine.com/ext/resources/2023/12/20/SEC_Top-Cybersecurity-Predictions-for-2023.jpg?height=635&t=1704488910&width=1200\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://images.ctfassets.net/o2pgk9gufvga/18LBE92ZZHGYQuI9FrZNjk/d885d389d333afcc0429fb968bae9dfd/Blog_CTI__1_.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.cm-alliance.com/hubfs/217059643_m_normal_none%20%281%29.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "Recent cyber attacks and data breaches in 2024 have been reported across various sectors, including finance, healthcare, and government institutions. This analysis synthesizes information from credible sources to provide an overview of the most significant incidents, highlighting threat actors, tactics, techniques, and procedures (TTPs), and offering actionable recommendations for mitigation and response. [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "*   **Threat Actor Groups**: Russian-aligned cyber threat actors have significantly increased their global threat activity in September 2024, according to Trellix telemetry data. These groups have been linked to various high-profile attacks, including the breach of HSBC and Barclays.\n",
            "*   **Cyber Gangs**: Scattered Spider, a notorious cybercrime gang, has been acting as an initial access broker for the RansomHub ransomware operation, employing advanced social engineering tactics to obtain privileged access and deploy the encryptor to impact critical environments.\n",
            "*   **Geo-specific**: Chinese-backed hackers have been conducting large data exfiltration operations against Thailand's government institutions, highlighting the need for enhanced cybersecurity measures in the region.\n",
            "*   **Tactics, Techniques, and Procedures (TTPs)**: Threat actors have been observed using various TTPs, including SEO poisoning, malvertising, and password spray attacks, to gain unauthorized access to systems and data. [Tavily Search](https://thehackernews.com/2024/11/thn-recap-top-cybersecurity-threats_18.html)\n",
            "\n",
            "**Most Recent Relevant Activities**\n",
            "\n",
            "*   **September 2024**: Major cyber attacks, data breaches, and ransomware attacks have been reported, affecting organizations such as TFL, Planned Parenthood, and Florida-based Slim CD.\n",
            "*   **October 2024**: The Red Cross has been investigating a cyber attack on its systems, following reports that a ransomware group leaked sensitive information on the dark web.\n",
            "*   **May 2024**: Dell warned customers about a significant data breach after a threat actor claimed to have stolen information on approximately 49,000 employees. [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/september-2024-major-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "**Actionable Recommendations**\n",
            "\n",
            "1.  **Implement robust cybersecurity measures**: Organizations should prioritize the implementation of robust cybersecurity measures, including endpoint detection and response (EDR) agents, to prevent unauthorized access and data breaches.\n",
            "2.  **Conduct regular security audits**: Regular security audits should be conducted to identify vulnerabilities and address potential weaknesses in systems and networks.\n",
            "3.  **Enhance employee training**: Employees should receive regular training on cybersecurity best practices, including password management and social engineering awareness.\n",
            "4.  **Stay informed**: Organizations should stay informed about the latest threat actor groups, TTPs, and cybersecurity trends to stay ahead of potential threats. [Vector Search](No URL)\n",
            "\n",
            "**Long-Term Forecast and Monitoring**\n",
            "\n",
            "*   **Threat actor evolution**: Threat actors are expected to continue evolving their TTPs and techniques, making it essential for organizations to stay informed and adapt their cybersecurity measures accordingly.\n",
            "*   **Cybersecurity trends**: Cybersecurity trends, including the use of AI and machine learning, are expected to continue shaping the threat landscape in 2024 and beyond. [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "**Source Citations and Evidence**\n",
            "\n",
            "*   Trellix telemetry data (2024)\n",
            "*   SOCRadar (2024)\n",
            "*   CM-Alliance (2024)\n",
            "*   Unitrends (2024)\n",
            "*   Security Magazine (2024) [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "**Visual and Media Analysis**\n",
            "\n",
            "*   Visuals: [Image 1](https://www.cm-alliance.com/hubfs/217059643_m_normal_none%20%281%29.webp)\n",
            "*   Media references: [Source Name](URL) format for each major claim; include media references when applicable.\n",
            "\n",
            "By synthesizing and presenting the latest intelligence on recent cyber attacks and data breaches, this analysis provides actionable recommendations for mitigation and response, supported by credible, source-backed evidence. [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper Image Search](https://erepublic.brightspotcdn.com/dims4/default/798c9d9/2147483647/strip/true/crop/7203x3501+0+105/resize/1440x700!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg)\n",
            "- [Google Serper Image Search](https://images.ctfassets.net/o2pgk9gufvga/18LBE92ZZHGYQuI9FrZNjk/d885d389d333afcc0429fb968bae9dfd/Blog_CTI__1_.png)\n",
            "- [Google Serper](https://purplesec.us/breach-report/)\n",
            "- [Google Programmable Search](https://socradar.io/hsbc-barclays-and-uk-gov-databases-compromised/)\n",
            "- [Tavily Search](https://thehackernews.com/2024/11/thn-recap-top-cybersecurity-threats_18.html)\n",
            "- [Google Serper](https://www.spanning.com/blog/spooky-cyber-incidents-2024/)\n",
            "- [Google Serper Image Search](https://res.cloudinary.com/reliaquest/images/f_svg,q_auto:best/fl_sanitize/v1731675742/wordpress/Chart-with-logo-2024-Threat-Predictions-Blog-1_878278c027/Chart-with-logo-2024-Threat-Predictions-Blog-1_878278c027.svg?_i=AA)\n",
            "- [Tavily Search](https://www.cm-alliance.com/cybersecurity-blog/september-2024-major-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Programmable Search](https://www.techtarget.com/whatis/definition/threat-actor)\n",
            "- [Google Serper Image Search](https://ciobusinessworld.com/wp-content/uploads/2023/12/The-3-Cyber-Security-Incidents-in-2024.jpg)\n",
            "- [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "- [Google Serper](https://www.csis.org/programs/strategic-technologies-program/significant-cyber-incidents)\n",
            "- [Google Serper](https://insights.integrity360.com/biggest-cyber-attacks-of-the-year-so-far..-2024-part-2)\n",
            "- [Google Serper](https://securityboulevard.com/2024/11/major-cyber-attacks-and-data-breaches-of-2024/)\n",
            "- [Google Serper Image Search](https://quointelligence.eu/wp-content/uploads/2024/01/Analysis-of-The-Red-Cross-Rules-Of-Engagement-For-Hacktivists-1024x576.png)\n",
            "- [Google Serper](https://socradar.io/major-cyber-attacks-in-review-april-2024/)\n",
            "- [Google Serper Image Search](https://threatmon.io/storage/180/global-cyber-threat-report-mid-year-2024.jpg)\n",
            "- [Tavily Search](https://www.techradar.com/pro/top-data-breaches-and-cyber-attacks-in-2024)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/10/major-cyber-attacks-in-review-september-2024.jpg)\n",
            "- [Tavily Search](https://www.trellix.com/advanced-research-center/threat-reports/november-2024/)\n",
            "- [Google Serper Image Search](https://erepublic.brightspotcdn.com/dims4/default/9aef090/2147483647/strip/true/crop/466x229+0+0/resize/840x413!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2F05%2Fe5%2F87a4003047a9aedcd572b2a1d68f%2Fzerofox-2024.png)\n",
            "- [Tavily Search](https://www.avg.com/en/signal/malware-statistics)\n",
            "- [Google Serper Image Search](https://www.cm-alliance.com/hubfs/217059643_m_normal_none%20%281%29.webp)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/october-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Serper Image Search](https://www.securitymagazine.com/ext/resources/2023/12/20/SEC_Top-Cybersecurity-Predictions-for-2023.jpg?height=635&t=1704488910&width=1200)\n",
            "- [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}