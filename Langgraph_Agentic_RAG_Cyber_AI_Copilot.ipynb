{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "b09119d7-7edb-499a-f35f-6e4706eb474a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "2024-11-14 12:27:18.839841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 12:27:18.881361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 12:27:18.894317: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 12:27:21.216964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "model.safetensors: 100% 499M/499M [00:08<00:00, 59.9MB/s]\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecbc33d-0a4d-4243-e7bf-7d3fb8adb594"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent\"\n",
        "    return enhanced_query\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "\n",
        "    5. **Actionable Recommendations**:\n",
        "       - Offer precise, immediate actions and mitigation strategies.\n",
        "       - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "       - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    6. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor tactics, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomeware Gang?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7612f2da-2b15-4144-ab53-ec422d15eb1a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomeware Gang? 2024 OR 2024 recent\n",
            "ERROR in Google Programmable Search: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2578)\n",
            "ERROR in Google Programmable Image Search: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2578)\n",
            "DEBUG: Raw results from Exa Search: Title: Ransomware fiends boast they've stolen 1.4TB from US pharmacy network\n",
            "URL: https://www.theregister.com/2024/11/13/embargo_ransomware_breach_aap/\n",
            "ID: https://www.theregister.com/2024/11/13/embargo_ransomware_breach_aap/\n",
            "Score: 0.16414600610733032\n",
            "Published Date: 2024-11-13T00:00:00.000Z\n",
            "Author: Connor Jones\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: American Associated Pharmacies (AAP) is the latest US healthcare organization to have had its data stolen and encrypted by cyber-crooks, it is feared.\n",
            "The criminals over at the Embargo ransomware operation claimed responsibility for the hit job, allegedly stealing 1.469 TB of AAP's data, scrambling its files, and demanding payment to restore the information.\n",
            "AAP, which oversees a few thousand independent pharmacies in the country, hasn't officially confirmed an attack, nor has it responded to The Register's request for input on the claims. At the time of writing, its website warns all user passwords were recently force-reset. It did not explain why the resets were forced nor mention a cyberattack.\n",
            "\"All user passwords associated with both APIRx.com and RxAAP.com have been reset, so existing credentials will no longer be valid to access the sites,\" a website notice reads. \"Please click 'forgot password' on the log in screen and follow the prompts accordingly to reset your password.\"\n",
            "     American Associated Pharmacies' website informing customers their passwords were reset amid suspected ransomware attack ... Click to enlarge\n",
            "The notice also states that API Warehouse, an AAP subsidiary devoted to helping partners save on branded and generic prescription drugs through wholesale buying plans, had some nondescript inventory issues, which are now resolved.\n",
            "As ever with these things, take claims by ransomware miscreants with a pinch of salt, but Embargo's own site claims AAP paid $1.3 million to have their systems decrypted and that it's demanding an additional $1.3 million to keep a lid on the pilfered documents.\n",
            "If true, the demands made by Embargo exceed the average, which the FBI said earlier this year is in the region of $1.5 million.\n",
            "It's not said what kind of data the ransomware group stole from AAP, if any at all. The pharmacy network was given a deadline of November 20 to pay the remaining \"balance\" before its data would be leaked online – a classic double extortion scenario.\n",
            "What isn't a classic move is Embargo's tendency to assign blame to specific individuals after deciding to leak a victim's data. In a number of cases where victims have let the countdown timer run down, Embargo has listed the names, email addresses, and phone numbers of key figures in the organization that it believes hindered the payment and negotiation process. Sometimes this also included the third-party incident responders drafted to help handle an attack.\n",
            " Microsoft says more ransomware stopped before reaching encryption \n",
            " Schneider Electric ransomware crew demands $125k paid in baguettes \n",
            " US healthcare org admits up to 400,000 people's personal info was snatched \n",
            " Healthcare giant to pay $65M settlement after crooks stole and leaked nude patient pics \n",
            "Embargo is a relatively new group on the ransomware scene. Researchers at ESET first noticed it as recently as June, and it is among a number of gangs using endpoint detection and response (EDR) killing tools to deploy its main payload.\n",
            "Despite only being around for mere months, it's garnered attention from established cybercriminals, with the likes of Storm-0501 also seen using its Rust-based ransomware kit.\n",
            "As for AAP, it was founded in 2009 through the merger of Phoenix-based United Drugs and Alabama-based Associated Pharmacies. According to its website, the co-operative oversees more than 2,000 independent pharmacies across the US.\n",
            "Other than the sparsely detailed notice slapped on its website, AAP hasn't publicly acknowledged anything about the alleged robbery. Some interested folks have queried the \"outage\" to its social media channels, and they haven't received a response. ®\n",
            "Highlights: [\"American Associated Pharmacies (AAP) is the latest US healthcare organization to have had its data stolen and encrypted by cyber-crooks, it is feared. The criminals over at the Embargo ransomware operation claimed responsibility for the hit job, allegedly stealing 1.469 TB of AAP's data, scrambling its files, and demanding payment to restore the information. AAP, which oversees a few thousand independent pharmacies in the country, hasn't officially confirmed an attack, nor has it responded to The Register's request for input on the claims. At the time of writing, its website warns all user passwords were recently force-reset. It did not explain why the resets were forced nor mention a cyberattack.\"]\n",
            "Highlight Scores: [0.5223809480667114]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: BianLian ransomware claims attack on Boston Children's Health Physicians\n",
            "URL: https://www.bleepingcomputer.com/news/security/bianlian-ransomware-claims-attack-on-boston-childrens-health-physicians/\n",
            "ID: https://www.bleepingcomputer.com/news/security/bianlian-ransomware-claims-attack-on-boston-childrens-health-physicians/\n",
            "Score: 0.1633998304605484\n",
            "Published Date: 2024-10-17T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The BianLian ransomware group has claimed the cyberattack on Boston Children's Health Physicians (BCHP) and threatens to leak stolen files unless a ransom is paid.\n",
            "BHCP is a network of over 300 pediatric physicians and specialists operating over 60 locations across New York's Hudson Valley and Connecticut, offering patient care in clinics, community hospitals, and health centers affiliated with Boston Children's Hospital.\n",
            "According to the announcement BHCP published on its website, a cyberattack compromised its IT vendor on September 6 and a few days later BHCP detected unauthorized activity on its network.\n",
            "\"On September 6, 2024, our IT vendor informed us that it identified unusual activity in its systems. On September 10, 2024, we detected unauthorized activity on limited parts of the BCHP network and immediately initiated our incident response protocols, including shutting down our systems as a protective measure.\" - BHCP\n",
            "The investigation that followed, conducted with the help of a third-party forensic expert, confirmed that the threat actors had gained unauthorized access to BHCP systems and also exfiltrated files.\n",
            "The exposure impacts current and former employees, patients, and guarantors. The exposed data includes the following, depending on the information customers provided to BHCP:\n",
            " Full names\n",
            "Social Security numbers\n",
            "Addresses\n",
            "Dates of birth\n",
            "Driver's license numbers\n",
            "Medical record numbers\n",
            "Health insurance information\n",
            "Billing information\n",
            "Treatment information (limited)\n",
            " BHCP clarifies that the cyberattack did not impact its electronic medical record systems, as they are hosted on a separate network.\n",
            "Individuals confirmed to have been affected by the incident will receive a letter from BHCP by October 25. Those who had their SSN and driver's license exposed will also receive credit monitoring and protection services.\n",
            "BianLian claims the attack\n",
            "Earlier this week, the BianLian ransomware group claimed the attack by ading BHCP to their extortion portal.\n",
            "The threat actors claim to have finance and HR data, email correspondence, database dumps, personally identifiable and health records, health insurance records, and data related to children.\n",
            "  Source: BleepingCompuer   \n",
            "The threat actors have not leaked anything yet, and there is no deadline for exposing the stolen information, indicating that they still expect to negotiate with BHCP.\n",
            "Attacking children healthcare organizations and stealing the data of minors is typically avoided by ransomware groups, or at least they claim so, but some threat actors lack the moral guidelines to draw the line at that.\n",
            "Earlier this year, the Rhysida ransomware group demanded a ransom payment of $3.6 million from Lurie Children's Hospital in Chicago after stealing 600GB of sensitive data from its systems and causing operational disruptions that led to delays in medical care.\n",
            "Highlights: [\"The threat actors have not leaked anything yet, and there is no deadline for exposing the stolen information, indicating that they still expect to negotiate with BHCP. Attacking children healthcare organizations and stealing the data of minors is typically avoided by ransomware groups, or at least they claim so, but some threat actors lack the moral guidelines to draw the line at that. Earlier this year, the Rhysida ransomware group demanded a ransom payment of $3.6 million from Lurie Children's Hospital in Chicago after stealing 600GB of sensitive data from its systems and causing operational disruptions that led to delays in medical care.\"]\n",
            "Highlight Scores: [0.48400428891181946]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Critical Veeam Vulnerability Exploited to Spread Akira and Fog Ransomware\n",
            "URL: https://thehackernews.com/2024/10/critical-veeam-vulnerability-exploited.html\n",
            "ID: https://thehackernews.com/2024/10/critical-veeam-vulnerability-exploited.html\n",
            "Score: 0.16313186287879944\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Threat actors are actively attempting to exploit a now-patched security flaw in Veeam Backup &amp; Replication to deploy Akira and Fog ransomware.\n",
            "Cybersecurity vendor Sophos said it has been tracking a series of attacks in the past month leveraging compromised VPN credentials and CVE-2024-40711 to create a local account and deploy the ransomware.\n",
            "CVE-2024-40711, rated 9.8 out of 10.0 on the CVSS scale, refers to a critical vulnerability that allows for unauthenticated remote code execution. It was addressed by Veeam in Backup &amp; Replication version 12.2 in early September 2024.\n",
            "Security researcher Florian Hauser of Germany-based CODE WHITE has been credited with discovering and reporting security shortcomings.\n",
            " \n",
            "\"In each of the cases, attackers initially accessed targets using compromised VPN gateways without multifactor authentication enabled,\" Sophos said. \"Some of these VPNs were running unsupported software versions.\"\n",
            "\"Each time, the attackers exploited VEEAM on the URI /trigger on port 8000, triggering the Veeam.Backup.MountService.exe to spawn net.exe. The exploit creates a local account, 'point,' adding it to the local Administrators and Remote Desktop Users groups.\"\n",
            "In the attack that led to the Fog ransomware deployment, the threat actors are said to have drop the ransomware to an unprotected Hyper-V server, while using the rclone utility to exfiltrate data. The other ransomware deployments were unsuccessful.\n",
            "The active exploitation of CVE-2024-40711 has prompted an advisory from NHS England, which noted that \"enterprise backup and disaster recovery applications are valuable targets for cyber threat groups.\"\n",
            "The disclosure comes as Palo Alto Networks Unit 42 detailed a successor to INC ransomware named Lynx that has been active since July 2024, targeting organizations in retail, real estate, architecture, financial, and environmental services sectors in the U.S. and U.K.\n",
            "   \n",
            "The emergence of Lynx is said to have been spurred by the sale of INC ransomware's source code on the criminal underground market as early as March 2024, prompting malware authors to repackage the locker and spawn new variants.\n",
            "\"Lynx ransomware shares a significant portion of its source code with INC ransomware,\" Unit 42 said. \"INC ransomware initially surfaced in August 2023 and had variants compatible with both Windows and Linux.\"\n",
            "It also follows an advisory from the U.S. Department of Health and Human Services (HHS) Health Sector Cybersecurity Coordination Center (HC3) that at least one healthcare entity in the country has fallen victim to Trinity ransomware, another relatively new ransomware player that first became known in May 2024 and is believed to be a rebrand of 2023Lock and Venus ransomware.\n",
            " \n",
            "\"It is a type of malicious software that infiltrates systems through several attack vectors, including phishing emails, malicious websites, and exploitation of software vulnerabilities,\" HC3 said. \"Once inside the system, Trinity ransomware employs a double extortion strategy to target its victims.\"\n",
            "Cyber attacks have also been observed delivering a MedusaLocker ransomware variant dubbed BabyLockerKZ by a financially motivated threat actor known to be active since October 2022, with targets primarily located in the E.U. countries and South America.\n",
            "\"This attacker uses several publicly known attack tools and living-off-the-land binaries (LoLBins), a set of tools built by the same developer (possibly the attacker) to assist in credential theft and lateral movement in compromised organizations,\" Talos researchers said.\n",
            "\"These tools are mostly wrappers around publicly available tools that include additional functionality to streamline the attack process and provide graphical or command-line interfaces.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['countries and South America. \"This attacker uses several publicly known attack tools and living-off-the-land binaries (LoLBins), a set of tools built by the same developer (possibly the attacker) to assist in credential theft and lateral movement in compromised organizations,\" Talos researchers said. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.32604384422302246]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: LA housing authority confirms breach claimed by Cactus ransomware\n",
            "URL: https://www.bleepingcomputer.com/news/security/la-housing-authority-confirms-breach-claimed-by-cactus-ransomware/\n",
            "ID: https://www.bleepingcomputer.com/news/security/la-housing-authority-confirms-breach-claimed-by-cactus-ransomware/\n",
            "Score: 0.1607683300971985\n",
            "Published Date: 2024-11-01T00:00:00.000Z\n",
            "Author: Sergiu Gatlan\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The Housing Authority of the City of Los Angeles (HACLA), one of the largest public housing authorities in the United States, confirmed that a cyberattack hit its IT network after recent breach claims from the Cactus ransomware gang.\n",
            "HACLA provides affordable public housing and assistance programs to low-income families, children, and seniors in Los Angeles, California. As a state-chartered public agency, it administers over 32,000 public housing units on an annual budget of over $1 billion.\n",
            "\"We've been by affected an attack on our IT network. As soon as we became aware of this, we hired external forensic IT specialists to help us investigate and respond appropriately,\" a HACLA spokesperson told BleepingComputer.\n",
            "\"Our systems remain operational, we're taking expert advice, and we remain committed to delivering important services for low income and vulnerable people in Los Angeles.\"\n",
            "The organization has yet to disclose when the attack was detected and if any sensitive data was exposed or stolen during the incident.\n",
            "While HACLA didn't reveal the nature of the cyberattack, the Cactus ransomware gang has claimed the breach, saying it allegedly stole 891 GB of files from the compromised network.\n",
            "Cactus claims this stolen data includes \"personal Identifiable Information, actual database backups, financial documents, executives\\employees personal data, customer personal information, corporate confidential data and correspondence,\" and has already published some screenshots of sensitive documents on its leak site as proof.\n",
            "The ransomware gang has also uploaded an archive containing allegedly stolen files to prove their claims.\n",
            "  HACLA Cactus ransomware entry (BleepingComputer)   \n",
            " Cactus ransomware surfaced in March 2023 with double-extortion attacks and has since added over 260 companies to its dark web data leak site.\n",
            "Its operators breach corporate networks in partnerships with various malware distributors, using purchased credentials, phishing attacks, or exploiting security vulnerabilities in their targets' Internet-exposed systems.\n",
            "HACLA was also breached by the LockBit ransomware gang two years ago, as the organization disclosed in March 2023.\n",
            " The data breach notice revealed that the attackers had access to HACLA's systems for an entire year, between January 15, 2022, and December 31, 2022.\n",
            "Before encrypting devices on the breached network on December 31, 2022, the attackers had access to HACLA members' sensitive personal information, including (but not limited to) names, social security numbers, contact information, driver's licenses, credit card and financial account numbers, as well as their health insurance and medical information.\n",
            "The LockBit ransomware group leaked all stolen files on January 27, 2023, after the government agency refused to pay the ransom demanded by the cybercriminals.\n",
            "Highlights: [\" The data breach notice revealed that the attackers had access to HACLA's systems for an entire year, between January 15, 2022, and December 31, 2022. Before encrypting devices on the breached network on December 31, 2022, the attackers had access to HACLA members' sensitive personal information, including (but not limited to) names, social security numbers, contact information, driver's licenses, credit card and financial account numbers, as well as their health insurance and medical information. The LockBit ransomware group leaked all stolen files on January 27, 2023, after the government agency refused to pay the ransom demanded by the cybercriminals.\"]\n",
            "Highlight Scores: [0.5460923910140991]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Black Basta greift über MS-Teams-Chats an\n",
            "URL: https://www.csoonline.com/article/3592549/black-basta-greift-uber-ms-teams-chats-an.html\n",
            "ID: https://www.csoonline.com/article/3592549/black-basta-greift-uber-ms-teams-chats-an.html\n",
            "Score: 0.15780054032802582\n",
            "Published Date: 2024-10-28T00:00:00.000Z\n",
            "Author: Julia Mutzbauer\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Die Ransomware-Bande Black Basta setzt jetzt auf eine neue Angriffstaktik, die über Microsoft Teams-Chats läuft.\t\n",
            "  Die Ransomware-Bande Black Basta nutzt MS-Teams-Chatnachrichten, um Zugriff auf die Systeme von Unternehmen zu erhalten.   Ink Drop – Shutterstock.com \n",
            "Die berüchtigte Ransomware-Gruppe Black Basta hat es auf Organisationen weltweit abgesehen. Bisher war die Bande dafür bekannt, ihre Opfer zunächst mit Spam-E-Mails zu überhäufen. Anschließend gaben sich die Hacker als IT-Support aus, um sich Zugriff auf die Systeme zu verschaffen. Diese Methode wurde jetzt offenbar weiterentwickelt.\n",
            "Sicherheitsforscher von ReliaQuest haben kürzlich herausgefunden, dass Black Basta nun Microsoft Teams-Chatnachrichten verwendet, um potenzielle Opfer in Gespräche zu verwickeln. Auch bei dieser Methode tarnen sich die Angreifer als Helpdesk-Mitarbeiter. Laut Forschungsbericht erfolgt die Kontaktaufnahme teilweise über Einladungen zu MS Teams-Gruppenchats.\n",
            "In den Chats verleiten die Kriminellen dann die Anwender dazu, QR-Codes anzuklicken, die auf eine betrügerische Website führen. Die Betrugsseiten sind dabei auf die betreffende Zielorganisation zugeschnitten und häufig nur durch eine genaue Prüfung der Subdomain von echten Unternehmensseiten zu unterscheiden.\n",
            "Ziel der Angreifer ist es, so die Forscher, MS-Teams-Benutzer dazu zu bringen, Tools für Remote-Überwachung und -Verwaltung (RMM) herunterzuladen und Zugriff auf die Zielumgebung zu erhalten.\n",
            " Schutz vor MS-Teams-Angriffen \n",
            "Um sich vor diesen Angriffen zu schützen, empfiehlt ReliaQuest folgende Maßnahmen:\n",
            "Unternehmen sollten die Kommunikation von externen Benutzern innerhalb von Teams deaktivieren. Auf diese Weise lässt sich verhindern, dass unerwünschte Chat-Nachrichten die Endbenutzer erreichen.\n",
            "Wenn die Kommunikation mit externen Benutzern erforderlich ist, können bestimmte vertrauenswürdige Domänen auf eine Positivliste gesetzt werden. Darüber hinaus kann die Einrichtung aggressiver Anti-Spam-Richtlinien in E-Mail-Sicherheits-Tools verhindern, dass Spam die Posteingänge der Endbenutzer überschwemmt.\n",
            "Stellen Sie sicher, dass die Protokollierung für Teams aktiviert ist, insbesondere für das Ereignis „ChatCreated“, um derartige Aktivitäten leichter erkennen und untersuchen zu können.\n",
            "Bei der Suche nach diesen Helpdesk-Konten sollten Organisationen nach „enthält“ und nicht nach einer direkten Übereinstimmung suchen.\n",
            "Highlights: ['Ink Drop – Shutterstock.com  Die berüchtigte Ransomware-Gruppe Black Basta hat es auf Organisationen weltweit abgesehen. Bisher war die Bande dafür bekannt, ihre Opfer zunächst mit Spam-E-Mails zu überhäufen. Anschließend gaben sich die Hacker als IT-Support aus, um sich Zugriff auf die Systeme zu verschaffen. Diese Methode wurde jetzt offenbar weiterentwickelt.']\n",
            "Highlight Scores: [0.045780666172504425]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is a recent cyber incident from the Blackbasta ransomware gang in 2024:\n",
            "Resolved Search Type: 2024-10-14T12:29:13.680Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://spin.ai/resources/ransomware-tracker/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://spin.ai/resources/ransomware-tracker/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://spin.ai/resources/ransomware-tracker/, success: True, time taken: 35.52 seconds\n",
            "[LOG] 🚀 Content extracted for https://spin.ai/resources/ransomware-tracker/, success: True, time taken: 55.60 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://spin.ai/resources/ransomware-tracker/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://spin.ai/resources/ransomware-tracker/, time taken: 70.53 seconds.\n",
            "[LOG] ✅ Crawled https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, success: True, time taken: 102.41 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, success: True, time taken: 11.22 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, time taken: 13.83 seconds.\n",
            "[LOG] ✅ Crawled https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, success: True, time taken: 140.54 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, success: True, time taken: 11.78 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, time taken: 13.33 seconds.\n",
            "Body visibility debug info: {'display': 'block', 'visibility': 'visible', 'opacity': '1', 'hasContent': 122293, 'classList': []}\n",
            "Proceeding despite hidden body...\n",
            "[LOG] ✅ Crawled https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, success: True, time taken: 173.95 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, success: True, time taken: 1.49 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/, time taken: 1.99 seconds.\n",
            "[LOG] ✅ Crawled https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/, success: True, time taken: 172.24 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/, success: True, time taken: 0.75 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/, time taken: 1.19 seconds.\n",
            "Crawled Results: [[{'links': 'javascript:void(0);', 'images': 'https://www.cm-alliance.com/hubfs/Staging%20Site/icons/Group%20108759.svg'}], [{'links': 'https://spin.ai/browser-extension-risk-assessment-plugin/', 'images': 'https://spin.ai/wp-content/themes/spin.ai/assets/img/login.svg'}], [{'links': 'https://www.csoonline.com', 'images': 'https://www.csoonline.com/wp-content/uploads/2024/11/3428-0-77376400-1731500642-author_photo_Julia-Mutzbauer_1729061602.jpg?quality=50&strip=all&w=150'}], [{'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}], [{'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'javascript:void(0);', 'images': 'https://www.cm-alliance.com/hubfs/Staging%20Site/icons/Group%20108759.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://spin.ai/browser-extension-risk-assessment-plugin/', 'images': 'https://spin.ai/wp-content/themes/spin.ai/assets/img/login.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.csoonline.com', 'images': 'https://www.csoonline.com/wp-content/uploads/2024/11/3428-0-77376400-1731500642-author_photo_Julia-Mutzbauer_1729061602.jpg?quality=50&strip=all&w=150'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.bleepingcomputer.com/', 'images': 'https://www.bleepstatic.com/images/site/logo.png'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://assets.infosecurity-magazine.com/content/span/cc94e881-f6db-4012-949a-eb4408ee237f.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://packetwatch.com/hubfs/Blog%20Featured%20Images/05-20-2024.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-10-13-145431.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.threatdown.com/wp-content/uploads/2024/07/known-ransomware-attacks-by-gang-june-2024.jpg?w=1024\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/07/Screenshot-2024-07-22-124510-1024x584.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.picussecurity.com/hubfs/5-1.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.reliaquest.com/wp-content/uploads/2024/07/rr-q2-graph4.svg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "The Black Basta ransomware gang has been actively targeting organizations across various sectors, including healthcare, with a focus on exploiting critical vulnerabilities. According to recent reports, the gang has impacted over 500 organizations worldwide, resulting in significant financial losses. This analysis provides an overview of the current situation, highlighting key findings, technical breakdowns, and contextual implications. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Key Findings**\n",
            "\n",
            "* The Black Basta ransomware gang has been linked to multiple high-profile attacks, including those on healthcare organizations and critical infrastructure.\n",
            "* The gang has been using a new attack tactic, posing as IT support on Microsoft Teams to breach networks.\n",
            "* Recent reports indicate that the gang has been exploiting a Windows privilege escalation vulnerability (CVE-2024-26169) as a zero-day attack.\n",
            "* The gang has been demanding significant ransom payments, with some reports suggesting that victims have paid over $100 million. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Technical Breakdown**\n",
            "\n",
            "* The Black Basta ransomware is a type of ransomware-as-a-service (RaaS) that has been gaining popularity in recent months.\n",
            "* The gang has been using a combination of social engineering and technical attacks to compromise networks and steal sensitive data.\n",
            "* The ransomware is highly customizable, allowing affiliates to tailor the attack to specific targets and sectors.\n",
            "* The gang has been using encryption and other obfuscation techniques to evade detection and maintain control over compromised systems. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Contextual and Industry Impact**\n",
            "\n",
            "* The Black Basta ransomware gang's attacks have had significant implications for the healthcare sector, with multiple organizations reporting losses and disruptions.\n",
            "* The gang's use of social engineering tactics has also raised concerns about the potential for phishing and other types of cyber attacks.\n",
            "* The incident highlights the need for organizations to prioritize cybersecurity and implement robust measures to prevent and respond to ransomware attacks. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Most Recent Relevant Activities**\n",
            "\n",
            "* The Black Basta ransomware gang has been actively targeting organizations across various sectors, including healthcare and critical infrastructure.\n",
            "* Recent reports indicate that the gang has been exploiting a Windows privilege escalation vulnerability (CVE-2024-26169) as a zero-day attack.\n",
            "* The gang has been demanding significant ransom payments, with some reports suggesting that victims have paid over $100 million. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Source Citations and Evidence**\n",
            "\n",
            "* [1] \"Black Basta ransomware gang linked to Windows zero-day attacks\" (Bleeping Computer)\n",
            "* [2] \"Black Basta ransomware poses as IT support on Microsoft Teams to breach networks\" (Bleeping Computer)\n",
            "* [3] \"Black Basta ransomware gang\" (Picus Security)\n",
            "* [4] \"Ransomware and Cyber Extortion in Q2 2024\" (ReliaQuest)\n",
            "* [5] \"Black Basta ransomware victims have paid over $100 million\" (Elliptic) [Google Serper](https://www.bleepingcomputer.com/tag/black-basta/)\n",
            "\n",
            "**Actionable Recommendations**\n",
            "\n",
            "* Organizations should prioritize cybersecurity and implement robust measures to prevent and respond to ransomware attacks.\n",
            "* The use of encryption and other obfuscation techniques should be avoided, as they can make it more difficult for organizations to detect and respond to attacks.\n",
            "* Affiliates should be cautious when using social engineering tactics, as they can be easily detected and countered by organizations with robust cybersecurity measures in place. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Long-Term Forecast and Monitoring**\n",
            "\n",
            "* The Black Basta ransomware gang is likely to continue its attacks on organizations across various sectors, including healthcare and critical infrastructure.\n",
            "* Organizations should remain vigilant and monitor for signs of ransomware attacks, including suspicious activity and unusual network behavior.\n",
            "* The use of advanced threat detection and response tools can help organizations to detect and respond to ransomware attacks more effectively. [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper Image Search](https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png)\n",
            "- [Google Serper Image Search](https://www.picussecurity.com/hubfs/5-1.jpg)\n",
            "- [Tavily Search](https://www.aha.org/system/files/media/file/2024/05/federal-government-issues-joint-advisory-as-black-basta-ransomware-group-accelerates-attacks-on-the-health-care-sector-advisory-5-10-24.pdf)\n",
            "- [Google Serper Image Search](https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg)\n",
            "- [Tavily Search](https://www.securityweek.com/black-basta-ransomware-hit-over-500-organizations/)\n",
            "- [Tavily Search](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-gang-linked-to-windows-zero-day-attacks/)\n",
            "- [Google Serper](https://www.reliaquest.com/blog/q2-2024-ransomware/)\n",
            "- [Google Serper](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "- [Google Serper Image Search](https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png)\n",
            "- [Google Serper](https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html)\n",
            "- [Google Serper Image Search](https://www.threatdown.com/wp-content/uploads/2024/07/known-ransomware-attacks-by-gang-june-2024.jpg?w=1024)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/07/Screenshot-2024-07-22-124510-1024x584.png)\n",
            "- [Google Serper](https://www.cisa.gov/stopransomware/official-alerts-statements-fbi)\n",
            "- [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "- [Google Serper Image Search](https://www.reliaquest.com/wp-content/uploads/2024/07/rr-q2-graph4.svg)\n",
            "- [Google Serper](https://therecord.media/tag/black-basta)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-10-13-145431.webp)\n",
            "- [Google Serper](https://spin.ai/resources/ransomware-tracker/)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/tag/black-basta/)\n",
            "- [Google Serper Image Search](https://packetwatch.com/hubfs/Blog%20Featured%20Images/05-20-2024.png)\n",
            "- [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n",
            "- [Google Serper Image Search](https://assets.infosecurity-magazine.com/content/span/cc94e881-f6db-4012-949a-eb4408ee237f.png)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}