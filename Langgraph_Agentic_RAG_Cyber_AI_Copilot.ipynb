{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "76f9a748-bc7e-45be-8802-c893a6cdb2b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/408.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 5.83MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 5.91MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 38.7MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 14.1MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.78MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 12.5MB/s]\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 229MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Downloading Chromium 129.0.6668.29 (playwright build v1134)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1134/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164 MiB [] 0% 10.8s\u001b[0K\u001b[1G164 MiB [] 0% 44.3s\u001b[0K\u001b[1G164 MiB [] 0% 33.0s\u001b[0K\u001b[1G164 MiB [] 0% 13.1s\u001b[0K\u001b[1G164 MiB [] 1% 7.4s\u001b[0K\u001b[1G164 MiB [] 1% 5.0s\u001b[0K\u001b[1G164 MiB [] 2% 3.9s\u001b[0K\u001b[1G164 MiB [] 3% 3.4s\u001b[0K\u001b[1G164 MiB [] 4% 2.9s\u001b[0K\u001b[1G164 MiB [] 6% 2.4s\u001b[0K\u001b[1G164 MiB [] 7% 2.2s\u001b[0K\u001b[1G164 MiB [] 8% 2.1s\u001b[0K\u001b[1G164 MiB [] 9% 2.0s\u001b[0K\u001b[1G164 MiB [] 10% 1.9s\u001b[0K\u001b[1G164 MiB [] 11% 1.9s\u001b[0K\u001b[1G164 MiB [] 13% 1.8s\u001b[0K\u001b[1G164 MiB [] 14% 1.7s\u001b[0K\u001b[1G164 MiB [] 15% 1.6s\u001b[0K\u001b[1G164 MiB [] 17% 1.6s\u001b[0K\u001b[1G164 MiB [] 18% 1.6s\u001b[0K\u001b[1G164 MiB [] 19% 1.6s\u001b[0K\u001b[1G164 MiB [] 20% 1.5s\u001b[0K\u001b[1G164 MiB [] 21% 1.5s\u001b[0K\u001b[1G164 MiB [] 22% 1.5s\u001b[0K\u001b[1G164 MiB [] 24% 1.5s\u001b[0K\u001b[1G164 MiB [] 26% 1.4s\u001b[0K\u001b[1G164 MiB [] 27% 1.4s\u001b[0K\u001b[1G164 MiB [] 29% 1.3s\u001b[0K\u001b[1G164 MiB [] 30% 1.2s\u001b[0K\u001b[1G164 MiB [] 32% 1.2s\u001b[0K\u001b[1G164 MiB [] 33% 1.1s\u001b[0K\u001b[1G164 MiB [] 34% 1.1s\u001b[0K\u001b[1G164 MiB [] 35% 1.1s\u001b[0K\u001b[1G164 MiB [] 37% 1.1s\u001b[0K\u001b[1G164 MiB [] 38% 1.0s\u001b[0K\u001b[1G164 MiB [] 40% 1.0s\u001b[0K\u001b[1G164 MiB [] 42% 0.9s\u001b[0K\u001b[1G164 MiB [] 44% 0.9s\u001b[0K\u001b[1G164 MiB [] 45% 0.9s\u001b[0K\u001b[1G164 MiB [] 46% 0.8s\u001b[0K\u001b[1G164 MiB [] 47% 0.8s\u001b[0K\u001b[1G164 MiB [] 48% 0.8s\u001b[0K\u001b[1G164 MiB [] 49% 0.8s\u001b[0K\u001b[1G164 MiB [] 50% 0.8s\u001b[0K\u001b[1G164 MiB [] 51% 0.8s\u001b[0K\u001b[1G164 MiB [] 52% 0.8s\u001b[0K\u001b[1G164 MiB [] 53% 0.8s\u001b[0K\u001b[1G164 MiB [] 54% 0.8s\u001b[0K\u001b[1G164 MiB [] 55% 0.8s\u001b[0K\u001b[1G164 MiB [] 56% 0.8s\u001b[0K\u001b[1G164 MiB [] 57% 0.7s\u001b[0K\u001b[1G164 MiB [] 58% 0.7s\u001b[0K\u001b[1G164 MiB [] 59% 0.7s\u001b[0K\u001b[1G164 MiB [] 60% 0.7s\u001b[0K\u001b[1G164 MiB [] 61% 0.7s\u001b[0K\u001b[1G164 MiB [] 62% 0.7s\u001b[0K\u001b[1G164 MiB [] 63% 0.7s\u001b[0K\u001b[1G164 MiB [] 64% 0.7s\u001b[0K\u001b[1G164 MiB [] 65% 0.7s\u001b[0K\u001b[1G164 MiB [] 66% 0.6s\u001b[0K\u001b[1G164 MiB [] 67% 0.6s\u001b[0K\u001b[1G164 MiB [] 68% 0.6s\u001b[0K\u001b[1G164 MiB [] 69% 0.6s\u001b[0K\u001b[1G164 MiB [] 70% 0.6s\u001b[0K\u001b[1G164 MiB [] 71% 0.6s\u001b[0K\u001b[1G164 MiB [] 71% 0.5s\u001b[0K\u001b[1G164 MiB [] 72% 0.5s\u001b[0K\u001b[1G164 MiB [] 73% 0.5s\u001b[0K\u001b[1G164 MiB [] 74% 0.5s\u001b[0K\u001b[1G164 MiB [] 75% 0.5s\u001b[0K\u001b[1G164 MiB [] 76% 0.5s\u001b[0K\u001b[1G164 MiB [] 77% 0.5s\u001b[0K\u001b[1G164 MiB [] 78% 0.4s\u001b[0K\u001b[1G164 MiB [] 80% 0.4s\u001b[0K\u001b[1G164 MiB [] 82% 0.3s\u001b[0K\u001b[1G164 MiB [] 84% 0.3s\u001b[0K\u001b[1G164 MiB [] 86% 0.3s\u001b[0K\u001b[1G164 MiB [] 88% 0.2s\u001b[0K\u001b[1G164 MiB [] 89% 0.2s\u001b[0K\u001b[1G164 MiB [] 90% 0.2s\u001b[0K\u001b[1G164 MiB [] 92% 0.1s\u001b[0K\u001b[1G164 MiB [] 93% 0.1s\u001b[0K\u001b[1G164 MiB [] 95% 0.1s\u001b[0K\u001b[1G164 MiB [] 96% 0.1s\u001b[0K\u001b[1G164 MiB [] 98% 0.0s\u001b[0K\u001b[1G164 MiB [] 99% 0.0s\u001b[0K\u001b[1G164 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 129.0.6668.29 (playwright build v1134) downloaded to /root/.cache/ms-playwright/chromium-1134\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 10% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 28% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 60% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Firefox 130.0 (playwright build v1463)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1463/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G86.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 0% 20.0s\u001b[0K\u001b[1G86.4 MiB [] 0% 13.4s\u001b[0K\u001b[1G86.4 MiB [] 0% 7.2s\u001b[0K\u001b[1G86.4 MiB [] 1% 3.9s\u001b[0K\u001b[1G86.4 MiB [] 2% 2.7s\u001b[0K\u001b[1G86.4 MiB [] 4% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 6% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 8% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 10% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 13% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 16% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 18% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 21% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 24% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 27% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 30% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 34% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 38% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 42% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 46% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 50% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 55% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 59% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 61% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 64% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 66% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 68% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 70% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 73% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 74% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 77% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 80% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 83% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 85% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 88% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 90% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 93% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 95% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 98% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 130.0 (playwright build v1463) downloaded to /root/.cache/ms-playwright/firefox-1463\n",
            "Downloading Webkit 18.0 (playwright build v2070)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2070/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G88.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 0% 32.3s\u001b[0K\u001b[1G88.2 MiB [] 0% 16.9s\u001b[0K\u001b[1G88.2 MiB [] 0% 9.7s\u001b[0K\u001b[1G88.2 MiB [] 1% 5.5s\u001b[0K\u001b[1G88.2 MiB [] 2% 2.9s\u001b[0K\u001b[1G88.2 MiB [] 4% 2.2s\u001b[0K\u001b[1G88.2 MiB [] 6% 1.7s\u001b[0K\u001b[1G88.2 MiB [] 8% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 10% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 12% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 15% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 17% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 19% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 21% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 23% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 25% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 27% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 27% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 28% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 28% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 29% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 31% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 33% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 34% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 35% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 36% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 37% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 37% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 38% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 40% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 41% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 42% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 44% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 45% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 46% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 47% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 50% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 53% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 54% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 55% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 56% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 59% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 61% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 63% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 64% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 65% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 67% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 69% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 71% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 72% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 73% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 74% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 75% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 77% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 80% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 81% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 82% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 84% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 86% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 89% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 91% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 94% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.0 (playwright build v2070) downloaded to /root/.cache/ms-playwright/webkit-2070\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all] langchain-openai\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76915226-b12f-4779-d063-986e0b3bfbce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "# Exa search function\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, exa_py.api.SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "async def extract_content_from_url(url):\n",
        "    schema = {\n",
        "        \"name\": \"Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "        return extracted_content\n",
        "\n",
        "async def scrape_links_and_content(urls):\n",
        "    tasks = [extract_content_from_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return [result for result in results if result is not None]"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search),\n",
        "        (\"Google Serper Image Search\", google_serper_image_search),\n",
        "        (\"Google Programmable Image Search\", google_programmable_image_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in {name}: {str(e)}\")\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        parsed_date = parse_date(x.date)\n",
        "        return (parsed_date is not None, parsed_date or datetime.min, x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 10 most relevant and recent results\n",
        "    top_results = all_results[:10]\n",
        "\n",
        "    # Extract URLs for further crawling\n",
        "    urls_to_crawl = [result.url for result in top_results if result.url]\n",
        "\n",
        "    # Scrape links and content from the extracted URLs\n",
        "    crawled_results = asyncio.run(scrape_links_and_content(urls_to_crawl))\n",
        "\n",
        "    # Add crawled results to the state\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Crawled Results\", \"crawled_results\": crawled_results})\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state"
      ],
      "metadata": {
        "id": "cjMU8sqFJu18"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                         if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines and data sources to provide comprehensive, query-specific responses.\n",
        "\n",
        "SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "1. Primary Source Evaluation:\n",
        "   - Assess credibility of each source domain\n",
        "   - Verify publication dates for temporal relevance\n",
        "   - Cross-reference information across multiple sources\n",
        "   - Identify and flag potential misinformation or conflicting data\n",
        "\n",
        "2. Content Processing Guidelines:\n",
        "   - Extract and normalize key data points\n",
        "   - Identify patterns and correlations across sources\n",
        "   - Prioritize information based on:\n",
        "     * Temporal relevance (newest to oldest)\n",
        "     * Source reliability\n",
        "     * Direct query relevance\n",
        "     * Technical depth\n",
        "     * Actionable insights\n",
        "\n",
        "3. Media Content Analysis:\n",
        "   - Evaluate included images, diagrams, or screenshots\n",
        "   - Extract relevant technical indicators from visual data\n",
        "   - Correlate visual evidence with textual information\n",
        "   - Note any visual proof of concepts or attack demonstrations\n",
        "\n",
        "RESPONSE STRUCTURE:\n",
        "1. Executive Summary (2-3 sentences)\n",
        "   - Core findings\n",
        "   - Critical alerts or time-sensitive information\n",
        "   - Confidence level in findings\n",
        "\n",
        "2. Detailed Analysis:\n",
        "   a) Key Findings\n",
        "      - Bullet points of critical discoveries\n",
        "      - Emerging threats or developments\n",
        "      - Statistical data or metrics\n",
        "\n",
        "   b) Technical Details\n",
        "      - Specific vulnerabilities or exploits\n",
        "      - Attack vectors and techniques\n",
        "      - System impacts and affected components\n",
        "\n",
        "   c) Contextual Analysis\n",
        "      - Industry impact\n",
        "      - Threat actor attribution (if applicable)\n",
        "      - Historical context or similar incidents\n",
        "\n",
        "3. Evidence and Citations:\n",
        "   - Link every major claim to source material\n",
        "   - Include relevant quote snippets\n",
        "   - Provide context for technical indicators\n",
        "   - Reference related media content\n",
        "\n",
        "4. Actionable Intelligence:\n",
        "   - Immediate response recommendations\n",
        "   - Mitigation strategies\n",
        "   - Detection methods\n",
        "   - Prevention measures\n",
        "\n",
        "5. Future Implications:\n",
        "   - Projected developments\n",
        "   - Potential cascade effects\n",
        "   - Areas requiring monitoring\n",
        "\n",
        "SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "1. For Threat Intelligence:\n",
        "   - Extract and validate IOCs\n",
        "   - Identify TTPs and map to MITRE ATT&CK\n",
        "   - Analyze malware behaviors\n",
        "   - Document C2 infrastructure\n",
        "\n",
        "2. For Vulnerability Analysis:\n",
        "   - Verify CVE details\n",
        "   - Document exploit requirements\n",
        "   - Assess patch availability\n",
        "   - Evaluate real-world exploitation\n",
        "\n",
        "3. For Incident Response:\n",
        "   - Timeline reconstruction\n",
        "   - Attack path analysis\n",
        "   - Impact assessment\n",
        "   - Recovery recommendations\n",
        "\n",
        "4. For Trend Analysis:\n",
        "   - Identify pattern changes\n",
        "   - Map threat evolution\n",
        "   - Project future developments\n",
        "   - Compare against historical data\n",
        "\n",
        "Previous conversation context: {chat_history}\n",
        "Current query: {input}\n",
        "Available search results: {search_results}\n",
        "Crawled results: {crawled_results}\n",
        "Current timestamp: {current_date}\n",
        "\n",
        "RESPONSE REQUIREMENTS:\n",
        "1. Maintain clinical precision and technical accuracy\n",
        "2. Prioritize actionable intelligence over general information\n",
        "3. Include explicit confidence levels for all assessments\n",
        "4. Cite ALL sources using [Source Name](URL) format\n",
        "5. Highlight time-sensitive information\n",
        "6. Address any information gaps or uncertainties\n",
        "7. Format output for maximum readability\n",
        "8. Include relevant media references\n",
        "9. Provide specific, implementable recommendations\n",
        "10. Maintain proper technical context throughout\n",
        "\n",
        "Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the search results:\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                if media.get(\"type\") == \"image\":\n",
        "                    display(Image(url=media.get(\"url\"), width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = graph.invoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Share some details on currently active Infostealer malware and give me their TTPs and IOCs\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Processing Query and Generating Response from Cyber AI Copilot Please Wait...:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "365053d7-25bf-4e00-ec2f-c75cca25d4d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Share some details on currently active Infostealer malware and give me their TTPs and IOCs\n",
            "DEBUG: Raw results from Exa Search: Title: IsaacWiper (Malware Family)\n",
            "URL: https://malpedia.caad.fkie.fraunhofer.de/details/win.isaacwiper\n",
            "ID: https://malpedia.caad.fkie.fraunhofer.de/details/win.isaacwiper\n",
            "Score: 0.1695963591337204\n",
            "Published Date: 2022-09-26T00:00:00.000Z\n",
            "Author: Fraunhofer FKIE\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: According to Recorded Future, IsaacWiper is a destructive malware that overwrites all physical disks and logical volumes on a victim’s machine.\n",
            "Highlights: ['According to Recorded Future, IsaacWiper is a destructive malware that overwrites all physical disks and logical volumes on a victim’s machine.']\n",
            "Highlight Scores: [0.29472413659095764]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Reports | Triage\n",
            "URL: https://tria.ge/s/tag:infostealer\n",
            "ID: https://tria.ge/s/tag:infostealer\n",
            "Score: 0.1687609851360321\n",
            "Published Date: 2023-02-06T00:00:00.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Sample ID\n",
            "Created\n",
            "Filename Tags\n",
            "Status/Score\n",
            "SHA256\n",
            " \n",
            "240615-e2hsysyfnf\n",
            "15-06-2024 04:26\n",
            "UTC\n",
            "16d52786d50d7c0bad6ef837ec04aa3c.exe \n",
            " evasion dcrat infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-e2clyasfrp\n",
            "15-06-2024 04:25\n",
            "UTC\n",
            "522f0a2aa0ab0cc5a76abe35435f83f1c4ad38328df296fd308f8e5825cad713 \n",
            " 0e6740 @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer lumma redline risepro infostealer persistence spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-e11blssfqn\n",
            "15-06-2024 04:25\n",
            "UTC\n",
            "d21c1dae567563d5e9bd69de0eaa4822b5274fb9ccf5026b2c2b0adaaed5cf3b \n",
            " @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer lumma redline infostealer persistence spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-d2216axgla\n",
            "15-06-2024 03:31\n",
            "UTC\n",
            "0c1b8d8d00d578c844a14875fb7599d3.exe \n",
            " evasion dcrat infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-dk6m1s1cjq\n",
            "15-06-2024 03:05\n",
            "UTC\n",
            "f5567fd47eb3b902426098c8c06d99df.bin \n",
            " logsdiller cloud (tg: @logsdillabot) redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-daxblazhmk\n",
            "15-06-2024 02:48\n",
            "UTC\n",
            "da5844b02ebfa56b4c036ea50136e7766922fa1591d344130f5492e5624fdf5d \n",
            " duc evasion redline infostealer persistence themida trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-c6matszgkn\n",
            "15-06-2024 02:41\n",
            "UTC\n",
            "f109e5691b8fedddeec53b85ced5c60cb3010b6219964b7f0262c5ebfd191242.exe \n",
            " hackus_checker_cracked discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cy1s9awepb\n",
            "15-06-2024 02:29\n",
            "UTC\n",
            "6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cykf9swenb\n",
            "15-06-2024 02:29\n",
            "UTC\n",
            "WiFiService.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cx7v6szekp\n",
            "15-06-2024 02:28\n",
            "UTC\n",
            "970a1609fc0058e529a6114875cd9b708d0377057f191e4ed8b5c870f0d4f3f8 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cla23awbkg\n",
            "15-06-2024 02:09\n",
            "UTC\n",
            "aa2ed5d30a8cfbc39f9a129bc3ca6fd4d07863c0e72ea945af19d3cb674c479e \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-clpkfszbkl\n",
            "15-06-2024 02:10\n",
            "UTC\n",
            "bruno-wi1.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ck49hswbjh\n",
            "15-06-2024 02:09\n",
            "UTC\n",
            "bruno-sis2.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ck2tdswbje\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "c35983b0754cf274ea4c071bd2f9c0fb45972e893d805f62ec8a1fc8ee97e9e3 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ckq2wszaqn\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "3b2749bfd91ab0a27ef988b84aeafbca81d4d9f3d63ab7e594655b266f27a30f \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ckkj4swarg\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "551d61685f69372204f69da1394b4155a4f732d5a4ff5bdb67f11ae7a8e7ee50 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cdsbdavgnb\n",
            "15-06-2024 01:57\n",
            "UTC\n",
            "a68d68e210a3ee6e9d8ef30c63de4658ef1023fbf27ca7c8d7b79bc6351f5677.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b8xznsyeqn\n",
            "15-06-2024 01:49\n",
            "UTC\n",
            "88b889a1477c81510c62a46c9eb1d77d386c59dceb0523e8b5734b6dde252573.exe \n",
            " cheat execution redline sectoprat infostealer rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b7qtzayekm\n",
            "15-06-2024 01:47\n",
            "UTC\n",
            "ac78fe3a2934efdfda73727906502f6a_JaffaCakes118 \n",
            " android banker collection credential_access discovery evasion anubis infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b7pa5syekl\n",
            "15-06-2024 01:47\n",
            "UTC\n",
            "83037ad76ddddabca05efe07e731d65c5d9069ad889e46306b753cbc7561fa59.exe \n",
            " 0011 discovery redline xworm infostealer rat spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b479vavcqf\n",
            "15-06-2024 01:43\n",
            "UTC\n",
            "6f42327cec9d52b7b30a0efab03df9d30fc1bd5e9a5b6a6d0c1c23d99cdb1349.exe \n",
            " kir redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b4qd2avcpa\n",
            "15-06-2024 01:42\n",
            "UTC\n",
            "6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bpxvkaxglj\n",
            "15-06-2024 01:19\n",
            "UTC\n",
            "2849878b8913c66392f6202039c1d38e2b7061daec60947671795f1e1cd63db5.exe \n",
            " execution dcrat infostealer rat spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bpbmbstfmh\n",
            "15-06-2024 01:18\n",
            "UTC\n",
            "26351c3679d0ab48f8fb8503b165237c1f4b194a2ceb16df788b53b3875bf9b7.xll \n",
            " warzonerat infostealer persistence rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bjx85sxenn\n",
            "15-06-2024 01:11\n",
            "UTC\n",
            "2a27295081e9c2b7e02815fd6fc556ec.exe \n",
            " evasion dcrat infostealer rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bc9zcsxcnl\n",
            "15-06-2024 01:01\n",
            "UTC\n",
            "a554f91e1f53fd240d00614f4b42cc47db6c0d389ada198cd0fed62ba855741b.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bc9ctstbrd\n",
            "15-06-2024 01:01\n",
            "UTC\n",
            "00b1ea6a2a6a6cc82331e94e37af46027fbfdb340ed465d5d01d136b6f777240.exe \n",
            " logsdiller cloud (tg: @logsdillabot) redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-a9wybstbja\n",
            "15-06-2024 00:55\n",
            "UTC\n",
            "aef7507afedaa98be7220764512b7884453db0a3af40aa6a5c2b627a031a95d6 \n",
            " aspackv2 evasion warzonerat infostealer persistence rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-a42avsshph\n",
            "15-06-2024 00:46\n",
            "UTC\n",
            "1849639219e47189b9836bb17998f72714ca9e785b6ee9fbce5933e050754a3d.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-anag7ssdkh\n",
            "15-06-2024 00:20\n",
            "UTC\n",
            "e35e252ec2c793fe44c09e744495b6d841c3e0c81807d08fb2923b08bf89d6e6.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2yjmeatcln\n",
            "14-06-2024 22:59\n",
            "UTC\n",
            "EnergyBetaCrack.exe \n",
            " evasion dcrat infostealer rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2qengsyhnc\n",
            "14-06-2024 22:46\n",
            "UTC\n",
            "Loader.exe \n",
            " @bloodyrain12 redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2nh8wssgrm\n",
            "14-06-2024 22:43\n",
            "UTC\n",
            "80f4b2a7dff768ce67ca462260e1157d267aacbd397f90421373458952e06aca.bin \n",
            " android banker collection credential_access discovery evasion execution hook impact infostealer persistence rat stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2gbkgasenm\n",
            "14-06-2024 22:32\n",
            "UTC\n",
            "7f2b1dcdb486730421552c9dd6bf09e9d386e2d59ca5fc542bbc66b0b2181c47.bin \n",
            " android collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-17js4ayaqd\n",
            "14-06-2024 22:17\n",
            "UTC\n",
            "abb7d2d64ee38fb8fb9b0191126a280c_JaffaCakes118 \n",
            " oski infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-16ccmayamc\n",
            "14-06-2024 22:15\n",
            "UTC\n",
            "bda689dc6532b4978c5a97a87c2acc3cad767489414bbebbafe1325d03132fbc.bin \n",
            " android banker collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-11214a1hjj\n",
            "14-06-2024 22:07\n",
            "UTC\n",
            "9073568a1855141fa0de08a35fb37216b8571bd92cc45a564af2d787e8924aca.bin \n",
            " android collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-1mlf7axckh\n",
            "14-06-2024 21:46\n",
            "UTC\n",
            "15f948da0e0786ee883bc9714ee6b47a.exe \n",
            " cheat discovery execution redline sectoprat infostealer rat spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-1htl1axara\n",
            "14-06-2024 21:39\n",
            "UTC\n",
            "ab95b07eeb30a98ec33aa2cb0c8d7929_JaffaCakes118 \n",
            " azorult infostealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-xfmtwswdrp\n",
            "14-06-2024 18:47\n",
            "UTC\n",
            "Malware with taskmgr.zip \n",
            " 0011 0e6740 @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer gh0strat phorphiex purplefox redline risepro tofsee xehook xworm infostealer loader persistence ransomware rat rootkit spyware stealer themida trojan upx worm \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zhy12sygjl\n",
            "14-06-2024 20:43\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zgtd6sveqf\n",
            "14-06-2024 20:41\n",
            "UTC\n",
            "build.exe \n",
            " 1 redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zfq8esyfjp\n",
            "14-06-2024 20:39\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zelxbavekb\n",
            "14-06-2024 20:37\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zdq5msvdqb\n",
            "14-06-2024 20:36\n",
            "UTC\n",
            "build.exe \n",
            " 1 redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-y5fmjsyblk\n",
            "14-06-2024 20:21\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-y3hdcsyanr\n",
            "14-06-2024 20:18\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yzc98sxhlp\n",
            "14-06-2024 20:13\n",
            "UTC\n",
            "ab43169d586372e2f42989aa10b89cce_JaffaCakes118 \n",
            " azorult infostealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yrcczaxemr\n",
            "14-06-2024 20:00\n",
            "UTC\n",
            "ab378a9483a51f89fe59858d6ad7e3c9_JaffaCakes118 \n",
            " collection hawkeye_reborn m00nd3v_logger infostealer keylogger spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yn3qraxdnr\n",
            "14-06-2024 19:56\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            "Highlights: [' duc evasion redline infostealer persistence themida trojan  f109e5691b8fedddeec53b85ced5c60cb3010b6219964b7f0262c5ebfd191242.exe   hackus_checker_cracked discovery redline infostealer spyware stealer  6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe   android collection discovery evasion tispy infostealer persistence spyware trojan ']\n",
            "Highlight Scores: [0.26290252804756165]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: From DarkGate to AsyncRAT: Malware Detected and Shared As Unit 42 Timely Threat Intelligence\n",
            "URL: https://unit42.paloaltonetworks.com/unit42-threat-intelligence-roundup/\n",
            "ID: https://unit42.paloaltonetworks.com/unit42-threat-intelligence-roundup/\n",
            "Score: 0.1658405363559723\n",
            "Published Date: 2023-12-29T00:00:00.000Z\n",
            "Author: Samantha Stallings, Brad Duncan\n",
            "Image: https://unit42.paloaltonetworks.com/wp-content/uploads/2023/12/Unit42-timely-threat-intelligence-23-illustration_Blue-wo-logo.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This post is also available in:\n",
            "  日本語   (Japanese)       Executive Summary\n",
            "This article summarizes the malware families (and groups pushing malware) seen by Unit 42 and shared with the broader threat hunting community through our social channels. Some malware – such as IcedID and DarkGate – came up repeatedly. We also included a number of posts about the cybercrime group TA577 – who have distributed multiple malware families but here favor Pikabot. In other cases, we posted about newer malware such as JinxLoader.\n",
            "By sharing timely threat intelligence via social media channels, we report on malware infections and other threat intelligence of note in an expedited manner. These posts summarize the infection chain, offer helpful screenshots of active traffic and point towards indicators of compromise (IoCs). In 2023, our 93 timely threat intelligence posts in total generated 1.6 million-plus impressions, showing the value of getting IoCs out to the community quickly.\n",
            "This article reviews all our timely threat intelligence released from October through December 2023. Summarizing these threat intelligence posts provides an opportunity to spot trends that are less visible in single posts. We’ve included a table in the Indicators of Compromise section that lists all the posts in full by date posted, name, links to social media channels and IoCs on GitHub.\n",
            "Many of these posts contain screenshots of infection traffic filtered in Wireshark, links to the network IoCs and comments linking to packet captures (pcaps) of the associated activity, so this article also provides readers an opportunity to practice and improve their Wireshark skills.\n",
            " The IoCs shared in the social posts are all considered malicious by Palo Alto Networks products. These verdicts are used, for example, by   cloud-delivered security services   such as   Advanced WildFire   and   Advanced URL Filtering   for the   Next-Generation Firewall  .  If you think you might have been compromised or have an urgent matter, contact the   Unit 42 Incident Response team  . \n",
            "To see our timely threat intelligence posts as we publish them, follow Unit 42 on X and LinkedIn.\n",
            " Related Unit 42 Topics \n",
            " Wireshark, Malware, RATs, Trojan \n",
            " Malware Families Mentioned \n",
            "DarkGate, Pikabot, IcedID, AsyncRAT, JinxLoader\n",
            "  Table of Contents\n",
            " Timely Threat Intelligence \n",
            "Timely Threat Intelligence: October \n",
            "DarkGate \n",
            "Pikabot \n",
            "IcedID (Bokbot) \n",
            "WS_FTP Server Critical Vulnerability \n",
            "AsyncRAT \n",
            "Citrix NetScaler \n",
            "Timely Threat Intelligence: November \n",
            "IcedID \n",
            "DarkGate \n",
            "Pikabot \n",
            "JinxLoader \n",
            "Timely Threat Intelligence: December \n",
            "Loader EXE \n",
            "DarkGate \n",
            "Astaroth/Guildma \n",
            "Pikabot \n",
            "Conclusion \n",
            "Protections and Mitigations \n",
            "Indicators of Compromise \n",
            "Additional Resources \n",
            "  Timely Threat Intelligence\n",
            "In addition to the in-depth articles published on this site, Unit 42 also shares timely threat intelligence – IoCs, TTPs and other observations about active campaigns – through our social channels.\n",
            "Unit 42 shared the first public post about JinxLoader. This information led at least one other vendor, ProofPoint’s Emerging Threats (ET) Labs team, to create a new signature triggering on traffic patterns generated by this malware.\n",
            "Besides recapping all social media posts published from October to December, we’ve included a table at the end of this review that includes links to the original posts as well as to all of the IoCs on our GitHub. These original posts include the images from the threat intelligence shared, which range from screen captures of malware and artifacts to the associated traffic filtered in Wireshark. Here we’ll only include the infection chain (as applicable), but head over to X (formerly Twitter) or LinkedIn to review the rest.\n",
            "A note: The infection date is not always the same as the date when shared on social channels. The table in the IoCs section includes the posting date. The infection date itself is included in all of the infection chain images. Don’t get confused if you’re comparing the infection date to the posting date and they don’t match up!\n",
            "  Timely Threat Intelligence: October\n",
            "  DarkGate\n",
            "We reported on two instances of DarkGate in October. The first instance was DarkGate malware distributed through Microsoft Teams. The attacker posed as the target organization's CEO and sent victims a Teams invite. The message sent contains a password-protected .zip archive. See the entire infection chain below in Figure 1.\n",
            " Figure 1. DarkGate infection chain from Microsoft Teams message. \n",
            "The second instance saw DarkGate malware distributed through fake invoice or billing emails with PDF attachments that spoof DocuSign. Figure 2 illustrates how the process worked. An attentive reader will be able to spot the differences between these two infection chains.\n",
            " Figure 2. DarkGate infection chain from email. \n",
            "  Pikabot\n",
            "We also reported on two instances of Pikabot in October. The first was a Pikabot infection leading to Cobalt Strike HTTPS C2 traffic using zzerxc[.]com on 179.60.149[.]244:443. Figure 3 shows the full series of events.\n",
            " Figure 3. Pikabot infection chain. \n",
            "The second instance saw the cybercrime threat actor TA577 pushing a Pikabot infection with HTTPS Cobalt Strike traffic on 45.155.249[.]171:443 using ponturded[.]com. We’ll definitely see more TA577 activity in this roundup – the infection chain below in Figure 4 will differ from other TA577 activity.\n",
            " Figure 4. Pikabot infection chain. \n",
            "  IcedID (Bokbot)\n",
            "Our only report in October of banking Trojan IcedID saw a forked variant infection with BackConnect, Anubis VNC, CobaltStrike and ConnectWise ScreenConnect. We also saw \"hands on the keyboard\" approximately 95 minutes after initial infection! Figure 5 lays out how this variant worked.\n",
            " Figure 5. IcedID forked variant infection chain. \n",
            "  WS_FTP Server Critical Vulnerability\n",
            "We observed multiple attempts to exploit the WS_FTP Server Critical Vulnerability, where threat actors attempted to deliver a Meterpreter payload via the URL 103[.]163.187.12:8080/cz3eKnhcaD0Fik7Eexo66A. Figure 6 includes not only the infection chain but the command line used.\n",
            " Figure 6. Infection chain exploiting WS_FTP. \n",
            "  AsyncRAT\n",
            "A 404 TDS URL chain led to an infection by an AsyncRAT variant. Figure 7 shows the simple infection chain.\n",
            " Figure 7. AsyncRAT variant infection chain. \n",
            "  Citrix NetScaler\n",
            "October 2023 saw several indicators of criminals exploiting the Citrix remote-code execution vulnerability CVE-2023-3519. Monitoring this vulnerability in the wild led to a timely snapshot of associated activity. Figure 8 displays the information in a simple column graph. We saw the most detections of this exploit – over 300 – from jscloud[.]biz.\n",
            " Figure 8. Snapshot of data from Oct. 17, 2023 showing instances of Citrix RCE vulnerability CVE-2023-3519 in the wild. \n",
            "  Timely Threat Intelligence: November\n",
            "  IcedID\n",
            "Our first timely threat intelligence post in November saw an IcedID (aka Bokbot) infection from an .msi file. Along with the regular HTTPS C2 traffic, we saw IcedID BackConnect activity on 159.89.124[.]188:443. Note the activity highlighted in red in Figure 9.\n",
            " Figure 9. IcedID infection chain stemming from unknown source. \n",
            "Cybercrime group TA577 once more distributed an IcedID (aka Bokbot) variant via a disk image downloaded from an emailed link. Figure 10 illustrates this process. As we continue to review TA577 activity, trends will begin to emerge for analysts to be mindful of.\n",
            " Figure 10. Infection chain where cybercrime group TA577 distributes an IcedID variant. \n",
            "  DarkGate\n",
            "At least two instances of DarkGate reared their heads in November. In our first sighting, a probable email led the victim to a password-protected .zip file. See it in full with Figure 11.\n",
            " Figure 11. DarkGate infection chain stemming from probable email. \n",
            "The second November appearance of DarkGate also came from an unknown source distributing a password-protected .zip. Comparing and contrasting to the DarkGate activity seen elsewhere in this post shows the variations attackers implement.\n",
            " Figure 12. DarkGate infection chain from unknown source.   \n",
            "  Pikabot\n",
            "A 10-hour infection run led to our list of IoCs from the Pikabot sighting found in November, once more spearheaded by TA577. This infection was from an email and led to a persistent Pikabot DLL. See Figure 13 for the details.\n",
            " Figure 13. Infection chain of Pikabot distributed by TA577. \n",
            "  JinxLoader\n",
            "Reportedly named for a League of Legends character, JinxLoader is written in Go. Symantec issued a protection bulletin on JinxLoader just a short while ago. In our post about it (the first public post!) we note that JinxLoader is a relatively new malware service first posted to hackforums[.]net on April 30, 2023. The eight steps of this infection chain are detailed in Figure 14.\n",
            " Figure 14. Infection chain of JinxLoader distributing Formbook/Xloader. \n",
            "  Timely Threat Intelligence: December\n",
            "  Loader EXE\n",
            "We started off December by spotting an EXE Loader leading to unidentified malware with C2 using encoded/encrypted TCP traffic on 91.92.120[.]119:62520. See Figure 15.\n",
            " Figure 15. Infection chain of loader to unidentified malware. \n",
            "While one security vendor has identified the loader as “PureLoader” and the unidentified malware as a “PureLogs” stealer, we have seen little else shared publicly on this unidentified malware.\n",
            "  DarkGate\n",
            "We reported on one example of DarkGate malware in December 2023. This example was distributed though a PDF file found on VirusTotal. The PDF file has a link that downloaded a malicious ZIP archive for DarkGate. Figure 16 shows a screenshot of the PDF file.\n",
            " Figure 16. DarkGate infection from PDF that links to malicious .zip archive. \n",
            "  Astaroth/Guildma\n",
            "Astaroth and Guildma may sound like characters from a 1970s pulp sci-fi novel with three moons and blue-skinned aliens on the cover, but they’re actually the name of malware we saw in a Portuguese-language email impersonating Brazil’s State Transport Department (Detran) as shown in Figure 17. This Detran-themed malspam tempted the end user with a link for a zip download – which is how the Guildma (aka Astaroth) malware infection would start.\n",
            " Figure 17. Screenshot of email written in Portuguese that links to malicious download. \n",
            "  Pikabot\n",
            "Our second-to-last report of TA577 in 2023 sees the group distributing Pikabot from an email link. Figure 18 shows the sequence. How does this differ from the previous entries about TA577?\n",
            " Figure 18. Infection chain of Pikabot pushed by threat group TA577. \n",
            "In the last of our threat intelligence shares for the year (barring additional incidents), we see that, once more, TA577 is spreading a Pikabot infection.\n",
            "In this instance, it led to Cobalt Strike on 207.246.99[.]159:443 using masterunis[.]net as its domain. Figure 19 dissects the traffic.\n",
            " Figure 19. Pikabot malware traffic shown in Wireshark. \n",
            "  Conclusion\n",
            " Much of our timely threat intelligence focuses on Windows malware, and we seek to post on malware families of current interest to the community. \n",
            "If you’re interested in following our updates in real time, follow us on LinkedIn or X (formerly Twitter). If you track hashtags, follow #Unit42ThreatIntel to always catch the latest posts. Another option is to sign up for notifications on our GitHub repo.\n",
            "As soon as you’re in the know, you’re also welcome to participate: comment, share or ask questions.\n",
            "  Protections and Mitigations\n",
            " The IoCs shared in the social posts are all considered malicious by Palo Alto Networks products. These verdicts are used, for example, by   cloud-delivered security services   such as   Advanced WildFire   and   Advanced URL Filtering   for the   Next-Generation Firewall  .  If you think you might have been compromised or have an urgent matter, contact the   Unit 42 Incident Response team  . \n",
            "  Indicators of Compromise\n",
            " Date Posted \n",
            " Infection \n",
            " Links \n",
            " IoCs \n",
            " 10/03/2023 \n",
            " WS_FTP vulnerability \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            " IoC available in the posts  \n",
            " 10/03/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/12/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/17/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/18/2023 \n",
            " RCE affecting Citrix NetScaler in the wild \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/20/2023 \n",
            " IcedID  \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/23/2023 \n",
            " AsyncRAT \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/25/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/01/2023 \n",
            " IcedID \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/03/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/21/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/28/2023 \n",
            " IcedID variant \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/30/2023 \n",
            " JinxLoader \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/30/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/06/2023 \n",
            " Loader EXE leads to unidentified malware \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/07/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/12/2023 \n",
            " Astaroth/Guildma \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/15/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/18/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            "   \n",
            "Additional Resources\n",
            " Malware Traffic Analysis.net \n",
            " Wireshark Tutorials – Unit 42, Palo Alto Networks\n",
            " IoCs of Unit 42 Timely Threat Intelligence – GitHub\n",
            " SID 2049408 by Emerging Threats Labs – X (Twitter)\n",
            " New Malvertising Campaign Distributing PikaBot Disguised as Popular Software – The Hacker News\n",
            "Get updates from  Palo Alto Networks!\n",
            "Sign up to receive the latest news, cyber threat intelligence and research from us\n",
            "Highlights: ['Symantec issued a protection bulletin on JinxLoader just a short while ago. In our post about it (the first public post! ) we note that JinxLoader is a relatively new malware service first posted to hackforums[. ]net on April 30, 2023. The eight steps of this infection chain are detailed in Figure 14.']\n",
            "Highlight Scores: [0.4174140393733978]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: malware-samples/binaries/fickerstealer/2020/November at master · jstrosch/malware-samples\n",
            "URL: https://github.com/jstrosch/malware-samples/tree/master/binaries/fickerstealer/2020/November\n",
            "ID: https://github.com/jstrosch/malware-samples/tree/master/binaries/fickerstealer/2020/November\n",
            "Score: 0.16380491852760315\n",
            "Published Date: 2020-11-24T00:00:00.000Z\n",
            "Author: jstrosch\n",
            "Image: https://opengraph.githubassets.com/29e8b6e716b574378f216102ae3b0b2cc0cd23f0c62133c6d38055d6b48430f8/jstrosch/malware-samples\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: FickerStealer w/ activity and IP check    \n",
            "Trojan (MD5): 1db6bd4d13cb9966e8875b3812aef71d.bin\n",
            "PCAP: b0f2d519ccae5bf1db6bd4d13cb9966e8875b3812aef71d1435264e0979770ce.pcap\n",
            "See the README for information about the archive password.\n",
            "Analysis source: Cuckoo 2.0.7\n",
            "Date: 11/24/2020\n",
            "This sample highlights FickerStealer activity with IP check.\n",
            "  Process Activity    \n",
            "   \n",
            "Process activity\n",
            "  Network Activity    \n",
            "   \n",
            "Command and Control activity\n",
            "   \n",
            "IDS alerts generated by Suricata\n",
            "Highlights: ['Trojan (MD5): 1db6bd4d13cb9966e8875b3812aef71d.bin PCAP: b0f2d519ccae5bf1db6bd4d13cb9966e8875b3812aef71d1435264e0979770ce.pcap See the README for information about the archive password. This sample highlights FickerStealer activity with IP check.']\n",
            "Highlight Scores: [0.06578176468610764]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Nefilim (Malware Family)\n",
            "URL: https://malpedia.caad.fkie.fraunhofer.de/details/win.nefilim\n",
            "ID: https://malpedia.caad.fkie.fraunhofer.de/details/win.nefilim\n",
            "Score: 0.16228768229484558\n",
            "Published Date: 2022-03-17T00:00:00.000Z\n",
            "Author: Fraunhofer FKIE\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: According to Vitali Kremez and Michael Gillespie, this ransomware shares much code with Nemty 2.5. A difference is removal of the RaaS component, which was switched to email communications for payments. Uses AES-128, which is then protected RSA2048.\n",
            "Highlights: ['According to Vitali Kremez and Michael Gillespie, this ransomware shares much code with Nemty 2.5. A difference is removal of the RaaS component, which was switched to email communications for payments. Uses AES-128, which is then protected RSA2048.']\n",
            "Highlight Scores: [0.2974186837673187]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here are some details on currently active Infostealer malware, including their TTPs and IOCs:\n",
            "ERROR in Exa Search: name 'exa_py' is not defined\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.73\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://flashpoint.io/blog/protecting-against-infostealer-malware/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[ERROR] 🚫 arun(): Failed to crawl https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png, error: [ERROR] 🚫 crawl(): Failed to crawl https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png: Page.goto: net::ERR_ABORTED at https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png\n",
            "Call log:\n",
            "navigating to \"https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2024/06/risepro-fig01c.png\n",
            "[LOG] 🕸️ Crawling https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E, success: True, time taken: 5.79 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.cisecurity.org/-/media/project/cisecurity/cisecurity/data/media/img/insights_images/blog_post_img/2024/05/top-10-malware-(4).png?rev=be940dd893ba43b09b3a8652bffef9da&hash=E36AC24CE1686D27B219D15DBB0FAA9E, time taken: 0.00 seconds.\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp successfully!\n",
            "[LOG] 🚀 Crawling done for https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp, success: True, time taken: 7.08 seconds\n",
            "[LOG] 🚀 Content extracted for https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp, success: True, time taken: 0.00 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://cdn.prod.website-files.com/626ff19cdd07d1258d49238d/64d3b3c501dad065b00001fe_SOC-Threat%20Research.webp, time taken: 0.00 seconds.\n",
            "[ERROR] 🚫 arun(): Failed to crawl https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/, error: [ERROR] 🚫 crawl(): Failed to crawl https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/: Page.goto: net::ERR_HTTP2_PROTOCOL_ERROR at https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/\n",
            "Call log:\n",
            "navigating to \"https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/\n",
            "[LOG] ✅ Crawled https://flashpoint.io/blog/protecting-against-infostealer-malware/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://flashpoint.io/blog/protecting-against-infostealer-malware/, success: True, time taken: 10.59 seconds\n",
            "[LOG] 🚀 Content extracted for https://flashpoint.io/blog/protecting-against-infostealer-malware/, success: True, time taken: 0.39 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://flashpoint.io/blog/protecting-against-infostealer-malware/, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://flashpoint.io/blog/protecting-against-infostealer-malware/, time taken: 0.47 seconds.\n",
            "[LOG] ✅ Crawled https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/, success: True, time taken: 8.95 seconds\n",
            "[LOG] 🚀 Content extracted for https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/, success: True, time taken: 0.37 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://cyberint.com/blog/research/the-lumma-stealer-infostealer-the-details/, time taken: 0.47 seconds.\n",
            "[LOG] ✅ Crawled https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer successfully!\n",
            "[LOG] 🚀 Crawling done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 11.83 seconds\n",
            "[LOG] 🚀 Content extracted for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 0.23 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, time taken: 0.31 seconds.\n",
            "[LOG] ✅ Crawled https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer successfully!\n",
            "[LOG] 🚀 Crawling done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 11.99 seconds\n",
            "[LOG] 🚀 Content extracted for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 0.23 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, time taken: 0.31 seconds.\n",
            "[LOG] ✅ Crawled https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer successfully!\n",
            "[LOG] 🚀 Crawling done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 11.42 seconds\n",
            "[LOG] 🚀 Content extracted for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, success: True, time taken: 0.24 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://darktrace.com/blog/the-rise-of-the-lumma-info-stealer, time taken: 0.33 seconds.\n",
            "[LOG] ✅ Crawled https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, success: True, time taken: 13.78 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, success: True, time taken: 0.10 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, Strategy: AsyncWebCrawler\n",
            "[LOG] 🚀 Extraction done for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, time taken: 0.13 seconds.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cdb100e399d2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Share some details on currently active Infostealer malware and give me their TTPs and IOCs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-4f9aad4ce31b>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query, memory)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m                     \u001b[0mmanager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                 ):\n\u001b[0;32m-> 1336\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mrun_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-4f9aad4ce31b>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcrawled_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mformatted_crawled_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Content: {item['content']}\\nLinks: {item['links']}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'content'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIp1L2bcInA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7mH99gGInD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VWtEsGIInHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}