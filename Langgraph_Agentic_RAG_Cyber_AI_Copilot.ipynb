{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "233f30cd-5ab1-458e-8b3c-9cb175f568ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "2024-12-09 06:51:28.620092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-09 06:51:28.671456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-09 06:51:28.688776: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-09 06:51:30.664880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from langgraph.graph import StateGraph, END\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import math\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY = \"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID = \"63053004a7e2445c3\"\n",
        "TAVILY_API_KEY = \"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ad5dfe-6b57-4234-e381-fced14058692"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    # Query expansion with related terms and synonyms\n",
        "    related_terms = get_related_terms(query)\n",
        "    if related_terms:\n",
        "        enhanced_query += f\" related_terms:{', '.join(related_terms)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "def get_related_terms(query: str) -> List[str]:\n",
        "    # Use an ontology or knowledge graph to identify related concepts and terms\n",
        "    related_terms = {\n",
        "        \"cyber attack\": [\"hacking\", \"data breach\", \"malware\", \"ransomware\"],\n",
        "        \"threat actor\": [\"cyber gang\", \"hacker group\", \"APT\"],\n",
        "        \"vulnerability\": [\"exploit\", \"CVE\", \"security flaw\"],\n",
        "        \"phishing\": [\"spear phishing\", \"email scam\", \"social engineering\"],\n",
        "        # Add more related terms as needed\n",
        "    }\n",
        "\n",
        "    # Find related terms for the query\n",
        "    query_terms = query.lower().split()\n",
        "    found_terms = []\n",
        "    for term in query_terms:\n",
        "        if term in related_terms:\n",
        "            found_terms.extend(related_terms[term])\n",
        "\n",
        "    return found_terms\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            print(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "def highlight_keywords(text: str, keywords: List[str]) -> str:\n",
        "    \"\"\"Highlight specific keywords in the text.\"\"\"\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, f\"**{keyword}**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "       - **Actionable Recommendations**:\n",
        "         - Offer precise, immediate actions and mitigation strategies.\n",
        "         - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "         - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    5. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor capabilities, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    **Highlighted Keywords**:\n",
        "    - **Threat Actor Group**\n",
        "    - **Cyber Gangs**\n",
        "    - **City**\n",
        "    - **Countries**\n",
        "    - **Geo-specific**\n",
        "    - **Malware**\n",
        "    - **Ransomware**\n",
        "    - **Vulnerability**\n",
        "    - **Exploit**\n",
        "    - **Phishing**\n",
        "    - **Data Breach**\n",
        "    - **Cyber Attack**\n",
        "    - **Incident Response**\n",
        "    - **MITRE ATT&CK**\n",
        "    - **Indicators of Compromise (IOCs)**\n",
        "    - **Command and Control (C2)**\n",
        "    - **Dates**\n",
        "    - **Times**\n",
        "    - **Trojans**\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {highlight_keywords(result.snippet, ['Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific', 'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing', 'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK', 'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates', 'Times', 'Trojans'])}\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Highlight important information\n",
        "    important_keywords = [\n",
        "        'Threat Actor Group', 'Cyber Gangs', 'City', 'Countries', 'Geo-specific',\n",
        "        'Malware', 'Ransomware', 'Vulnerability', 'Exploit', 'Phishing',\n",
        "        'Data Breach', 'Cyber Attack', 'Incident Response', 'MITRE ATT&CK',\n",
        "        'Indicators of Compromise (IOCs)', 'Command and Control (C2)', 'Dates',\n",
        "        'Times', 'Trojans'\n",
        "    ]\n",
        "    highlighted_response = highlight_keywords(processed_response, important_keywords)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": highlighted_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {highlighted_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result\n",
        "\n",
        "# Named Entity Recognition (NER) for entity extraction\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "def extract_entities(query: str) -> Dict[str, List[str]]:\n",
        "    ner_results = ner_pipeline(query)\n",
        "    entities = {\n",
        "        \"threat_actors\": [],\n",
        "        \"locations\": [],\n",
        "        \"organizations\": [],\n",
        "        \"dates\": []\n",
        "    }\n",
        "\n",
        "    for entity in ner_results:\n",
        "        entity_text = entity['word'].lower()\n",
        "        entity_label = entity['entity'].lower()\n",
        "\n",
        "        if \"threat\" in entity_label or \"actor\" in entity_label:\n",
        "            entities[\"threat_actors\"].append(entity_text)\n",
        "        elif \"location\" in entity_label or \"geo\" in entity_label:\n",
        "            entities[\"locations\"].append(entity_text)\n",
        "        elif \"organization\" in entity_label:\n",
        "            entities[\"organizations\"].append(entity_text)\n",
        "        elif \"date\" in entity_label:\n",
        "            entities[\"dates\"].append(entity_text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Enhanced query rewriting with entity extraction and variations\n",
        "def enhance_search_query_with_entities(query: str) -> str:\n",
        "    entities = extract_entities(query)\n",
        "    enhanced_query = query\n",
        "\n",
        "    if entities[\"threat_actors\"]:\n",
        "        enhanced_query += f\" threat_actors:{', '.join(entities['threat_actors'])}\"\n",
        "    if entities[\"locations\"]:\n",
        "        enhanced_query += f\" locations:{', '.join(entities['locations'])}\"\n",
        "    if entities[\"organizations\"]:\n",
        "        enhanced_query += f\" organizations:{', '.join(entities['organizations'])}\"\n",
        "    if entities[\"dates\"]:\n",
        "        enhanced_query += f\" dates:{', '.join(entities['dates'])}\"\n",
        "\n",
        "    # Add variations based on the most important entities\n",
        "    important_entities = entities[\"threat_actors\"] + entities[\"locations\"] + entities[\"organizations\"]\n",
        "    if important_entities:\n",
        "        enhanced_query += f\" OR {' OR '.join(important_entities)}\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "# General query optimization\n",
        "def optimize_query_for_variables(query: str) -> str:\n",
        "    # Extract keywords and related terms\n",
        "    keywords = extract_keywords(query)\n",
        "    related_terms = get_related_terms(query)\n",
        "\n",
        "    # Enhance the query with keywords and related terms\n",
        "    enhanced_query = f\"{query} {', '.join(keywords)} {', '.join(related_terms)}\"\n",
        "\n",
        "    # Add date and geo-location specific terms\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query += f\" 2024 OR {current_year} recent threat actor groups gangs companies locations\"\n",
        "\n",
        "    return enhanced_query\n",
        "\n",
        "# Improved concurrency\n",
        "async def run_concurrent_tasks(tasks):\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    return results\n",
        "\n",
        "def advanced_entity_query_expansion(query: str, entities: Dict[str, List[str]]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate advanced query variations based on extracted entities\n",
        "\n",
        "    Args:\n",
        "        query (str): Original query\n",
        "        entities (dict): Extracted named entities\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Expanded query variations\n",
        "    \"\"\"\n",
        "    query_variations = []\n",
        "\n",
        "    # Base variations\n",
        "    base_variations = [\n",
        "        query,\n",
        "        f\"recent {query}\",\n",
        "        f\"latest developments {query}\",\n",
        "        f\"cybersecurity analysis {query}\"\n",
        "    ]\n",
        "\n",
        "    # Entity-specific expansions\n",
        "    if entities.get(\"threat_actors\"):\n",
        "        for actor in entities[\"threat_actors\"]:\n",
        "            base_variations.extend([\n",
        "                f\"{actor} cyber incidents\",\n",
        "                f\"{actor} threat intelligence\",\n",
        "                f\"recent activities of {actor}\",\n",
        "                f\"cyber operations by {actor}\"\n",
        "            ])\n",
        "\n",
        "    if entities.get(\"locations\"):\n",
        "        for location in entities[\"locations\"]:\n",
        "            base_variations.extend([\n",
        "                f\"cyber threats in {location}\",\n",
        "                f\"{location} cybersecurity landscape\",\n",
        "                f\"threat actor activities {location}\"\n",
        "            ])\n",
        "\n",
        "    if entities.get(\"organizations\"):\n",
        "        for org in entities[\"organizations\"]:\n",
        "            base_variations.extend([\n",
        "                f\"{org} cyber defense\",\n",
        "                f\"cyber incidents affecting {org}\",\n",
        "                f\"{org} threat intelligence\"\n",
        "            ])\n",
        "\n",
        "    # Advanced query transformations\n",
        "    query_variations.extend([\n",
        "        f'intitle:\"{query}\"',  # Titles containing exact phrase\n",
        "        f'inurl:{query.replace(\" \", \"-\").lower()}',  # URL-friendly version\n",
        "        f'\"{query}\" cybersecurity',  # Exact phrase with context\n",
        "        f'site:*.gov {query}',  # Government sources\n",
        "        f'site:*.mil {query}',  # Military sources\n",
        "        f'site:*.org {query}',  # Organization sources\n",
        "    ])\n",
        "\n",
        "    return list(set(base_variations + query_variations))\n",
        "\n",
        "def advanced_semantic_query_expansion(query: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate semantically related query variations using advanced NLP techniques\n",
        "\n",
        "    Args:\n",
        "        query (str): Original query\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Semantically expanded queries\n",
        "    \"\"\"\n",
        "    # Use sentence transformer for semantic similarity\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    semantic_variations = [\n",
        "        # Cybersecurity domain-specific variations\n",
        "        \"threat intelligence for \" + query,\n",
        "        \"cyber incidents related to \" + query,\n",
        "        \"security analysis of \" + query,\n",
        "        \"emerging cyber threats with \" + query,\n",
        "\n",
        "        # Technical variations\n",
        "        \"IOCs associated with \" + query,\n",
        "        \"MITRE ATT&CK tactics for \" + query,\n",
        "        \"vulnerability landscape of \" + query\n",
        "    ]\n",
        "\n",
        "    # Advanced semantic embeddings and similarity\n",
        "    query_embedding = model.encode(query)\n",
        "    semantic_scores = []\n",
        "\n",
        "    # You would integrate with a large corpus or knowledge base here\n",
        "    # For demonstration, using a mock corpus\n",
        "    mock_corpus = [\n",
        "        \"Cyber threat intelligence\",\n",
        "        \"Advanced persistent threats\",\n",
        "        \"Cybersecurity incident response\",\n",
        "        \"Threat actor methodologies\"\n",
        "    ]\n",
        "\n",
        "    for corpus_text in mock_corpus:\n",
        "        corpus_embedding = model.encode(corpus_text)\n",
        "        similarity = util.pytorch_cos_sim(query_embedding, corpus_embedding)[0][0].item()\n",
        "\n",
        "        if similarity > 0.5:  # Adjust threshold as needed\n",
        "            semantic_variations.append(f\"{corpus_text} {query}\")\n",
        "\n",
        "    return list(set(semantic_variations))\n",
        "\n",
        "def advanced_time_based_query_filtering(results: List[SearchResult], max_age_days: int = 180) -> List[SearchResult]:\n",
        "    \"\"\"\n",
        "    Filter search results based on recency and time-based relevance\n",
        "\n",
        "    Args:\n",
        "        results (List[SearchResult]): Original search results\n",
        "        max_age_days (int): Maximum age of results to consider\n",
        "\n",
        "    Returns:\n",
        "        List[SearchResult]: Filtered and time-weighted results\n",
        "    \"\"\"\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    filtered_results = []\n",
        "\n",
        "    for result in results:\n",
        "        result_date = parse_date(result.date) or current_date\n",
        "        days_since_publication = (current_date - result_date).days\n",
        "\n",
        "        # Apply exponential decay for time relevance\n",
        "        if days_since_publication <= max_age_days:\n",
        "            time_weight = math.exp(-0.1 * days_since_publication)\n",
        "            result.final_score = (result.final_score or 0) * time_weight\n",
        "            filtered_results.append(result)\n",
        "\n",
        "    return sorted(filtered_results, key=lambda x: x.final_score, reverse=True)\n",
        "\n",
        "def advanced_threat_intelligence_scoring(results: List[SearchResult]) -> List[SearchResult]:\n",
        "    \"\"\"\n",
        "    Score and rank results based on threat intelligence relevance\n",
        "\n",
        "    Args:\n",
        "        results (List[SearchResult]): Search results\n",
        "\n",
        "    Returns:\n",
        "        List[SearchResult]: Scored and ranked results\n",
        "    \"\"\"\n",
        "    threat_keywords = [\n",
        "        'threat actor', 'cyber attack', 'malware',\n",
        "        'vulnerability', 'exploit', 'IOC',\n",
        "        'command and control', 'data breach'\n",
        "    ]\n",
        "\n",
        "    for result in results:\n",
        "        # Calculate threat intelligence score\n",
        "        threat_score = sum(\n",
        "            1.5 if keyword in result.snippet.lower() else\n",
        "            1.0 if keyword in result.title.lower() else 0\n",
        "            for keyword in threat_keywords\n",
        "        )\n",
        "\n",
        "        # Adjust final score based on threat intelligence relevance\n",
        "        result.final_score = (result.final_score or 0) + threat_score\n",
        "\n",
        "    return sorted(results, key=lambda x: x.final_score, reverse=True)\n",
        "\n",
        "def comprehensive_query_optimization(query: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Comprehensive query optimization with multi-level processing\n",
        "\n",
        "    Args:\n",
        "        query (str): Original search query\n",
        "\n",
        "    Returns:\n",
        "        Dict containing optimized query details\n",
        "    \"\"\"\n",
        "    entities = extract_entities(query)\n",
        "\n",
        "    return {\n",
        "        \"original_query\": query,\n",
        "        \"entities\": entities,\n",
        "        \"query_variations\": advanced_entity_query_expansion(query, entities),\n",
        "        \"semantic_variations\": advanced_semantic_query_expansion(query),\n",
        "        \"optimized_query\": enhance_search_query_with_entities(query)\n",
        "    }"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL",
        "outputId": "f3f13a04-e68c-40a3-d986-c8f75ca088a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents on LunarsGo Threat Actor?\"\n",
        "    enhanced_query = enhance_search_query_with_entities(query)\n",
        "    print(f\"Enhanced Query: {enhanced_query}\")\n",
        "    result = asyncio.run(run_agent(enhanced_query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0cab977-03f7-4f21-edf4-56502669ab3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Query: Latest Cyber Incidents on LunarsGo Threat Actor?\n",
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents on LunarsGo Threat Actor? 2024 OR 2024 recent threat actor groups gangs companies locations\n",
            "ERROR in Tavily Search: 'str' object has no attribute 'get'\n",
            "DEBUG: Raw results from Exa Search: Title: Flashpoint Year In Review: 2022 Cryptocurrency Threat Landscape\n",
            "URL: https://www.oodaloop.com/technology/2022/12/22/flashpoint-year-in-review-2022-cryptocurrency-threat-landscape/\n",
            "ID: https://www.oodaloop.com/technology/2022/12/22/flashpoint-year-in-review-2022-cryptocurrency-threat-landscape/\n",
            "Score: 0.17804977297782898\n",
            "Published Date: 2023-02-06T20:38:14.000Z\n",
            "Author: OODA Analyst\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Bitcoin remained the most-discussed crypto in the threat actor community and the most-used crypto for accepting illicit payments this year. Flashpoint analysts identified over 50,000 unique Bitcoin addresses circulating in Flashpoint collections in 2022. Flashpoint observed 125,513 mentions of Bitcoin addresses within our collections since January 1, with 54,629 distinct addresses. These addresses have transacted on the blockchain 20,621 times during 2022. Throughout 2022, threat actors committed fraud targeting cryptocurrency entities, investors, and users. Centralized exchanges (CEXs) and nonfungible token (NFT) markets were the primary targets of fraud schemes over the past year. CEXs are exchange platforms that allow users to buy and sell crypto and function as an intermediary service between buyers and sellers of digital currencies. Decentralized exchanges (DEXs), in contrast, do not use intermediaries to execute crypto asset exchanges, and instead facilitate trades through self-executing smart contracts. Throughout 2022, automatic transfer system kits, fraudulent verified accounts, one-time password bypasses, and account checkers all represented major threats to CEXs. The largest threats to NFT markets were account takeover (ATO) attacks, third-party compromises, spoofed pages, and various scams. In general, the NFT fraud landscape grew dramatically from 2021 to 2022. Threat actors increasingly leveraged the emerging technology of NFTs to steal from inexperienced users unfamiliar with the platforms or general best security practices. Full report : Flashpoint Year In Review: 2022 Cryptocurrency Threat Landscape.  At the doorstep of 2023, we can say that 2022 was the worst year for cryptocurrencies, NFTs, Defi, etc. If you want to have a comprehensive look at all the Web3 attacks that have been happening over the years, you can check out our comprehensive Web3 incident database based on our research to categorize what compromises are taking place as well as document the root causes that plague Cryptos, DeFi, NFTs, and Web3 in general. Tracking root causes provides comprehensive insights into how innovators can create robust cyber risk management approaches and reduce the potential for consequential attacks. You can access the OODA comprehensive Crypto Incident tracker here. ||||I|||| * Understand tomorrow, today.\n",
            "* Home\n",
            "* OODA Analysis\n",
            "* News Briefs\n",
            "+ Cyber\n",
            "+ Technology\n",
            "+ Global Risk\n",
            "+ Business\n",
            "* Join OODA Loop\n",
            "* About OODA Loop\n",
            "+ About OODA Loop\n",
            "+ Consulting\n",
            "+ Contact\n",
            "* Member Menu\n",
            "+ Member Menu\n",
            "+ Sign in\n",
            "+ The Forum\n",
            "+ Explore OODA Research and Analysis\n",
            "+ The OODA Community\n",
            "+ OODAcast Video and Podcast Series\n",
            "+ OODA C-Suite Report\n",
            "+ Cryptocurrency Incident Database\n",
            "*\n",
            "*\n",
            "* Home\n",
            "* OODA Analysis\n",
            "* News Briefs\n",
            "+ Cyber\n",
            "+ Technology\n",
            "+ Global Risk\n",
            "+ Business\n",
            "* Join OODA Loop\n",
            "* About OODA Loop\n",
            "+ About OODA Loop\n",
            "+ Consulting\n",
            "+ Contact\n",
            "* Member Menu\n",
            "+ Member Menu\n",
            "+ Sign in\n",
            "+ The Forum\n",
            "+ Explore OODA Research and Analysis\n",
            "+ The OODA Community\n",
            "+ OODAcast Video and Podcast Series\n",
            "+ OODA C-Suite Report\n",
            "+ Cryptocurrency Incident Database\n",
            "Technology\n",
            "Flashpoint Year In Review: 2022 Cryptocurrency Threat Landscape\n",
            "OODA Analyst 2022-12-22\n",
            "22 Dec 2022 OODA Analyst\n",
            "Share Tweet Post Reddit\n",
            "Bitcoin remained the most-discussed crypto in the threat actor community and the most-used crypto for accepting illicit payments this year. Flashpoint analysts identified over 50,000 unique Bitcoin addresses circulating in Flashpoint collections in 2022. Flashpoint observed 125,513 mentions of Bitcoin addresses within our collections since January 1, with 54,629 distinct addresses. These addresses have transacted on the blockchain 20,621 times during 2022. Throughout 2022, threat actors committed fraud targeting cryptocurrency entities, investors, and users. Centralized exchanges (CEXs) and nonfungible token (NFT) markets were the primary targets of fraud schemes over the past year. CEXs are exchange platforms that allow users to buy and sell crypto and function as an intermediary service between buyers and sellers of digital currencies. Decentralized exchanges (DEXs), in contrast, do not use intermediaries to execute crypto asset exchanges, and instead facilitate trades through self-executing smart contracts. Throughout 2022, automatic transfer system kits, fraudulent verified accounts, one-time password bypasses, and account checkers all represented major threats to CEXs. The largest threats to NFT markets were account takeover (ATO) attacks, third-party compromises, spoofed pages, and various scams. In general, the NFT fraud landscape grew dramatically from 2021 to 2022. Threat actors increasingly leveraged the emerging technology of NFTs to steal from inexperienced users unfamiliar with the platforms or general best security practices.\n",
            "Full report : Flashpoint Year In Review: 2022 Cryptocurrency Threat Landscape.\n",
            "At the doorstep of 2023, we can say that 2022 was the worst year for cryptocurrencies, NFTs, Defi, etc. If you want to have a comprehensive look at all the Web3 attacks that have been happening over the years, you can check out our comprehensive Web3 incident database based on our research to categorize what compromises are taking place as well as document the root causes that plague Cryptos, DeFi, NFTs, and Web3 in general. Tracking root causes provides comprehensive insights into how innovators can create robust cyber risk management approaches and reduce the potential for consequential attacks. You can access the OODA comprehensive Crypto Incident tracker here.\n",
            "Tags: 2022Crypto FraudsCrypto HackingCrypto ScamscryptocurrencyDeFiDeFi HacksNFTsreport\n",
            "OODA Analyst\n",
            "OODA is comprised of a unique team of international experts capable of providing advanced intelligence and analysis, strategy and planning support, risk and threat management, training, decision support, crisis response, and security services to global corporations and governments.\n",
            "You Might Also Like\n",
            "Orion Protocol Hacked for $3 Million Through Reentrancy Attack\n",
            "February 6, 2023\n",
            "Crypto’s Decentralized Finance Security Problem\n",
            "February 6, 2023\n",
            "OODA Analysis\n",
            "Designing, Quantifying, and Measuring Exponential Innovation\n",
            "The Chinese IoT Threat: CBS News Features OODA Network Expert Charlie Parton\n",
            "The free OODA Daily Pulse Report provides a detailed summary of the top cybersecurity, technology, and global risk stories of the day. Subscribe today!\n",
            "OODA Briefs\n",
            "China says it ‘reserves the right’ to deal with ‘similar situations’ after US jets shoot down suspected spy balloon\n",
            "Huge quake toppled buildings in Turkey and Syria as people slept\n",
            "US Man Charged in $110m Crypto Trading Scheme\n",
            "UK Car Retailer Arnold Clark Hit by Ransomware\n",
            "Spain, Morocco seek reset of testy relationship at Rabat summit\n",
            "US tracking suspected Chinese surveillance balloon\n",
            "Israel and Sudan finalize text of peace agreement, says Israeli foreign minister\n",
            "City of London on High Alert After Ransomware Attack\n",
            "Are You In the Loop?\n",
            "An OODA Loop membership gives you access to all premium content, risk intelligence reporting, and other special resources. Members also help support the production of targeted analysis and the daily curated OSINT.\n",
            "Free Daily OSINT Report\n",
            "Each day our analysts hand-curate the top OSINT stories of the day from hundreds of international sources. The free Daily OSINT report is delivered to your inbox at 10:00 a.m. daily.SUBSCRIBE TO DAILY OSINT\n",
            "Copyright © - All Rights Reserved.\n",
            "* About OODA Loop\n",
            "* Terms of Use\n",
            "* General Inquiry\n",
            "* Get In The Loop – Join Today!\n",
            "* Podcast\n",
            "SHARE\n",
            "Flashpoint Year In Review: 2022 Cryptocurrency Threat Landscape\n",
            "Highlights: ['At the doorstep of 2023, we can say that 2022 was the worst year for cryptocurrencies, NFTs, Defi, etc. If you want to have a comprehensive look at all the Web3 attacks that have been happening over the years, you can check out our comprehensive Web3 incident database based on our research to categorize what compromises are taking place as well as document the root causes that plague Cryptos, DeFi, NFTs, and Web3 in general. Tracking root causes provides comprehensive insights into how innovators can create robust cyber risk management approaches and reduce the potential for consequential attacks. You can access the OODA comprehensive Crypto Incident tracker here. ||||I|||| * Understand tomorrow, today.']\n",
            "Highlight Scores: [0.44027823209762573]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Weekly Threat Briefs | FortiGuard Labs\n",
            "URL: https://www.fortiguard.com/resources/threat-brief/2022/12/09/fortiguard-threat-intelligence-brief-december-09-2022\n",
            "ID: https://www.fortiguard.com/resources/threat-brief/2022/12/09/fortiguard-threat-intelligence-brief-december-09-2022\n",
            "Score: 0.17275412380695343\n",
            "Published Date: 2022-12-09T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://filestore.fortinet.com/fortiguard/static/images/fortiguard-logo-dark-theme.svg?v=27986\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: FortiGuard Threat Intelligence Brief - December 09, 2022\n",
            "Highlights: ['FortiGuard Threat Intelligence Brief - December 09, 2022']\n",
            "Highlight Scores: [0.08303090184926987]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: 1-15 November 2021 Cyber Attacks Timeline\n",
            "URL: https://www.hackmageddon.com/2021/12/16/1-15-november-2021-cyber-attacks-timeline/\n",
            "ID: https://www.hackmageddon.com/2021/12/16/1-15-november-2021-cyber-attacks-timeline/\n",
            "Score: 0.17003273963928223\n",
            "Published Date: 2021-12-16T00:00:00.000Z\n",
            "Author: Post author:Paolo Passeri\n",
            "Image: https://i0.wp.com/www.hackmageddon.com/wp-content/uploads/2021/12/network-6869734_1280.webp?fit=1280%2C853&ssl=1\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Donate  \n",
            " About \n",
            " Timelines \n",
            " Statistics \n",
            " Cloud-Native Threats \n",
            " Data Breaches \n",
            " Leaky Buckets \n",
            " Infographics \n",
            " Submit \n",
            "  Toggle website search   \n",
            " \n",
            "   Views: 9,387\n",
            " Related Posts \n",
            " Tags:  Android , Babuk , CVE-2021-1048 , CVE-2021-35211 , CVE-2021-4053 , Cyber Attacks , Cyber Crime , Cyber Espionage , Cyber Warfare , DEV-0322 , Hacktivism , Kimsuky , Lazarus Group , Lyceum , PhoneSpy , ProxyShell , Ransomware , Robinhood , Solarwinds , Timeline , Tortilla , Zoho  \n",
            "Read more articles\n",
            " Leave a Reply  \n",
            " \n",
            "This site uses Akismet to reduce spam. Learn how your comment data is processed.\n",
            "Highlights: [' Tags:  Android , Babuk , CVE-2021-1048 , CVE-2021-35211 , CVE-2021-4053 , Cyber Attacks , Cyber Crime , Cyber Espionage , Cyber Warfare , DEV-0322 , Hacktivism , Kimsuky , Lazarus Group , Lyceum , PhoneSpy , ProxyShell , Ransomware , Robinhood , Solarwinds , Timeline , Tortilla , Zoho   This site uses Akismet to reduce spam. Learn how your comment data is processed.']\n",
            "Highlight Scores: [0.0870533362030983]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Cyber Threat Intelligence Report: December 13th to December 20th, 2023\n",
            "URL: https://krypt3ia.wordpress.com/2023/12/20/cyber-threat-intelligence-report-december-13th-to-december-20th-2023/\n",
            "ID: https://krypt3ia.wordpress.com/2023/12/20/cyber-threat-intelligence-report-december-13th-to-december-20th-2023/\n",
            "Score: 0.16971971094608307\n",
            "Published Date: 2023-12-20T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://krypt3ia.wordpress.com/wp-content/uploads/2023/12/screenshot-2023-12-20-at-3.27.48e280afpm.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This threat intelligence report was created in tandem with ChatGPT4 by Scot Terban using the Icebreaker Threat Intelligence Analyst created by Scot Terban \n",
            "    \n",
            " Threat Actors and Activities: \n",
            " Ransomware Attacks: Ransomware remains a primary threat, with about 2,000 ransomware breach events reported in the first half of 2023. LockBit 3.0 was particularly impactful, accounting for over 500 breaches​ ​.\n",
            " Pro-Russian Hacktivism: Due to the Russia-Ukraine conflict, pro-Russian hacktivism has been prominent, although its activity declined in the second quarter of 2023​ ​.\n",
            " Access Sales: Over 2,000 instances were observed where access vendors offered to sell compromised credentials and unauthorized network or system access​ ​.\n",
            " AI and Law Enforcement Operations: An increase in discussions and activities related to artificial intelligence and law enforcement operations was noted, alongside a decrease in activities related to dump shops, ATM malware, and PoS malware​ ​.\n",
            " Recent Incidents: \n",
            " MongoDB Security Breach: MongoDB disclosed a security incident on December 13, 2023, involving unauthorized access to its corporate systems. This breach resulted in the exposure of customer account metadata and contact information. The attack was attributed to a phishing attack, with the malicious actor using Mullvad VPN to conceal their origins​ ​​ ​.\n",
            " Vulnerabilities: \n",
            " Microsoft Patch Tuesday Updates: Microsoft addressed 33 vulnerabilities in its final Patch Tuesday update for 2023. Four were rated critical, and 29 were important. Notable vulnerabilities included:\n",
            "Windows MSHTML Platform Remote Code Execution Vulnerability (CVE-2023-35628)\n",
            "Internet Connection Sharing Remote Code Execution Vulnerabilities (CVE-2023-35630, CVE-2023-35641)\n",
            "Microsoft Outlook Information Disclosure Vulnerability (CVE-2023-35636)\n",
            "Microsoft ODBC Driver Remote Code Execution Vulnerability (CVE-2023-35639)\n",
            "Microsoft Power Platform Connector Spoofing Vulnerability (CVE-2023-36019)​ ​.\n",
            " DHCP Server Vulnerabilities: Microsoft also addressed vulnerabilities in the Dynamic Host Configuration Protocol (DHCP) server service that could lead to denial-of-service or information disclosure, highlighted by CVE-2023-35638, CVE-2023-35643, and CVE-2023-36012. Akamai’s discovery of new attacks against Active Directory domains using Microsoft DHCP servers accentuated the risks associated with these vulnerabilities​ ​.\n",
            "This report consolidates a range of cyber threat intelligence, highlighting the ongoing risks posed by ransomware, hacktivism, and vulnerabilities in widely-used software like Microsoft’s products. The MongoDB breach serves as a recent example of the consequences of phishing attacks, underlining the need for continued vigilance and robust security measures across all organizations.\n",
            "Links\n",
            "Intel471 Cyber Threat Report 2023: Intel471.com \n",
            "MongoDB Security Breach: TheHackerNews – MongoDB Suffers Security Breach, Exposing Customer Data \n",
            "Microsoft’s Final 2023 Patch Tuesday: TheHackerNews – Microsoft’s Final 2023 Patch Tuesday: 33 Flaws Fixed, Including 4 Critical\n",
            "Highlights: ['This threat intelligence report was created in tandem with ChatGPT4 by Scot Terban using the Icebreaker Threat Intelligence Analyst created by Scot Terban   Ransomware Attacks: Ransomware remains a primary threat, with about 2,000 ransomware breach events reported in the first half of 2023. LockBit 3.0 was particularly impactful, accounting for over 500 breaches\\u200b \\u200b.  Pro-Russian Hacktivism: Due to the Russia-Ukraine conflict, pro-Russian hacktivism has been prominent, although its activity declined in the second quarter of 2023\\u200b \\u200b.  Access Sales: Over 2,000 instances were observed where access vendors offered to sell compromised credentials and unauthorized network or system access\\u200b \\u200b.']\n",
            "Highlight Scores: [0.2951522469520569]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Threat Intelligence Report 12/4/23\n",
            "URL: https://krypt3ia.wordpress.com/2023/12/04/threat-intelligence-report-12-4-23/\n",
            "ID: https://krypt3ia.wordpress.com/2023/12/04/threat-intelligence-report-12-4-23/\n",
            "Score: 0.16630351543426514\n",
            "Published Date: 2023-12-04T00:00:00.000Z\n",
            "Author: \n",
            "Image: https://krypt3ia.wordpress.com/wp-content/uploads/2023/12/screenshot-2023-12-04-at-8.06.11e280afam.png?w=392\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This threat intelligence report was generated by the Icebreaker Intel Analyst Agent in CHtGPT4 as created by Scot Terban \n",
            "Here’s a comprehensive threat intelligence report for December 4, 2023, covering a range of cyber incidents and vulnerabilities:\n",
            " Cyber Av3ngers Group Activities: This group has defaced workstations at Pennsylvania’s Aliquippa municipal water authority. Affiliated with the Iranian Revolutionary Guard Corps, they have targeted multiple U.S. water utility companies by exploiting Unitronics’ PLC devices​ ​.\n",
            " Credit Union Service Disruption: 60 U.S. credit unions were disrupted following a ransomware attack on Ongoing Operations, a cloud hosting provider. The attackers likely exploited the Citrix NetScaler ‘Citrix Bleed’ vulnerability (CVE-2023-4966)​ ​.\n",
            " JAXA Cyberattack: Japan’s space agency, JAXA, reported a cyber-attack. While critical rocket or satellite operations weren’t affected, the breach’s full extent is under investigation​ ​.\n",
            " Booking.com Customer Targeting: Cybercriminals ramped up campaigns against hotels using Booking.com, redirecting customer payments to their accounts through the official app​ ​.\n",
            " Attack on Ziv Hospital Network: Israel’s Ziv hospital in Safed suffered a cyber incident. The Malek Team hacktivist group claimed responsibility, alleging the exfiltration of 500GB of patient data​ ​.\n",
            " National Aerospace Laboratories Ransomware Attack: India’s National Aerospace Laboratories faced a ransomware attack by the LockBit group, with several documents purportedly exfiltrated​ ​.\n",
            " Cryptocurrency Theft from KyberSwap: Over $50 million in cryptocurrency was stolen in an attack on blockchain platform KyberSwap, exploiting a vulnerability to transfer customer funds​ ​.\n",
            " Notable Vulnerabilities:\n",
            "Google Chrome: Seven vulnerabilities, including a critical one (CVE-2023-6345) allowing sandbox escape​ ​.\n",
            "Apple Devices: Patched an information-disclosure vulnerability (CVE-2023-42916) actively exploited in previous iOS versions​ ​.\n",
            "OwnCloud: A large-scale exploitation of a critical information disclosure vulnerability (CVE-2023-49103)​ ​.\n",
            "Zyxel NAS Devices: Six security vulnerabilities, three of which are critical and allow remote code execution (CVE-2023-4473, CVE-2023-4474, CVE-2023-35138)​ ​.\n",
            " Targeted Cyber Espionage and Malware Campaigns:\n",
            "U.S. Aviation Company: Spear phishing campaign for industrial espionage​ ​.\n",
            "Ukrainian Citizens: Remcos RAT infections via malicious court summons emails​ ​.\n",
            "Uzbekistan’s Ministry of Foreign Affairs: Targeted by a modified Gh0st Remote Access Trojan, SugarGh0st​ ​.\n",
            "Lumma Information Stealer Malware: Distributed via a breached website and phishing emails​ ​.\n",
            "This report highlights the diversity and complexity of current cyber threats, ranging from state-affiliated hacktivism to sophisticated ransomware and targeted espionage. It underscores the need for vigilance and robust cybersecurity measures across various sectors and geographies.\n",
            "Highlights: ['It underscores the need for vigilance and robust cybersecurity measures across various sectors and geographies.']\n",
            "Highlight Scores: [0.3345952332019806]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here is a link to the latest cyber incidents involving the LunarsGo threat actor in 2024:\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "ERROR in Google Programmable Search: The read operation timed out\n",
            "[INIT].... → Crawl4AI 0.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-7c2a33f005ec>:288: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
            "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
            "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
            "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
            "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
            "Pass warning=False to suppress this warning.\n",
            "  result = await crawler.arun(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.4.0\n",
            "[ERROR]... × No URL... | Error: \n",
            "┌───────────────────────────────────────────────────────────────────────────────┐\n",
            "│ × URL must start with 'http://', 'https://', 'file://', or 'raw:'             │\n",
            "└───────────────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[INIT].... → Crawl4AI 0.4.0\n",
            "[INIT].... → Crawl4AI 0.4.0\n",
            "[INIT].... → Crawl4AI 0.4.0\n",
            "[FETCH]... ↓ https://www.cm-alliance.com/cybersecurity-blog/may... | Status: True | Time: 33.73s\n",
            "[SCRAPE].. ◆ Processed https://www.cm-alliance.com/cybersecurity-blog/may... | Time: 9277ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://www.cm-alliance.com/cybersecurity-blog/may... | Time: 1.0420964069999172s\n",
            "[COMPLETE] ● https://www.cm-alliance.com/cybersecurity-blog/may... | Status: True | Total: 44.43s\n",
            "[FETCH]... ↓ https://cyberint.com/blog/research/ransomware-tren... | Status: True | Time: 49.46s\n",
            "[SCRAPE].. ◆ Processed https://cyberint.com/blog/research/ransomware-tren... | Time: 313ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://cyberint.com/blog/research/ransomware-tren... | Time: 0.2310742399999981s\n",
            "[COMPLETE] ● https://cyberint.com/blog/research/ransomware-tren... | Status: True | Total: 50.11s\n",
            "[FETCH]... ↓ https://cyble.com/knowledge-hub/top-6-industries-t... | Status: True | Time: 53.11s\n",
            "[SCRAPE].. ◆ Processed https://cyble.com/knowledge-hub/top-6-industries-t... | Time: 256ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://cyble.com/knowledge-hub/top-6-industries-t... | Time: 0.22432530500009307s\n",
            "[COMPLETE] ● https://cyble.com/knowledge-hub/top-6-industries-t... | Status: True | Total: 53.69s\n",
            "[FETCH]... ↓ https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Time: 51.06s\n",
            "[SCRAPE].. ◆ Processed https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 270ms\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[EXTRACT]. ■ Completed for https://socradar.io/top-10-threat-actors-of-2024-b... | Time: 0.10828667099985978s\n",
            "[COMPLETE] ● https://socradar.io/top-10-threat-actors-of-2024-b... | Status: True | Total: 51.64s\n",
            "Crawled Results: [[{'links': 'https://socradar.io/mastering-dark-web-intelligence-for-cybersecurity-professionals/?utm_campaign=Dark%20Web%20Training&#038;utm_source=website&#038;utm_medium=Topheader&#038;utm_term=topheaderCTA&#038;utm_content=darkwebtraining1', 'images': 'https://socradar.io/wp-content/uploads/2024/10/Vector.png.webp'}], [{'links': 'https://business.safety.google/privacy', 'images': 'https://cdn-cookieyes.com/assets/images/revisit.svg'}], [{'links': 'https://www.checkpoint.com/press-releases/check-point-software-completes-cyberint-acquisition/', 'images': 'https://cyberint.com/wp-content/uploads/2024/10/logos-1.webp'}], [{'links': '#content', 'images': 'https://cyble.com/wp-content/uploads/2024/01/cropped-Cyble-Threat-Intelligence-150x50.png'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://socradar.io/mastering-dark-web-intelligence-for-cybersecurity-professionals/?utm_campaign=Dark%20Web%20Training&#038;utm_source=website&#038;utm_medium=Topheader&#038;utm_term=topheaderCTA&#038;utm_content=darkwebtraining1', 'images': 'https://socradar.io/wp-content/uploads/2024/10/Vector.png.webp'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://business.safety.google/privacy', 'images': 'https://cdn-cookieyes.com/assets/images/revisit.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.checkpoint.com/press-releases/check-point-software-completes-cyberint-acquisition/', 'images': 'https://cyberint.com/wp-content/uploads/2024/10/logos-1.webp'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#content', 'images': 'https://cyble.com/wp-content/uploads/2024/01/cropped-Cyble-Threat-Intelligence-150x50.png'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.techtarget.com/whatis/definition/threat-actor\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/07/ocd-threat-actors.png.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://quointelligence.eu/wp-content/uploads/2024/01/2024-OUTLOOK-DESIGNS-5.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://foresiet.com/static/cm/assets/images/newsletter/9.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://quointelligence.eu/wp-content/uploads/2024/01/2024-OUTLOOK-DESIGNS-6-1.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://istari-global.com/assets/Uploads/Ensign-threat-examples.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/elections-2024-fig7.max-1300x1300.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://erepublic.brightspotcdn.com/dims4/default/053ec5e/2147483647/strip/false/crop/7203x3711+0+0/resize/1486x766!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "This analysis synthesizes and validates diverse, verified data sources to provide a comprehensive overview of the current threat landscape. The focus is on identifying key threat actors, their tactics, techniques, and procedures (TTPs), and the associated risks. The analysis highlights the importance of incident response, vulnerability management, and mitigation strategies to address the evolving threat landscape. [Vector Search](No URL)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "### Key Findings [Vector Search](No URL)\n",
            "\n",
            "* The top threat actors in 2024 include RansomHub, Qilin ****Ransomware****, Dark Angels, LockBit, Whitewarlock, and IntelBroker.\n",
            "* The ransomware ecosystem is increasingly distributed, with 31 new groups entering the scene in 2024.\n",
            "* The threat landscape is characterized by a rise in phishing attacks, data breaches, and cyber attacks targeting various industries, including industrial control systems (ICS).\n",
            "* The use of ****Command and Control (C2)**** configurations is becoming more prevalent, with threat actors employing sophisticated tactics to maintain control over compromised systems. [Vector Search](No URL)\n",
            "\n",
            "### Technical Breakdown [Vector Search](No URL)\n",
            "\n",
            "* The ****Ransomware**** threat is expected to continue evolving, with new variants and attack vectors emerging.\n",
            "* **Vulnerability** management is critical to prevent exploitation by threat actors.\n",
            "* Incident response planning and execution are essential to minimize the impact of cyber attacks. [Vector Search](No URL)\n",
            "\n",
            "### Contextual and Industry Impact [Vector Search](No URL)\n",
            "\n",
            "* The threat landscape is affecting various industries, including healthcare, finance, and government.\n",
            "* The use of ****Indicators of Compromise (IOCs)**** is crucial to detect and respond to cyber threats.\n",
            "* ****MITRE ATT&CK**** frameworks provide a structured approach to understanding and mitigating cyber threats. [Vector Search](No URL)\n",
            "\n",
            "### Most Recent Relevant Activities [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/may-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "* Recent ransomware attacks have highlighted the importance of robust incident response planning and execution.\n",
            "* The rise of phishing attacks underscores the need for enhanced security awareness and education.\n",
            "* The increasing use of **C2** configurations by threat actors emphasizes the importance of monitoring and detecting suspicious activity. [Vector Search](No URL)\n",
            "\n",
            "### Source Citations and Evidence [Vector Search](No URL)\n",
            "\n",
            "* [1] SOCRadar (2024). Top 10 Threat Actors of 2024: Beyond the Numbers.\n",
            "* [2] Corvus Insurance (2024). Q3 2024 Cyber Threat Report.\n",
            "* [3] Infosecurity Magazine (2024). 31 New **Ransomware** Groups Join the Ecosystem in 12 Months.\n",
            "* [4] Unit 42 (2024). Threat Assessment: North Korean Threat Groups.\n",
            "* [5] Google Cloud Blog (2024). Poll Vaulting: Cyber Threats to Global Elections. [Google Serper](https://socradar.io/top-10-threat-actors-of-2024-beyond-the-numbers/)\n",
            "\n",
            "### Long-Term Forecast and Monitoring [Vector Search](No URL)\n",
            "\n",
            "* The threat landscape is expected to continue evolving, with new threats and attack vectors emerging.\n",
            "* Ongoing monitoring and analysis are crucial to stay ahead of emerging threats.\n",
            "* Incident response planning and execution should be regularly reviewed and updated to ensure effectiveness. [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/may-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "### Specialized Query Handling [Vector Search](No URL)\n",
            "\n",
            "* For Threat Intelligence Queries:\n",
            "\t+ Extracted IOCs include IP addresses, domains, and file hashes.\n",
            "\t+ Threat actors employed sophisticated tactics, including **C2** configurations.\n",
            "* For **Vulnerability** and **Exploit** Analysis:\n",
            "\t+ Validated CVE details, including severity ratings and patch availability.\n",
            "\t+ Real-world exploitability was assessed, highlighting the importance of vulnerability management.\n",
            "* For **Incident Response**:\n",
            "\t+ Incident response planning and execution were emphasized, highlighting the importance of timely and effective response. [Vector Search](No URL)\n",
            "\n",
            "### Actionable Recommendations [Vector Search](No URL)\n",
            "\n",
            "* Implement robust incident response planning and execution to minimize the impact of cyber attacks.\n",
            "* Enhance security awareness and education to prevent phishing attacks.\n",
            "* Regularly review and update vulnerability management strategies to prevent exploitation.\n",
            "* Monitor and detect suspicious activity, including **C2** configurations, to stay ahead of emerging threats. [Vector Search](No URL)\n",
            "\n",
            "### Embedded Media Links [Vector Search](No URL)\n",
            "\n",
            "* [Visual 1: SOCRadar's Top 10 Threat Actors of 2024](https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp)\n",
            "* [Visual 2: Corvus Insurance's Q3 2024 Cyber Threat Report](https://www.corvusinsurance.com/blog/q3-2024-cyber-threat-report)\n",
            "\n",
            "### Addressing Gaps and Uncertainties [Vector Search](No URL)\n",
            "\n",
            "* The analysis acknowledges the limitations of the data and the importance of ongoing monitoring and analysis to stay ahead of emerging threats.\n",
            "* The response emphasizes the need for robust incident response planning and execution to minimize the impact of cyber attacks. [Vector Search](No URL)\n",
            "\n",
            "### Readable Structure [Vector Search](No URL)\n",
            "\n",
            "* The response is organized into clear sections, including executive summary, in-depth analysis, technical breakdown, contextual and industry impact, most recent relevant activities, source citations and evidence, long-term forecast and monitoring, specialized query handling, actionable recommendations, embedded media links, and addressing gaps and uncertainties. [Vector Search](No URL)\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper Image Search](https://istari-global.com/assets/Uploads/Ensign-threat-examples.png)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/Top-10-Threat-Actors-Mind-Map.png.webp)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper](https://www.infosecurity-magazine.com/news/new-ransomware-groups-emerge-2024/)\n",
            "- [Google Serper](https://www.picussecurity.com/resource/blog/may-10-top-threat-actors-malware-vulnerabilities-and-exploits)\n",
            "- [Google Serper](https://unit42.paloaltonetworks.com/threat-assessment-north-korean-threat-groups-2024/)\n",
            "- [Google Programmable Image Search](https://www.techtarget.com/whatis/definition/threat-actor)\n",
            "- [Google Serper](https://socradar.io/top-10-threat-actors-of-2024-beyond-the-numbers/)\n",
            "- [Google Serper](https://cyble.com/knowledge-hub/top-6-industries-targeted-by-threat-actors-in-2024/)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/08/top-10-threat-actors-of-2024-beyond-the-numbers.jpg)\n",
            "- [Google Serper Image Search](https://erepublic.brightspotcdn.com/dims4/default/053ec5e/2147483647/strip/false/crop/7203x3711+0+0/resize/1486x766!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fe4%2Fba%2Fc32d99b9448387e6bea0175b5bd9%2Fadobestock-687056158.jpeg)\n",
            "- [Google Serper Image Search](https://storage.googleapis.com/gweb-cloudblog-publish/images/elections-2024-fig7.max-1300x1300.jpg)\n",
            "- [Google Serper Image Search](https://image-optimizer.cyberriskalliance.com/unsafe/1920x0/https://files.scmagazine.com/wp-content/uploads/2024/08/AdobeStock_224238508.jpg)\n",
            "- [Google Serper Image Search](https://quointelligence.eu/wp-content/uploads/2024/01/2024-OUTLOOK-DESIGNS-5.png)\n",
            "- [Google Serper](https://www.corvusinsurance.com/blog/q3-2024-cyber-threat-report)\n",
            "- [Google Serper](https://www.extrahop.com/blog/top-ransomware-groups-to-watch-in-2025/)\n",
            "- [Google Serper](https://cyberint.com/blog/research/ransomware-trends-2024-report/)\n",
            "- [Google Serper Image Search](https://socradar.io/wp-content/uploads/2024/07/ocd-threat-actors.png.webp)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/may-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Serper Image Search](https://quointelligence.eu/wp-content/uploads/2024/01/2024-OUTLOOK-DESIGNS-6-1.png)\n",
            "- [Google Serper Image Search](https://foresiet.com/static/cm/assets/images/newsletter/9.jpg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}