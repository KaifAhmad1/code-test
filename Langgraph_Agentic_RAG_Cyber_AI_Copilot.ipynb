{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "%pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from exa_py import Exa\n",
        "from langchain_core.tools import tool\n",
        "import re\n",
        "from typing import List, Union\n",
        "from goose3 import Goose\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = \"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\"\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM and embeddings\n",
        "llm = ChatGroq(temperature=0, model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper(k=5)\n",
        "\n",
        "# Initialize Exa search tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Goose3 for web scraping\n",
        "goose = Goose()"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[Dict[str, str]]]\n",
        "    images: Optional[List[str]]\n",
        "    videos: Optional[List[str]]\n",
        "    content: Optional[str]\n",
        "\n",
        "def scrape_content(url: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        article = goose.extract(url=url)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        images = [img['src'] for img in soup.find_all('img') if 'src' in img.attrs]\n",
        "        videos = [video['src'] for video in soup.find_all('video') if 'src' in video.attrs]\n",
        "        return {\n",
        "            \"text\": article.cleaned_text[:1000],  # Limit to first 1000 characters\n",
        "            \"images\": images[:3],  # Limit to first 3 images\n",
        "            \"videos\": videos[:1]  # Limit to first video\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "        return {\"text\": \"\", \"images\": [], \"videos\": []}\n",
        "\n",
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=3)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content[:200],\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            **scrape_content(doc.metadata.get(\"source\", \"\"))\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\")[:200],\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            **scrape_content(result.get(\"link\", \"\"))\n",
        "        ) for result in results.get(\"organic\", [])[:3]\n",
        "    ]\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    results = exa.search_and_contents(query, use_autoprompt=True, num_results=3)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Exa Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"text\", \"No text\")[:200],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_urls\", [])[:3],\n",
        "            **scrape_content(result.get(\"url\", \"\"))\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    results = tavily_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Tavily Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"content\", \"No content\")[:200],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_url\", [])[:3] if result.get(\"image_url\") else [],\n",
        "            **scrape_content(result.get(\"url\", \"\"))\n",
        "        ) for result in results[:3]\n",
        "    ]\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    results = google_search.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Programmable Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\")[:200],\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            **scrape_content(result.get(\"link\", \"\"))\n",
        "        ) for result in results[:3]\n",
        "    ]"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            print(f\"Results from {name}:\")\n",
        "            for result in results:\n",
        "                print(f\"Title: {result.title}\")\n",
        "                print(f\"Snippet: {result.snippet}\")\n",
        "                print(f\"URL: {result.url}\")\n",
        "                print(f\"Date: {result.date}\")\n",
        "                print(f\"Images: {result.images}\")\n",
        "                print(f\"Videos: {result.videos}\")\n",
        "                print(f\"Content: {result.content[:200]}...\")\n",
        "                print(\"\\n\")\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        return (x.date is not None, x.date or \"\", x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 5 most relevant results\n",
        "    top_results = all_results[:5]\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state\n",
        "\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"]) if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence. Your task is to provide highly relevant, actionable, and up-to-date information based on the user's query. Follow these guidelines:\n",
        "\n",
        "1. Analyze the user's query: {input}\n",
        "2. Examine the search results, prioritizing recent and relevant information from reputable sources.\n",
        "3. Identify emerging patterns, trends, and potential implications related to the query.\n",
        "4. Provide a concise, structured response tailored to the query, including:\n",
        "   a. Key Findings (2-3 bullet points of the most critical information)\n",
        "   b. Brief Analysis (focused examination of the key points, directly addressing the query)\n",
        "   c. Recommendations (1-2 actionable items with rationale)\n",
        "\n",
        "5. Include clear citations for ALL information using the format [Source Name](URL).\n",
        "6. If search results contain images or videos, mention their content and relevance.\n",
        "7. Adjust the response length based on the query complexity and available information.\n",
        "8. Include technical details when appropriate, such as specific vulnerabilities or mitigation strategies.\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "Human query: {input}\n",
        "Search Results: {search_results}\n",
        "\n",
        "Current date: {current_date}\n",
        "\n",
        "Provide a concise, actionable response based on the query and latest findings, ensuring every piece of information is properly cited:\n",
        "\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join([f\"{result.title}\\n{result.snippet}\\n{result.content[:200]}...\\nImages: {', '.join(result.images[:3])}\\nVideos: {', '.join(result.videos[:1])}\\n{format_source_link(result.source, result.url)}\\nDate: {result.date or 'Not specified'}\\n\" for result in search_results]),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"}\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not re.search(r'', paragraph) and not paragraph.startswith('**'):\n",
        "            paragraph += ' [Source needed]()'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    return graph.invoke(state)"
      ],
      "metadata": {
        "id": "EGkMBAYB2V9U"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Incidents and attacks by Blackbasta Ransomware Gang?\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"AI Copilot Analysis:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "cdyh1-kcW0L0",
        "outputId": "deba0903-4d2c-4217-92a3-e40921fe1a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping : Either url or raw_html should be provided\n",
            "AI Copilot Analysis:\n",
            "Based on your query \"Search Results,\" I've analyzed the latest information available. However, it seems that the query is too broad and doesn't provide enough context for a specific response. I'll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\n",
            "* Search results can be influenced by various factors, including personalization, location, and search history [2].\n",
            "* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\n",
            "\n",
            "**Brief Analysis:**\n",
            "Search results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it's essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\n",
            "2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\n",
            "\n",
            "**References:**\n",
            "\n",
            "[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\n",
            "[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\n",
            "[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\n",
            "\n",
            "Please provide a more specific query for a more detailed and actionable response.\n",
            "\n",
            "**Sources**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFLQkgkNW0OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6LjD1kyW0Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}