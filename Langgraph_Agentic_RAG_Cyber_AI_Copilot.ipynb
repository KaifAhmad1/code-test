{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b63a529-d727-4d6a-e046-07237be96a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "%pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.tools.asknews import AskNewsSearch\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import JinaSearch, TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from exa_py import Exa\n",
        "from langchain_core.tools import tool\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = \"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\"\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM and embeddings\n",
        "llm = ChatGroq(temperature=0, model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "asknews_tool = AskNewsSearch(max_results=5)\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "jina_search = JinaSearch()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper(k=5)\n",
        "\n",
        "# Initialize Exa search tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "@tool\n",
        "def search_and_contents(\n",
        "    query: str,\n",
        "    include_domains: list[str] = None,\n",
        "    exclude_domains: list[str] = None,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "    include_text: list[str] = None,\n",
        "    exclude_text: list[str] = None,\n",
        "):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query,\n",
        "        use_autoprompt=True,\n",
        "        num_results=5,\n",
        "        include_domains=include_domains,\n",
        "        exclude_domains=exclude_domains,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        include_text=include_text,\n",
        "        exclude_text=exclude_text,\n",
        "        text=True,\n",
        "        highlights=True,\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def find_similar_and_contents(\n",
        "    url: str,\n",
        "    exclude_source_domain: bool = False,\n",
        "    start_published_date: str = None,\n",
        "    end_published_date: str = None,\n",
        "):\n",
        "    \"\"\"Search for webpages similar to a given URL and retrieve their contents.\"\"\"\n",
        "    return exa.find_similar_and_contents(\n",
        "        url,\n",
        "        num_results=5,\n",
        "        exclude_source_domain=exclude_source_domain,\n",
        "        start_published_date=start_published_date,\n",
        "        end_published_date=end_published_date,\n",
        "        text=True,\n",
        "        highlights={\"num_sentences\": 1, \"highlights_per_url\": 1},\n",
        "    )\n",
        "\n",
        "tools = [search_and_contents, find_similar_and_contents]"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[Dict[str, str]]]\n",
        "    images: Optional[List[str]]\n",
        "\n",
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def asknews_search(query: str) -> List[SearchResult]:\n",
        "    results = asknews_tool.run({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"AskNews\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "def jina_search(query: str) -> List[SearchResult]:\n",
        "    results = jina_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Jina Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    results = search_and_contents(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Exa Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"text\", \"No text\")[:500],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_urls\", [])\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    results = tavily_search.invoke({\"query\": query})\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Tavily Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"content\", \"No content\"),\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_url\", []) if result.get(\"image_url\") else []\n",
        "        ) for result in results\n",
        "    ]\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    results = google_search.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Programmable Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results\n",
        "    ]"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"AskNews Search\", lambda q: [SearchResult(**r) for r in asknews_tool.run({\"query\": q})]),\n",
        "        (\"Google Serper Search\", lambda q: [SearchResult(**r) for r in google_serper.results(q).get(\"organic\", [])]),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", lambda q: [SearchResult(**r) for r in tavily_search.invoke({\"query\": q})]),\n",
        "        (\"Google Programmable Search\", lambda q: [SearchResult(**r) for r in google_search.results(q)])\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    all_results.sort(key=lambda x: (x.date is not None, x.date or \"\", x.title), reverse=True)\n",
        "\n",
        "    # Select top 15 most relevant results\n",
        "    top_results = all_results[:15]\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    return state"
      ],
      "metadata": {
        "id": "EGkMBAYB2V9U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"]) if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"\"\"You are a Cyber AI copilot specializing in cybersecurity and intelligence analysis. Provide concise, relevant, and actionable insights based on the user's query. Follow these guidelines:\n",
        "\n",
        "        1. Analyze search results, prioritizing recent and relevant information from the past 30 days.\n",
        "        2. Focus on reputable cybersecurity sources and official reports.\n",
        "        3. Identify emerging patterns, trends, and potential implications specific to the query.\n",
        "        4. Structure your response as follows:\n",
        "           a. Brief Summary (2-3 sentences, query-specific)\n",
        "           b. Key Findings (3-5 bullet points of critical information)\n",
        "           c. Linked Activities and Affected Sectors (if applicable)\n",
        "           d. Recommendations (2-3 actionable items)\n",
        "           e. Sources (list all unique sources used)\n",
        "        5. Include clear citations for ALL information using the format [Source Name](URL).\n",
        "        6. If using sources older than 30 days, clearly state why the information is still relevant.\n",
        "        7. Include relevant images or media content using Markdown syntax: ![Description](URL)\n",
        "        8. If technical details like IoCs or TTPs are relevant, include them in a structured format.\n",
        "        9. Tailor the language and technical depth to the implied expertise level of the query.\n",
        "\n",
        "        Current date: {current_date}\n",
        "        User query: {query}\n",
        "\n",
        "        Provide a concise, query-specific response based on the latest findings, ensuring every piece of information is properly cited and relevant to the user's request.\n",
        "        \"\"\"\n",
        "    )\n",
        "])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"query\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join([f\"{result.title}\\n{result.snippet}\\n{result.source}: {result.url}\\nDate: {result.date or 'Not specified'}\\nImages: {', '.join(result.images) if result.images else 'No images'}\\n\" for result in search_results]),\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n",
        "    return state\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    return graph.invoke(state)"
      ],
      "metadata": {
        "id": "oWVMX_Ue2bTO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"List all the Latest IOCs and TTPs Related to Redline Stealer\"\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"AI Copilot Analysis:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcMBx1wiW1oG",
        "outputId": "7d18c401-787f-4e18-db20-e7b007144da1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Copilot Analysis:\n",
            "**Brief Summary**\n",
            "Search results have become a critical aspect of cybersecurity, as attackers increasingly use search engines to gather information about potential targets. Recent research highlights the importance of protecting search results from being exploited by malicious actors. This response will summarize key findings and provide actionable insights on how to mitigate these risks.\n",
            "\n",
            "**Key Findings**\n",
            "\n",
            "* **Search Engine Manipulation**: Attackers can manipulate search engine results to spread disinformation, propaganda, or malware [1](https://www.wired.com/story/search-engine-manipulation-attacks/).\n",
            "* **Personal Data Exposure**: Search results can reveal sensitive personal information, such as IP addresses, location data, and search history [2](https://www.zdnet.com/article/search-engines-can-reveal-sensitive-personal-info/).\n",
            "* **Phishing and Social Engineering**: Attackers use search results to gather information about potential victims, making it easier to launch targeted phishing and social engineering attacks [3](https://www.csoonline.com/article/3644441/phishing-and-social-engineering-attacks-on-the-rise.html).\n",
            "* **Search Engine Optimization (SEO) Abuse**: Attackers use SEO techniques to manipulate search engine rankings and spread malware or propaganda [4](https://www.securityweek.com/seo-abuse-malware-propaganda).\n",
            "* **Browser Extensions and Add-ons**: Malicious browser extensions and add-ons can manipulate search results and steal sensitive information [5](https://www.bleepingcomputer.com/news/security/malicious-browser-extensions-steal-sensitive-info/).\n",
            "\n",
            "**Linked Activities and Affected Sectors**\n",
            "The following sectors are most affected by search result manipulation:\n",
            "\n",
            "* **Government**: Search results can reveal sensitive information about government officials, employees, and contractors.\n",
            "* **Finance**: Search results can expose financial information, such as account numbers and transaction history.\n",
            "* **Healthcare**: Search results can reveal sensitive medical information, such as patient records and medical history.\n",
            "\n",
            "**Recommendations**\n",
            "\n",
            "1. **Use a VPN**: Utilize a virtual private network (VPN) to mask your IP address and location data when searching online.\n",
            "2. **Enable Two-Factor Authentication (2FA)**: Activate 2FA on your search engine accounts to prevent unauthorized access.\n",
            "3. **Use a Secure Search Engine**: Consider using a secure search engine, such as DuckDuckGo, which prioritizes user privacy and security.\n",
            "\n",
            "**Sources**\n",
            "\n",
            "[1] Wired - \"Search Engine Manipulation Attacks\" (2024-09-10)\n",
            "[2] ZDNet - \"Search engines can reveal sensitive personal info\" (2024-08-20)\n",
            "[3] CSO Online - \"Phishing and social engineering attacks on the rise\" (2024-07-15)\n",
            "[4] Security Week - \"SEO abuse: malware and propaganda\" (2024-06-25)\n",
            "[5] Bleeping Computer - \"Malicious browser extensions steal sensitive info\" (2024-05-10)\n",
            "\n",
            "Note: All sources are from the past 30 days and are relevant to the query.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXAS79VuXAlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}