{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "910e4a70-fb69-48a7-ea86-c2dfd46efdfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbd6360-1bd3-45fc-b9c1-6ae6b4afbbf1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced reranking function with semantic similarity and metadata scoring\n",
        "def enhanced_rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = (datetime.now() - date).days if date else 0\n",
        "        final_score = similarity + metadata_score - date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = (datetime.now() - date).days if date else 0\n",
        "            final_score = weight + source_weight - date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            print(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Enhanced reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = enhanced_rerank_results(query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\"\n",
        "    You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines, APIs, and data sources to provide comprehensive, up-to-date, query-specific responses.\n",
        "\n",
        "    SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "    1. Primary Source Evaluation:\n",
        "        - Assess the credibility of each source domain.\n",
        "        - Verify publication dates to ensure the most recent information.\n",
        "        - Cross-reference information across multiple sources.\n",
        "        - Identify and flag potential misinformation or conflicting data.\n",
        "\n",
        "    2. Content Processing Guidelines:\n",
        "        - Extract and normalize key data points.\n",
        "        - Identify patterns and correlations across sources.\n",
        "        - Prioritize information based on:\n",
        "            * Temporal relevance (newest to oldest).\n",
        "            * Source reliability.\n",
        "            * Direct query relevance.\n",
        "            * Technical depth.\n",
        "            * Actionable insights.\n",
        "\n",
        "    3. Media Content Analysis:\n",
        "        - Evaluate included images, diagrams, or screenshots.\n",
        "        - Extract relevant technical indicators from visual data.\n",
        "        - Correlate visual evidence with textual information.\n",
        "        - Note any visual proof of concepts or attack demonstrations.\n",
        "\n",
        "    RESPONSE STRUCTURE:\n",
        "    1. Executive Summary (8-10 sentences):\n",
        "        - Core findings.\n",
        "        - Critical alerts or time-sensitive information.\n",
        "\n",
        "    2. Detailed Analysis:\n",
        "        a) Key Findings:\n",
        "            - Bullet points of critical discoveries.\n",
        "            - Emerging threats or developments.\n",
        "            - Statistical data or metrics.\n",
        "\n",
        "        b) Technical Details:\n",
        "            - Specific vulnerabilities or exploits.\n",
        "            - Attack vectors and techniques.\n",
        "            - System impacts and affected components.\n",
        "\n",
        "        c) Contextual Analysis:\n",
        "            - Industry impact.\n",
        "            - Threat actor attribution (if applicable).\n",
        "            - Historical context or similar incidents.\n",
        "\n",
        "    3. Evidence and Citations:\n",
        "        - Link every major claim to source material.\n",
        "        - Include relevant quote snippets.\n",
        "        - Provide context for technical indicators.\n",
        "        - Reference related media content.\n",
        "\n",
        "    4. Actionable Intelligence:\n",
        "        - Immediate response recommendations.\n",
        "        - Mitigation strategies.\n",
        "        - Detection methods.\n",
        "        - Prevention measures.\n",
        "\n",
        "    5. Future Implications\n",
        "        - Projected developments.\n",
        "        - Potential cascade effects.\n",
        "        - Areas requiring monitoring.\n",
        "\n",
        "    SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "    1. For Threat Intelligence:\n",
        "        - Extract and validate Indicators of Compromise (IOCs).\n",
        "        - Identify Tactics, Techniques, and Procedures (TTPs) and map to MITRE ATT&CK.\n",
        "        - Analyze malware behaviors.\n",
        "        - Document Command and Control (C2) infrastructure.\n",
        "\n",
        "    2. For Vulnerability Analysis:\n",
        "        - Verify Common Vulnerabilities and Exposures (CVE) details.\n",
        "        - Document exploit requirements.\n",
        "        - Assess patch availability.\n",
        "        - Evaluate real-world exploitation scenarios.\n",
        "\n",
        "    3. For Incident Response:\n",
        "        - Reconstruct attack timelines.\n",
        "        - Analyze attack paths.\n",
        "        - Perform impact assessments.\n",
        "        - Provide recovery and remediation recommendations.\n",
        "\n",
        "    4. For Trend Analysis:\n",
        "        - Identify pattern changes.\n",
        "        - Map threat evolution.\n",
        "        - Project future developments.\n",
        "        - Compare against historical data.\n",
        "\n",
        "    PREVIOUS CONVERSATION CONTEXT: {chat_history}\n",
        "    CURRENT QUERY: {input}\n",
        "    AVAILABLE SEARCH RESULTS: {search_results}\n",
        "    CRAWLED RESULTS: {crawled_results}\n",
        "    CURRENT TIMESTAMP: {current_date}\n",
        "\n",
        "    RESPONSE REQUIREMENTS:\n",
        "    1. Maintain clinical precision and technical accuracy.\n",
        "    2. Prioritize actionable intelligence over general information.\n",
        "    3. Include explicit confidence levels for all assessments.\n",
        "    4. Cite ALL sources using [Source Name](URL) format.\n",
        "    5. Highlight time-sensitive information.\n",
        "    6. Address any information gaps or uncertainties.\n",
        "    7. Format output for maximum readability.\n",
        "    8. Include relevant media references.\n",
        "    9. Provide specific, implementable recommendations.\n",
        "    10. Maintain proper technical context throughout.\n",
        "\n",
        "    Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the latest search results.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\"\n",
        "    You are an advanced AI copilot specializing in cybersecurity and intelligence analysis. Your primary function is to synthesize and analyze information from multiple search engines, APIs, and data sources to provide comprehensive, up-to-date, query-specific responses.\n",
        "\n",
        "    SEARCH RESULTS ANALYSIS PROTOCOL:\n",
        "    1. Primary Source Evaluation:\n",
        "        - Assess the credibility of each source domain.\n",
        "        - Verify publication dates to ensure the most recent information.\n",
        "        - Cross-reference information across multiple sources.\n",
        "        - Identify and flag potential misinformation or conflicting data.\n",
        "\n",
        "    2. Content Processing Guidelines:\n",
        "        - Extract and normalize key data points.\n",
        "        - Identify patterns and correlations across sources.\n",
        "        - Prioritize information based on:\n",
        "            * Temporal relevance (newest to oldest).\n",
        "            * Source reliability.\n",
        "            * Direct query relevance.\n",
        "            * Technical depth.\n",
        "            * Actionable insights.\n",
        "\n",
        "    3. Media Content Analysis:\n",
        "        - Evaluate included images, diagrams, or screenshots.\n",
        "        - Extract relevant technical indicators from visual data.\n",
        "        - Correlate visual evidence with textual information.\n",
        "        - Note any visual proof of concepts or attack demonstrations.\n",
        "\n",
        "    RESPONSE STRUCTURE:\n",
        "    1. Executive Summary (8-10 sentences):\n",
        "        - Core findings.\n",
        "        - Critical alerts or time-sensitive information.\n",
        "\n",
        "    2. Detailed Analysis:\n",
        "        a) Key Findings:\n",
        "            - Bullet points of critical discoveries.\n",
        "            - Emerging threats or developments.\n",
        "            - Statistical data or metrics.\n",
        "\n",
        "        b) Technical Details:\n",
        "            - Specific vulnerabilities or exploits.\n",
        "            - Attack vectors and techniques.\n",
        "            - System impacts and affected components.\n",
        "\n",
        "        c) Contextual Analysis:\n",
        "            - Industry impact.\n",
        "            - Threat actor attribution (if applicable).\n",
        "            - Historical context or similar incidents.\n",
        "\n",
        "    3. Evidence and Citations:\n",
        "        - Link every major claim to source material.\n",
        "        - Include relevant quote snippets.\n",
        "        - Provide context for technical indicators.\n",
        "        - Reference related media content.\n",
        "\n",
        "    4. Actionable Intelligence:\n",
        "        - Immediate response recommendations.\n",
        "        - Mitigation strategies.\n",
        "        - Detection methods.\n",
        "        - Prevention measures.\n",
        "\n",
        "    5. Future Implications\n",
        "        - Projected developments.\n",
        "        - Potential cascade effects.\n",
        "        - Areas requiring monitoring.\n",
        "\n",
        "    SPECIALIZED PROCESSING INSTRUCTIONS:\n",
        "    1. For Threat Intelligence:\n",
        "        - Extract and validate Indicators of Compromise (IOCs).\n",
        "        - Identify Tactics, Techniques, and Procedures (TTPs) and map to MITRE ATT&CK.\n",
        "        - Analyze malware behaviors.\n",
        "        - Document Command and Control (C2) infrastructure.\n",
        "\n",
        "    2. For Vulnerability Analysis:\n",
        "        - Verify Common Vulnerabilities and Exposures (CVE) details.\n",
        "        - Document exploit requirements.\n",
        "        - Assess patch availability.\n",
        "        - Evaluate real-world exploitation scenarios.\n",
        "\n",
        "    3. For Incident Response:\n",
        "        - Reconstruct attack timelines.\n",
        "        - Analyze attack paths.\n",
        "        - Perform impact assessments.\n",
        "        - Provide recovery and remediation recommendations.\n",
        "\n",
        "    4. For Trend Analysis:\n",
        "        - Identify pattern changes.\n",
        "        - Map threat evolution.\n",
        "        - Project future developments.\n",
        "        - Compare against historical data.\n",
        "\n",
        "    PREVIOUS CONVERSATION CONTEXT: {chat_history}\n",
        "    CURRENT QUERY: {input}\n",
        "    AVAILABLE SEARCH RESULTS: {search_results}\n",
        "    CRAWLED RESULTS: {crawled_results}\n",
        "    CURRENT TIMESTAMP: {current_date}\n",
        "\n",
        "    RESPONSE REQUIREMENTS:\n",
        "    1. Maintain clinical precision and technical accuracy.\n",
        "    2. Prioritize actionable intelligence over general information.\n",
        "    3. Include explicit confidence levels for all assessments.\n",
        "    4. Cite ALL sources using [Source Name](URL) format.\n",
        "    5. Highlight time-sensitive information.\n",
        "    6. Address any information gaps or uncertainties.\n",
        "    7. Format output for maximum readability.\n",
        "    8. Include relevant media references.\n",
        "    9. Provide specific, implementable recommendations.\n",
        "    10. Maintain proper technical context throughout.\n",
        "\n",
        "    Generate a comprehensive response that directly addresses the query while synthesizing all available intelligence from the latest search results.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Enhanced Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1905456-3b78-4107-85cf-9aeb27b151c5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomware in India past three months?\n",
            "ERROR in Exa Search: Request failed with status code 502: <!DOCTYPE html>\n",
            "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
            "<head>\n",
            "\n",
            "\n",
            "<title>api.exa.ai | 502: Bad gateway</title>\n",
            "<meta charset=\"UTF-8\" />\n",
            "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
            "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
            "<meta name=\"robots\" content=\"noindex, nofollow\" />\n",
            "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
            "<link rel=\"stylesheet\" id=\"cf_styles-css\" href=\"/cdn-cgi/styles/main.css\" />\n",
            "\n",
            "\n",
            "</head>\n",
            "<body>\n",
            "<div id=\"cf-wrapper\">\n",
            "    <div id=\"cf-error-details\" class=\"p-0\">\n",
            "        <header class=\"mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-8\">\n",
            "            <h1 class=\"inline-block sm:block sm:mb-2 font-light text-60 lg:text-4xl text-black-dark leading-tight mr-2\">\n",
            "              <span class=\"inline-block\">Bad gateway</span>\n",
            "              <span class=\"code-label\">Error code 502</span>\n",
            "            </h1>\n",
            "            <div>\n",
            "               Visit <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_502&utm_campaign=api.exa.ai\" target=\"_blank\" rel=\"noopener noreferrer\">cloudflare.com</a> for more information.\n",
            "            </div>\n",
            "            <div class=\"mt-3\">2024-11-12 13:27:31 UTC</div>\n",
            "        </header>\n",
            "        <div class=\"my-8 bg-gradient-gray\">\n",
            "            <div class=\"w-240 lg:w-full mx-auto\">\n",
            "                <div class=\"clearfix md:px-8\">\n",
            "                  \n",
            "<div id=\"cf-browser-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
            "  <div class=\"relative mb-10 md:m-0\">\n",
            "    \n",
            "    <span class=\"cf-icon-browser block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
            "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
            "    \n",
            "  </div>\n",
            "  <span class=\"md:block w-full truncate\">You</span>\n",
            "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
            "    \n",
            "    Browser\n",
            "    \n",
            "  </h3>\n",
            "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
            "</div>\n",
            "\n",
            "<div id=\"cf-cloudflare-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
            "  <div class=\"relative mb-10 md:m-0\">\n",
            "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_502&utm_campaign=api.exa.ai\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
            "    <span class=\"cf-icon-cloud block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
            "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
            "    </a>\n",
            "  </div>\n",
            "  <span class=\"md:block w-full truncate\">Ashburn</span>\n",
            "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
            "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_502&utm_campaign=api.exa.ai\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
            "    Cloudflare\n",
            "    </a>\n",
            "  </h3>\n",
            "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
            "</div>\n",
            "\n",
            "<div id=\"cf-host-status\" class=\"cf-error-source relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
            "  <div class=\"relative mb-10 md:m-0\">\n",
            "    \n",
            "    <span class=\"cf-icon-server block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
            "    <span class=\"cf-icon-error w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
            "    \n",
            "  </div>\n",
            "  <span class=\"md:block w-full truncate\">api.exa.ai</span>\n",
            "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
            "    \n",
            "    Host\n",
            "    \n",
            "  </h3>\n",
            "  <span class=\"leading-1.3 text-2xl text-red-error\">Error</span>\n",
            "</div>\n",
            "\n",
            "                </div>\n",
            "            </div>\n",
            "        </div>\n",
            "\n",
            "        <div class=\"w-240 lg:w-full mx-auto mb-8 lg:px-8\">\n",
            "            <div class=\"clearfix\">\n",
            "                <div class=\"w-1/2 md:w-full float-left pr-6 md:pb-10 md:pr-0 leading-relaxed\">\n",
            "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What happened?</h2>\n",
            "                    <p>The web server reported a bad gateway error.</p>\n",
            "                </div>\n",
            "                <div class=\"w-1/2 md:w-full float-left leading-relaxed\">\n",
            "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What can I do?</h2>\n",
            "                    <p class=\"mb-6\">Please try again in a few minutes.</p>\n",
            "                </div>\n",
            "            </div>\n",
            "        </div>\n",
            "\n",
            "        <div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\">\n",
            "  <p class=\"text-13\">\n",
            "    <span class=\"cf-footer-item sm:block sm:mb-1\">Cloudflare Ray ID: <strong class=\"font-semibold\">8e16dc21baa23932</strong></span>\n",
            "    <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
            "    <span id=\"cf-footer-item-ip\" class=\"cf-footer-item hidden sm:block sm:mb-1\">\n",
            "      Your IP:\n",
            "      <button type=\"button\" id=\"cf-footer-ip-reveal\" class=\"cf-footer-ip-reveal-btn\">Click to reveal</button>\n",
            "      <span class=\"hidden\" id=\"cf-footer-ip\">34.48.162.204</span>\n",
            "      <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
            "    </span>\n",
            "    <span class=\"cf-footer-item sm:block sm:mb-1\"><span>Performance &amp; security by</span> <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_502&utm_campaign=api.exa.ai\" id=\"brand_link\" target=\"_blank\">Cloudflare</a></span>\n",
            "    \n",
            "  </p>\n",
            "  <script>(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();</script>\n",
            "</div><!-- /.error-footer -->\n",
            "\n",
            "\n",
            "    </div>\n",
            "</div>\n",
            "</body>\n",
            "</html>\n",
            "\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 2.81 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, success: True, time taken: 0.12 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta, time taken: 0.16 seconds.\n",
            "[LOG] ✅ Crawled https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics successfully!\n",
            "[LOG] 🚀 Crawling done for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, success: True, time taken: 2.93 seconds\n",
            "[LOG] 🚀 Content extracted for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, success: True, time taken: 0.23 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics, time taken: 0.29 seconds.\n",
            "[LOG] ✅ Crawled https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a, success: True, time taken: 3.56 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a, success: True, time taken: 0.51 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a, time taken: 0.61 seconds.\n",
            "[LOG] ✅ Crawled https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/, success: True, time taken: 5.58 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/, success: True, time taken: 0.74 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/, time taken: 0.91 seconds.\n",
            "[LOG] ✅ Crawled https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, success: True, time taken: 6.12 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, success: True, time taken: 0.51 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html, time taken: 0.63 seconds.\n",
            "Crawled Results: [[{'links': 'https://www.barracuda.com', 'images': '/content/dam/barracuda-corp/images/site/header/logo_barracuda_primary_reversed.svg'}], [{'links': 'https://www.cookiebot.com/en/what-is-behind-powered-by-cookiebot/', 'images': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAuCAYAAABXuSs3AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAdMSURBVGhD7ZdrcFXVFce3JhFM7vueewlJSCJEbMNDIKKiMmEMEsi9ASJGqa+gqEBQRsH4JEaFe/MAnfqIz+GRgjwCdZA3ljRFCpR22k6n43T6wWn95PjNL46fHP39d/bNOLXtxMTxMd7/zJq91n+tvfY6e++zzz4miyyyyCKLLL6Ghk1x09Q/2lnfOXKQ3AH1a8hzrUUkEimuEtfye7+p77jBJNPzTM1j0WAwGMatPF+Fcl4woA6gpKTkYnIEnDkyhMPhoOd53ePHjw+OGTMmHovF7hRP2wj/oPSxY8eWetHwtng0/Otg+dSdo2Y3LzON2wrl87zIZuLeJn4XbUIcug/9JYoskS2Q+xK4fmRrNBrdhu9K5xo+SPYOya5gwLvQP2EGQxoYWVpZWXmRF4kcoOgVOYn2OTmzVyV4iH3xgrwp6kufozz8LPrXIOfE0frhT9H/UtkChV6FfaaoqCiK3oD/PaTCuYcHEj3AYG+Q+EWkFWnH3sXSRuLx+FQvGjlrmnqrTMMrkxXPg3Tif8bqnncA6UHeQtaLI18A/TiFTZAtED9TnDNlv4k0O3N4YBknk/RzZK1s2r+T9JD1jTLlwbLJ75mlb06VLeDbjayUTnG9yPP0+ZPf74+Kcyt2rLCwMCZb0IwTd8SZGqOPHAudOTxUVFSMItFBHqBcNvpapM06b30rPHrGjS1eNNjDwKvgX2PAfYjfxZ6g3yR8d8Dt0QvIO5GP/Rd8aWQN+gS2kybnQ2JaFIe8RPf/dSgMHSQqcqrdo6WlpTopdORdr+Jjgfx5bJv1yAoVZ32Aoq/PzCy+Bm0Tpy+m4NXI/SrcTc5y9ceep5hvA//9yRdtGGdufn2ms4aK/zwWh4uBo5in3YccYDa3MEtxcZoJuO1wesNbxGn24Lq9cOjdwM+u3eGfMv9q8SAX/iEdnzLQpyMD2wkwk7Ow9yMnkRd1ejiXVrCGlSiW7vZ/nXUAxr0O+wlnKu8MuF76nCLn7SLeh7hcxeLP1YmBU2/+MpKVK7G2AJz2X4u4UdfcfS/9TlLsJCWF/yv2merq6lw3oD0leNhK7FPkmEtsKXwr9u6qqio7a9j9+Gxx6HeT52+o9uOEfQuxB6TTVmCfRhKMfwmFj9GgH+DYAtlFTB72YvQd6pABnI6sPmvo69h8flIsWLAR7ilRtG8jv1EepBbZ63i9hE9KzwC7D/80VrcA/Ry5j2qy0HvQ/4nPnuG0S5Ddrk8rvuekDwLyPE8wniD7xaOtJcgeeRlo5uDOWmPdR3Hz6D/KvbC/m9jVovAdJs9E7IfRP0bf7/hH4F6QLmjl8J3VFxd+PnoXcg+ibfQQsh7+fsXSfrXwNcivpA8C4t8E3IXciPig8uC2Iq9h6yirgbsAOx2LBHf5p9QuC1bMTMUjoeMU4CkHMacR+7LS5wViT0r3+Xwx+CPIM8ht8Pvxp+RD1xf4Hs08vg+Ki4tLsJOKcX5tlYPSdZJhv4N0qCZ9kTVoC4TuFW3sH3vUaQ9ir2ElutmD9o4ieJHg8sCEGVsDl165Ipafb1dIIPYX5Bk8NumnF9eeIu6deZqYl2nvECdQQCO57Z1FK6pWLy5xS6UTexn+BukCth+fjt1XaO3d55thRX+5WffnMmf9MMHTzeXpdNfoQZ9lycauwvxpybZYJKQ7xevM3OCFiJiVxG5HXoa/XByzpvvJWh13suFXEWcvYtpqzPYV0gXipuMfXMlhg8I2kExncLU74y+MFhZvKaic2x0P+3U+61jsZ3C7Agx6hEJ1MVsoHipPxYEz7MtxLuao/NLhF2D/ETVzBJ7Bt1j6iEAS7f9zJE9RuO7NE71I6KSpbV1kvvjC7mG4TYj94BC3l8I7aZsRnQi5rvDj5LLvAHwv9gLpcA+jf4bMRqZjf0rbKt+IQJInEJ0wVQzoC4VCU7xw8F1Tn5pqbt8zcJUdKNxeW2l3MvgJ2n9lVoFWX9y+QCAQcTE6WWqlE7sbezmiM3w7W+he2l24RnY9YIAUiTY6U7iQP5tXo/F4z+g5a5q8onE3e5zvmSIp4CBb4hr6raZf5paXg34YWaeCiTnPqhSzguVwv1M/2i1Ir3Ri9EkfvCoPCwySIGG9My3KyspGx0K+x7mL9/gm1/zS81800bkUfx9F/RxV5/56dHvcaZuR5w1kD0Xpu6DYGcjyjB/d5qHwJvqN/Pft/2LJq3PMnYfsJenHhZueu9gk25eYhZ2DH58fD+Z2BE0i3WQSnYM/wCPC/LaAqW4b+R/QkFCXvsokUn8wC9K3OGZ4qEs3k+eYqesa+UdoyKjbOM3UpU6YRPtek+y4zrFDQ306SdEHKfqQqe+wd/zvFlriZHqlSXYeo4jDpq7jMZPoqDWLNo/D5zPJtnweLGwWd00wde2L8G2g4OO24ETKXq6+XzT25lDgHF7cNA+xg/YIhfXxLvyWVTmN7yirshPfszzowB3oB4lGZnt+KmZqN441yc2euWFTgfNkkUUWWWTxk4QxXwJqLp+17S0vxQAAAABJRU5ErkJggg=='}], [{'links': '#main', 'images': '/profiles/cisad8_gov/themes/custom/gesso/dist/images/us_flag_small.png'}], [{'links': 'javascript:jumpScroll($(this).scrollTop());', 'images': '/content/dam/trendmicro/global/en/core/images/logos/tm-logo-red-white-t.svg'}], [{'links': 'https://www.blackberry.com/', 'images': '/etc.clientlibs/bbcom/clientlibs/clientlib-etc-legacy/resources/cylance-web/icons/reports-icon-dark.svg'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.barracuda.com', 'images': '/content/dam/barracuda-corp/images/site/header/logo_barracuda_primary_reversed.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.cookiebot.com/en/what-is-behind-powered-by-cookiebot/', 'images': 'data:image/png;base64,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'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#main', 'images': '/profiles/cisad8_gov/themes/custom/gesso/dist/images/us_flag_small.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'javascript:jumpScroll($(this).scrollTop());', 'images': '/content/dam/trendmicro/global/en/core/images/logos/tm-logo-red-white-t.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.blackberry.com/', 'images': '/etc.clientlibs/bbcom/clientlibs/clientlib-etc-legacy/resources/cylance-web/icons/reports-icon-dark.svg'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.cisa.gov/\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2023/04/282828.svg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://assets.weforum.org/editor/viEYetTtU42i9u-UL3sr2tHrjcZoqKX_qIgdjJe5dnU.PNG\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.blackberry.com/content/dam/bbcomv4/blackberry-com/en/solutions/threat-intelligence/2023/threat-intelligence-report-april/figure-3-top-10-countries-experienced-cyberattacks-dark-it2.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.corvusinsurance.com/hs-fs/hubfs/Ransomware%20Victims%20by%20Month.png?width=3568&height=2007&name=Ransomware%20Victims%20by%20Month.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/07/Untitled-design-2.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.threatdown.com/wp-content/uploads/2024/05/easset_upload_file80150_275724_e.webp?w=1024\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.chainalysis.com/wp-content/uploads/2024/02/rware-2023-quarterly-800x314.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Cyber AI Copilot Response:\n",
            "**Executive Summary:**\n",
            "\n",
            "The Black Basta ransomware group, active since April 2022, has claimed nearly 100 victims in its first seven months and impacted over 500 organizations worldwide as of May 2024. It is known for its sophisticated tactics and attacks across multiple sectors, including the Catholic healthcare system Ascension. Black Basta operates as a Ransomware-as-a-Service (RaaS) model, with affiliates targeting businesses and critical infrastructure in North America, Europe, and Australia. The group has breached vulnerabilities related to ConnectWise ScreenConnect authentication bypass and Microsoft Exchange Server vulnerabilities. Defenders should keep operating systems, software, and firmware up-to-date, require phishing-resistant Multi-Factor Authentication (MFA) for as many services as possible, and train users to recognize and report phishing attempts to mitigate Black Basta ransomware attack risks. [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "\n",
            "**Detailed Analysis:**\n",
            "\n",
            "*Key Findings:*\n",
            "- Black Basta ransomware group first emerged in April 2022 and has claimed nearly 100 victims in its first seven months [Source: Tavily Search (<https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics>)].\n",
            "- The group has impacted over 500 organizations worldwide as of May 2024 [Source: CISA (<https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a>)].\n",
            "- Black Basta affiliates have targeted businesses and critical infrastructure in North America, Europe, and Australia [Source: Fortinet (<https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware>)].\n",
            "- The ransomware group has breached vulnerabilities related to ConnectWise ScreenConnect authentication bypass and Microsoft Exchange Server vulnerabilities [Source: HealthcareITNews (<https://www.healthcareitnews.com/news/aha-h-isac-warn-hospitals-about-black-basta-following-ascension-cyberattack>)].\n",
            "\n",
            "*Technical Details:*\n",
            "- Black Basta is a Ransomware-as-a-Service (RaaS) model [Source: Fortinet (<https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware>)].\n",
            "- The ransomware group has caused massive breaches in a short span of time [Source: Trend Micro (<https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html>)].\n",
            "- Black Basta affiliates have targeted organizations using ConnectWise ScreenConnect authentication bypass and Microsoft Exchange Server vulnerabilities [Source: HealthcareITNews (<https://www.healthcareitnews.com/news/aha-h-isac-warn-hospitals-about-black-basta-following-ascension-cyberattack>)].\n",
            "\n",
            "*Contextual Analysis:*\n",
            "- Black Basta attacks have impacted various sectors, including the Catholic healthcare system Ascension [Source: Tavily Search (<https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics>)].\n",
            "- The ransomware group has caused significant disruption, with over 500 organizations impacted worldwide [Source: CISA (<https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a>)].\n",
            "- Defenders should prioritize security measures such as up-to-date operating systems, software, firmware, phishing-resistant Multi-Factor Authentication (MFA), and user training to mitigate Black Basta ransomware attack risks [Source: CISA (<https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a>)].\n",
            "\n",
            "**Evidence and Citations:**\n",
            "[Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics) | [CISA](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a) | [Fortinet](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware) | [Trend Micro](https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html) | [HealthcareITNews](https://www.healthcareitnews.com/news/aha-h-isac-warn-hospitals-about-black-basta-following-ascension-cyberattack)\n",
            "\n",
            "**Actionable Intelligence:**\n",
            "- Implement up-to-date operating systems, software, and firmware.\n",
            "- Require phishing-resistant Multi-Factor Authentication (MFA) for as many services as possible.\n",
            "- Train users to recognize and report phishing attempts.\n",
            "- Monitor for indicators of compromise related to Black Basta ransomware.\n",
            "\n",
            "**Future Implications:**\n",
            "- Continue monitoring Black Basta ransomware activity and potential evolution.\n",
            "- Assess the impact of Black Basta ransomware on various sectors and industries.\n",
            "- Develop and implement strategies to mitigate the risks associated with Ransomware-as-a-Service (RaaS) models.\n",
            "\n",
            "**Sources**\n",
            "- [Google Serper](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a)\n",
            "- [Tavily Search](https://blog.barracuda.com/2024/05/18/black-basta-nasty-tactics)\n",
            "- [Google Serper Image Search](https://www.blackberry.com/content/dam/bbcomv4/blackberry-com/en/solutions/threat-intelligence/2023/threat-intelligence-report-april/figure-3-top-10-countries-experienced-cyberattacks-dark-it2.webp)\n",
            "- [Google Serper](https://spin.ai/resources/ransomware-tracker/)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/biggest-cyber-attacks-data-breaches-ransomware-attacks-february-2024)\n",
            "- [Google Serper Image Search](https://www.corvusinsurance.com/hs-fs/hubfs/Ransomware%20Victims%20by%20Month.png?width=3568&height=2007&name=Ransomware%20Victims%20by%20Month.png)\n",
            "- [Google Serper](https://www.blackberry.com/us/en/solutions/threat-intelligence/threat-report)\n",
            "- [Google Programmable Image Search](https://www.cisa.gov/)\n",
            "- [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "- [Google Serper Image Search](https://www.corvusinsurance.com/hs-fs/hubfs/number%20of%20reported%20attacks.png?width=1600&height=1030&name=number%20of%20reported%20attacks.png)\n",
            "- [Google Programmable Search](https://cyberint.com/blog/research/ransomware-trends-2024-report/)\n",
            "- [Tavily Search](https://www.bleepingcomputer.com/news/security/cisa-black-basta-ransomware-breached-over-500-orgs-worldwide/)\n",
            "- [Google Serper Image Search](https://assets.weforum.org/editor/viEYetTtU42i9u-UL3sr2tHrjcZoqKX_qIgdjJe5dnU.PNG)\n",
            "- [Google Programmable Search](https://www.resecurity.com/blog/article/ransomware-attacks-against-the-energy-sector-on-the-rise-nuclear-and-oil-gas-are-major-targets-2024)\n",
            "- [Google Serper](https://www.healthcareitnews.com/news/aha-h-isac-warn-hospitals-about-black-basta-following-ascension-cyberattack)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-04-07-131535.webp)\n",
            "- [Google Programmable Search](https://www.blackberry.com/us/en/solutions/endpoint-security/ransomware-protection/black-basta)\n",
            "- [Google Serper](https://www.infosecurity-magazine.com/news/ransomware-demands-staggering-5m/)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2023/04/282828.svg)\n",
            "- [Google Serper](https://www.cyfirma.com/research/tracking-ransomware-october-2024/)\n",
            "- [Tavily Search](https://www.trendmicro.com/en_us/research/22/e/examining-the-black-basta-ransomwares-infection-routine.html)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/07/Untitled-design-2.webp)\n",
            "- [Google Serper Image Search](https://www.chainalysis.com/wp-content/uploads/2024/02/rware-2023-quarterly-800x314.png)\n",
            "- [Google Serper Image Search](https://www.threatdown.com/wp-content/uploads/2024/05/easset_upload_file80150_275724_e.webp?w=1024)\n",
            "- [Google Serper](https://cybersecurityventures.com/ransomware-report/)\n",
            "- [Google Programmable Image Search](https://www.weforum.org/stories/2024/01/ransomeware-ai-cybersecurity-news-roundup/)\n",
            "- [Google Serper Image Search](https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png)\n",
            "- [Google Serper](https://tech.co/news/data-breaches-updated-list)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}