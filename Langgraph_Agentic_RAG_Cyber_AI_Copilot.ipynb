{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "85227c1f-7882-49b0-ccc4-e2abb999698a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "2024-11-13 09:17:22.819763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-13 09:17:22.847277: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-13 09:17:22.855689: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-13 09:17:24.357616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38239ef1-f282-4df5-bd22-2a6a2994ba47"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = (datetime.now() - date).days if date else 0\n",
        "        final_score = similarity + metadata_score - date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = (datetime.now() - date).days if date else 0\n",
        "            final_score = weight + source_weight - date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are a highly advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse data sources, yielding a response that combines comprehensive detail with actionable intelligence, customized to the specific context of each query.\n",
        "\n",
        "---\n",
        "\n",
        "**ANALYSIS PROTOCOL** *(Structured in Phases)*:\n",
        "\n",
        "1. **Source and Credibility Verification**:\n",
        "   - **Evaluate Domain Reliability**: Prioritize reputable cybersecurity and intelligence sources.\n",
        "   - **Ensure Timeliness**: Confirm that information is recent and directly relevant to the query.\n",
        "   - **Cross-Reference Findings**: Validate key data points across multiple credible sources, flagging any discrepancies.\n",
        "   - **Misinformation Detection**: Identify any unsupported or exaggerated claims, ensuring accuracy throughout.\n",
        "\n",
        "2. **Content Extraction and Relevance Filtering**:\n",
        "   - **Core Data Identification**: Extract essential information, such as threat vectors, indicators, metrics, and statistics.\n",
        "   - **Pattern Recognition**: Look for and highlight recurring themes, correlations, and emerging trends across sources.\n",
        "   - **Contextual Prioritization**:\n",
        "     * **Temporal Relevance**: Focus on the most recent and high-impact data.\n",
        "     * **Technical Depth**: Prioritize technical details pertinent to the query’s context.\n",
        "     * **Query Alignment**: Rank findings by their direct applicability to the user’s stated question.\n",
        "\n",
        "3. **Visual and Media Analysis**:\n",
        "   - **Visual Validation**: Assess images, diagrams, and screenshots for relevance, especially for technical PoCs or attack evidence.\n",
        "   - **Extract Technical Indicators**: Capture relevant data from visual media, such as IPs, file hashes, or attack paths.\n",
        "   - **Correlate Visual and Textual Information**: Ensure any media content aligns with textual data, emphasizing its technical implications.\n",
        "\n",
        "---\n",
        "\n",
        "**ADAPTIVE RESPONSE STRUCTURE** *(Dynamic format based on query type)*:\n",
        "\n",
        "1. **Executive Summary** *(Concise overview, 8-10 sentences)*:\n",
        "   - Summarize core findings, noting any urgent or high-priority insights and recommendations.\n",
        "\n",
        "2. **In-Depth Analysis** *(Tailored to query specifics)*:\n",
        "   - **Key Findings**:\n",
        "     * Bullet-point list of critical discoveries, emerging threats, or significant events.\n",
        "     * Include metrics, trends, or any statistical data directly relevant to the query.\n",
        "   - **Technical Breakdown**:\n",
        "     * Detailed information on specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "     * Affected components and dependencies, along with remediation measures if applicable.\n",
        "   - **Contextual and Industry Impact**:\n",
        "     * Analyze industry or sector-specific implications.\n",
        "     * Attribute threat actors or TTPs (Tactics, Techniques, and Procedures) if identifiable.\n",
        "     * Link with historical incidents or precedents for added context.\n",
        "\n",
        "3. **Source Citations and Evidence**:\n",
        "   - Cite all findings in the format [Source Name](URL), explicitly linking sources to major claims.\n",
        "   - Provide quote snippets when referencing specific assertions, and ensure context.\n",
        "   - **Media Links**: Reference any visual media, linking to screenshots or diagrams if available, with a brief description of their relevance.\n",
        "\n",
        "4. **Actionable Recommendations**:\n",
        "   - Provide specific, immediate actions and mitigation steps.\n",
        "   - Outline detection and prevention strategies directly relevant to the identified threats.\n",
        "   - Highlight operational security measures, particularly for high-severity issues.\n",
        "\n",
        "5. **Long-Term Forecast and Monitoring**:\n",
        "   - Discuss projected threat evolution and potential impacts.\n",
        "   - Highlight trends for ongoing observation and provide recommendations for continuous monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "**SPECIALIZED QUERY HANDLING** *(Select based on context)*:\n",
        "\n",
        "1. **For Threat Intelligence Queries**:\n",
        "   - Extract IOCs (Indicators of Compromise) such as IPs, domains, and hashes.\n",
        "   - Map relevant data to MITRE ATT&CK TTPs and assess malware behavior patterns.\n",
        "   - Document Command and Control (C2) configurations if applicable.\n",
        "\n",
        "2. **For Vulnerability and Exploit Analysis**:\n",
        "   - Validate CVE details, including severity, affected systems, and patch status.\n",
        "   - Assess real-world exploitability, contextualizing with any reported attacks.\n",
        "\n",
        "3. **For Incident Response**:\n",
        "   - Construct attack timelines, reconstructing paths and points of entry.\n",
        "   - Provide recovery steps and guidance for immediate containment and response.\n",
        "\n",
        "4. **For Trend Analysis**:\n",
        "   - Identify changes in attack patterns, mapping against historical baselines.\n",
        "   - Forecast likely developments in tactics or threat actors’ capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "**PROMPT VARIABLES**:\n",
        "- **Previous Context**: {chat_history}\n",
        "- **Current Query**: {input}\n",
        "- **Search Results**: {search_results}\n",
        "- **Additional Crawled Data**: {crawled_results}\n",
        "- **Current Date**: {current_date}\n",
        "\n",
        "---\n",
        "\n",
        "**RESPONSE REQUIREMENTS**:\n",
        "1. **Precision and Depth**: Ensure technical accuracy and concise, detailed insights.\n",
        "2. **Confidence Levels**: State the certainty of all assessments, highlighting uncertainties.\n",
        "3. **Citation Accuracy**: Use the [Source Name](URL) format for each major source; include relevant media references where applicable.\n",
        "4. **Highlight Urgency**: Mark any urgent or time-sensitive findings.\n",
        "5. **Readable Structure**: Use clear headings, subheadings, and bullet points for readability.\n",
        "6. **Address Gaps and Uncertainties**: Note any information limitations.\n",
        "7. **Embedded Media Links**: Include links to any referenced media (screenshots, diagrams) with brief contextual descriptions.\n",
        "8. **Actionable and Context-Specific Recommendations**: Tailor guidance based on query context.\n",
        "9. **Technical Context Maintenance**: Retain technical rigor and avoid generalizations.\n",
        "\n",
        "---\n",
        "\n",
        "Generate a comprehensive, precise response that directly addresses the query by synthesizing and presenting the latest, most relevant intelligence, and ensuring actionable insights and credible, source-backed evidence.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Share some details on currently active Infostealer malware and give me their TTPs and IOCs?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5be0611d-4f78-4b45-d711-ea2e86efd448"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Share some details on currently active Infostealer malware and give me their TTPs and IOCs?\n",
            "DEBUG: Raw results from Exa Search: Title: IsaacWiper (Malware Family)\n",
            "URL: https://malpedia.caad.fkie.fraunhofer.de/details/win.isaacwiper\n",
            "ID: https://malpedia.caad.fkie.fraunhofer.de/details/win.isaacwiper\n",
            "Score: 0.1686544120311737\n",
            "Published Date: 2023-03-15T00:00:00.000Z\n",
            "Author: Fraunhofer FKIE\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: According to Recorded Future, IsaacWiper is a destructive malware that overwrites all physical disks and logical volumes on a victim’s machine.\n",
            "Highlights: ['According to Recorded Future, IsaacWiper is a destructive malware that overwrites all physical disks and logical volumes on a victim’s machine.']\n",
            "Highlight Scores: [0.29397985339164734]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Reports | Triage\n",
            "URL: https://tria.ge/s/tag:infostealer\n",
            "ID: https://tria.ge/s/tag:infostealer\n",
            "Score: 0.16774824261665344\n",
            "Published Date: 2024-06-15T04:33:47.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Sample ID\n",
            "Created\n",
            "Filename Tags\n",
            "Status/Score\n",
            "SHA256\n",
            " \n",
            "240615-e2hsysyfnf\n",
            "15-06-2024 04:26\n",
            "UTC\n",
            "16d52786d50d7c0bad6ef837ec04aa3c.exe \n",
            " evasion dcrat infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-e2clyasfrp\n",
            "15-06-2024 04:25\n",
            "UTC\n",
            "522f0a2aa0ab0cc5a76abe35435f83f1c4ad38328df296fd308f8e5825cad713 \n",
            " 0e6740 @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer lumma redline risepro infostealer persistence spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-e11blssfqn\n",
            "15-06-2024 04:25\n",
            "UTC\n",
            "d21c1dae567563d5e9bd69de0eaa4822b5274fb9ccf5026b2c2b0adaaed5cf3b \n",
            " @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer lumma redline infostealer persistence spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-d2216axgla\n",
            "15-06-2024 03:31\n",
            "UTC\n",
            "0c1b8d8d00d578c844a14875fb7599d3.exe \n",
            " evasion dcrat infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-dk6m1s1cjq\n",
            "15-06-2024 03:05\n",
            "UTC\n",
            "f5567fd47eb3b902426098c8c06d99df.bin \n",
            " logsdiller cloud (tg: @logsdillabot) redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-daxblazhmk\n",
            "15-06-2024 02:48\n",
            "UTC\n",
            "da5844b02ebfa56b4c036ea50136e7766922fa1591d344130f5492e5624fdf5d \n",
            " duc evasion redline infostealer persistence themida trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-c6matszgkn\n",
            "15-06-2024 02:41\n",
            "UTC\n",
            "f109e5691b8fedddeec53b85ced5c60cb3010b6219964b7f0262c5ebfd191242.exe \n",
            " hackus_checker_cracked discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cy1s9awepb\n",
            "15-06-2024 02:29\n",
            "UTC\n",
            "6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cykf9swenb\n",
            "15-06-2024 02:29\n",
            "UTC\n",
            "WiFiService.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cx7v6szekp\n",
            "15-06-2024 02:28\n",
            "UTC\n",
            "970a1609fc0058e529a6114875cd9b708d0377057f191e4ed8b5c870f0d4f3f8 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cla23awbkg\n",
            "15-06-2024 02:09\n",
            "UTC\n",
            "aa2ed5d30a8cfbc39f9a129bc3ca6fd4d07863c0e72ea945af19d3cb674c479e \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-clpkfszbkl\n",
            "15-06-2024 02:10\n",
            "UTC\n",
            "bruno-wi1.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ck49hswbjh\n",
            "15-06-2024 02:09\n",
            "UTC\n",
            "bruno-sis2.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ck2tdswbje\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "c35983b0754cf274ea4c071bd2f9c0fb45972e893d805f62ec8a1fc8ee97e9e3 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ckq2wszaqn\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "3b2749bfd91ab0a27ef988b84aeafbca81d4d9f3d63ab7e594655b266f27a30f \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-ckkj4swarg\n",
            "15-06-2024 02:08\n",
            "UTC\n",
            "551d61685f69372204f69da1394b4155a4f732d5a4ff5bdb67f11ae7a8e7ee50 \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-cdsbdavgnb\n",
            "15-06-2024 01:57\n",
            "UTC\n",
            "a68d68e210a3ee6e9d8ef30c63de4658ef1023fbf27ca7c8d7b79bc6351f5677.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b8xznsyeqn\n",
            "15-06-2024 01:49\n",
            "UTC\n",
            "88b889a1477c81510c62a46c9eb1d77d386c59dceb0523e8b5734b6dde252573.exe \n",
            " cheat execution redline sectoprat infostealer rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b7qtzayekm\n",
            "15-06-2024 01:47\n",
            "UTC\n",
            "ac78fe3a2934efdfda73727906502f6a_JaffaCakes118 \n",
            " android banker collection credential_access discovery evasion anubis infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b7pa5syekl\n",
            "15-06-2024 01:47\n",
            "UTC\n",
            "83037ad76ddddabca05efe07e731d65c5d9069ad889e46306b753cbc7561fa59.exe \n",
            " 0011 discovery redline xworm infostealer rat spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b479vavcqf\n",
            "15-06-2024 01:43\n",
            "UTC\n",
            "6f42327cec9d52b7b30a0efab03df9d30fc1bd5e9a5b6a6d0c1c23d99cdb1349.exe \n",
            " kir redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-b4qd2avcpa\n",
            "15-06-2024 01:42\n",
            "UTC\n",
            "6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe.apk \n",
            " android collection discovery evasion tispy infostealer persistence spyware trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bpxvkaxglj\n",
            "15-06-2024 01:19\n",
            "UTC\n",
            "2849878b8913c66392f6202039c1d38e2b7061daec60947671795f1e1cd63db5.exe \n",
            " execution dcrat infostealer rat spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bpbmbstfmh\n",
            "15-06-2024 01:18\n",
            "UTC\n",
            "26351c3679d0ab48f8fb8503b165237c1f4b194a2ceb16df788b53b3875bf9b7.xll \n",
            " warzonerat infostealer persistence rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bjx85sxenn\n",
            "15-06-2024 01:11\n",
            "UTC\n",
            "2a27295081e9c2b7e02815fd6fc556ec.exe \n",
            " evasion dcrat infostealer rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bc9zcsxcnl\n",
            "15-06-2024 01:01\n",
            "UTC\n",
            "a554f91e1f53fd240d00614f4b42cc47db6c0d389ada198cd0fed62ba855741b.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-bc9ctstbrd\n",
            "15-06-2024 01:01\n",
            "UTC\n",
            "00b1ea6a2a6a6cc82331e94e37af46027fbfdb340ed465d5d01d136b6f777240.exe \n",
            " logsdiller cloud (tg: @logsdillabot) redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-a9wybstbja\n",
            "15-06-2024 00:55\n",
            "UTC\n",
            "aef7507afedaa98be7220764512b7884453db0a3af40aa6a5c2b627a031a95d6 \n",
            " aspackv2 evasion warzonerat infostealer persistence rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-a42avsshph\n",
            "15-06-2024 00:46\n",
            "UTC\n",
            "1849639219e47189b9836bb17998f72714ca9e785b6ee9fbce5933e050754a3d.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240615-anag7ssdkh\n",
            "15-06-2024 00:20\n",
            "UTC\n",
            "e35e252ec2c793fe44c09e744495b6d841c3e0c81807d08fb2923b08bf89d6e6.bin \n",
            " android banker collection discovery evasion xloader_apk impact infostealer persistence stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2yjmeatcln\n",
            "14-06-2024 22:59\n",
            "UTC\n",
            "EnergyBetaCrack.exe \n",
            " evasion dcrat infostealer rat \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2qengsyhnc\n",
            "14-06-2024 22:46\n",
            "UTC\n",
            "Loader.exe \n",
            " @bloodyrain12 redline infostealer spyware \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2nh8wssgrm\n",
            "14-06-2024 22:43\n",
            "UTC\n",
            "80f4b2a7dff768ce67ca462260e1157d267aacbd397f90421373458952e06aca.bin \n",
            " android banker collection credential_access discovery evasion execution hook impact infostealer persistence rat stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-2gbkgasenm\n",
            "14-06-2024 22:32\n",
            "UTC\n",
            "7f2b1dcdb486730421552c9dd6bf09e9d386e2d59ca5fc542bbc66b0b2181c47.bin \n",
            " android collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-17js4ayaqd\n",
            "14-06-2024 22:17\n",
            "UTC\n",
            "abb7d2d64ee38fb8fb9b0191126a280c_JaffaCakes118 \n",
            " oski infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-16ccmayamc\n",
            "14-06-2024 22:15\n",
            "UTC\n",
            "bda689dc6532b4978c5a97a87c2acc3cad767489414bbebbafe1325d03132fbc.bin \n",
            " android banker collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat stealth trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-11214a1hjj\n",
            "14-06-2024 22:07\n",
            "UTC\n",
            "9073568a1855141fa0de08a35fb37216b8571bd92cc45a564af2d787e8924aca.bin \n",
            " android collection credential_access discovery evasion execution ermac hook impact infostealer persistence rat trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-1mlf7axckh\n",
            "14-06-2024 21:46\n",
            "UTC\n",
            "15f948da0e0786ee883bc9714ee6b47a.exe \n",
            " cheat discovery execution redline sectoprat infostealer rat spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-1htl1axara\n",
            "14-06-2024 21:39\n",
            "UTC\n",
            "ab95b07eeb30a98ec33aa2cb0c8d7929_JaffaCakes118 \n",
            " azorult infostealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-xfmtwswdrp\n",
            "14-06-2024 18:47\n",
            "UTC\n",
            "Malware with taskmgr.zip \n",
            " 0011 0e6740 @logscloudyt_bot e76b71 livetraffic newbild discovery evasion execution amadey exelastealer gh0strat phorphiex purplefox redline risepro tofsee xehook xworm infostealer loader persistence ransomware rat rootkit spyware stealer themida trojan upx worm \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zhy12sygjl\n",
            "14-06-2024 20:43\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zgtd6sveqf\n",
            "14-06-2024 20:41\n",
            "UTC\n",
            "build.exe \n",
            " 1 redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zfq8esyfjp\n",
            "14-06-2024 20:39\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zelxbavekb\n",
            "14-06-2024 20:37\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-zdq5msvdqb\n",
            "14-06-2024 20:36\n",
            "UTC\n",
            "build.exe \n",
            " 1 redline infostealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-y5fmjsyblk\n",
            "14-06-2024 20:21\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-y3hdcsyanr\n",
            "14-06-2024 20:18\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yzc98sxhlp\n",
            "14-06-2024 20:13\n",
            "UTC\n",
            "ab43169d586372e2f42989aa10b89cce_JaffaCakes118 \n",
            " azorult infostealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yrcczaxemr\n",
            "14-06-2024 20:00\n",
            "UTC\n",
            "ab378a9483a51f89fe59858d6ad7e3c9_JaffaCakes118 \n",
            " collection hawkeye_reborn m00nd3v_logger infostealer keylogger spyware stealer trojan \n",
            "10 \n",
            "Reported\n",
            " \n",
            " \n",
            "240614-yn3qraxdnr\n",
            "14-06-2024 19:56\n",
            "UTC\n",
            "build.exe \n",
            " 1 discovery redline infostealer spyware stealer \n",
            "10 \n",
            "Reported\n",
            "Highlights: [' duc evasion redline infostealer persistence themida trojan  f109e5691b8fedddeec53b85ced5c60cb3010b6219964b7f0262c5ebfd191242.exe   hackus_checker_cracked discovery redline infostealer spyware stealer  6a386248e8856ebd0841cb70e0433189b251c4dbe9bc2dce2096d6996266abbe   android collection discovery evasion tispy infostealer persistence spyware trojan ']\n",
            "Highlight Scores: [0.254659503698349]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: From DarkGate to AsyncRAT: Malware Detected and Shared As Unit 42 Timely Threat Intelligence\n",
            "URL: https://unit42.paloaltonetworks.com/unit42-threat-intelligence-roundup/\n",
            "ID: https://unit42.paloaltonetworks.com/unit42-threat-intelligence-roundup/\n",
            "Score: 0.16535604000091553\n",
            "Published Date: 2023-12-29T14:00:38.000Z\n",
            "Author: Samantha Stallings, Brad Duncan\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: This post is also available in:\n",
            "  日本語   (Japanese)       Executive Summary\n",
            "This article summarizes the malware families (and groups pushing malware) seen by Unit 42 and shared with the broader threat hunting community through our social channels. Some malware – such as IcedID and DarkGate – came up repeatedly. We also included a number of posts about the cybercrime group TA577 – who have distributed multiple malware families but here favor Pikabot. In other cases, we posted about newer malware such as JinxLoader.\n",
            "By sharing timely threat intelligence via social media channels, we report on malware infections and other threat intelligence of note in an expedited manner. These posts summarize the infection chain, offer helpful screenshots of active traffic and point towards indicators of compromise (IoCs). In 2023, our 93 timely threat intelligence posts in total generated 1.6 million-plus impressions, showing the value of getting IoCs out to the community quickly.\n",
            "This article reviews all our timely threat intelligence released from October through December 2023. Summarizing these threat intelligence posts provides an opportunity to spot trends that are less visible in single posts. We’ve included a table in the Indicators of Compromise section that lists all the posts in full by date posted, name, links to social media channels and IoCs on GitHub.\n",
            "Many of these posts contain screenshots of infection traffic filtered in Wireshark, links to the network IoCs and comments linking to packet captures (pcaps) of the associated activity, so this article also provides readers an opportunity to practice and improve their Wireshark skills.\n",
            " The IoCs shared in the social posts are all considered malicious by Palo Alto Networks products. These verdicts are used, for example, by   cloud-delivered security services   such as   Advanced WildFire   and   Advanced URL Filtering   for the   Next-Generation Firewall  .  If you think you might have been compromised or have an urgent matter, contact the   Unit 42 Incident Response team  . \n",
            "To see our timely threat intelligence posts as we publish them, follow Unit 42 on X and LinkedIn.\n",
            " Related Unit 42 Topics \n",
            " Wireshark, Malware, RATs, Trojan \n",
            " Malware Families Mentioned \n",
            "DarkGate, Pikabot, IcedID, AsyncRAT, JinxLoader\n",
            "  Table of Contents\n",
            " Timely Threat Intelligence \n",
            "Timely Threat Intelligence: October \n",
            "DarkGate \n",
            "Pikabot \n",
            "IcedID (Bokbot) \n",
            "WS_FTP Server Critical Vulnerability \n",
            "AsyncRAT \n",
            "Citrix NetScaler \n",
            "Timely Threat Intelligence: November \n",
            "IcedID \n",
            "DarkGate \n",
            "Pikabot \n",
            "JinxLoader \n",
            "Timely Threat Intelligence: December \n",
            "Loader EXE \n",
            "DarkGate \n",
            "Astaroth/Guildma \n",
            "Pikabot \n",
            "Conclusion \n",
            "Protections and Mitigations \n",
            "Indicators of Compromise \n",
            "Additional Resources \n",
            "  Timely Threat Intelligence\n",
            "In addition to the in-depth articles published on this site, Unit 42 also shares timely threat intelligence – IoCs, TTPs and other observations about active campaigns – through our social channels.\n",
            "Unit 42 shared the first public post about JinxLoader. This information led at least one other vendor, ProofPoint’s Emerging Threats (ET) Labs team, to create a new signature triggering on traffic patterns generated by this malware.\n",
            "Besides recapping all social media posts published from October to December, we’ve included a table at the end of this review that includes links to the original posts as well as to all of the IoCs on our GitHub. These original posts include the images from the threat intelligence shared, which range from screen captures of malware and artifacts to the associated traffic filtered in Wireshark. Here we’ll only include the infection chain (as applicable), but head over to X (formerly Twitter) or LinkedIn to review the rest.\n",
            "A note: The infection date is not always the same as the date when shared on social channels. The table in the IoCs section includes the posting date. The infection date itself is included in all of the infection chain images. Don’t get confused if you’re comparing the infection date to the posting date and they don’t match up!\n",
            "  Timely Threat Intelligence: October\n",
            "  DarkGate\n",
            "We reported on two instances of DarkGate in October. The first instance was DarkGate malware distributed through Microsoft Teams. The attacker posed as the target organization's CEO and sent victims a Teams invite. The message sent contains a password-protected .zip archive. See the entire infection chain below in Figure 1.\n",
            " Figure 1. DarkGate infection chain from Microsoft Teams message. \n",
            "The second instance saw DarkGate malware distributed through fake invoice or billing emails with PDF attachments that spoof DocuSign. Figure 2 illustrates how the process worked. An attentive reader will be able to spot the differences between these two infection chains.\n",
            " Figure 2. DarkGate infection chain from email. \n",
            "  Pikabot\n",
            "We also reported on two instances of Pikabot in October. The first was a Pikabot infection leading to Cobalt Strike HTTPS C2 traffic using zzerxc[.]com on 179.60.149[.]244:443. Figure 3 shows the full series of events.\n",
            " Figure 3. Pikabot infection chain. \n",
            "The second instance saw the cybercrime threat actor TA577 pushing a Pikabot infection with HTTPS Cobalt Strike traffic on 45.155.249[.]171:443 using ponturded[.]com. We’ll definitely see more TA577 activity in this roundup – the infection chain below in Figure 4 will differ from other TA577 activity.\n",
            " Figure 4. Pikabot infection chain. \n",
            "  IcedID (Bokbot)\n",
            "Our only report in October of banking Trojan IcedID saw a forked variant infection with BackConnect, Anubis VNC, CobaltStrike and ConnectWise ScreenConnect. We also saw \"hands on the keyboard\" approximately 95 minutes after initial infection! Figure 5 lays out how this variant worked.\n",
            " Figure 5. IcedID forked variant infection chain. \n",
            "  WS_FTP Server Critical Vulnerability\n",
            "We observed multiple attempts to exploit the WS_FTP Server Critical Vulnerability, where threat actors attempted to deliver a Meterpreter payload via the URL 103[.]163.187.12:8080/cz3eKnhcaD0Fik7Eexo66A. Figure 6 includes not only the infection chain but the command line used.\n",
            " Figure 6. Infection chain exploiting WS_FTP. \n",
            "  AsyncRAT\n",
            "A 404 TDS URL chain led to an infection by an AsyncRAT variant. Figure 7 shows the simple infection chain.\n",
            " Figure 7. AsyncRAT variant infection chain. \n",
            "  Citrix NetScaler\n",
            "October 2023 saw several indicators of criminals exploiting the Citrix remote-code execution vulnerability CVE-2023-3519. Monitoring this vulnerability in the wild led to a timely snapshot of associated activity. Figure 8 displays the information in a simple column graph. We saw the most detections of this exploit – over 300 – from jscloud[.]biz.\n",
            " Figure 8. Snapshot of data from Oct. 17, 2023 showing instances of Citrix RCE vulnerability CVE-2023-3519 in the wild. \n",
            "  Timely Threat Intelligence: November\n",
            "  IcedID\n",
            "Our first timely threat intelligence post in November saw an IcedID (aka Bokbot) infection from an .msi file. Along with the regular HTTPS C2 traffic, we saw IcedID BackConnect activity on 159.89.124[.]188:443. Note the activity highlighted in red in Figure 9.\n",
            " Figure 9. IcedID infection chain stemming from unknown source. \n",
            "Cybercrime group TA577 once more distributed an IcedID (aka Bokbot) variant via a disk image downloaded from an emailed link. Figure 10 illustrates this process. As we continue to review TA577 activity, trends will begin to emerge for analysts to be mindful of.\n",
            " Figure 10. Infection chain where cybercrime group TA577 distributes an IcedID variant. \n",
            "  DarkGate\n",
            "At least two instances of DarkGate reared their heads in November. In our first sighting, a probable email led the victim to a password-protected .zip file. See it in full with Figure 11.\n",
            " Figure 11. DarkGate infection chain stemming from probable email. \n",
            "The second November appearance of DarkGate also came from an unknown source distributing a password-protected .zip. Comparing and contrasting to the DarkGate activity seen elsewhere in this post shows the variations attackers implement.\n",
            " Figure 12. DarkGate infection chain from unknown source.   \n",
            "  Pikabot\n",
            "A 10-hour infection run led to our list of IoCs from the Pikabot sighting found in November, once more spearheaded by TA577. This infection was from an email and led to a persistent Pikabot DLL. See Figure 13 for the details.\n",
            " Figure 13. Infection chain of Pikabot distributed by TA577. \n",
            "  JinxLoader\n",
            "Reportedly named for a League of Legends character, JinxLoader is written in Go. Symantec issued a protection bulletin on JinxLoader just a short while ago. In our post about it (the first public post!) we note that JinxLoader is a relatively new malware service first posted to hackforums[.]net on April 30, 2023. The eight steps of this infection chain are detailed in Figure 14.\n",
            " Figure 14. Infection chain of JinxLoader distributing Formbook/Xloader. \n",
            "  Timely Threat Intelligence: December\n",
            "  Loader EXE\n",
            "We started off December by spotting an EXE Loader leading to unidentified malware with C2 using encoded/encrypted TCP traffic on 91.92.120[.]119:62520. See Figure 15.\n",
            " Figure 15. Infection chain of loader to unidentified malware. \n",
            "While one security vendor has identified the loader as “PureLoader” and the unidentified malware as a “PureLogs” stealer, we have seen little else shared publicly on this unidentified malware.\n",
            "  DarkGate\n",
            "We reported on one example of DarkGate malware in December 2023. This example was distributed though a PDF file found on VirusTotal. The PDF file has a link that downloaded a malicious ZIP archive for DarkGate. Figure 16 shows a screenshot of the PDF file.\n",
            " Figure 16. DarkGate infection from PDF that links to malicious .zip archive. \n",
            "  Astaroth/Guildma\n",
            "Astaroth and Guildma may sound like characters from a 1970s pulp sci-fi novel with three moons and blue-skinned aliens on the cover, but they’re actually the name of malware we saw in a Portuguese-language email impersonating Brazil’s State Transport Department (Detran) as shown in Figure 17. This Detran-themed malspam tempted the end user with a link for a zip download – which is how the Guildma (aka Astaroth) malware infection would start.\n",
            " Figure 17. Screenshot of email written in Portuguese that links to malicious download. \n",
            "  Pikabot\n",
            "Our second-to-last report of TA577 in 2023 sees the group distributing Pikabot from an email link. Figure 18 shows the sequence. How does this differ from the previous entries about TA577?\n",
            " Figure 18. Infection chain of Pikabot pushed by threat group TA577. \n",
            "In the last of our threat intelligence shares for the year (barring additional incidents), we see that, once more, TA577 is spreading a Pikabot infection.\n",
            "In this instance, it led to Cobalt Strike on 207.246.99[.]159:443 using masterunis[.]net as its domain. Figure 19 dissects the traffic.\n",
            " Figure 19. Pikabot malware traffic shown in Wireshark. \n",
            "  Conclusion\n",
            " Much of our timely threat intelligence focuses on Windows malware, and we seek to post on malware families of current interest to the community. \n",
            "If you’re interested in following our updates in real time, follow us on LinkedIn or X (formerly Twitter). If you track hashtags, follow #Unit42ThreatIntel to always catch the latest posts. Another option is to sign up for notifications on our GitHub repo.\n",
            "As soon as you’re in the know, you’re also welcome to participate: comment, share or ask questions.\n",
            "  Protections and Mitigations\n",
            " The IoCs shared in the social posts are all considered malicious by Palo Alto Networks products. These verdicts are used, for example, by   cloud-delivered security services   such as   Advanced WildFire   and   Advanced URL Filtering   for the   Next-Generation Firewall  .  If you think you might have been compromised or have an urgent matter, contact the   Unit 42 Incident Response team  . \n",
            "  Indicators of Compromise\n",
            " Date Posted \n",
            " Infection \n",
            " Links \n",
            " IoCs \n",
            " 10/03/2023 \n",
            " WS_FTP vulnerability \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            " IoC available in the posts  \n",
            " 10/03/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/12/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/17/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/18/2023 \n",
            " RCE affecting Citrix NetScaler in the wild \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/20/2023 \n",
            " IcedID  \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/23/2023 \n",
            " AsyncRAT \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 10/25/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/01/2023 \n",
            " IcedID \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/03/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/21/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/28/2023 \n",
            " IcedID variant \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/30/2023 \n",
            " JinxLoader \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 11/30/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/06/2023 \n",
            " Loader EXE leads to unidentified malware \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/07/2023 \n",
            " DarkGate \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/12/2023 \n",
            " Astaroth/Guildma \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/15/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            " 12/18/2023 \n",
            " Pikabot \n",
            "  Twitter (X)  ,   LinkedIn  \n",
            "  IoCs  \n",
            "   \n",
            "Additional Resources\n",
            " Malware Traffic Analysis.net \n",
            " Wireshark Tutorials – Unit 42, Palo Alto Networks\n",
            " IoCs of Unit 42 Timely Threat Intelligence – GitHub\n",
            " SID 2049408 by Emerging Threats Labs – X (Twitter)\n",
            " New Malvertising Campaign Distributing PikaBot Disguised as Popular Software – The Hacker News\n",
            "Get updates from  Palo Alto Networks!\n",
            "Sign up to receive the latest news, cyber threat intelligence and research from us\n",
            "Highlights: ['Symantec issued a protection bulletin on JinxLoader just a short while ago. In our post about it (the first public post! ) we note that JinxLoader is a relatively new malware service first posted to hackforums[. ]net on April 30, 2023. The eight steps of this infection chain are detailed in Figure 14.']\n",
            "Highlight Scores: [0.4281970262527466]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: malware-samples/binaries/fickerstealer/2020/November at master · jstrosch/malware-samples\n",
            "URL: https://github.com/jstrosch/malware-samples/tree/master/binaries/fickerstealer/2020/November\n",
            "ID: https://github.com/jstrosch/malware-samples/tree/master/binaries/fickerstealer/2020/November\n",
            "Score: 0.16413679718971252\n",
            "Published Date: 2019-12-23T17:47:20.000Z\n",
            "Author: jstrosch\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: FickerStealer w/ activity and IP check    \n",
            "Trojan (MD5): 1db6bd4d13cb9966e8875b3812aef71d.bin\n",
            "PCAP: b0f2d519ccae5bf1db6bd4d13cb9966e8875b3812aef71d1435264e0979770ce.pcap\n",
            "See the README for information about the archive password.\n",
            "Analysis source: Cuckoo 2.0.7\n",
            "Date: 11/24/2020\n",
            "This sample highlights FickerStealer activity with IP check.\n",
            "  Process Activity    \n",
            "   \n",
            "Process activity\n",
            "  Network Activity    \n",
            "   \n",
            "Command and Control activity\n",
            "   \n",
            "IDS alerts generated by Suricata\n",
            "Highlights: ['Trojan (MD5): 1db6bd4d13cb9966e8875b3812aef71d.bin PCAP: b0f2d519ccae5bf1db6bd4d13cb9966e8875b3812aef71d1435264e0979770ce.pcap See the README for information about the archive password. This sample highlights FickerStealer activity with IP check.']\n",
            "Highlight Scores: [0.06239201873540878]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Here are some details on currently active Infostealer malware, including their TTPs and IOCs:\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "ERROR in Google Programmable Search: The read operation timed out\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://flashpoint.io/blog/protecting-against-infostealer-malware/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.wired.com/story/infostealer-malware-password-theft/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling No URL using AsyncPlaywrightCrawlerStrategy...\n",
            "[ERROR] 🚫 arun(): Failed to crawl No URL, error: [ERROR] 🚫 crawl(): Failed to crawl No URL: Page.goto: Protocol error (Page.navigate): Cannot navigate to invalid URL\n",
            "Call log:\n",
            "navigating to \"No URL\", waiting until \"domcontentloaded\"\n",
            "\n",
            "ERROR: Failed to crawl the page No URL\n",
            "[LOG] ✅ Crawled https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, success: True, time taken: 3.78 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, success: True, time taken: 0.20 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.packetlabs.net/posts/what-is-infostealer-malware-and-how-does-it-work/, time taken: 0.26 seconds.\n",
            "[LOG] ✅ Crawled https://flashpoint.io/blog/protecting-against-infostealer-malware/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://flashpoint.io/blog/protecting-against-infostealer-malware/, success: True, time taken: 5.71 seconds\n",
            "[LOG] 🚀 Content extracted for https://flashpoint.io/blog/protecting-against-infostealer-malware/, success: True, time taken: 0.55 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://flashpoint.io/blog/protecting-against-infostealer-malware/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://flashpoint.io/blog/protecting-against-infostealer-malware/, time taken: 0.72 seconds.\n",
            "[LOG] ✅ Crawled https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a, success: True, time taken: 5.92 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a, success: True, time taken: 1.55 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-075a, time taken: 1.86 seconds.\n",
            "[LOG] ✅ Crawled https://www.wired.com/story/infostealer-malware-password-theft/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.wired.com/story/infostealer-malware-password-theft/, success: True, time taken: 10.55 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.wired.com/story/infostealer-malware-password-theft/, success: True, time taken: 0.38 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.wired.com/story/infostealer-malware-password-theft/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.wired.com/story/infostealer-malware-password-theft/, time taken: 0.50 seconds.\n",
            "Crawled Results: [[{'links': '/', 'images': 'https://flashpoint.io/wp-content/themes/flashpoint/img/flashpoint-logo@2x.png'}], [{'links': '#main', 'images': '/profiles/cisad8_gov/themes/custom/gesso/dist/images/us_flag_small.png'}], [{'links': '/', 'images': '/img/logo.svg'}], [{'links': '#main-content', 'images': '/verso/static/wired-us/assets/logo-header.svg'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://flashpoint.io/wp-content/themes/flashpoint/img/flashpoint-logo@2x.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#main', 'images': '/profiles/cisad8_gov/themes/custom/gesso/dist/images/us_flag_small.png'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': '/img/logo.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '#main-content', 'images': '/verso/static/wired-us/assets/logo-header.svg'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIStatusError",
          "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `mixtral-8x7b-32768` in organization `org_01j5e07dy7e5db32yaffzg1td2` on tokens per minute (TPM): Limit 5000, Requested 5103, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-80181182567b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Share some details on currently active Infostealer malware and give me their TTPs and IOCs?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f6477db1b228>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query, memory)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m         async for chunk in self.astream(\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mastream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1685\u001b[0m                     \u001b[0mmanager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m                 ):\n\u001b[0;32m-> 1687\u001b[0;31m                     async for _ in runner.atick(\n\u001b[0m\u001b[1;32m   1688\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36matick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0marun_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_astream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36mainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecutor_or_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_or_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Use default executor with context copied from current context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         return await asyncio.get_running_loop().run_in_executor(\n\u001b[0m\u001b[1;32m    589\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# StopIteration can't be set on an asyncio.Future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f6477db1b228>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     response = chain.invoke({\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;34m\"search_results\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         }\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         )\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `mixtral-8x7b-32768` in organization `org_01j5e07dy7e5db32yaffzg1td2` on tokens per minute (TPM): Limit 5000, Requested 5103, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}