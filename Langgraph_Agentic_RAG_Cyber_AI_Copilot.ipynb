{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "7b8afe52-3873-477f-c305-c7d318d3d4de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.7/248.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[LOG] Welcome to the Crawl4AI Model Downloader!\n",
            "[LOG] This script will download all the models required for Crawl4AI.\n",
            "[LOG] Downloading text classifier...\n",
            "tokenizer_config.json: 100% 1.30k/1.30k [00:00<00:00, 6.11MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 14.4MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 3.57MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 22.0MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 768kB/s]\n",
            "config.json: 100% 1.88k/1.88k [00:00<00:00, 10.3MB/s]\n",
            "2024-11-14 09:09:32.009176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 09:09:32.037199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 09:09:32.045662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 09:09:32.067066: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 09:09:33.605937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 170MB/s]\n",
            "[LOG] Text classifier loaded on cpu\n",
            "[LOG] Downloading custom NLTK Punkt model...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[LOG] ✅ All models downloaded successfully.\n",
            "Downloading Chromium 129.0.6668.29 (playwright build v1134)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1134/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164 MiB [] 0% 10.8s\u001b[0K\u001b[1G164 MiB [] 0% 35.9s\u001b[0K\u001b[1G164 MiB [] 0% 18.2s\u001b[0K\u001b[1G164 MiB [] 0% 12.5s\u001b[0K\u001b[1G164 MiB [] 0% 8.7s\u001b[0K\u001b[1G164 MiB [] 1% 5.9s\u001b[0K\u001b[1G164 MiB [] 1% 5.1s\u001b[0K\u001b[1G164 MiB [] 2% 4.5s\u001b[0K\u001b[1G164 MiB [] 3% 4.3s\u001b[0K\u001b[1G164 MiB [] 3% 4.4s\u001b[0K\u001b[1G164 MiB [] 4% 4.2s\u001b[0K\u001b[1G164 MiB [] 4% 4.0s\u001b[0K\u001b[1G164 MiB [] 5% 3.9s\u001b[0K\u001b[1G164 MiB [] 5% 3.6s\u001b[0K\u001b[1G164 MiB [] 6% 3.6s\u001b[0K\u001b[1G164 MiB [] 6% 3.4s\u001b[0K\u001b[1G164 MiB [] 7% 3.2s\u001b[0K\u001b[1G164 MiB [] 8% 3.1s\u001b[0K\u001b[1G164 MiB [] 9% 3.0s\u001b[0K\u001b[1G164 MiB [] 9% 2.9s\u001b[0K\u001b[1G164 MiB [] 10% 2.9s\u001b[0K\u001b[1G164 MiB [] 11% 2.8s\u001b[0K\u001b[1G164 MiB [] 11% 2.9s\u001b[0K\u001b[1G164 MiB [] 12% 2.8s\u001b[0K\u001b[1G164 MiB [] 13% 2.9s\u001b[0K\u001b[1G164 MiB [] 13% 3.0s\u001b[0K\u001b[1G164 MiB [] 13% 3.1s\u001b[0K\u001b[1G164 MiB [] 14% 3.1s\u001b[0K\u001b[1G164 MiB [] 14% 3.2s\u001b[0K\u001b[1G164 MiB [] 14% 3.3s\u001b[0K\u001b[1G164 MiB [] 15% 3.3s\u001b[0K\u001b[1G164 MiB [] 15% 3.4s\u001b[0K\u001b[1G164 MiB [] 15% 3.5s\u001b[0K\u001b[1G164 MiB [] 16% 3.5s\u001b[0K\u001b[1G164 MiB [] 17% 3.3s\u001b[0K\u001b[1G164 MiB [] 18% 3.2s\u001b[0K\u001b[1G164 MiB [] 19% 3.1s\u001b[0K\u001b[1G164 MiB [] 20% 3.1s\u001b[0K\u001b[1G164 MiB [] 21% 3.1s\u001b[0K\u001b[1G164 MiB [] 22% 3.0s\u001b[0K\u001b[1G164 MiB [] 23% 3.0s\u001b[0K\u001b[1G164 MiB [] 24% 2.9s\u001b[0K\u001b[1G164 MiB [] 25% 2.9s\u001b[0K\u001b[1G164 MiB [] 26% 2.8s\u001b[0K\u001b[1G164 MiB [] 27% 2.7s\u001b[0K\u001b[1G164 MiB [] 28% 2.6s\u001b[0K\u001b[1G164 MiB [] 29% 2.5s\u001b[0K\u001b[1G164 MiB [] 30% 2.5s\u001b[0K\u001b[1G164 MiB [] 31% 2.4s\u001b[0K\u001b[1G164 MiB [] 32% 2.3s\u001b[0K\u001b[1G164 MiB [] 33% 2.3s\u001b[0K\u001b[1G164 MiB [] 34% 2.2s\u001b[0K\u001b[1G164 MiB [] 35% 2.1s\u001b[0K\u001b[1G164 MiB [] 36% 2.0s\u001b[0K\u001b[1G164 MiB [] 38% 2.0s\u001b[0K\u001b[1G164 MiB [] 38% 1.9s\u001b[0K\u001b[1G164 MiB [] 39% 1.9s\u001b[0K\u001b[1G164 MiB [] 40% 1.8s\u001b[0K\u001b[1G164 MiB [] 41% 1.8s\u001b[0K\u001b[1G164 MiB [] 42% 1.7s\u001b[0K\u001b[1G164 MiB [] 43% 1.7s\u001b[0K\u001b[1G164 MiB [] 44% 1.6s\u001b[0K\u001b[1G164 MiB [] 45% 1.6s\u001b[0K\u001b[1G164 MiB [] 46% 1.6s\u001b[0K\u001b[1G164 MiB [] 47% 1.5s\u001b[0K\u001b[1G164 MiB [] 48% 1.5s\u001b[0K\u001b[1G164 MiB [] 49% 1.4s\u001b[0K\u001b[1G164 MiB [] 50% 1.4s\u001b[0K\u001b[1G164 MiB [] 51% 1.4s\u001b[0K\u001b[1G164 MiB [] 52% 1.3s\u001b[0K\u001b[1G164 MiB [] 53% 1.3s\u001b[0K\u001b[1G164 MiB [] 54% 1.2s\u001b[0K\u001b[1G164 MiB [] 56% 1.2s\u001b[0K\u001b[1G164 MiB [] 56% 1.1s\u001b[0K\u001b[1G164 MiB [] 58% 1.1s\u001b[0K\u001b[1G164 MiB [] 59% 1.1s\u001b[0K\u001b[1G164 MiB [] 60% 1.0s\u001b[0K\u001b[1G164 MiB [] 61% 1.0s\u001b[0K\u001b[1G164 MiB [] 62% 1.0s\u001b[0K\u001b[1G164 MiB [] 63% 0.9s\u001b[0K\u001b[1G164 MiB [] 64% 0.9s\u001b[0K\u001b[1G164 MiB [] 65% 0.9s\u001b[0K\u001b[1G164 MiB [] 66% 0.9s\u001b[0K\u001b[1G164 MiB [] 67% 0.8s\u001b[0K\u001b[1G164 MiB [] 68% 0.8s\u001b[0K\u001b[1G164 MiB [] 69% 0.8s\u001b[0K\u001b[1G164 MiB [] 70% 0.7s\u001b[0K\u001b[1G164 MiB [] 71% 0.7s\u001b[0K\u001b[1G164 MiB [] 72% 0.7s\u001b[0K\u001b[1G164 MiB [] 73% 0.7s\u001b[0K\u001b[1G164 MiB [] 74% 0.6s\u001b[0K\u001b[1G164 MiB [] 75% 0.6s\u001b[0K\u001b[1G164 MiB [] 76% 0.6s\u001b[0K\u001b[1G164 MiB [] 77% 0.6s\u001b[0K\u001b[1G164 MiB [] 78% 0.5s\u001b[0K\u001b[1G164 MiB [] 79% 0.5s\u001b[0K\u001b[1G164 MiB [] 80% 0.5s\u001b[0K\u001b[1G164 MiB [] 81% 0.4s\u001b[0K\u001b[1G164 MiB [] 82% 0.4s\u001b[0K\u001b[1G164 MiB [] 83% 0.4s\u001b[0K\u001b[1G164 MiB [] 84% 0.4s\u001b[0K\u001b[1G164 MiB [] 85% 0.3s\u001b[0K\u001b[1G164 MiB [] 86% 0.3s\u001b[0K\u001b[1G164 MiB [] 87% 0.3s\u001b[0K\u001b[1G164 MiB [] 88% 0.3s\u001b[0K\u001b[1G164 MiB [] 89% 0.2s\u001b[0K\u001b[1G164 MiB [] 90% 0.2s\u001b[0K\u001b[1G164 MiB [] 91% 0.2s\u001b[0K\u001b[1G164 MiB [] 92% 0.2s\u001b[0K\u001b[1G164 MiB [] 93% 0.2s\u001b[0K\u001b[1G164 MiB [] 94% 0.1s\u001b[0K\u001b[1G164 MiB [] 95% 0.1s\u001b[0K\u001b[1G164 MiB [] 96% 0.1s\u001b[0K\u001b[1G164 MiB [] 97% 0.1s\u001b[0K\u001b[1G164 MiB [] 98% 0.0s\u001b[0K\u001b[1G164 MiB [] 99% 0.0s\u001b[0K\u001b[1G164 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 129.0.6668.29 (playwright build v1134) downloaded to /root/.cache/ms-playwright/chromium-1134\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 3% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 29% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 39% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 77% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Firefox 130.0 (playwright build v1463)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1463/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G86.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 0% 21.1s\u001b[0K\u001b[1G86.4 MiB [] 0% 13.6s\u001b[0K\u001b[1G86.4 MiB [] 0% 7.0s\u001b[0K\u001b[1G86.4 MiB [] 1% 5.0s\u001b[0K\u001b[1G86.4 MiB [] 2% 3.6s\u001b[0K\u001b[1G86.4 MiB [] 3% 3.0s\u001b[0K\u001b[1G86.4 MiB [] 4% 2.6s\u001b[0K\u001b[1G86.4 MiB [] 5% 2.5s\u001b[0K\u001b[1G86.4 MiB [] 5% 2.4s\u001b[0K\u001b[1G86.4 MiB [] 7% 2.3s\u001b[0K\u001b[1G86.4 MiB [] 7% 2.4s\u001b[0K\u001b[1G86.4 MiB [] 8% 2.4s\u001b[0K\u001b[1G86.4 MiB [] 9% 2.5s\u001b[0K\u001b[1G86.4 MiB [] 10% 2.3s\u001b[0K\u001b[1G86.4 MiB [] 12% 2.2s\u001b[0K\u001b[1G86.4 MiB [] 12% 2.1s\u001b[0K\u001b[1G86.4 MiB [] 14% 2.0s\u001b[0K\u001b[1G86.4 MiB [] 15% 1.9s\u001b[0K\u001b[1G86.4 MiB [] 16% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 17% 1.8s\u001b[0K\u001b[1G86.4 MiB [] 18% 1.7s\u001b[0K\u001b[1G86.4 MiB [] 19% 1.7s\u001b[0K\u001b[1G86.4 MiB [] 20% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 22% 1.6s\u001b[0K\u001b[1G86.4 MiB [] 23% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 24% 1.5s\u001b[0K\u001b[1G86.4 MiB [] 25% 1.4s\u001b[0K\u001b[1G86.4 MiB [] 26% 1.4s\u001b[0K\u001b[1G86.4 MiB [] 28% 1.3s\u001b[0K\u001b[1G86.4 MiB [] 29% 1.3s\u001b[0K\u001b[1G86.4 MiB [] 30% 1.3s\u001b[0K\u001b[1G86.4 MiB [] 31% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 33% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 34% 1.2s\u001b[0K\u001b[1G86.4 MiB [] 35% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 36% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 38% 1.1s\u001b[0K\u001b[1G86.4 MiB [] 39% 1.0s\u001b[0K\u001b[1G86.4 MiB [] 40% 1.0s\u001b[0K\u001b[1G86.4 MiB [] 42% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 44% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 45% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 46% 0.9s\u001b[0K\u001b[1G86.4 MiB [] 47% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 49% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 51% 0.8s\u001b[0K\u001b[1G86.4 MiB [] 53% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 55% 0.7s\u001b[0K\u001b[1G86.4 MiB [] 56% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 58% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 60% 0.6s\u001b[0K\u001b[1G86.4 MiB [] 62% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 64% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 65% 0.5s\u001b[0K\u001b[1G86.4 MiB [] 68% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 70% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 72% 0.4s\u001b[0K\u001b[1G86.4 MiB [] 75% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 77% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 79% 0.3s\u001b[0K\u001b[1G86.4 MiB [] 80% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 82% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 83% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 85% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 87% 0.2s\u001b[0K\u001b[1G86.4 MiB [] 88% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 90% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 92% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 94% 0.1s\u001b[0K\u001b[1G86.4 MiB [] 96% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 98% 0.0s\u001b[0K\u001b[1G86.4 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 130.0 (playwright build v1463) downloaded to /root/.cache/ms-playwright/firefox-1463\n",
            "Downloading Webkit 18.0 (playwright build v2070)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2070/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G88.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 0% 20.4s\u001b[0K\u001b[1G88.2 MiB [] 0% 11.3s\u001b[0K\u001b[1G88.2 MiB [] 0% 7.4s\u001b[0K\u001b[1G88.2 MiB [] 1% 5.0s\u001b[0K\u001b[1G88.2 MiB [] 2% 3.3s\u001b[0K\u001b[1G88.2 MiB [] 3% 3.1s\u001b[0K\u001b[1G88.2 MiB [] 3% 3.0s\u001b[0K\u001b[1G88.2 MiB [] 4% 2.8s\u001b[0K\u001b[1G88.2 MiB [] 5% 2.6s\u001b[0K\u001b[1G88.2 MiB [] 6% 2.7s\u001b[0K\u001b[1G88.2 MiB [] 7% 2.6s\u001b[0K\u001b[1G88.2 MiB [] 8% 2.4s\u001b[0K\u001b[1G88.2 MiB [] 9% 2.3s\u001b[0K\u001b[1G88.2 MiB [] 10% 2.2s\u001b[0K\u001b[1G88.2 MiB [] 11% 2.0s\u001b[0K\u001b[1G88.2 MiB [] 12% 1.9s\u001b[0K\u001b[1G88.2 MiB [] 14% 1.9s\u001b[0K\u001b[1G88.2 MiB [] 15% 1.8s\u001b[0K\u001b[1G88.2 MiB [] 16% 1.7s\u001b[0K\u001b[1G88.2 MiB [] 17% 1.7s\u001b[0K\u001b[1G88.2 MiB [] 18% 1.6s\u001b[0K\u001b[1G88.2 MiB [] 20% 1.5s\u001b[0K\u001b[1G88.2 MiB [] 21% 1.5s\u001b[0K\u001b[1G88.2 MiB [] 22% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 23% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 24% 1.4s\u001b[0K\u001b[1G88.2 MiB [] 26% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 28% 1.3s\u001b[0K\u001b[1G88.2 MiB [] 29% 1.2s\u001b[0K\u001b[1G88.2 MiB [] 30% 1.2s\u001b[0K\u001b[1G88.2 MiB [] 32% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 33% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 34% 1.1s\u001b[0K\u001b[1G88.2 MiB [] 36% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 38% 1.0s\u001b[0K\u001b[1G88.2 MiB [] 39% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 41% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 42% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 43% 0.9s\u001b[0K\u001b[1G88.2 MiB [] 45% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 46% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 48% 0.8s\u001b[0K\u001b[1G88.2 MiB [] 49% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 51% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 52% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 53% 0.7s\u001b[0K\u001b[1G88.2 MiB [] 55% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 56% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 58% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 60% 0.6s\u001b[0K\u001b[1G88.2 MiB [] 61% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 62% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 64% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 65% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 66% 0.5s\u001b[0K\u001b[1G88.2 MiB [] 68% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 70% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 71% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 73% 0.4s\u001b[0K\u001b[1G88.2 MiB [] 75% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 76% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 78% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 80% 0.3s\u001b[0K\u001b[1G88.2 MiB [] 82% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 83% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 86% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 87% 0.2s\u001b[0K\u001b[1G88.2 MiB [] 88% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 90% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 91% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 93% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 95% 0.1s\u001b[0K\u001b[1G88.2 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 98% 0.0s\u001b[0K\u001b[1G88.2 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.0 (playwright build v2070) downloaded to /root/.cache/ms-playwright/webkit-2070\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:626:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:724:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:713:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph\n",
        "!crawl4ai-download-models\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from IPython.display import Image, display\n",
        "import getpass\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "import json\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "COHERE_API_KEY = \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3abea5b-f10c-4bf0-d6b1-7f9140b4651c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Cohere Reranker\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "# Initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    results: List[SearchResult]\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\")\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\")\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def search_and_contents(query: str):\n",
        "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
        "    return exa.search_and_contents(\n",
        "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "    )\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"DEBUG: Starting Exa Search with query: {query}\")\n",
        "        response = search_and_contents(query)\n",
        "        print(f\"DEBUG: Raw results from Exa Search: {response}\")\n",
        "\n",
        "        if not isinstance(response, SearchResponse):\n",
        "            print(f\"DEBUG: Exa Search results are not a SearchResponse. Type: {type(response)}\")\n",
        "            return []\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"DEBUG: Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Tavily search tool\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = tavily_tool.invoke({\"query\": query})\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# New Google Programmable Search function\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Google Serper Image Search\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "# Google Programmable Image Search\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Enhanced recency scoring using exponential decay\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9\n",
        "\n",
        "# Enhanced source classification\n",
        "def classify_source(source: str) -> float:\n",
        "    if \"advisory\" in source.lower() or \"threat intelligence\" in source.lower():\n",
        "        return 1.0  # Highest weight for official security advisories and threat intelligence platforms\n",
        "    elif \"news\" in source.lower():\n",
        "        return 0.8  # High weight for news sources\n",
        "    elif \"blog\" in source.lower():\n",
        "        return 0.6  # Moderate weight for blogs\n",
        "    else:\n",
        "        return 0.5  # Default weight for other sources\n",
        "\n",
        "# Enhanced search query\n",
        "def enhance_search_query(query: str) -> str:\n",
        "    current_year = datetime.now().year\n",
        "    enhanced_query = f\"{query} 2024 OR {current_year} recent\"\n",
        "    return enhanced_query\n",
        "\n",
        "# Reranking function with semantic similarity and metadata scoring\n",
        "def rerank_results(query: str, results: List[SearchResult], state: AgentState) -> List[SearchResult]:\n",
        "    # Create embeddings for query and results\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Combine snippets with crawled content for richer context\n",
        "    enhanced_results = []\n",
        "    for result in results:\n",
        "        # Get crawled content for this URL if available\n",
        "        crawled_content = \"\"\n",
        "        for m in state[\"messages\"]:\n",
        "            if m[\"role\"] == \"tool\" and \"crawled_results\" in m:\n",
        "                for cr in m[\"crawled_results\"]:\n",
        "                    if isinstance(cr, dict) and cr.get(\"url\") == result.url:\n",
        "                        crawled_content = cr.get(\"content\", \"\")\n",
        "                        break\n",
        "\n",
        "        # Combine snippet with crawled content\n",
        "        full_content = f\"{result.snippet}\\n{crawled_content}\"\n",
        "        content_embedding = embeddings.embed_query(full_content)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarity = cosine_similarity(\n",
        "            [query_embedding],\n",
        "            [content_embedding]\n",
        "        )[0][0]\n",
        "\n",
        "        # Add metadata scoring (e.g., source weight, date)\n",
        "        metadata_score = result.source_weight or 0\n",
        "        date = parse_date(result.date)\n",
        "        date_score = calculate_recency_score(date)\n",
        "        final_score = similarity + metadata_score + date_score\n",
        "\n",
        "        enhanced_results.append((final_score, result))\n",
        "\n",
        "    # Sort by final score\n",
        "    enhanced_results.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [result for _, result in enhanced_results]\n",
        "\n",
        "# Enhanced content extraction with media handling\n",
        "async def extract_content_from_url(url: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"name\": \"Enhanced Content Extractor\",\n",
        "        \"baseSelector\": \"body\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"content\",\n",
        "                \"selector\": \"body\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"links\",\n",
        "                \"selector\": \"a[href]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"images\",\n",
        "                \"selector\": \"img[src]\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"meta_description\",\n",
        "                \"selector\": \"meta[name='description']\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"content\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"publication_date\",\n",
        "                \"selector\": [\n",
        "                    \"meta[property='article:published_time']\",\n",
        "                    \"time[datetime]\",\n",
        "                    \"meta[name='publicationDate']\"\n",
        "                ],\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": [\"content\", \"datetime\", \"content\"],\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=url,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            bypass_cache=True,\n",
        "        )\n",
        "\n",
        "        if not result.success:\n",
        "            print(f\"ERROR: Failed to crawl the page {url}\")\n",
        "            return None\n",
        "\n",
        "        extracted_content = json.loads(result.extracted_content)\n",
        "\n",
        "        # Process and validate images\n",
        "        if \"images\" in extracted_content:\n",
        "            valid_images = []\n",
        "            for img_url in extracted_content[\"images\"]:\n",
        "                if is_valid_image_url(img_url):\n",
        "                    valid_images.append(img_url)\n",
        "            extracted_content[\"valid_images\"] = valid_images\n",
        "\n",
        "        return extracted_content\n",
        "\n",
        "def is_valid_image_url(url: str) -> bool:\n",
        "    \"\"\"Validate image URLs and filter out common web elements.\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "\n",
        "    # Filter out common web elements\n",
        "    excluded_patterns = [\n",
        "        'favicon', 'logo', 'icon', 'sprite', 'pixel',\n",
        "        'tracking', 'advertisement', 'banner'\n",
        "    ]\n",
        "    return not any(pattern in url.lower() for pattern in excluded_patterns)\n",
        "\n",
        "# Enhanced search aggregation with deduplication and metadata scoring\n",
        "def aggregate_search_results(\n",
        "    query: str,\n",
        "    *args: List[SearchResult]\n",
        ") -> List[SearchResult]:\n",
        "\n",
        "    # Combine all results with metadata scoring\n",
        "    all_results = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [1.0, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "\n",
        "    # Deduplicate results based on URL and calculate final score\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            # Add source and weight to result metadata\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            # Calculate final score based on weight, source_weight, and date\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by final score\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results"
      ],
      "metadata": {
        "id": "48JIG0EUJjqx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced execute_searches function with improved concurrency and error handling\n",
        "async def execute_searches(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "    # Enhance the search query\n",
        "    enhanced_query = enhance_search_query(query)\n",
        "\n",
        "    # Execute all searches in parallel with improved error handling\n",
        "    search_functions = [\n",
        "        vector_search,\n",
        "        google_serper_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_programmable_search,\n",
        "        google_serper_image_search,\n",
        "        google_programmable_image_search\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(search_func, enhanced_query) for search_func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            logging.error(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results = aggregate_search_results(\n",
        "        enhanced_query, *successful_results\n",
        "    )\n",
        "\n",
        "    # Reranking with semantic similarity and metadata scoring\n",
        "    reranked_results = rerank_results(enhanced_query, combined_results, state)\n",
        "\n",
        "    # Extract URLs for crawling with improved concurrency\n",
        "    urls_to_crawl = [result.url for result in reranked_results[:5]]  # Limit to top 5\n",
        "    crawl_tasks = [extract_content_from_url(url) for url in urls_to_crawl]\n",
        "    crawled_results = await asyncio.gather(*crawl_tasks)\n",
        "\n",
        "    # Filter out None results and add to state\n",
        "    valid_crawled_results = [r for r in crawled_results if r is not None]\n",
        "\n",
        "    state[\"messages\"].append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": \"Enhanced Search Results\",\n",
        "        \"results\": reranked_results,\n",
        "        \"crawled_results\": valid_crawled_results\n",
        "    })\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "iuF6b8-Wn1F_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced response generation with better prompt engineering and media content handling\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"])\n",
        "                           if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "\n",
        "    crawled_results = next((m[\"crawled_results\"] for m in reversed(state[\"messages\"])\n",
        "                            if m[\"role\"] == \"tool\" and \"crawled_results\" in m), [])\n",
        "\n",
        "    print(\"Crawled Results:\", crawled_results)  # Add this line to inspect the crawled results\n",
        "\n",
        "    # Generate adaptive prompt based on the query and search results\n",
        "    prompt_template = \"\"\" You are an advanced AI copilot specializing in cybersecurity, intelligence analysis, and technical response. Your task is to synthesize, validate, and provide query-focused insights from diverse, verified data sources, delivering a response that combines precision, actionable intelligence, and situational awareness. Your analysis should be tailored to each unique query, maintaining accuracy and relevance throughout.\n",
        "\n",
        "    **ANALYSIS PROTOCOL** *(Structured in Phases for comprehensive evaluation)*:\n",
        "\n",
        "    1. **Source and Credibility Verification**:\n",
        "       - **Domain Reliability**: Prioritize high-authority cybersecurity, intelligence, and technical sources.\n",
        "       - **Timeliness Validation**: Confirm that the data is current and directly relevant to the specific query.\n",
        "       - **Cross-Reference Key Data Points**: Validate critical information by cross-referencing with multiple reputable sources.\n",
        "       - **Misinformation Detection**: Identify and disregard any unsupported claims, exaggerations, or potentially misleading data.\n",
        "\n",
        "    2. **Content Extraction and Relevance Filtering**:\n",
        "       - **Identify Core Data**: Extract essential information such as threat vectors, indicators, metrics, and statistics.\n",
        "       - **Pattern Recognition and Correlation**: Detect recurring themes, correlations, and trends across data sources.\n",
        "       - **Contextual Prioritization**:\n",
        "         - **Temporal Relevance**: Emphasize the most recent and impactful data.\n",
        "         - **Technical Depth**: Focus on technical details directly pertinent to the query context.\n",
        "         - **Query Alignment**: Rank findings by their relevance to the query and the user’s specific question.\n",
        "\n",
        "    3. **Visual and Media Analysis**:\n",
        "       - **Visual Verification**: Evaluate images, diagrams, and screenshots for technical relevance and accuracy.\n",
        "       - **Technical Indicator Extraction**: Identify critical data from visuals, including IP addresses, file hashes, or attack paths.\n",
        "       - **Text-Visual Correlation**: Cross-reference media content with textual data, emphasizing technical implications and alignment.\n",
        "\n",
        "    **ADAPTIVE RESPONSE STRUCTURE** *(Dynamic, based on query type)*:\n",
        "\n",
        "    1. **Executive Summary**:\n",
        "       - Provide a concise, high-level overview summarizing key findings, highlighting high-priority insights and recommendations.\n",
        "\n",
        "    2. **In-Depth Analysis**:\n",
        "       - **Key Findings**:\n",
        "         - A bullet-point list of critical discoveries, emerging threats, and significant events.\n",
        "         - Include specific metrics, trends, or any quantitative data directly relevant to the query.\n",
        "       - **Technical Breakdown**:\n",
        "         - Detail specific vulnerabilities, exploits, attack vectors, or system impacts.\n",
        "         - Address affected components and dependencies, along with any recommended remediation actions.\n",
        "       - **Contextual and Industry Impact**:\n",
        "         - Analyze sector-specific or industry-wide implications.\n",
        "         - Attribute threat actors, where identifiable, and connect tactics to established frameworks (e.g., MITRE ATT&CK).\n",
        "         - Draw connections to historical incidents or patterns for enhanced context.\n",
        "\n",
        "    3. **Most Recent Relevant Activities**:\n",
        "       - **Latest Developments**:\n",
        "         - Summarize the most recent activities, incidents, or updates directly related to the query.\n",
        "         - Describe new vulnerabilities, patches, or emerging threats impacting the cybersecurity landscape.\n",
        "       - **Immediate Implications**:\n",
        "         - Assess the direct impact of these recent developments on the query context.\n",
        "         - Suggest any immediate actions or mitigations needed in response to recent changes.\n",
        "\n",
        "    4. **Source Citations and Evidence**:\n",
        "       - Cite all findings with accuracy, using the [Source Name](URL) format to link major claims.\n",
        "       - For specific assertions, provide direct quote snippets with context.\n",
        "       - **Embedded Media References**: Link to relevant media (e.g., screenshots, diagrams) with brief descriptions.\n",
        "\n",
        "    5. **Actionable Recommendations**:\n",
        "       - Offer precise, immediate actions and mitigation strategies.\n",
        "       - Outline relevant detection and prevention techniques pertinent to the identified threats.\n",
        "       - Suggest operational security measures for high-severity findings.\n",
        "\n",
        "    6. **Long-Term Forecast and Monitoring**:\n",
        "       - Discuss projected evolution in threat trends, actor tactics, or tool capabilities.\n",
        "       - Recommend specific trends or areas for ongoing monitoring and long-term response.\n",
        "\n",
        "    **SPECIALIZED QUERY HANDLING** *(Dynamic strategies based on context)*:\n",
        "\n",
        "    - **For Threat Intelligence Queries**:\n",
        "      - Extract Indicators of Compromise (IOCs) such as IPs, domains, and file hashes.\n",
        "      - Map findings to MITRE ATT&CK TTPs and assess behavior patterns of malware and threat actors.\n",
        "      - Document any identified Command and Control (C2) configurations.\n",
        "\n",
        "    - **For Vulnerability and Exploit Analysis**:\n",
        "      - Validate CVE details, including severity ratings, affected systems, and patch availability.\n",
        "      - Assess real-world exploitability, including any observed attacks or reports of active exploitation.\n",
        "\n",
        "    - **For Incident Response**:\n",
        "      - Construct a timeline of events, reconstructing points of compromise and attack paths.\n",
        "      - Provide clear recovery steps and immediate containment strategies.\n",
        "\n",
        "    - **For Trend Analysis**:\n",
        "      - Identify shifts in attack vectors, techniques, or actor capabilities, mapping against historical baselines.\n",
        "      - Forecast potential evolutions in tactics or capabilities based on observed trends.\n",
        "\n",
        "    **PROMPT VARIABLES**:\n",
        "    - **Previous Context**: {chat_history}\n",
        "    - **Current Query**: {input}\n",
        "    - **Search Results**: {search_results}\n",
        "    - **Additional Crawled Data**: {crawled_results}\n",
        "    - **Current Date**: {current_date}\n",
        "\n",
        "    **RESPONSE REQUIREMENTS**:\n",
        "    - **Precision and Depth**: Maintain technical accuracy and detailed insights throughout the response.\n",
        "    - **Confidence Levels**: Clearly state the confidence level of each assessment, highlighting uncertainties where applicable.\n",
        "    - **Citation Accuracy**: Ensure citations are accurate, using the [Source Name](URL) format for each major claim; include media references when applicable.\n",
        "    - **Urgency and Priority**: Highlight any urgent findings or time-sensitive information.\n",
        "    - **Readable Structure**: Use clear headings, subheadings, and bullet points for easy navigation.\n",
        "    - **Address Gaps and Uncertainties**: Acknowledge any data limitations or uncertainties within the response.\n",
        "    - **Embedded Media Links**: Include links to relevant visuals with contextual descriptions.\n",
        "    - **Actionable and Context-Specific Recommendations**: Customize suggestions based on query-specific context.\n",
        "    - **Technical Integrity**: Retain technical rigor throughout, avoiding over-generalization.\n",
        "\n",
        "    Generate a comprehensive, accurate response that addresses the query directly by synthesizing and presenting the latest, most relevant intelligence. Include insights into recent activities, incidents, and recommendations, supported by credible, source-backed evidence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", prompt_template\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "\n",
        "    # Enhanced search results formatting with media content handling\n",
        "    formatted_results = []\n",
        "    for result in search_results:\n",
        "        # Process media content\n",
        "        media_info = []\n",
        "        if result.media_content:\n",
        "            for media in result.media_content:\n",
        "                media_info.append({\n",
        "                    \"type\": media.get(\"type\", \"unknown\"),\n",
        "                    \"url\": media.get(\"url\", \"no url\"),\n",
        "                    \"description\": media.get(\"description\", \"\"),\n",
        "                    \"timestamp\": media.get(\"timestamp\", \"\")\n",
        "                })\n",
        "\n",
        "        # Create detailed result entry\n",
        "        result_str = (\n",
        "            f\"SOURCE ENTRY:\\n\"\n",
        "            f\"Title: {result.title}\\n\"\n",
        "            f\"Source: {result.source}\\n\"\n",
        "            f\"URL: {result.url}\\n\"\n",
        "            f\"Date: {result.date or 'Not specified'}\\n\"\n",
        "            f\"Content: {result.snippet}\\n\"\n",
        "        )\n",
        "\n",
        "        # Add media information if available\n",
        "        if media_info:\n",
        "            result_str += \"Media Content:\\n\"\n",
        "            for media in media_info:\n",
        "                result_str += (\n",
        "                    f\"- Type: {media['type']}\\n\"\n",
        "                    f\"  URL: {media['url']}\\n\"\n",
        "                    f\"  Description: {media['description']}\\n\"\n",
        "                    f\"  Timestamp: {media['timestamp']}\\n\"\n",
        "                )\n",
        "\n",
        "        # Add linked resources if available\n",
        "        if result.links:\n",
        "            result_str += \"Related Links:\\n\"\n",
        "            for link in result.links:\n",
        "                result_str += f\"- {link}\\n\"\n",
        "\n",
        "        result_str += \"-\" * 50 + \"\\n\"\n",
        "        formatted_results.append(result_str)\n",
        "\n",
        "    # Format crawled results with hyperlink extraction\n",
        "    formatted_crawled_results = []\n",
        "    for crawled_result in crawled_results:\n",
        "        for item in crawled_result:\n",
        "            if 'content' in item and 'links' in item:\n",
        "                formatted_crawled_results.append(f\"Content: {item['content']}\\nLinks: {item['links']}\\n\")\n",
        "                # Extract hyperlinks from content\n",
        "                hyperlinks = extract_hyperlinks(item['content'])\n",
        "                if hyperlinks:\n",
        "                    formatted_crawled_results.append(f\"Hyperlinks: {hyperlinks}\\n\")\n",
        "            else:\n",
        "                print(\"Missing 'content' or 'links' key in crawled result item:\", item)\n",
        "\n",
        "    # Generate response\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join(formatted_results),\n",
        "        \"crawled_results\": \"\\n\".join(formatted_crawled_results),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "\n",
        "    # Process response and ensure citations\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "\n",
        "    # Display media content\n",
        "    for result in search_results:\n",
        "        if hasattr(result, 'media') and result.media:\n",
        "            for media_url in result.media:\n",
        "                if is_valid_image_url(media_url):\n",
        "                    display(Image(url=media_url, width=400))\n",
        "\n",
        "    # Add crawled images\n",
        "    for crawled_result in crawled_results:\n",
        "        if crawled_result and 'valid_images' in crawled_result:\n",
        "            for img_url in crawled_result['valid_images']:\n",
        "                display(Image(url=img_url, width=400))\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\n",
        "        \"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"WARNING: No search results available for citation.\")\n",
        "        return text\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not any(result.url in paragraph for result in search_results) and not paragraph.startswith('**'):\n",
        "            most_relevant_source = max(search_results, key=lambda x: len(set(paragraph.lower().split()) & set(x.snippet.lower().split())))\n",
        "            paragraph += f' {format_source_link(most_relevant_source.source, most_relevant_source.url)}'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    return '\\n\\n'.join(cited_paragraphs)\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "def extract_hyperlinks(content: str) -> List[str]:\n",
        "    import re\n",
        "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return pattern.findall(content)\n",
        "\n",
        "# Workflow definition\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Asynchronous function to run the agent\n",
        "async def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = await graph.ainvoke(state)\n",
        "    return result"
      ],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents from Blackbasta Ransomeware Gang?\"\n",
        "    result = asyncio.run(run_agent(query))\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"Cyber AI Copilot Response:\")\n",
        "            print(message[\"content\"])"
      ],
      "metadata": {
        "id": "GwKcWCaOs2vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "918a727e-cf29-4da6-d81a-1bb358b3c019"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting Exa Search with query: Latest Cyber Incidents from Blackbasta Ransomeware Gang? 2024 OR 2024 recent\n",
            "DEBUG: Raw results from Exa Search: Title: BianLian ransomware claims attack on Boston Children's Health Physicians\n",
            "URL: https://www.bleepingcomputer.com/news/security/bianlian-ransomware-claims-attack-on-boston-childrens-health-physicians/\n",
            "ID: https://www.bleepingcomputer.com/news/security/bianlian-ransomware-claims-attack-on-boston-childrens-health-physicians/\n",
            "Score: 0.16840775310993195\n",
            "Published Date: 2024-10-17T00:00:00.000Z\n",
            "Author: Bill Toulas\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The BianLian ransomware group has claimed the cyberattack on Boston Children's Health Physicians (BCHP) and threatens to leak stolen files unless a ransom is paid.\n",
            "BHCP is a network of over 300 pediatric physicians and specialists operating over 60 locations across New York's Hudson Valley and Connecticut, offering patient care in clinics, community hospitals, and health centers affiliated with Boston Children's Hospital.\n",
            "According to the announcement BHCP published on its website, a cyberattack compromised its IT vendor on September 6 and a few days later BHCP detected unauthorized activity on its network.\n",
            "\"On September 6, 2024, our IT vendor informed us that it identified unusual activity in its systems. On September 10, 2024, we detected unauthorized activity on limited parts of the BCHP network and immediately initiated our incident response protocols, including shutting down our systems as a protective measure.\" - BHCP\n",
            "The investigation that followed, conducted with the help of a third-party forensic expert, confirmed that the threat actors had gained unauthorized access to BHCP systems and also exfiltrated files.\n",
            "The exposure impacts current and former employees, patients, and guarantors. The exposed data includes the following, depending on the information customers provided to BHCP:\n",
            " Full names\n",
            "Social Security numbers\n",
            "Addresses\n",
            "Dates of birth\n",
            "Driver's license numbers\n",
            "Medical record numbers\n",
            "Health insurance information\n",
            "Billing information\n",
            "Treatment information (limited)\n",
            " BHCP clarifies that the cyberattack did not impact its electronic medical record systems, as they are hosted on a separate network.\n",
            "Individuals confirmed to have been affected by the incident will receive a letter from BHCP by October 25. Those who had their SSN and driver's license exposed will also receive credit monitoring and protection services.\n",
            "BianLian claims the attack\n",
            "Earlier this week, the BianLian ransomware group claimed the attack by ading BHCP to their extortion portal.\n",
            "The threat actors claim to have finance and HR data, email correspondence, database dumps, personally identifiable and health records, health insurance records, and data related to children.\n",
            "  Source: BleepingCompuer   \n",
            "The threat actors have not leaked anything yet, and there is no deadline for exposing the stolen information, indicating that they still expect to negotiate with BHCP.\n",
            "Attacking children healthcare organizations and stealing the data of minors is typically avoided by ransomware groups, or at least they claim so, but some threat actors lack the moral guidelines to draw the line at that.\n",
            "Earlier this year, the Rhysida ransomware group demanded a ransom payment of $3.6 million from Lurie Children's Hospital in Chicago after stealing 600GB of sensitive data from its systems and causing operational disruptions that led to delays in medical care.\n",
            "Highlights: [\"The threat actors have not leaked anything yet, and there is no deadline for exposing the stolen information, indicating that they still expect to negotiate with BHCP. Attacking children healthcare organizations and stealing the data of minors is typically avoided by ransomware groups, or at least they claim so, but some threat actors lack the moral guidelines to draw the line at that. Earlier this year, the Rhysida ransomware group demanded a ransom payment of $3.6 million from Lurie Children's Hospital in Chicago after stealing 600GB of sensitive data from its systems and causing operational disruptions that led to delays in medical care.\"]\n",
            "Highlight Scores: [0.48400428891181946]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: MalwareHunterTeam on X: \"\"Our name is BlackBasta Syndicate, and we are the largest, most advanced, and most prolific organized group [...]. We are the ultimate cyber tradecraft with a credential record of taking down the most advanced, high-profile, and defended companies one can ever imagine.\"\n",
            "😂 https://t.co/skGHz4s8m9\" / X\n",
            "URL: https://x.com/malwrhunterteam/status/1850294636503531937\n",
            "ID: https://x.com/malwrhunterteam/status/1850294636503531937\n",
            "Score: 0.16644157469272614\n",
            "Published Date: 2024-10-26T00:00:00.000Z\n",
            "Author: \n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: None\n",
            "             Conversation                                      9:53 PM · Oct 26, 2024\n",
            "Highlights: ['None\\n             Conversation                                      9:53 PM · Oct 26, 2024']\n",
            "Highlight Scores: [0.035208288580179214]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Critical Veeam Vulnerability Exploited to Spread Akira and Fog Ransomware\n",
            "URL: https://thehackernews.com/2024/10/critical-veeam-vulnerability-exploited.html\n",
            "ID: https://thehackernews.com/2024/10/critical-veeam-vulnerability-exploited.html\n",
            "Score: 0.1642424464225769\n",
            "Published Date: 2024-10-14T00:00:00.000Z\n",
            "Author: The Hacker News\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Threat actors are actively attempting to exploit a now-patched security flaw in Veeam Backup &amp; Replication to deploy Akira and Fog ransomware.\n",
            "Cybersecurity vendor Sophos said it has been tracking a series of attacks in the past month leveraging compromised VPN credentials and CVE-2024-40711 to create a local account and deploy the ransomware.\n",
            "CVE-2024-40711, rated 9.8 out of 10.0 on the CVSS scale, refers to a critical vulnerability that allows for unauthenticated remote code execution. It was addressed by Veeam in Backup &amp; Replication version 12.2 in early September 2024.\n",
            "Security researcher Florian Hauser of Germany-based CODE WHITE has been credited with discovering and reporting security shortcomings.\n",
            " \n",
            "\"In each of the cases, attackers initially accessed targets using compromised VPN gateways without multifactor authentication enabled,\" Sophos said. \"Some of these VPNs were running unsupported software versions.\"\n",
            "\"Each time, the attackers exploited VEEAM on the URI /trigger on port 8000, triggering the Veeam.Backup.MountService.exe to spawn net.exe. The exploit creates a local account, 'point,' adding it to the local Administrators and Remote Desktop Users groups.\"\n",
            "In the attack that led to the Fog ransomware deployment, the threat actors are said to have drop the ransomware to an unprotected Hyper-V server, while using the rclone utility to exfiltrate data. The other ransomware deployments were unsuccessful.\n",
            "The active exploitation of CVE-2024-40711 has prompted an advisory from NHS England, which noted that \"enterprise backup and disaster recovery applications are valuable targets for cyber threat groups.\"\n",
            "The disclosure comes as Palo Alto Networks Unit 42 detailed a successor to INC ransomware named Lynx that has been active since July 2024, targeting organizations in retail, real estate, architecture, financial, and environmental services sectors in the U.S. and U.K.\n",
            "   \n",
            "The emergence of Lynx is said to have been spurred by the sale of INC ransomware's source code on the criminal underground market as early as March 2024, prompting malware authors to repackage the locker and spawn new variants.\n",
            "\"Lynx ransomware shares a significant portion of its source code with INC ransomware,\" Unit 42 said. \"INC ransomware initially surfaced in August 2023 and had variants compatible with both Windows and Linux.\"\n",
            "It also follows an advisory from the U.S. Department of Health and Human Services (HHS) Health Sector Cybersecurity Coordination Center (HC3) that at least one healthcare entity in the country has fallen victim to Trinity ransomware, another relatively new ransomware player that first became known in May 2024 and is believed to be a rebrand of 2023Lock and Venus ransomware.\n",
            " \n",
            "\"It is a type of malicious software that infiltrates systems through several attack vectors, including phishing emails, malicious websites, and exploitation of software vulnerabilities,\" HC3 said. \"Once inside the system, Trinity ransomware employs a double extortion strategy to target its victims.\"\n",
            "Cyber attacks have also been observed delivering a MedusaLocker ransomware variant dubbed BabyLockerKZ by a financially motivated threat actor known to be active since October 2022, with targets primarily located in the E.U. countries and South America.\n",
            "\"This attacker uses several publicly known attack tools and living-off-the-land binaries (LoLBins), a set of tools built by the same developer (possibly the attacker) to assist in credential theft and lateral movement in compromised organizations,\" Talos researchers said.\n",
            "\"These tools are mostly wrappers around publicly available tools that include additional functionality to streamline the attack process and provide graphical or command-line interfaces.\"\n",
            "Found this article interesting? Follow us on Twitter   and LinkedIn to read more exclusive content we post.\n",
            "Highlights: ['countries and South America. \"This attacker uses several publicly known attack tools and living-off-the-land binaries (LoLBins), a set of tools built by the same developer (possibly the attacker) to assist in credential theft and lateral movement in compromised organizations,\" Talos researchers said. Found this article interesting? Follow us on Twitter \\uf099  and LinkedIn to read more exclusive content we post.']\n",
            "Highlight Scores: [0.32604384422302246]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: Black Basta greift über MS-Teams-Chats an\n",
            "URL: https://www.csoonline.com/article/3592549/black-basta-greift-uber-ms-teams-chats-an.html\n",
            "ID: https://www.csoonline.com/article/3592549/black-basta-greift-uber-ms-teams-chats-an.html\n",
            "Score: 0.16407129168510437\n",
            "Published Date: 2024-10-28T00:00:00.000Z\n",
            "Author: Julia Mutzbauer\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Die Ransomware-Bande Black Basta setzt jetzt auf eine neue Angriffstaktik, die über Microsoft Teams-Chats läuft.\t\n",
            "  Die Ransomware-Bande Black Basta nutzt MS-Teams-Chatnachrichten, um Zugriff auf die Systeme von Unternehmen zu erhalten.   Ink Drop – Shutterstock.com \n",
            "Die berüchtigte Ransomware-Gruppe Black Basta hat es auf Organisationen weltweit abgesehen. Bisher war die Bande dafür bekannt, ihre Opfer zunächst mit Spam-E-Mails zu überhäufen. Anschließend gaben sich die Hacker als IT-Support aus, um sich Zugriff auf die Systeme zu verschaffen. Diese Methode wurde jetzt offenbar weiterentwickelt.\n",
            "Sicherheitsforscher von ReliaQuest haben kürzlich herausgefunden, dass Black Basta nun Microsoft Teams-Chatnachrichten verwendet, um potenzielle Opfer in Gespräche zu verwickeln. Auch bei dieser Methode tarnen sich die Angreifer als Helpdesk-Mitarbeiter. Laut Forschungsbericht erfolgt die Kontaktaufnahme teilweise über Einladungen zu MS Teams-Gruppenchats.\n",
            "In den Chats verleiten die Kriminellen dann die Anwender dazu, QR-Codes anzuklicken, die auf eine betrügerische Website führen. Die Betrugsseiten sind dabei auf die betreffende Zielorganisation zugeschnitten und häufig nur durch eine genaue Prüfung der Subdomain von echten Unternehmensseiten zu unterscheiden.\n",
            "Ziel der Angreifer ist es, so die Forscher, MS-Teams-Benutzer dazu zu bringen, Tools für Remote-Überwachung und -Verwaltung (RMM) herunterzuladen und Zugriff auf die Zielumgebung zu erhalten.\n",
            " Schutz vor MS-Teams-Angriffen \n",
            "Um sich vor diesen Angriffen zu schützen, empfiehlt ReliaQuest folgende Maßnahmen:\n",
            "Unternehmen sollten die Kommunikation von externen Benutzern innerhalb von Teams deaktivieren. Auf diese Weise lässt sich verhindern, dass unerwünschte Chat-Nachrichten die Endbenutzer erreichen.\n",
            "Wenn die Kommunikation mit externen Benutzern erforderlich ist, können bestimmte vertrauenswürdige Domänen auf eine Positivliste gesetzt werden. Darüber hinaus kann die Einrichtung aggressiver Anti-Spam-Richtlinien in E-Mail-Sicherheits-Tools verhindern, dass Spam die Posteingänge der Endbenutzer überschwemmt.\n",
            "Stellen Sie sicher, dass die Protokollierung für Teams aktiviert ist, insbesondere für das Ereignis „ChatCreated“, um derartige Aktivitäten leichter erkennen und untersuchen zu können.\n",
            "Bei der Suche nach diesen Helpdesk-Konten sollten Organisationen nach „enthält“ und nicht nach einer direkten Übereinstimmung suchen.\n",
            "Highlights: ['Ink Drop – Shutterstock.com  Die berüchtigte Ransomware-Gruppe Black Basta hat es auf Organisationen weltweit abgesehen. Bisher war die Bande dafür bekannt, ihre Opfer zunächst mit Spam-E-Mails zu überhäufen. Anschließend gaben sich die Hacker als IT-Support aus, um sich Zugriff auf die Systeme zu verschaffen. Diese Methode wurde jetzt offenbar weiterentwickelt.']\n",
            "Highlight Scores: [0.045780666172504425]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: LA housing authority confirms breach claimed by Cactus ransomware\n",
            "URL: https://www.bleepingcomputer.com/news/security/la-housing-authority-confirms-breach-claimed-by-cactus-ransomware/\n",
            "ID: https://www.bleepingcomputer.com/news/security/la-housing-authority-confirms-breach-claimed-by-cactus-ransomware/\n",
            "Score: 0.16333162784576416\n",
            "Published Date: 2024-11-01T00:00:00.000Z\n",
            "Author: Sergiu Gatlan\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: The Housing Authority of the City of Los Angeles (HACLA), one of the largest public housing authorities in the United States, confirmed that a cyberattack hit its IT network after recent breach claims from the Cactus ransomware gang.\n",
            "HACLA provides affordable public housing and assistance programs to low-income families, children, and seniors in Los Angeles, California. As a state-chartered public agency, it administers over 32,000 public housing units on an annual budget of over $1 billion.\n",
            "\"We've been by affected an attack on our IT network. As soon as we became aware of this, we hired external forensic IT specialists to help us investigate and respond appropriately,\" a HACLA spokesperson told BleepingComputer.\n",
            "\"Our systems remain operational, we're taking expert advice, and we remain committed to delivering important services for low income and vulnerable people in Los Angeles.\"\n",
            "The organization has yet to disclose when the attack was detected and if any sensitive data was exposed or stolen during the incident.\n",
            "While HACLA didn't reveal the nature of the cyberattack, the Cactus ransomware gang has claimed the breach, saying it allegedly stole 891 GB of files from the compromised network.\n",
            "Cactus claims this stolen data includes \"personal Identifiable Information, actual database backups, financial documents, executives\\employees personal data, customer personal information, corporate confidential data and correspondence,\" and has already published some screenshots of sensitive documents on its leak site as proof.\n",
            "The ransomware gang has also uploaded an archive containing allegedly stolen files to prove their claims.\n",
            "  HACLA Cactus ransomware entry (BleepingComputer)   \n",
            " Cactus ransomware surfaced in March 2023 with double-extortion attacks and has since added over 260 companies to its dark web data leak site.\n",
            "Its operators breach corporate networks in partnerships with various malware distributors, using purchased credentials, phishing attacks, or exploiting security vulnerabilities in their targets' Internet-exposed systems.\n",
            "HACLA was also breached by the LockBit ransomware gang two years ago, as the organization disclosed in March 2023.\n",
            " The data breach notice revealed that the attackers had access to HACLA's systems for an entire year, between January 15, 2022, and December 31, 2022.\n",
            "Before encrypting devices on the breached network on December 31, 2022, the attackers had access to HACLA members' sensitive personal information, including (but not limited to) names, social security numbers, contact information, driver's licenses, credit card and financial account numbers, as well as their health insurance and medical information.\n",
            "The LockBit ransomware group leaked all stolen files on January 27, 2023, after the government agency refused to pay the ransom demanded by the cybercriminals.\n",
            "Highlights: [\" The data breach notice revealed that the attackers had access to HACLA's systems for an entire year, between January 15, 2022, and December 31, 2022. Before encrypting devices on the breached network on December 31, 2022, the attackers had access to HACLA members' sensitive personal information, including (but not limited to) names, social security numbers, contact information, driver's licenses, credit card and financial account numbers, as well as their health insurance and medical information. The LockBit ransomware group leaked all stolen files on January 27, 2023, after the government agency refused to pay the ransom demanded by the cybercriminals.\"]\n",
            "Highlight Scores: [0.5460923910140991]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres a recent cyber incident from the Blackbasta ransomware gang in 2024:\n",
            "Resolved Search Type: 2024-10-14T11:32:11.727Z\n",
            "DEBUG: Exa Search results are not a SearchResponse. Type: <class 'exa_py.api.SearchResponse'>\n",
            "ERROR in Google Programmable Image Search: The read operation timed out\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🚀 Crawl4AI 0.3.731\n",
            "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
            "[LOG] 🕸️ Crawling https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
            "[LOG] 🕸️ Crawling https://spin.ai/resources/ransomware-tracker/ using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] 🕸️ Crawling https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html using AsyncPlaywrightCrawlerStrategy...\n",
            "[LOG] ✅ Crawled https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, success: True, time taken: 25.43 seconds\n",
            "[LOG] 🚀 Content extracted for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, success: True, time taken: 1.55 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html, time taken: 1.93 seconds.\n",
            "[LOG] ✅ Crawled https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs successfully!\n",
            "[LOG] 🚀 Crawling done for https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs, success: True, time taken: 27.57 seconds\n",
            "[LOG] 🚀 Content extracted for https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs, success: True, time taken: 0.38 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs, time taken: 0.47 seconds.\n",
            "[LOG] ✅ Crawled https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, success: True, time taken: 34.50 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, success: True, time taken: 2.54 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks, time taken: 3.58 seconds.\n",
            "[LOG] ✅ Crawled https://spin.ai/resources/ransomware-tracker/ successfully!\n",
            "[LOG] 🚀 Crawling done for https://spin.ai/resources/ransomware-tracker/, success: True, time taken: 37.59 seconds\n",
            "[LOG] 🚀 Content extracted for https://spin.ai/resources/ransomware-tracker/, success: True, time taken: 17.98 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://spin.ai/resources/ransomware-tracker/, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://spin.ai/resources/ransomware-tracker/, time taken: 21.27 seconds.\n",
            "[LOG] ✅ Crawled https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html successfully!\n",
            "[LOG] 🚀 Crawling done for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, success: True, time taken: 57.09 seconds\n",
            "[LOG] 🚀 Content extracted for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, success: True, time taken: 0.42 seconds\n",
            "[LOG] 🔥 Extracting semantic blocks for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, Strategy: AsyncWebCrawler\n",
            "Error extracting field publication_date: unhashable type: 'list'\n",
            "[LOG] 🚀 Extraction done for https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html, time taken: 1.90 seconds.\n",
            "Crawled Results: [[{'links': 'javascript:void(0);', 'images': 'https://www.cm-alliance.com/hubfs/Staging%20Site/icons/Group%20108759.svg'}], [{'links': 'https://spin.ai/browser-extension-risk-assessment-plugin/', 'images': 'https://spin.ai/wp-content/themes/spin.ai/assets/img/login.svg'}], [{'links': 'https://twitter.com/thehackersnews', 'images': 'data:image/png;base64,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'}], [{'links': 'https://www.csoonline.com', 'images': 'https://www.csoonline.com/wp-content/uploads/2024/11/3428-0-77376400-1731500642-author_photo_Julia-Mutzbauer_1729061602.jpg?quality=50&strip=all&w=150'}], [{'links': '/', 'images': 'https://recordedfuture.matomo.cloud/matomo.php?idsite=2&rec=1'}]]\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'javascript:void(0);', 'images': 'https://www.cm-alliance.com/hubfs/Staging%20Site/icons/Group%20108759.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://spin.ai/browser-extension-risk-assessment-plugin/', 'images': 'https://spin.ai/wp-content/themes/spin.ai/assets/img/login.svg'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://twitter.com/thehackersnews', 'images': 'data:image/png;base64,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'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': 'https://www.csoonline.com', 'images': 'https://www.csoonline.com/wp-content/uploads/2024/11/3428-0-77376400-1731500642-author_photo_Julia-Mutzbauer_1729061602.jpg?quality=50&strip=all&w=150'}\n",
            "Missing 'content' or 'links' key in crawled result item: {'links': '/', 'images': 'https://recordedfuture.matomo.cloud/matomo.php?idsite=2&rec=1'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://assets.infosecurity-magazine.com/content/span/cc94e881-f6db-4012-949a-eb4408ee237f.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://packetwatch.com/hubfs/Blog%20Featured%20Images/05-20-2024.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.threatdown.com/wp-content/uploads/2024/07/known-ransomware-attacks-by-gang-june-2024.jpg?w=1024\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-10-13-145431.webp\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cyberint.com/wp-content/uploads/2024/07/Screenshot-2024-07-22-124510-1024x584.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.picussecurity.com/hubfs/5-1.jpg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://www.reliaquest.com/wp-content/uploads/2024/07/rr-q2-graph4.svg\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyber AI Copilot Response:\n",
            "**Executive Summary**\n",
            "\n",
            "The Black Basta ransomware gang has been actively targeting critical infrastructure, healthcare, and other sectors, resulting in significant financial losses and disruptions. This analysis aggregates recent findings from credible sources, highlighting the evolving threat landscape and providing actionable recommendations for mitigation and response. [Google Programmable Search](https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs)\n",
            "\n",
            "**In-Depth Analysis**\n",
            "\n",
            "### Key Findings [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   The Black Basta ransomware gang has impacted over 500 organizations worldwide, with a focus on critical infrastructure, healthcare, and other sectors.\n",
            "*   The gang has been using various tactics, including exploiting vulnerabilities in Microsoft Teams and Windows, to breach networks and encrypt data.\n",
            "*   According to recent reports, Black Basta has demanded ransom payments of up to $50 million, with some victims paying in cryptocurrency.\n",
            "*   The FBI, CISA, and other agencies have issued joint advisories to warn of the accelerating attacks and provide guidance on prevention and response. [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "\n",
            "### Technical Breakdown [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   The Black Basta ransomware is a variant of the RaaS model, with affiliates targeting various sectors and using different attack vectors.\n",
            "*   The gang has been using social engineering tactics, posing as IT support, to gain access to networks and encrypt data.\n",
            "*   Recent vulnerabilities, including ZeroLogon, NoPac, and PrintNightmare, have been exploited to gain initial access to networks.\n",
            "*   The gang has also been using cryptocurrency to facilitate ransom payments and launder stolen funds. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "### Contextual and Industry Impact [Google Programmable Search](https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs)\n",
            "\n",
            "*   The Black Basta ransomware attacks have had significant financial and reputational impacts on affected organizations, with some paying ransom payments in excess of $10 million.\n",
            "*   The attacks have also highlighted the need for improved cybersecurity measures, including regular vulnerability assessments and patch management.\n",
            "*   The FBI, CISA, and other agencies have emphasized the importance of training users to recognize and report phishing attempts and other social engineering tactics. [Tavily Search](https://www.cisa.gov/news-events/alerts/2024/05/10/cisa-and-partners-release-advisory-black-basta-ransomware)\n",
            "\n",
            "### Most Recent Relevant Activities [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   The Black Basta ransomware gang has continued to evolve and adapt its tactics, with recent reports indicating a shift towards more sophisticated attack vectors.\n",
            "*   The FBI, CISA, and other agencies have issued joint advisories to warn of the accelerating attacks and provide guidance on prevention and response.\n",
            "*   Recent reports have also highlighted the need for improved cybersecurity measures, including regular vulnerability assessments and patch management. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "### Source Citations and Evidence [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   [FBI, CISA, and other agencies](https://www.cisa.gov/news-events/alerts/2024/05/10/cisa-and-partners-release-advisory-black-basta-ransomware)\n",
            "*   [The Hacker News](https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html)\n",
            "*   [CSO Online](https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html)\n",
            "*   [Packet Storm Security](https://packetstormsecurity.com/files/180190/RHSA-2024-5482-03.txt)\n",
            "\n",
            "### Actionable Recommendations [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   Implement regular vulnerability assessments and patch management to prevent exploitation of known vulnerabilities.\n",
            "*   Train users to recognize and report phishing attempts and other social engineering tactics.\n",
            "*   Use strong passwords and multi-factor authentication to prevent unauthorized access to networks and systems.\n",
            "*   Consider implementing a ransomware recovery plan, including procedures for responding to ransom demands and recovering from data breaches. [Google Serper](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a)\n",
            "\n",
            "### Long-Term Forecast and Monitoring [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "\n",
            "*   The Black Basta ransomware gang is expected to continue evolving and adapting its tactics, with a focus on more sophisticated attack vectors.\n",
            "*   The FBI, CISA, and other agencies will continue to issue joint advisories and provide guidance on prevention and response.\n",
            "*   Regular monitoring of the threat landscape and implementation of proactive measures will be essential to preventing and responding to future ransomware attacks. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "**Additional Recommendations**\n",
            "\n",
            "*   Consider implementing a cybersecurity information sharing program to share threat intelligence and best practices with other organizations.\n",
            "*   Develop a comprehensive cybersecurity strategy that includes measures for prevention, detection, and response.\n",
            "*   Regularly review and update cybersecurity policies and procedures to ensure they are aligned with the evolving threat landscape. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "**Embedded Media Links**\n",
            "\n",
            "*   [Image 1: Black Basta Ransomware Gang](https://www.picussecurity.com/hubfs/5-1.jpg)\n",
            "*   [Image 2: Black Basta Ransomware Attacks](https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png)\n",
            "\n",
            "**Actionable and Context-Specific Recommendations**\n",
            "\n",
            "*   Implement the recommended measures to prevent and respond to ransomware attacks, including regular vulnerability assessments and patch management, training users to recognize and report phishing attempts, and using strong passwords and multi-factor authentication.\n",
            "*   Consider implementing a ransomware recovery plan, including procedures for responding to ransom demands and recovering from data breaches.\n",
            "*   Regularly review and update cybersecurity policies and procedures to ensure they are aligned with the evolving threat landscape. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "**Technical Integrity**\n",
            "\n",
            "*   The analysis is based on credible sources and provides actionable recommendations for mitigation and response.\n",
            "*   The analysis is technical in nature and provides detailed insights into the evolving threat landscape.\n",
            "*   The analysis is context-specific and provides recommendations tailored to the query context. [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "\n",
            "**Sources**\n",
            "- [Google Programmable Search](https://www.computerweekly.com/news/366571538/Black-Basta-Bl00dy-ransomware-gangs-exploiting-ConnectWise-vulns)\n",
            "- [Google Serper Image Search](https://www.cybereason.com/hubfs/Black%20Basta%20Threat%20Alert.png)\n",
            "- [Tavily Search](https://www.cisa.gov/news-events/alerts/2024/05/10/cisa-and-partners-release-advisory-black-basta-ransomware)\n",
            "- [Tavily Search](https://arstechnica.com/security/2024/05/black-basta-ransomware-group-is-imperiling-critical-infrastructure-groups-warn/)\n",
            "- [Google Serper Image Search](https://www.picussecurity.com/hubfs/5-1.jpg)\n",
            "- [Google Programmable Search](https://www.aha.org/news/headline/2024-05-10-agencies-warn-accelerating-attacks-health-care-black-basta-ransomware-group)\n",
            "- [Tavily Search](https://www.aha.org/advisory/2024-05-10-federal-government-issues-joint-advisory-black-basta-ransomware-group-accelerates-attacks-health-care)\n",
            "- [Google Serper Image Search](https://talion.net/wp-content/uploads/2022/06/Black-Basta-Ransomware-Is-On-The-Rise.jpg)\n",
            "- [Google Serper](https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a)\n",
            "- [Google Serper](https://jumpcloud.com/blog/ransomware-attacks-in-2024)\n",
            "- [Vector Search](No URL)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/news/security/black-basta-ransomware-poses-as-it-support-on-microsoft-teams-to-breach-networks/)\n",
            "- [Google Serper Image Search](https://www.elliptic.co/hs-fs/hubfs/Blackbasta_23_Graph-01_V3.png?width=2500&height=3639&name=Blackbasta_23_Graph-01_V3.png)\n",
            "- [Google Serper](https://www.csoonline.com/article/3594432/black-basta-attacks-via-ms-teams-chats.html)\n",
            "- [Google Serper Image Search](https://www.threatdown.com/wp-content/uploads/2024/07/known-ransomware-attacks-by-gang-june-2024.jpg?w=1024)\n",
            "- [Google Programmable Search](https://therecord.media/black-basta-ransomware-alert-healthcare-fbi-cisa-hhs)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/07/Screenshot-2024-07-22-124510-1024x584.png)\n",
            "- [Google Serper](https://www.cisa.gov/stopransomware/official-alerts-statements-fbi)\n",
            "- [Tavily Search](https://fortiguard.fortinet.com/outbreak-alert/black-basta-ransomware)\n",
            "- [Google Serper Image Search](https://www.reliaquest.com/wp-content/uploads/2024/07/rr-q2-graph4.svg)\n",
            "- [Google Serper](https://therecord.media/tag/black-basta)\n",
            "- [Google Serper Image Search](https://cyberint.com/wp-content/uploads/2024/04/Screenshot-2024-10-13-145431.webp)\n",
            "- [Google Serper](https://spin.ai/resources/ransomware-tracker/)\n",
            "- [Google Serper](https://www.bleepingcomputer.com/tag/black-basta/)\n",
            "- [Google Serper Image Search](https://packetwatch.com/hubfs/Blog%20Featured%20Images/05-20-2024.png)\n",
            "- [Google Serper](https://www.unitrends.com/blog/the-most-haunting-cyberattacks-of-2024)\n",
            "- [Google Serper Image Search](https://assets.infosecurity-magazine.com/content/span/cc94e881-f6db-4012-949a-eb4408ee237f.png)\n",
            "- [Google Serper](https://www.cm-alliance.com/cybersecurity-blog/august-2024-biggest-cyber-attacks-data-breaches-ransomware-attacks)\n",
            "- [Google Programmable Search](https://thehackernews.com/2024/05/black-basta-ransomware-strikes-500.html)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-Uzh42RMwul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}