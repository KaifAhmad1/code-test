{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Langgraph_Agentic_RAG_Cyber_AI_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cyber AI Copilot for Security and Intelligence Domain**"
      ],
      "metadata": {
        "id": "M0rt8qzmxyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzih_tgLwXK6",
        "outputId": "a0b73789-1393-4c56-9c83-7f6e9c0e4093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/164.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain-core asknews langgraph\n",
        "%pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "from exa_py import Exa\n",
        "from langchain_core.tools import tool\n",
        "import re\n",
        "from typing import List, Union\n",
        "from goose3 import Goose\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_community.document_loaders.firecrawl import FireCrawlLoader\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = \"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\"\n",
        "PINECONE_API_KEY = \"8e15b925-3b96-497d-b20a-08d308782b83\"\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
        "ASKNEWS_CLIENT_ID = \"a0de4609-b760-4c83-9609-5c04d7743b84\"\n",
        "ASKNEWS_CLIENT_SECRET = \"D5Mlhkztk4TcW24diUgcW0FA2w\"\n",
        "SERPER_API_KEY = \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\"\n",
        "EXA_API_KEY = \"953b5801-11be-4b37-a313-f8df8f37027c\"\n",
        "GOOGLE_API_KEY=\"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\"\n",
        "GOOGLE_CSE_ID=\"63053004a7e2445c3\"\n",
        "Tavily_API_KEY=\"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\"\n",
        "FIRECRAWL_API_KEY = \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\"\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = Tavily_API_KEY"
      ],
      "metadata": {
        "id": "FY2ZeXgvxMxp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM and embeddings\n",
        "llm = ChatGroq(temperature=0, model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "print(\"DEBUG: LLM and embeddings initialized\")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "print(\"DEBUG: Pinecone and vector store initialized\")\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper(k=5)\n",
        "\n",
        "# Initialize Exa search tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Initialize Goose3 for web scraping\n",
        "goose = Goose()\n",
        "\n",
        "# Initialize FireCrawl\n",
        "firecrawl_loader = FireCrawlLoader(api_key=FIRECRAWL_API_KEY, url=\"\")\n",
        "\n",
        "print(\"DEBUG: Search tools initialized\")"
      ],
      "metadata": {
        "id": "XxTld11_xT0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4296feed-04f1-4be2-f75a-7dbdc889f30f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: LLM and embeddings initialized\n",
            "DEBUG: Pinecone and vector store initialized\n",
            "DEBUG: Search tools initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Dict[str, str]]\n",
        "    memory: Optional[Dict[str, Any]]\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[Dict[str, str]]] = []\n",
        "    images: Optional[List[str]] = []\n",
        "    videos: Optional[List[str]] = []\n",
        "    content: Optional[str] = \"\"\n",
        "\n",
        "async def scrape_content(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    print(f\"DEBUG: Scraping content from {len(urls)} URLs\")\n",
        "    results = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Use FireCrawl to extract content\n",
        "            firecrawl_loader.url = url\n",
        "            firecrawl_loader.mode = \"scrape\"\n",
        "            docs = firecrawl_loader.load()\n",
        "\n",
        "            if docs:\n",
        "                doc = docs[0]\n",
        "                content = doc.page_content\n",
        "                images = [img for img in doc.metadata.get(\"images\", [])]\n",
        "                videos = [vid for vid in doc.metadata.get(\"videos\", [])]\n",
        "\n",
        "                results.append({\n",
        "                    \"url\": url,\n",
        "                    \"text\": content[:1000],  # Limit to first 1000 characters\n",
        "                    \"images\": images[:3],  # Limit to first 3 images\n",
        "                    \"videos\": videos[:1]  # Limit to first video\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"url\": url,\n",
        "                    \"text\": \"\",\n",
        "                    \"images\": [],\n",
        "                    \"videos\": []\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR scraping {url}: {str(e)}\")\n",
        "            results.append({\n",
        "                \"url\": url,\n",
        "                \"text\": \"\",\n",
        "                \"images\": [],\n",
        "                \"videos\": []\n",
        "            })\n",
        "    return results\n",
        "\n",
        "def vector_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"DEBUG: Performing vector search for query: {query}\")\n",
        "    results = vector_store.similarity_search(query, k=3)\n",
        "    urls = [doc.metadata.get(\"source\", \"\") for doc in results]\n",
        "    scraped_contents = asyncio.run(scrape_content(urls))\n",
        "    search_results = [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content[:200],\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            **scraped_contents[i]\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "    print(f\"DEBUG: Vector search results: {search_results}\")\n",
        "    return search_results\n",
        "\n",
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"DEBUG: Performing Google Serper search for query: {query}\")\n",
        "    results = google_serper.results(query)\n",
        "    urls = [result.get(\"link\", \"\") for result in results.get(\"organic\", [])[:3]]\n",
        "    scraped_contents = asyncio.run(scrape_content(urls))\n",
        "    search_results = [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\")[:200],\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            **scraped_contents[i]\n",
        "        ) for i, result in enumerate(results.get(\"organic\", [])[:3])\n",
        "    ]\n",
        "    print(f\"DEBUG: Google Serper search results: {search_results}\")\n",
        "    return search_results\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"DEBUG: Performing Exa search for query: {query}\")\n",
        "    results = exa.search_and_contents(query, use_autoprompt=True, num_results=3)\n",
        "    urls = [result.get(\"url\", \"\") for result in results]\n",
        "    scraped_contents = asyncio.run(scrape_content(urls))\n",
        "    search_results = [\n",
        "        SearchResult(\n",
        "            source=\"Exa Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"text\", \"No text\")[:200],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_urls\", [])[:3],\n",
        "            **scraped_contents[i]\n",
        "        ) for i, result in enumerate(results)\n",
        "    ]\n",
        "    print(f\"DEBUG: Exa search results: {search_results}\")\n",
        "    return search_results\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"DEBUG: Performing Tavily search for query: {query}\")\n",
        "    results = tavily_search.invoke({\"query\": query})\n",
        "    urls = [result.get(\"url\", \"\") for result in results[:3]]\n",
        "    scraped_contents = asyncio.run(scrape_content(urls))\n",
        "    search_results = [\n",
        "        SearchResult(\n",
        "            source=\"Tavily Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"content\", \"No content\")[:200],\n",
        "            url=result.get(\"url\", \"No URL\"),\n",
        "            date=result.get(\"published_date\"),\n",
        "            images=result.get(\"image_url\", [])[:3] if result.get(\"image_url\") else [],\n",
        "            **scraped_contents[i]\n",
        "        ) for i, result in enumerate(results[:3])\n",
        "    ]\n",
        "    print(f\"DEBUG: Tavily search results: {search_results}\")\n",
        "    return search_results\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"DEBUG: Performing Google Programmable search for query: {query}\")\n",
        "    results = google_search.results(query, num_results=5)\n",
        "    urls = [result.get(\"link\", \"\") for result in results[:3]]\n",
        "    scraped_contents = asyncio.run(scrape_content(urls))\n",
        "    search_results = [\n",
        "        SearchResult(\n",
        "            source=\"Google Programmable Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\")[:200],\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            **scraped_contents[i]\n",
        "        ) for i, result in enumerate(results[:3])\n",
        "    ]\n",
        "    print(f\"DEBUG: Google Programmable search results: {search_results}\")\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "rnQiDPU8xddo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_searches(state: AgentState) -> AgentState:\n",
        "    print(\"DEBUG: Executing searches\")\n",
        "    query = state[\"messages\"][-1][\"content\"]\n",
        "    print(f\"DEBUG: Search query: {query}\")\n",
        "    searches = [\n",
        "        (\"Vector Search\", vector_search),\n",
        "        (\"Google Serper Search\", google_serper_search),\n",
        "        (\"Exa Search\", exa_search),\n",
        "        (\"Tavily Search\", tavily_search),\n",
        "        (\"Google Programmable Search\", google_programmable_search)\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    for name, func in searches:\n",
        "        try:\n",
        "            results = func(query)\n",
        "            print(f\"DEBUG: Results from {name}:\")\n",
        "            for result in results:\n",
        "                print(f\"Title: {result.title}\")\n",
        "                print(f\"Snippet: {result.snippet}\")\n",
        "                print(f\"URL: {result.url}\")\n",
        "                print(f\"Date: {result.date}\")\n",
        "                print(f\"Images: {result.images}\")\n",
        "                print(f\"Videos: {result.videos}\")\n",
        "                print(f\"Content: {result.content[:200]}...\")\n",
        "                print(\"\\n\")\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR in {name}: {str(e)}\")\n",
        "            state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
        "\n",
        "    # Sort results by date (if available) and relevance\n",
        "    def sort_key(x):\n",
        "        return (x.date is not None, x.date or \"\", x.title)\n",
        "\n",
        "    all_results.sort(key=sort_key, reverse=True)\n",
        "\n",
        "    # Select top 5 most relevant results\n",
        "    top_results = all_results[:5]\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"tool\", \"content\": \"Search Results\", \"results\": top_results})\n",
        "    print(f\"DEBUG: Final search results added to state: {top_results}\")\n",
        "    return state\n",
        "\n",
        "def generate_response(state: AgentState) -> AgentState:\n",
        "    print(\"DEBUG: Generating response\")\n",
        "    memory = state.get(\"memory\", {})\n",
        "    chat_history = memory.get(\"chat_history\", \"\")\n",
        "\n",
        "    search_results = next((m[\"results\"] for m in reversed(state[\"messages\"]) if m[\"role\"] == \"tool\" and \"results\" in m), [])\n",
        "    print(f\"DEBUG: Search results for response generation: {search_results}\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([(\n",
        "        \"system\", \"\"\"You are an advanced AI copilot specializing in cybersecurity and intelligence. Your task is to provide highly relevant, actionable, and up-to-date information based on the user's query. Follow these guidelines:\n",
        "\n",
        "1. Analyze the user's query: {input}\n",
        "2. Examine the search results, prioritizing recent and relevant information from reputable sources.\n",
        "3. Identify emerging patterns, trends, and potential implications related to the query.\n",
        "4. Provide a concise, structured response tailored to the query, including:\n",
        "   a. Key Findings (2-3 bullet points of the most critical information)\n",
        "   b. Brief Analysis (focused examination of the key points, directly addressing the query)\n",
        "   c. Recommendations (1-2 actionable items with rationale)\n",
        "\n",
        "5. Include clear citations for ALL information using the format [Source Name](URL).\n",
        "6. If search results contain images or videos, mention their content and relevance.\n",
        "7. Adjust the response length based on the query complexity and available information.\n",
        "8. Include technical details when appropriate, such as specific vulnerabilities or mitigation strategies.\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "Human query: {input}\n",
        "Search Results: {search_results}\n",
        "\n",
        "Current date: {current_date}\n",
        "\n",
        "Provide a concise, actionable response based on the query and latest findings, ensuring every piece of information is properly cited:\n",
        "\"\"\"\n",
        "    )])\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    print(\"DEBUG: Invoking LLM chain\")\n",
        "    response = chain.invoke({\n",
        "        \"input\": state[\"messages\"][-1][\"content\"],\n",
        "        \"search_results\": \"\\n\".join([f\"{result.title}\\n{result.snippet}\\n{result.content[:200]}...\\nImages: {', '.join(result.images[:3])}\\nVideos: {', '.join(result.videos[:1])}\\n{format_source_link(result.source, result.url)}\\nDate: {result.date or 'Not specified'}\\n\" for result in search_results]),\n",
        "        \"chat_history\": chat_history,\n",
        "        \"current_date\": current_date\n",
        "    })\n",
        "    print(f\"DEBUG: Raw LLM response: {response.content}\")\n",
        "\n",
        "    processed_response = ensure_citations(response.content, search_results)\n",
        "    print(f\"DEBUG: Processed response with citations: {processed_response}\")\n",
        "\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": processed_response})\n",
        "    state[\"memory\"] = {\"chat_history\": chat_history + f\"\\nHuman: {state['messages'][-2]['content']}\\nAI: {processed_response}\"}\n",
        "    return state\n",
        "\n",
        "def ensure_citations(text: str, search_results: List[SearchResult]) -> str:\n",
        "    print(\"DEBUG: Ensuring citations\")\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cited_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if not re.search(r'', paragraph) and not paragraph.startswith('**'):\n",
        "            paragraph += ' [Source needed]()'\n",
        "        cited_paragraphs.append(paragraph)\n",
        "\n",
        "    if not any(p.startswith('**Sources**') for p in cited_paragraphs):\n",
        "        sources = set(f\"- {format_source_link(result.source, result.url)}\" for result in search_results)\n",
        "        cited_paragraphs.append(\"**Sources**\\n\" + \"\\n\".join(sources))\n",
        "\n",
        "    result = '\\n\\n'.join(cited_paragraphs)\n",
        "    print(f\"DEBUG: Citations ensured. Final text: {result[:200]}...\")  # Print first 200 characters\n",
        "    return result\n",
        "\n",
        "def format_source_link(source: str, url: str) -> str:\n",
        "    return f\"[{source}]({url})\"\n",
        "\n",
        "# Workflow definition\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"execute_searches\", execute_searches)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_edge(\"execute_searches\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", END)\n",
        "workflow.set_entry_point(\"execute_searches\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "print(\"DEBUG: Workflow defined and compiled\")\n",
        "\n",
        "def run_agent(query: str, memory: Optional[Dict[str, Any]] = None) -> AgentState:\n",
        "    print(f\"DEBUG: Running agent with query: {query}\")\n",
        "    state = AgentState(messages=[{\"role\": \"human\", \"content\": query}], memory=memory or {})\n",
        "    result = graph.invoke(state)\n",
        "    print(f\"DEBUG: Agent run completed. Final state: {result}\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "EGkMBAYB2V9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52e9888-88fc-4b90-8a7e-bebfabbdff65"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Workflow defined and compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Incidents and attacks by Blackbasta Ransomware Gang?\"\n",
        "    print(f\"DEBUG: Starting main execution with query: {query}\")\n",
        "    result = run_agent(query)\n",
        "    for message in result[\"messages\"]:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            print(\"AI Copilot Analysis:\")\n",
        "            print(message[\"content\"])\n",
        "    print(\"DEBUG: Main execution completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdyh1-kcW0L0",
        "outputId": "320f9f61-285d-45cb-df4f-740c2197705c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Starting main execution with query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "DEBUG: Running agent with query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "DEBUG: Executing searches\n",
            "DEBUG: Search query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "DEBUG: Performing vector search for query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "ERROR in Vector Search: asyncio.run() cannot be called from a running event loop\n",
            "DEBUG: Performing Google Serper search for query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-f6145f2ba02b>:30: RuntimeWarning: coroutine 'scrape_content' was never awaited\n",
            "  state[\"messages\"].append({\"role\": \"tool\", \"content\": f\"{name} Error: {str(e)}\"})\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR in Google Serper Search: asyncio.run() cannot be called from a running event loop\n",
            "DEBUG: Performing Exa search for query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "ERROR in Exa Search: 'SearchResponse' object is not iterable\n",
            "DEBUG: Performing Tavily search for query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "ERROR in Tavily Search: 'function' object has no attribute 'invoke'\n",
            "DEBUG: Performing Google Programmable search for query: Latest Incidents and attacks by Blackbasta Ransomware Gang?\n",
            "ERROR in Google Programmable Search: asyncio.run() cannot be called from a running event loop\n",
            "DEBUG: Final search results added to state: []\n",
            "DEBUG: Generating response\n",
            "DEBUG: Search results for response generation: []\n",
            "DEBUG: Invoking LLM chain\n",
            "DEBUG: Raw LLM response: Based on your query \"Search Results,\" I've analyzed the latest information available. However, it seems that the query is too broad and doesn't provide enough context for a specific response. I'll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\n",
            "* Search results can be influenced by various factors, including personalization, location, and search history [2].\n",
            "* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\n",
            "\n",
            "**Brief Analysis:**\n",
            "Search results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it's essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\n",
            "2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\n",
            "\n",
            "**References:**\n",
            "\n",
            "[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\n",
            "[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\n",
            "[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\n",
            "\n",
            "Please provide a more specific query for a more detailed and actionable response.\n",
            "DEBUG: Ensuring citations\n",
            "DEBUG: Citations ensured. Final text: Based on your query \"Search Results,\" I've analyzed the latest information available. However, it seems that the query is too broad and doesn't provide enough context for a specific response. I'll pro...\n",
            "DEBUG: Processed response with citations: Based on your query \"Search Results,\" I've analyzed the latest information available. However, it seems that the query is too broad and doesn't provide enough context for a specific response. I'll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\n",
            "* Search results can be influenced by various factors, including personalization, location, and search history [2].\n",
            "* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\n",
            "\n",
            "**Brief Analysis:**\n",
            "Search results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it's essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\n",
            "2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\n",
            "\n",
            "**References:**\n",
            "\n",
            "[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\n",
            "[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\n",
            "[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\n",
            "\n",
            "Please provide a more specific query for a more detailed and actionable response.\n",
            "\n",
            "**Sources**\n",
            "\n",
            "DEBUG: Agent run completed. Final state: {'messages': [{'role': 'human', 'content': 'Latest Incidents and attacks by Blackbasta Ransomware Gang?'}, {'role': 'tool', 'content': 'Vector Search Error: asyncio.run() cannot be called from a running event loop'}, {'role': 'tool', 'content': 'Google Serper Search Error: asyncio.run() cannot be called from a running event loop'}, {'role': 'tool', 'content': \"Exa Search Error: 'SearchResponse' object is not iterable\"}, {'role': 'tool', 'content': \"Tavily Search Error: 'function' object has no attribute 'invoke'\"}, {'role': 'tool', 'content': 'Google Programmable Search Error: asyncio.run() cannot be called from a running event loop'}, {'role': 'tool', 'content': 'Search Results', 'results': []}, {'role': 'assistant', 'content': 'Based on your query \"Search Results,\" I\\'ve analyzed the latest information available. However, it seems that the query is too broad and doesn\\'t provide enough context for a specific response. I\\'ll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\\n\\n**Key Findings:**\\n\\n* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\\n* Search results can be influenced by various factors, including personalization, location, and search history [2].\\n* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\\n\\n**Brief Analysis:**\\nSearch results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it\\'s essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\\n\\n**Recommendations:**\\n\\n1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\\n2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\\n\\n**References:**\\n\\n[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\\n[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\\n[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\\n\\nPlease provide a more specific query for a more detailed and actionable response.\\n\\n**Sources**\\n'}], 'memory': {'chat_history': '\\nHuman: Search Results\\nAI: Based on your query \"Search Results,\" I\\'ve analyzed the latest information available. However, it seems that the query is too broad and doesn\\'t provide enough context for a specific response. I\\'ll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\\n\\n**Key Findings:**\\n\\n* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\\n* Search results can be influenced by various factors, including personalization, location, and search history [2].\\n* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\\n\\n**Brief Analysis:**\\nSearch results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it\\'s essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\\n\\n**Recommendations:**\\n\\n1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\\n2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\\n\\n**References:**\\n\\n[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\\n[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\\n[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\\n\\nPlease provide a more specific query for a more detailed and actionable response.\\n\\n**Sources**\\n'}}\n",
            "AI Copilot Analysis:\n",
            "Based on your query \"Search Results,\" I've analyzed the latest information available. However, it seems that the query is too broad and doesn't provide enough context for a specific response. I'll provide a general overview of search results and their relevance in the context of cybersecurity and intelligence.\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "* Search engines like Google, Bing, and DuckDuckGo use complex algorithms to rank search results, taking into account factors such as relevance, authority, and user behavior [1].\n",
            "* Search results can be influenced by various factors, including personalization, location, and search history [2].\n",
            "* In the context of cybersecurity, search results can be used to gather intelligence on potential threats, vulnerabilities, and mitigation strategies [3].\n",
            "\n",
            "**Brief Analysis:**\n",
            "Search results are a crucial aspect of modern information gathering, and their relevance and accuracy can significantly impact decision-making. In the context of cybersecurity, search results can be used to identify potential threats, vulnerabilities, and mitigation strategies. However, it's essential to be aware of the factors that influence search results, such as personalization and location, to ensure accurate and unbiased information.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "1. **Use multiple search engines**: To gather a more comprehensive understanding of a topic, use multiple search engines and compare the results to identify potential biases and inconsistencies [1].\n",
            "2. **Verify information through reputable sources**: When using search results for cybersecurity purposes, verify the information through reputable sources, such as government websites, academic journals, and established cybersecurity organizations [3].\n",
            "\n",
            "**References:**\n",
            "\n",
            "[1] Google Search Algorithm (2024). Retrieved from <https://developers.google.com/search/docs/advanced/optimization>\n",
            "[2] Bing Search Algorithm (2024). Retrieved from <https://www.bing.com/docs/advanced-search>\n",
            "[3] Cybersecurity and Infrastructure Security Agency (CISA). (2024). Search Results for Cybersecurity Threats. Retrieved from <https://www.cisa.gov/search-results>\n",
            "\n",
            "Please provide a more specific query for a more detailed and actionable response.\n",
            "\n",
            "**Sources**\n",
            "\n",
            "DEBUG: Main execution completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFLQkgkNW0OY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6LjD1kyW0Q0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKgGbgXaW0UL"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}