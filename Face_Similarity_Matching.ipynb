{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Face_Similarity_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reverse Image Search System for Defensive Forensics**"
      ],
      "metadata": {
        "id": "AVEgMKQJFRqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJzbvTscFMq1"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain langgraph numpy pillow requests vllm aiohttp opencv-python-headless networkx matplotlib nest_asyncio groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "import nest_asyncio\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "import os\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Union, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ExifTags\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import aiohttp\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "# LLM orchestration with Groq and LangChain\n",
        "from groq import Groq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Multi-agent workflow using LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Patch asyncio for environments with an existing event loop\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "y8qfEoZQK2hA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams"
      ],
      "metadata": {
        "id": "k0tRnwLsA_7V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_api_keys():\n",
        "    \"\"\"Get API keys from environment variables or prompt user for input.\"\"\"\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "    google_cse_id = os.environ.get(\"GOOGLE_CSE_ID\") or getpass.getpass(\"Enter your Google CSE ID: \")\n",
        "    google_api_key = os.environ.get(\"GOOGLE_API_KEY\") or getpass.getpass(\"Enter your Google API Key: \")\n",
        "    return groq_api_key, google_cse_id, google_api_key\n",
        "\n",
        "def load_models(device=None):\n",
        "    \"\"\"Load and initialize all required models.\"\"\"\n",
        "    print(\"Loading models...\")\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # Try to load models using vLLM for memory efficiency\n",
        "    try:\n",
        "        # CLIP model for image embeddings\n",
        "        clip_model = LLM(\n",
        "            model=\"openai/clip-vit-large-patch14\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"clip_model\"] = clip_model\n",
        "        models[\"clip_processor\"] = None  # Will be handled internally by vLLM\n",
        "        print(\"CLIP model loaded successfully\")\n",
        "\n",
        "        # BLIP-2 model for image semantic understanding and description generation\n",
        "        blip_model = LLM(\n",
        "            model=\"Salesforce/blip2-opt-2.7b\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"blip_model\"] = blip_model\n",
        "        models[\"blip_processor\"] = None  # Will be handled internally by vLLM\n",
        "        print(\"BLIP-2 model loaded successfully\")\n",
        "\n",
        "        # DINOv2 model for robust feature extraction\n",
        "        dinov2_model = LLM(\n",
        "            model=\"facebook/dinov2-base\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"dinov2_model\"] = dinov2_model\n",
        "        models[\"dinov2_processor\"] = None  # Will be handled internally by vLLM\n",
        "        print(\"DINOv2 model loaded successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models with vLLM: {e}\")\n",
        "        print(\"Falling back to direct imports...\")\n",
        "\n",
        "        # Fallback to direct imports if vLLM fails\n",
        "        try:\n",
        "            # For direct imports, we need to use the transformers library\n",
        "            # But we'll import it only if needed to avoid potential conflicts\n",
        "            from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel\n",
        "\n",
        "            # Initialize CLIP model for image embeddings\n",
        "            clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            models[\"clip_model\"] = clip_model.to(device)\n",
        "            models[\"clip_processor\"] = clip_processor\n",
        "            print(\"CLIP model loaded successfully (direct)\")\n",
        "\n",
        "            # Initialize BLIP-2 model for image semantic understanding and description generation\n",
        "            blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "            blip_model = AutoModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "            models[\"blip_model\"] = blip_model.to(device)\n",
        "            models[\"blip_processor\"] = blip_processor\n",
        "            print(\"BLIP-2 model loaded successfully (direct)\")\n",
        "\n",
        "            # Initialize DINOv2 model for robust feature extraction\n",
        "            dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "            dinov2_processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "            models[\"dinov2_model\"] = dinov2_model.to(device)\n",
        "            models[\"dinov2_processor\"] = dinov2_processor\n",
        "            print(\"DINOv2 model loaded successfully (direct)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models with direct imports: {e}\")\n",
        "            print(\"Falling back to OpenCV-based alternatives...\")\n",
        "\n",
        "            # Fallback to OpenCV-based alternatives if direct imports also fail\n",
        "            import cv2\n",
        "            import numpy as np\n",
        "\n",
        "            # OpenCV-based feature extractors as fallback\n",
        "            models[\"feature_detector\"] = cv2.SIFT_create()\n",
        "            models[\"orb_detector\"] = cv2.ORB_create()\n",
        "            models[\"brisk_detector\"] = cv2.BRISK_create()\n",
        "            print(\"OpenCV-based feature detectors loaded successfully\")\n",
        "\n",
        "    models[\"device\"] = device\n",
        "    print(f\"Model initialization complete on {device}\")\n",
        "    return models\n",
        "\n",
        "def initialize_llm(api_key):\n",
        "    \"\"\"Initialize the LLM client.\"\"\"\n",
        "    from groq import Groq\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    # Configure sampling parameters\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    return client, sampling_params"
      ],
      "metadata": {
        "id": "O1c6PH_HqJBP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. IMAGE PROCESSING FUNCTIONS\n",
        "#############################################\n",
        "async def fetch_image_async(image_url: str) -> bytes:\n",
        "    \"\"\"Asynchronously fetch an image from a URL.\"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(image_url) as response:\n",
        "            if response.status != 200:\n",
        "                raise ValueError(f\"Failed to fetch image. HTTP status code: {response.status}\")\n",
        "            return await response.read()\n",
        "\n",
        "def load_local_image(image_path: str) -> bytes:\n",
        "    \"\"\"Load an image from a local file path.\"\"\"\n",
        "    with open(image_path, 'rb') as f:\n",
        "        return f.read()\n",
        "\n",
        "async def preprocess_image_async(image_source: Union[str, bytes], enhance: bool = True) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Download and preprocess the image asynchronously.\n",
        "    Enhances contrast, sharpens, and returns a cleaned-up RGB PIL image.\n",
        "\n",
        "    Args:\n",
        "        image_source: URL string, local path, or bytes of the image\n",
        "        enhance: Whether to enhance the image or just load it\n",
        "\n",
        "    Returns:\n",
        "        Processed PIL Image\n",
        "    \"\"\"\n",
        "    # Determine if the source is a URL, local path, or bytes\n",
        "    if isinstance(image_source, str):\n",
        "        if image_source.startswith(('http://', 'https://')):\n",
        "            image_bytes = await fetch_image_async(image_source)\n",
        "        else:\n",
        "            # Assume it's a local file path\n",
        "            image_bytes = load_local_image(image_source)\n",
        "    else:\n",
        "        image_bytes = image_source\n",
        "\n",
        "    image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "    if enhance:\n",
        "        # Enhance contrast and sharpen the image\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        image = image.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "    # Resize image if any dimension exceeds 1024 pixels\n",
        "    if max(image.size) > 1024:\n",
        "        image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "\n",
        "    return image\n",
        "\n",
        "def detect_faces(image: Image.Image, cascade_file=None) -> list:\n",
        "    \"\"\"\n",
        "    Detect faces in the image using Haar Cascade from OpenCV.\n",
        "    Returns a list of cropped face images as PIL Image objects.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        cascade_file: Optional custom cascade file path\n",
        "\n",
        "    Returns:\n",
        "        List of detected face images\n",
        "    \"\"\"\n",
        "    img_cv = np.array(image)\n",
        "    img_cv = img_cv[:, :, ::-1].copy()  # Convert from RGB to BGR format\n",
        "\n",
        "    if cascade_file and os.path.exists(cascade_file):\n",
        "        face_cascade = cv2.CascadeClassifier(cascade_file)\n",
        "    else:\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Try different parameters for better face detection\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    # If no faces found, try more aggressive parameters\n",
        "    if len(faces) == 0:\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.05, 3)\n",
        "\n",
        "    face_images = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_img = image.crop((x, y, x+w, y+h))\n",
        "        face_img = face_img.resize((224, 224), Image.LANCZOS)\n",
        "        face_images.append({\n",
        "            \"image\": face_img,\n",
        "            \"coords\": (x, y, w, h)\n",
        "        })\n",
        "\n",
        "    return face_images\n",
        "\n",
        "def extract_exif_data(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Extract EXIF metadata from an image.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with EXIF data\n",
        "    \"\"\"\n",
        "    exif_data = {}\n",
        "    try:\n",
        "        # Get EXIF data if available\n",
        "        exif = image._getexif()\n",
        "        if exif:\n",
        "            for tag_id, value in exif.items():\n",
        "                tag = ExifTags.TAGS.get(tag_id, tag_id)\n",
        "                # Skip binary data\n",
        "                if isinstance(value, bytes) or tag == 'MakerNote':\n",
        "                    exif_data[tag] = \"Binary data\"\n",
        "                else:\n",
        "                    exif_data[tag] = value\n",
        "    except (AttributeError, KeyError, IndexError):\n",
        "        pass\n",
        "\n",
        "    return exif_data\n",
        "\n",
        "def perform_ocr(image: Image.Image) -> str:\n",
        "    \"\"\"\n",
        "    Perform OCR on the image if pytesseract is available.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Extracted text or empty string\n",
        "    \"\"\"\n",
        "    if not OCR_AVAILABLE:\n",
        "        return \"OCR not available. Install pytesseract to enable this feature.\"\n",
        "\n",
        "    try:\n",
        "        # Preprocess for better OCR\n",
        "        ocr_img = image.copy()\n",
        "        enhancer = ImageEnhance.Contrast(ocr_img)\n",
        "        ocr_img = enhancer.enhance(2.0)\n",
        "\n",
        "        # Convert to grayscale for better OCR\n",
        "        ocr_img = ocr_img.convert('L')\n",
        "\n",
        "        # Run OCR\n",
        "        text = pytesseract.image_to_string(ocr_img)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"OCR error: {str(e)}\"\n",
        "\n",
        "def save_faces(faces: list, output_dir=\"faces_output\"):\n",
        "    \"\"\"\n",
        "    Save detected face images to the output directory.\n",
        "\n",
        "    Args:\n",
        "        faces: List of face images\n",
        "        output_dir: Directory to save faces\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    for idx, face_data in enumerate(faces):\n",
        "        face = face_data[\"image\"]\n",
        "        face_file = os.path.join(output_dir, f\"face_{timestamp}_{idx+1}.jpg\")\n",
        "        face.save(face_file)\n",
        "\n",
        "    print(f\"Saved {len(faces)} face image(s) in '{output_dir}'.\")\n",
        "\n",
        "def image_hash(image: Image.Image, hash_size=8) -> str:\n",
        "    \"\"\"\n",
        "    Compute perceptual hash for the image for deduplication.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        hash_size: Size of the hash\n",
        "\n",
        "    Returns:\n",
        "        Hex string of the hash\n",
        "    \"\"\"\n",
        "    # Resize and convert to grayscale\n",
        "    img = image.resize((hash_size, hash_size), Image.LANCZOS).convert('L')\n",
        "    pixels = list(img.getdata())\n",
        "\n",
        "    # Calculate average pixel value\n",
        "    avg = sum(pixels) / len(pixels)\n",
        "\n",
        "    # Create hash: 1 if pixel > avg, 0 otherwise\n",
        "    bits = ''.join('1' if pixel > avg else '0' for pixel in pixels)\n",
        "\n",
        "    # Convert to hexadecimal\n",
        "    hex_hash = hex(int(bits, 2))[2:].zfill(hash_size**2 // 4)\n",
        "    return hex_hash\n",
        "\n",
        "def detect_image_manipulation(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Detect potential image manipulation using ELA (Error Level Analysis).\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with manipulation assessment\n",
        "    \"\"\"\n",
        "    # Save image with a specific quality level\n",
        "    temp_file = BytesIO()\n",
        "    image.save(temp_file, format='JPEG', quality=90)\n",
        "    temp_file.seek(0)\n",
        "    saved_image = Image.open(temp_file).convert('RGB')\n",
        "\n",
        "    # Calculate the difference\n",
        "    ela_image = ImageChops.difference(image, saved_image)\n",
        "    extrema = ela_image.getextrema()\n",
        "\n",
        "    # Determine if manipulation is likely\n",
        "    max_diff = max([ex[1] for ex in extrema])\n",
        "\n",
        "    return {\n",
        "        \"max_diff\": max_diff,\n",
        "        \"manipulation_score\": min(max_diff / 40.0, 1.0),  # Scale from 0-1\n",
        "        \"likely_manipulated\": max_diff > 20\n",
        "    }"
      ],
      "metadata": {
        "id": "06zu0aWBItFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. EMBEDDING & DESCRIPTION FUNCTIONS\n",
        "#############################################\n",
        "async def generate_clip_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the CLIP model.\"\"\"\n",
        "    inputs = models[\"clip_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"clip_model\"].get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_blip_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using BLIP-2 model for semantic representation.\"\"\"\n",
        "    inputs = models[\"blip_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"blip_model\"].get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_dinov2_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the DINOv2 model (CLS token).\"\"\"\n",
        "    inputs = models[\"dinov2_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"dinov2_model\"](**inputs).last_hidden_state[:, 0]\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_image_description(image: Image.Image, models: dict, prompt=None) -> str:\n",
        "    \"\"\"\n",
        "    Generate a detailed description of the image using BLIP-2.\n",
        "    Focuses on identifiable people, objects, and locations.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        models: Dictionary with loaded models\n",
        "        prompt: Optional custom prompt\n",
        "    \"\"\"\n",
        "    if prompt is None:\n",
        "        prompt = \"Describe this image in detail with focus on identifiable people, objects, and locations:\"\n",
        "\n",
        "    inputs = models[\"blip_processor\"](images=image, text=prompt, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"blip_model\"].generate(**inputs, max_new_tokens=100)\n",
        "    description = models[\"blip_processor\"].batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return description.strip()\n",
        "\n",
        "async def extract_classical_features(image: Image.Image) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Extract classical image features using ORB and optionally SIFT.\n",
        "    Returns descriptors selected based on the number of keypoints.\n",
        "    \"\"\"\n",
        "    image_np = np.array(image)\n",
        "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Extract ORB features\n",
        "    orb = cv2.ORB_create(nfeatures=1000)\n",
        "    keypoints_orb, descriptors_orb = orb.detectAndCompute(gray, None)\n",
        "\n",
        "    # Extract SIFT features if available\n",
        "    descriptors_sift = None\n",
        "    keypoints_sift = None\n",
        "    try:\n",
        "        sift = cv2.SIFT_create()\n",
        "        keypoints_sift, descriptors_sift = sift.detectAndCompute(gray, None)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Use the better descriptor\n",
        "    if descriptors_orb is None and descriptors_sift is None:\n",
        "        return None, None\n",
        "    elif descriptors_orb is None:\n",
        "        return keypoints_sift, descriptors_sift\n",
        "    elif descriptors_sift is None:\n",
        "        return keypoints_orb, descriptors_orb\n",
        "    else:\n",
        "        if len(keypoints_sift) > len(keypoints_orb):\n",
        "            return keypoints_sift, descriptors_sift\n",
        "        else:\n",
        "            return keypoints_orb, descriptors_orb\n",
        "\n",
        "def get_histogram_features(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Extract color histogram features from the image.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with histogram features\n",
        "    \"\"\"\n",
        "    # Convert to NumPy array\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    # Calculate histograms for each channel\n",
        "    hist_r = cv2.calcHist([img_np], [0], None, [256], [0, 256])\n",
        "    hist_g = cv2.calcHist([img_np], [1], None, [256], [0, 256])\n",
        "    hist_b = cv2.calcHist([img_np], [2], None, [256], [0, 256])\n",
        "\n",
        "    # Normalize histograms\n",
        "    hist_r = cv2.normalize(hist_r, hist_r).flatten()\n",
        "    hist_g = cv2.normalize(hist_g, hist_g).flatten()\n",
        "    hist_b = cv2.normalize(hist_b, hist_b).flatten()\n",
        "\n",
        "    return {\n",
        "        \"histogram_r\": hist_r.tolist(),\n",
        "        \"histogram_g\": hist_g.tolist(),\n",
        "        \"histogram_b\": hist_b.tolist()\n",
        "    }\n",
        "\n",
        "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two image embeddings.\n",
        "    Returns a float between -1 and 1 (1 indicates identical embeddings).\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(embedding1.flatten(), embedding2.flatten())\n",
        "    norm1 = np.linalg.norm(embedding1)\n",
        "    norm2 = np.linalg.norm(embedding2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(dot_product / (norm1 * norm2))\n",
        "\n",
        "async def generate_consensus_embedding(clip_emb, blip_emb, dinov2_emb) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate a consensus embedding by averaging the CLIP, BLIP-2, and DINOv2 embeddings.\n",
        "    Weights the embeddings based on their typical performance.\n",
        "    \"\"\"\n",
        "    # Weight the embeddings (these weights can be adjusted)\n",
        "    weights = np.array([0.4, 0.3, 0.3])  # CLIP, BLIP, DINOv2\n",
        "\n",
        "    # Ensure all embeddings are flattened and normalized\n",
        "    embeddings = [\n",
        "        clip_emb.flatten() / np.linalg.norm(clip_emb),\n",
        "        blip_emb.flatten() / np.linalg.norm(blip_emb),\n",
        "        dinov2_emb.flatten() / np.linalg.norm(dinov2_emb)\n",
        "    ]\n",
        "\n",
        "    # Check if dimensions match, otherwise resize\n",
        "    min_dim = min(e.shape[0] for e in embeddings)\n",
        "    resized_embeddings = [e[:min_dim] for e in embeddings]\n",
        "\n",
        "    # Weighted average\n",
        "    consensus = np.average(np.array(resized_embeddings), axis=0, weights=weights)\n",
        "\n",
        "    # Normalize\n",
        "    norm = np.linalg.norm(consensus)\n",
        "    return consensus / norm if norm > 0 else consensus"
      ],
      "metadata": {
        "id": "EyCtUkF1ItH-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. SEARCH ENGINE FUNCTIONS\n",
        "#############################################\n",
        "async def search_private_db(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search in a private database using image embeddings.\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, replace this with an actual database search\n",
        "    await asyncio.sleep(0.5)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Private DB\",\n",
        "            \"match\": \"Person_123\",\n",
        "            \"score\": 0.91,\n",
        "            \"metadata\": {\"date\": \"2023-10-15\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        },\n",
        "        {\n",
        "            \"source\": \"Private DB\",\n",
        "            \"match\": \"Person_456\",\n",
        "            \"score\": 0.85,\n",
        "            \"metadata\": {\"date\": \"2023-09-22\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_twitter(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search Twitter using keywords from the image description.\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, replace with actual Twitter API calls\n",
        "    await asyncio.sleep(0.7)\n",
        "    keywords = description.split()[:5]\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Twitter\",\n",
        "            \"match\": \"Tweet_Image_456\",\n",
        "            \"score\": 0.87,\n",
        "            \"metadata\": {\n",
        "                \"username\": \"@user123\",\n",
        "                \"posted\": \"2023-11-01\",\n",
        "                \"keywords\": keywords\n",
        "            },\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_reddit(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search Reddit for matching posts.\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, replace with actual Reddit API calls\n",
        "    await asyncio.sleep(0.6)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Reddit\",\n",
        "            \"match\": \"Reddit_Post_321\",\n",
        "            \"score\": 0.89,\n",
        "            \"metadata\": {\"subreddit\": \"r/pics\", \"posted\": \"2023-10-25\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_instagram(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search Instagram for matching posts.\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, replace with actual Instagram API calls\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Instagram\",\n",
        "            \"match\": \"Insta_Post_654\",\n",
        "            \"score\": 0.88,\n",
        "            \"metadata\": {\"username\": \"user456\", \"posted\": \"2023-11-12\", \"location\": \"New York\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_osint_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search OSINT sources for matching content.\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, implement real OSINT APIs\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"OSINT\",\n",
        "            \"match\": \"DarkWeb_Post_999\",\n",
        "            \"score\": 0.83,\n",
        "            \"metadata\": {\"forum\": \"anonymous_forum\", \"date\": \"2023-09-10\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        },\n",
        "        {\n",
        "            \"source\": \"OSINT\",\n",
        "            \"match\": \"Telegram_Group_123\",\n",
        "            \"score\": 0.79,\n",
        "            \"metadata\": {\"group\": \"public_channel_xyz\", \"date\": \"2023-10-30\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_tineye(image: Image.Image) -> list:\n",
        "    \"\"\"\n",
        "    Perform a TinEye reverse image search.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, implement actual TinEye API\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format='JPEG')\n",
        "    img_byte_arr.seek(0)\n",
        "    await asyncio.sleep(1.2)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"TinEye\",\n",
        "            \"match\": \"Website_ABC\",\n",
        "            \"score\": 0.92,\n",
        "            \"metadata\": {\"domain\": \"example.com\", \"first_crawled\": \"2023-08-15\"},\n",
        "            \"embedding\": None\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_google_images(image: Image.Image, description: str, api_key: str, cse_id: str) -> list:\n",
        "    \"\"\"\n",
        "    Search Google Images using the CSE API.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        description: Image description\n",
        "        api_key: Google API key\n",
        "        cse_id: Google CSE ID\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, implement actual Google API calls\n",
        "    search_terms = \" \".join(description.split()[:7])\n",
        "    await asyncio.sleep(1.0)\n",
        "\n",
        "    # If API keys are provided, use the Google Custom Search API\n",
        "    if api_key and cse_id:\n",
        "        try:\n",
        "            # Actual API call would go here\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"Google API error: {str(e)}\")\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Google Images\",\n",
        "            \"match\": \"News_Site_XYZ\",\n",
        "            \"score\": 0.86,\n",
        "            \"metadata\": {\n",
        "                \"url\": \"https://example-news.com/article123\",\n",
        "                \"title\": \"Example article related to the image\"\n",
        "            },\n",
        "            \"embedding\": None\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def search_additional_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Search additional sources (Facebook, LinkedIn, etc).\n",
        "\n",
        "    Args:\n",
        "        embedding: Image embedding\n",
        "        description: Image description\n",
        "\n",
        "    Returns:\n",
        "        List of matching results\n",
        "    \"\"\"\n",
        "    # For production, implement actual API calls\n",
        "    await asyncio.sleep(0.9)\n",
        "    return [\n",
        "        {\n",
        "            \"source\": \"Facebook\",\n",
        "            \"match\": \"FB_Post_123\",\n",
        "            \"score\": 0.81,\n",
        "            \"metadata\": {\"user\": \"john.doe\", \"posted\": \"2023-10-05\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        },\n",
        "        {\n",
        "            \"source\": \"LinkedIn\",\n",
        "            \"match\": \"LinkedIn_Profile_456\",\n",
        "            \"score\": 0.78,\n",
        "            \"metadata\": {\"profile\": \"jane-smith\", \"updated\": \"2023-11-10\"},\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "async def merge_search_results(*results: list) -> list:\n",
        "    \"\"\"\n",
        "    Merge search results from all sources and sort them by score.\n",
        "\n",
        "    Args:\n",
        "        *results: Lists of search results\n",
        "\n",
        "    Returns:\n",
        "        Merged and sorted list of results\n",
        "    \"\"\"\n",
        "    merged = []\n",
        "\n",
        "    # Use a set to track unique matches by source and match ID\n",
        "    seen = set()\n",
        "\n",
        "    for result_list in results:\n",
        "        for result in result_list:\n",
        "            # Create a unique identifier for deduplication\n",
        "            result_id = (result['source'], result['match'])\n",
        "\n",
        "            if result_id not in seen:\n",
        "                seen.add(result_id)\n",
        "                # Convert embedding to list if it's a NumPy array\n",
        "                if isinstance(result.get('embedding'), np.ndarray):\n",
        "                    result['embedding'] = result['embedding'].tolist()\n",
        "                merged.append(result)\n",
        "\n",
        "    # Sort by score in descending order\n",
        "    return sorted(merged, key=lambda x: x.get('score', 0), reverse=True)"
      ],
      "metadata": {
        "id": "VmM_gT9yItKt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 5. ANALYSIS & THREAT ASSESSMENT FUNCTIONS\n",
        "#############################################\n",
        "async def analyze_results_with_llm(results: list, image_description: str, client) -> str:\n",
        "    \"\"\"\n",
        "    Analyze the reverse image search results using an LLM.\n",
        "\n",
        "    Args:\n",
        "        results: Search results\n",
        "        image_description: Image description\n",
        "        client: LLM client\n",
        "\n",
        "    Returns:\n",
        "        Forensic analysis report\n",
        "    \"\"\"\n",
        "    # Prepare data for LLM\n",
        "    clean_results = []\n",
        "    for result in results:\n",
        "        # Remove embedding to keep the data smaller\n",
        "        clean_result = {k: v for k, v in result.items() if k != 'embedding'}\n",
        "        clean_results.append(clean_result)\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "You are a forensic analyst. Analyze the following reverse image search results.\n",
        "\n",
        "IMAGE DESCRIPTION:\n",
        "{image_description}\n",
        "\n",
        "SEARCH RESULTS (in JSON):\n",
        "{search_results}\n",
        "\n",
        "Provide a structured forensic assessment that includes:\n",
        "1. Cross-referencing of entities.\n",
        "2. Temporal and geographic correlations.\n",
        "3. Evaluation of source reliability.\n",
        "4. Insights on image similarity between the current image and search results.\n",
        "5. Overall forensic insights about the image origin.\n",
        "\n",
        "Format your response as a clear multi-section report.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use LLM to analyze\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a forensic image analyst providing detailed assessments.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
        "                image_description=image_description,\n",
        "                search_results=json.dumps(clean_results, indent=2)\n",
        "            )}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "async def threat_assessment(analysis: str, client) -> dict:\n",
        "    \"\"\"\n",
        "    Assess potential threats based on the forensic analysis.\n",
        "\n",
        "    Args:\n",
        "        analysis: Forensic analysis text\n",
        "        client: LLM client\n",
        "\n",
        "    Returns:\n",
        "        Threat assessment dictionary\n",
        "    \"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "Based on the following forensic image analysis, assess potential threats or concerns.\n",
        "\n",
        "{analysis}\n",
        "\n",
        "Return a JSON object with the keys:\n",
        "- \"threat_level\": integer (0-10)\n",
        "- \"categories\": list of strings (e.g., [\"identity_theft\", \"privacy_breach\"])\n",
        "- \"reasoning\": a brief explanation\n",
        "- \"recommended_actions\": a list of recommended actions\n",
        "\n",
        "Return valid JSON only.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a threat assessment analyst providing structured evaluations.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_template.format(analysis=analysis)}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Extract and parse the JSON from the response\n",
        "        response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Handle responses that might have text before/after the JSON\n",
        "        json_start = response_text.find('{')\n",
        "        json_end = response_text.rfind('}') + 1\n",
        "        if json_start >= 0 and json_end > json_start:\n",
        "            json_str = response_text[json_start:json_end]\n",
        "            return json.loads(json_str)\n",
        "        else:\n",
        "            raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"Error parsing threat assessment: {str(e)}\")\n",
        "        return {\n",
        "            \"threat_level\": 5,\n",
        "            \"categories\": [\"unknown\"],\n",
        "            \"reasoning\": \"LLM response parsing failed. Check the forensic analysis manually.\",\n",
        "            \"recommended_actions\": [\"Review analysis manually.\"]\n",
        "        }\n",
        "\n",
        "async def graph_link_analysis(results: list) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Create a network graph visualization of search results.\n",
        "\n",
        "    Args:\n",
        "        results: List of search results\n",
        "\n",
        "    Returns:"
      ],
      "metadata": {
        "id": "A8ZPflppItNU",
        "outputId": "f9702a24-a608-4562-9461-618763da1c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-8-2b42652ea767>, line 114)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-2b42652ea767>\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HY5EIPfqItQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}