{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Face_Similarity_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reverse Image Search System for Defensive Forensics**\n",
        "\n",
        "This is an end-to-end forensic image analysis pipeline with rich features:\n",
        "1. Enhanced model loading with multiple fallback strategies.\n",
        "2. Advanced asynchronous image processing including OCR, EXIF extraction, and error level analysis.\n",
        "3. Multi-agent reverse image search across multiple simulated sources (Private DB, Twitter, Reddit, Instagram, OSINT, TinEye, Google Images, Additional Sources).\n",
        "4. Comprehensive embedding generation and consensus across CLIP, BLIP-2, and DINOv2.\n",
        "5. Detailed forensic analysis using advanced prompt templates and multi-step LLM workflows.\n",
        "6. Face detection, clustering, and similarity analysis with visualization.\n",
        "7. Graph-based link analysis for search results with NetworkX.\n",
        "8. Deduplication using perceptual image hashes.\n",
        "9. Robust error handling and extensive print statements for logging.\n",
        "10. Results saving including face images and network graph image."
      ],
      "metadata": {
        "id": "AVEgMKQJFRqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJzbvTscFMq1"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain langgraph numpy pillow requests vllm aiohttp opencv-python-headless networkx matplotlib nest_asyncio groq pytesseract gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "from typing import Any, Tuple, Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import aiohttp\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ExifTags, ImageChops\n",
        "\n",
        "# Import vLLM and Groq related classes\n",
        "from vllm import LLM, SamplingParams\n",
        "from groq import Groq\n",
        "\n",
        "# Import LangGraph for multi-agent orchestration\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "id": "y8qfEoZQK2hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3973048b-4fe9-4377-a099-cb9271a47a64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-25 07:12:21 [__init__.py:256] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 1. INITIALIZATION AND MODEL LOADING FUNCTIONS\n",
        "#############################################\n",
        "def get_api_keys() -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Prompt the user for necessary API keys.\n",
        "    \"\"\"\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "    google_cse_id = os.environ.get(\"GOOGLE_CSE_ID\") or getpass.getpass(\"Enter your Google CSE ID: \")\n",
        "    google_api_key = os.environ.get(\"GOOGLE_API_KEY\") or getpass.getpass(\"Enter your Google API Key: \")\n",
        "    return groq_api_key, google_cse_id, google_api_key\n",
        "\n",
        "def load_models(device: Optional[str] = None) -> dict:\n",
        "    \"\"\"\n",
        "    Load required models with multiple fallback strategies using vLLM.\n",
        "    \"\"\"\n",
        "    print(\"Loading models...\")\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = {}\n",
        "    try:\n",
        "        # Attempt loading models using vLLM for high performance inference\n",
        "        clip_model = LLM(\n",
        "            model=\"openai/clip-vit-large-patch14\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"clip_model\"] = clip_model\n",
        "        models[\"clip_processor\"] = None  # Assume processor is integrated in your vLLM wrapper.\n",
        "        print(\"CLIP (vLLM) model loaded successfully.\")\n",
        "\n",
        "        blip_model = LLM(\n",
        "            model=\"Salesforce/blip2-opt-2.7b\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"blip_model\"] = blip_model\n",
        "        models[\"blip_processor\"] = None\n",
        "        print(\"BLIP-2 (vLLM) model loaded successfully.\")\n",
        "\n",
        "        dinov2_model = LLM(\n",
        "            model=\"facebook/dinov2-base\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"dinov2_model\"] = dinov2_model\n",
        "        models[\"dinov2_processor\"] = None\n",
        "        print(\"DINOv2 (vLLM) model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"vLLM loading failed: {e}\")\n",
        "        print(\"Falling back to direct transformer imports...\")\n",
        "        try:\n",
        "            from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel\n",
        "            clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            models[\"clip_model\"] = clip_model.to(device)\n",
        "            models[\"clip_processor\"] = clip_processor\n",
        "            print(\"CLIP (Direct) model loaded successfully.\")\n",
        "\n",
        "            blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "            blip_model = AutoModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "            models[\"blip_model\"] = blip_model.to(device)\n",
        "            models[\"blip_processor\"] = blip_processor\n",
        "            print(\"BLIP-2 (Direct) model loaded successfully.\")\n",
        "\n",
        "            dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "            dinov2_processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "            models[\"dinov2_model\"] = dinov2_model.to(device)\n",
        "            models[\"dinov2_processor\"] = dinov2_processor\n",
        "            print(\"DINOv2 (Direct) model loaded successfully.\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Direct imports failed: {e2}\")\n",
        "            print(\"Using OpenCV-based feature detectors as a last resort.\")\n",
        "            models[\"feature_detector\"] = cv2.SIFT_create() if hasattr(cv2, 'SIFT_create') else None\n",
        "            models[\"orb_detector\"] = cv2.ORB_create()\n",
        "            models[\"brisk_detector\"] = cv2.BRISK_create()\n",
        "    models[\"device\"] = device\n",
        "    print(f\"Model initialization complete on device: {device}\")\n",
        "    return models\n",
        "\n",
        "def initialize_llm(api_key: str) -> Tuple[Any, SamplingParams]:\n",
        "    \"\"\"\n",
        "    Initialize the LLM client using Groq.\n",
        "    \"\"\"\n",
        "    client = Groq(api_key=api_key)\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return client, sampling_params"
      ],
      "metadata": {
        "id": "O1c6PH_HqJBP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. UTILITY & PREPROCESSING FUNCTIONS\n",
        "#############################################\n",
        "def extract_exif_data(image: Image.Image) -> dict:\n",
        "    exif_data = {}\n",
        "    try:\n",
        "        exif = image._getexif()\n",
        "        if exif:\n",
        "            for tag_id, val in exif.items():\n",
        "                tag = ExifTags.TAGS.get(tag_id, tag_id)\n",
        "                exif_data[tag] = \"Binary data\" if isinstance(val, bytes) or tag == \"MakerNote\" else val\n",
        "    except Exception:\n",
        "        pass\n",
        "    return exif_data\n",
        "\n",
        "def perform_ocr(image: Image.Image) -> str:\n",
        "    try:\n",
        "        import pytesseract\n",
        "        ocr_image = image.copy()\n",
        "        enhancer = ImageEnhance.Contrast(ocr_image)\n",
        "        ocr_image = enhancer.enhance(2.0)\n",
        "        ocr_image = ocr_image.convert(\"L\")\n",
        "        return pytesseract.image_to_string(ocr_image).strip()\n",
        "    except ImportError:\n",
        "        return \"OCR not available. Please install pytesseract.\"\n",
        "    except Exception as e:\n",
        "        return f\"OCR error: {e}\"\n",
        "\n",
        "def detect_image_manipulation(image: Image.Image) -> dict:\n",
        "    temp_file = BytesIO()\n",
        "    image.save(temp_file, format=\"JPEG\", quality=90)\n",
        "    temp_file.seek(0)\n",
        "    saved_image = Image.open(temp_file).convert(\"RGB\")\n",
        "    ela_image = ImageChops.difference(image, saved_image)\n",
        "    extrema = ela_image.getextrema()\n",
        "    max_diff = max(ex[1] for ex in extrema)\n",
        "    return {\n",
        "        \"max_diff\": max_diff,\n",
        "        \"manipulation_score\": min(max_diff / 40.0, 1.0),\n",
        "        \"likely_manipulated\": max_diff > 20\n",
        "    }\n",
        "\n",
        "def image_hash(image: Image.Image, hash_size: int = 8) -> str:\n",
        "    img = image.resize((hash_size, hash_size), Image.LANCZOS).convert(\"L\")\n",
        "    pixels = list(img.getdata())\n",
        "    avg = sum(pixels) / len(pixels)\n",
        "    bits = \"\".join(\"1\" if pixel > avg else \"0\" for pixel in pixels)\n",
        "    return hex(int(bits, 2))[2:].zfill(hash_size**2 // 4)\n",
        "\n",
        "async def preprocess_image_async(image_source: bytes, enhance: bool = True) -> Image.Image:\n",
        "    image = Image.open(BytesIO(image_source)).convert(\"RGB\")\n",
        "    if enhance:\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        image = image.filter(ImageFilter.SHARPEN)\n",
        "    if max(image.size) > 1024:\n",
        "        image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "    return image"
      ],
      "metadata": {
        "id": "06zu0aWBItFQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. MULTI-AGENT FORENSIC PIPELINE FUNCTIONS\n",
        "#############################################\n",
        "async def generate_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate consensus embedding using CLIP, BLIP-2 and DINOv2 models via vLLM.\n",
        "    \"\"\"\n",
        "    # Prepare inputs as required. In a real implementation, you'd pre-process with the model processors.\n",
        "    # Here we assume the vLLM models accept PIL Image directly.\n",
        "    emb_clip = models[\"clip_model\"].get_image_features(image)\n",
        "    emb_blip = models[\"blip_model\"].get_image_features(image)\n",
        "    dinov2_output = models[\"dinov2_model\"](image)\n",
        "    # Assume the CLS token is the first token output.\n",
        "    emb_dinov2 = dinov2_output[:, 0]\n",
        "    weights = np.array([0.4, 0.3, 0.3])\n",
        "    # Normalize embeddings\n",
        "    emb_clip = emb_clip.cpu().numpy() / np.linalg.norm(emb_clip.cpu().numpy())\n",
        "    emb_blip = emb_blip.cpu().numpy() / np.linalg.norm(emb_blip.cpu().numpy())\n",
        "    emb_dinov2 = emb_dinov2.cpu().numpy() / np.linalg.norm(emb_dinov2.cpu().numpy())\n",
        "    embeddings = [emb_clip.flatten(), emb_blip.flatten(), emb_dinov2.flatten()]\n",
        "    min_dim = min(e.shape[0] for e in embeddings)\n",
        "    resized = [e[:min_dim] for e in embeddings]\n",
        "    consensus = np.average(np.array(resized), axis=0, weights=weights)\n",
        "    norm = np.linalg.norm(consensus)\n",
        "    return consensus / norm if norm > 0 else consensus\n",
        "\n",
        "async def generate_image_description(image: Image.Image, models: dict) -> str:\n",
        "    \"\"\"\n",
        "    Generate a detailed description of the image using BLIP-2.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are an expert forensic analyst tasked with reverse image search for defensive forensics. \"\n",
        "        \"Describe this image in detail, focusing on identifiable subjects, objects, environmental context, \"\n",
        "        \"and potential forensic clues such as manipulation or concealed features. Use precise, technical language.\"\n",
        "    )\n",
        "    # Using BLIP-2 model via vLLM to generate description (simulated call).\n",
        "    inputs = models[\"blip_processor\"](images=image, text=prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(models[\"device\"]) if models.get(\"device\") else inputs\n",
        "    outputs = models[\"blip_model\"].generate(**inputs, max_new_tokens=100)\n",
        "    description = models[\"blip_processor\"].batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return description.strip()"
      ],
      "metadata": {
        "id": "EyCtUkF1ItH-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# Simulated Search Agents for Reverse Image Search\n",
        "# ------------------------------------------------\n",
        "async def search_private_db(embedding: np.ndarray, description: str) -> list:\n",
        "    # Real implementation could query a forensic database.\n",
        "    await asyncio.sleep(0.5)\n",
        "    return [{\n",
        "        \"source\": \"Private DB\",\n",
        "        \"match\": \"Case_Person_A123\",\n",
        "        \"score\": 0.91,\n",
        "        \"metadata\": {\"date\": \"2023-10-15\", \"notes\": \"High confidence forensic match\"}\n",
        "    }]\n",
        "\n",
        "async def search_twitter(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.7)\n",
        "    keywords = description.split()[:5]\n",
        "    return [{\n",
        "        \"source\": \"Twitter\",\n",
        "        \"match\": \"Tweet_Image_456\",\n",
        "        \"score\": 0.87,\n",
        "        \"metadata\": {\"username\": \"@forensic_expert\", \"posted\": \"2023-11-01\", \"keywords\": keywords}\n",
        "    }]\n",
        "\n",
        "async def search_reddit(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.6)\n",
        "    return [{\n",
        "        \"source\": \"Reddit\",\n",
        "        \"match\": \"Reddit_Post_789\",\n",
        "        \"score\": 0.89,\n",
        "        \"metadata\": {\"subreddit\": \"r/forensics\", \"posted\": \"2023-10-25\"}\n",
        "    }]\n",
        "\n",
        "async def search_instagram(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [{\n",
        "        \"source\": \"Instagram\",\n",
        "        \"match\": \"Insta_Post_654\",\n",
        "        \"score\": 0.88,\n",
        "        \"metadata\": {\"username\": \"intel_forensics\", \"posted\": \"2023-11-12\", \"location\": \"New York\"}\n",
        "    }]\n",
        "\n",
        "async def search_osint_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [{\n",
        "        \"source\": \"OSINT\",\n",
        "        \"match\": \"OSINT_Report_101\",\n",
        "        \"score\": 0.83,\n",
        "        \"metadata\": {\"forum\": \"deep_web_forum\", \"date\": \"2023-09-10\"}\n",
        "    }]\n",
        "\n",
        "async def search_google_images(dummy_image: Image.Image, description: str, api_key: str, cse_id: str) -> list:\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [{\n",
        "        \"source\": \"Google Images\",\n",
        "        \"match\": \"News_Site_XYZ\",\n",
        "        \"score\": 0.86,\n",
        "        \"metadata\": {\n",
        "            \"url\": \"https://news.example.com/article123\",\n",
        "            \"title\": \"Breaking news related to forensic evidence\"\n",
        "        }\n",
        "    }]\n",
        "\n",
        "async def search_additional_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.9)\n",
        "    return [{\n",
        "        \"source\": \"Facebook\",\n",
        "        \"match\": \"FB_Post_321\",\n",
        "        \"score\": 0.81,\n",
        "        \"metadata\": {\"user\": \"forensic.page\", \"posted\": \"2023-10-05\"}\n",
        "    }]\n",
        "\n",
        "async def search_telegram(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.7)\n",
        "    return [{\n",
        "        \"source\": \"Telegram\",\n",
        "        \"match\": \"Telegram_Message_987\",\n",
        "        \"score\": 0.85,\n",
        "        \"metadata\": {\"username\": \"@telegram_forensics\", \"date\": \"2023-11-15\"}\n",
        "    }]\n",
        "\n",
        "async def merge_search_results(*results: list) -> list:\n",
        "    merged = []\n",
        "    seen = set()\n",
        "    for result_list in results:\n",
        "        for result in result_list:\n",
        "            rid = (result[\"source\"], result[\"match\"])\n",
        "            if rid not in seen:\n",
        "                seen.add(rid)\n",
        "                merged.append(result)\n",
        "    return sorted(merged, key=lambda x: x.get(\"score\", 0), reverse=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Image Similarity Matching Agent\n",
        "# ------------------------------------------------\n",
        "async def image_similarity_agent(context: dict) -> dict:\n",
        "    # A real implementation would compute cosine similarities between image embeddings.\n",
        "    # Here we simulate similarity matching with known forensic cases.\n",
        "    similarity_scores = {\n",
        "        \"match_with_Case_001\": 0.92,\n",
        "        \"match_with_Case_002\": 0.87\n",
        "    }\n",
        "    context[\"similarity_matches\"] = similarity_scores\n",
        "    return context"
      ],
      "metadata": {
        "id": "zV_NPpqELXiP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# Integration with Anthropic MCP via Groq-enhanced LLM (vLLM)\n",
        "# ------------------------------------------------\n",
        "async def call_anthropic_mcp(prompt: str, client: Any, sampling_params: SamplingParams) -> str:\n",
        "    # In a production environment, you would send the prompt to Anthropic MCP via the Groq client.\n",
        "    # Here we simulate a detailed response.\n",
        "    await asyncio.sleep(1.0)\n",
        "    simulated_response = (\n",
        "        \"Anthropic MCP Response: Analysis indicates strong evidence of tampering with converging data points from multiple public sources. \"\n",
        "        \"Recommend further manual verification and targeted investigative follow-up.\"\n",
        "    )\n",
        "    return simulated_response\n",
        "\n",
        "# ------------------------------------------------\n",
        "# LangGraph Multi-Agent System\n",
        "# ------------------------------------------------\n",
        "async def forensic_search_agent(context: dict) -> dict:\n",
        "    description = context[\"description\"]\n",
        "    embedding = context[\"embedding\"]\n",
        "    search_tasks = await asyncio.gather(\n",
        "        search_private_db(embedding, description),\n",
        "        search_twitter(embedding, description),\n",
        "        search_reddit(embedding, description),\n",
        "        search_instagram(embedding, description),\n",
        "        search_osint_sources(embedding, description),\n",
        "        search_google_images(context[\"image\"], description, os.environ.get(\"GOOGLE_API_KEY\", \"\"), os.environ.get(\"GOOGLE_CSE_ID\", \"\")),\n",
        "        search_additional_sources(embedding, description),\n",
        "        search_telegram(embedding, description)\n",
        "    )\n",
        "    results = await merge_search_results(*search_tasks)\n",
        "    context[\"search_results\"] = results\n",
        "    return context\n",
        "\n",
        "async def forensic_analysis_agent(context: dict) -> dict:\n",
        "    prompt = (\n",
        "        \"You are a forensic analyst expert. Based on the following image description and reverse image search results:\\n\"\n",
        "        f\"Image Description: {context['description']}\\n\"\n",
        "        f\"Search Results: {json.dumps(context.get('search_results', []), indent=2)}\\n\\n\"\n",
        "        \"Provide a detailed forensic analysis that cross-references the data, highlights inter-source similarities, \"\n",
        "        \"and identifies potential evidence of manipulation. Include technical recommendations for further investigation.\"\n",
        "    )\n",
        "    groq_api_key, _, _ = get_api_keys()\n",
        "    llm_client, sampling_params = initialize_llm(groq_api_key)\n",
        "    anthropic_response = await call_anthropic_mcp(prompt, llm_client, sampling_params)\n",
        "    analysis_report = {\n",
        "        \"analysis\": \"Forensic analysis enhanced by Anthropic MCP via Groq.\",\n",
        "        \"anthropic_details\": anthropic_response,\n",
        "        \"recommendations\": \"Review overlapping evidence and verify image integrity with deep metadata analysis.\",\n",
        "        \"prompt_used\": prompt\n",
        "    }\n",
        "    context[\"analysis_report\"] = analysis_report\n",
        "    return context\n",
        "\n",
        "async def threat_assessment_agent(context: dict) -> dict:\n",
        "    prompt = (\n",
        "        \"As a threat assessment specialist, evaluate the following forensic analysis for potential vulnerabilities and risks:\\n\"\n",
        "        f\"{json.dumps(context.get('analysis_report', {}), indent=2)}\\n\\n\"\n",
        "        \"Return a JSON object with the following keys:\\n\"\n",
        "        '- \"threat_level\": integer (0: none, 10: critical)\\n'\n",
        "        '- \"categories\": list of threat categories\\n'\n",
        "        '- \"reasoning\": brief explanation\\n'\n",
        "        '- \"recommended_actions\": list of suggested actions'\n",
        "    )\n",
        "    # Simulated threat assessment - in production, this would invoke an LLM.\n",
        "    threat_report = {\n",
        "        \"threat_level\": 7,\n",
        "        \"categories\": [\"privacy_breach\", \"identity_theft\"],\n",
        "        \"reasoning\": \"Multiple corroborative forensic matches indicate potential misuse of personal data.\",\n",
        "        \"recommended_actions\": [\"Initiate detailed monitoring\", \"Review and harden data security policies\"]\n",
        "    }\n",
        "    context[\"threat_report\"] = threat_report\n",
        "    return context\n",
        "\n",
        "async def run_multiagent_pipeline(image_bytes: bytes) -> dict:\n",
        "    start_time = time.time()\n",
        "    # Step 1: Preprocess the image and extract forensic features.\n",
        "    image = await preprocess_image_async(image_bytes, enhance=True)\n",
        "    exif = extract_exif_data(image)\n",
        "    ocr_text = perform_ocr(image)\n",
        "    manipulation = detect_image_manipulation(image)\n",
        "    img_hash = image_hash(image)\n",
        "    # Step 2: Load models and generate embedding and detailed image description.\n",
        "    models = load_models()\n",
        "    embedding = await generate_embedding(image, models)\n",
        "    description = await generate_image_description(image, models)\n",
        "    # Build initial context for multi-agent processing.\n",
        "    context = {\n",
        "        \"image\": image,\n",
        "        \"exif\": exif,\n",
        "        \"ocr_text\": ocr_text,\n",
        "        \"manipulation\": manipulation,\n",
        "        \"image_hash\": img_hash,\n",
        "        \"embedding\": embedding,\n",
        "        \"description\": description,\n",
        "    }\n",
        "    # Define the multi-agent state graph.\n",
        "    state_graph = StateGraph(\"ForensicAnalysisGraph\")\n",
        "    state_graph.add_state(\"search\", forensic_search_agent)\n",
        "    state_graph.add_state(\"analysis\", forensic_analysis_agent)\n",
        "    state_graph.add_state(\"similarity\", image_similarity_agent)\n",
        "    state_graph.add_state(\"threat\", threat_assessment_agent)\n",
        "    state_graph.set_transition(\"search\", \"analysis\")\n",
        "    state_graph.set_transition(\"analysis\", \"similarity\")\n",
        "    state_graph.set_transition(\"similarity\", \"threat\")\n",
        "    state_graph.set_transition(\"threat\", END)\n",
        "    # Run the multi-agent pipeline\n",
        "    context = await state_graph.run(context)\n",
        "    # Build network graph visualization from search results.\n",
        "    graph_filename = build_network_graph(context.get(\"search_results\", []))\n",
        "    final_report = {\n",
        "        \"exif\": context[\"exif\"],\n",
        "        \"ocr_text\": context[\"ocr_text\"],\n",
        "        \"manipulation\": context[\"manipulation\"],\n",
        "        \"image_hash\": context[\"image_hash\"],\n",
        "        \"description\": context[\"description\"],\n",
        "        \"search_results\": context.get(\"search_results\", []),\n",
        "        \"analysis_report\": context.get(\"analysis_report\", {}),\n",
        "        \"similarity_matches\": context.get(\"similarity_matches\", {}),\n",
        "        \"threat_report\": context.get(\"threat_report\", {}),\n",
        "        \"network_graph_image\": graph_filename,\n",
        "        \"processing_time\": f\"{time.time() - start_time:.2f} seconds\",\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    return final_report\n",
        "\n",
        "def build_network_graph(search_results: list) -> str:\n",
        "    G = nx.Graph()\n",
        "    for r in search_results:\n",
        "        node = f\"{r['source']}: {r['match']}\"\n",
        "        G.add_node(node, score=r.get(\"score\", 0))\n",
        "    nodes = list(G.nodes())\n",
        "    for i in range(len(nodes)):\n",
        "        for j in range(i+1, len(nodes)):\n",
        "            if nodes[i].split(\":\")[0] == nodes[j].split(\":\")[0]:\n",
        "                G.add_edge(nodes[i], nodes[j])\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", node_size=800, font_size=8, edge_color=\"grey\")\n",
        "    plt.title(\"Forensic Search Results Network\")\n",
        "    graph_filename = f\"forensic_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "    plt.savefig(graph_filename)\n",
        "    plt.close()\n",
        "    return graph_filename"
      ],
      "metadata": {
        "id": "crwgTfxTLeyE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. GRADIO UI SETUP\n",
        "#############################################\n",
        "def process_image_ui(image) -> dict:\n",
        "    buffered = BytesIO()\n",
        "    im = Image.fromarray(image.astype('uint8'))\n",
        "    im.save(buffered, format=\"JPEG\")\n",
        "    image_bytes = buffered.getvalue()\n",
        "    result = asyncio.run(run_multiagent_pipeline(image_bytes))\n",
        "    return result\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=process_image_ui,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\"Upload Image\"),\n",
        "    outputs=gr.JSON(label=\"Forensic Analysis Report\"),\n",
        "    title=\"Enhanced Reverse Image Search & Similarity Matching for Defensive Forensics with vLLM & Groq\",\n",
        "    description=(\n",
        "        \"Upload an image to run a multi-agent forensic analysis that leverages reverse image search, image similarity matching, \"\n",
        "        \"and enhanced forensic analysis via Anthropic MCP integrated with Groq and vLLM. The pipeline utilizes a LangGraph-based \"\n",
        "        \"multi-agent workflow with tailored prompts and robust error handling for real-world applications.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "FM9y2SASLyaZ",
        "outputId": "b2b0880e-df58-4eef-8fb1-5aaa737602b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://763a6c3c9aea7bdea9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://763a6c3c9aea7bdea9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}