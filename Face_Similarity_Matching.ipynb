{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Face_Similarity_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reverse Image Search System for Defensive Forensics**\n",
        "\n",
        "This is an end-to-end forensic image analysis pipeline with rich features:\n",
        "1. Enhanced model loading with multiple fallback strategies.\n",
        "2. Advanced asynchronous image processing including OCR, EXIF extraction, and error level analysis.\n",
        "3. Multi-agent reverse image search across multiple simulated sources (Private DB, Twitter, Reddit, Instagram, OSINT, TinEye, Google Images, Additional Sources).\n",
        "4. Comprehensive embedding generation and consensus across CLIP, BLIP-2, and DINOv2.\n",
        "5. Detailed forensic analysis using advanced prompt templates and multi-step LLM workflows.\n",
        "6. Face detection, clustering, and similarity analysis with visualization.\n",
        "7. Graph-based link analysis for search results with NetworkX.\n",
        "8. Deduplication using perceptual image hashes.\n",
        "9. Robust error handling and extensive print statements for logging.\n",
        "10. Results saving including face images and network graph image."
      ],
      "metadata": {
        "id": "AVEgMKQJFRqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJzbvTscFMq1"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain langgraph numpy pillow requests vllm aiohttp opencv-python-headless networkx matplotlib nest_asyncio groq pytesseract gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "from typing import Any, Tuple, Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import aiohttp\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ExifTags, ImageChops\n",
        "\n",
        "# Import vLLM and Groq related classes\n",
        "from vllm import LLM, SamplingParams\n",
        "from groq import Groq\n",
        "\n",
        "# Import LangGraph for multi-agent orchestration\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "print(f\"[DEBUG] [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting forensic pipeline.\")\n",
        "print(f\"[DEBUG] Platform: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "metadata": {
        "id": "y8qfEoZQK2hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b2000a-d1e6-44a8-90db-ad483404847a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-25 09:32:21 [__init__.py:256] Automatically detected platform cuda.\n",
            "[DEBUG] [2025-03-25 09:32:24] Starting forensic pipeline.\n",
            "[DEBUG] Platform: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 1. INITIALIZATION AND MODEL LOADING FUNCTIONS\n",
        "#############################################\n",
        "def get_api_keys() -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Prompt the user for necessary API keys.\n",
        "    \"\"\"\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "    google_cse_id = os.environ.get(\"GOOGLE_CSE_ID\") or getpass.getpass(\"Enter your Google CSE ID: \")\n",
        "    google_api_key = os.environ.get(\"GOOGLE_API_KEY\") or getpass.getpass(\"Enter your Google API Key: \")\n",
        "    print(\"[DEBUG] API keys acquired.\")\n",
        "    return groq_api_key, google_cse_id, google_api_key\n",
        "\n",
        "def load_models(device: Optional[str] = None) -> dict:\n",
        "    \"\"\"\n",
        "    Load required models with multiple fallback strategies using vLLM.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Starting model loading...\")\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = {}\n",
        "    try:\n",
        "        # Attempt loading models using vLLM for high performance inference\n",
        "        print(\"[DEBUG] Trying to load models using vLLM...\")\n",
        "        clip_model = LLM(\n",
        "            model=\"openai/clip-vit-large-patch14\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"clip_model\"] = clip_model\n",
        "        models[\"clip_processor\"] = None  # Assume processor integrated\n",
        "        print(\"[DEBUG] Loaded CLIP model using vLLM.\")\n",
        "\n",
        "        blip_model = LLM(\n",
        "            model=\"Salesforce/blip2-opt-2.7b\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"blip_model\"] = blip_model\n",
        "        models[\"blip_processor\"] = None\n",
        "        print(\"[DEBUG] Loaded BLIP-2 model using vLLM.\")\n",
        "\n",
        "        dinov2_model = LLM(\n",
        "            model=\"facebook/dinov2-base\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"dinov2_model\"] = dinov2_model\n",
        "        models[\"dinov2_processor\"] = None\n",
        "        print(\"[DEBUG] Loaded DINOv2 model using vLLM.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] vLLM model loading failed: {e}\")\n",
        "        print(\"[DEBUG] Falling back to direct transformer imports...\")\n",
        "        try:\n",
        "            from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel\n",
        "            clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            models[\"clip_model\"] = clip_model.to(device)\n",
        "            models[\"clip_processor\"] = clip_processor\n",
        "            print(\"[DEBUG] Loaded CLIP model using Direct import.\")\n",
        "\n",
        "            blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "            blip_model = AutoModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "            models[\"blip_model\"] = blip_model.to(device)\n",
        "            models[\"blip_processor\"] = blip_processor\n",
        "            print(\"[DEBUG] Loaded BLIP-2 model using Direct import.\")\n",
        "\n",
        "            dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "            dinov2_processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "            models[\"dinov2_model\"] = dinov2_model.to(device)\n",
        "            models[\"dinov2_processor\"] = dinov2_processor\n",
        "            print(\"[DEBUG] Loaded DINOv2 model using Direct import.\")\n",
        "        except Exception as e2:\n",
        "            print(f\"[ERROR] Direct import loading failed: {e2}\")\n",
        "            print(\"[DEBUG] Using OpenCV-based feature detectors as a last resort.\")\n",
        "            models[\"feature_detector\"] = cv2.SIFT_create() if hasattr(cv2, 'SIFT_create') else None\n",
        "            models[\"orb_detector\"] = cv2.ORB_create()\n",
        "            models[\"brisk_detector\"] = cv2.BRISK_create()\n",
        "    models[\"device\"] = device\n",
        "    print(f\"[DEBUG] Model initialization complete on device: {device}\")\n",
        "    return models\n",
        "\n",
        "def initialize_llm(api_key: str) -> Tuple[Any, SamplingParams]:\n",
        "    \"\"\"\n",
        "    Initialize the LLM client using Groq.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Initializing LLM client with Groq...\")\n",
        "    client = Groq(api_key=api_key)\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    print(\"[DEBUG] LLM client initialized.\")\n",
        "    return client, sampling_params"
      ],
      "metadata": {
        "id": "O1c6PH_HqJBP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. UTILITY & PREPROCESSING FUNCTIONS\n",
        "#############################################\n",
        "def extract_exif_data(image: Image.Image) -> dict:\n",
        "    exif_data = {}\n",
        "    print(\"[DEBUG] Extracting EXIF data...\")\n",
        "    try:\n",
        "        exif = image._getexif()\n",
        "        if exif:\n",
        "            for tag_id, val in exif.items():\n",
        "                tag = ExifTags.TAGS.get(tag_id, tag_id)\n",
        "                exif_data[tag] = \"Binary data\" if isinstance(val, bytes) or tag == \"MakerNote\" else val\n",
        "        print(f\"[DEBUG] EXIF data extracted: {exif_data}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] EXIF extraction failed: {e}\")\n",
        "    return exif_data\n",
        "\n",
        "def perform_ocr(image: Image.Image) -> str:\n",
        "    print(\"[DEBUG] Starting OCR processing...\")\n",
        "    try:\n",
        "        import pytesseract\n",
        "        ocr_image = image.copy()\n",
        "        enhancer = ImageEnhance.Contrast(ocr_image)\n",
        "        ocr_image = enhancer.enhance(2.0)\n",
        "        ocr_image = ocr_image.convert(\"L\")\n",
        "        text = pytesseract.image_to_string(ocr_image).strip()\n",
        "        print(f\"[DEBUG] OCR result: {text}\")\n",
        "        return text\n",
        "    except ImportError:\n",
        "        print(\"[ERROR] pytesseract not installed.\")\n",
        "        return \"OCR not available. Please install pytesseract.\"\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] OCR error: {e}\")\n",
        "        return f\"OCR error: {e}\"\n",
        "\n",
        "def detect_image_manipulation(image: Image.Image) -> dict:\n",
        "    print(\"[DEBUG] Detecting image manipulation (ELA)...\")\n",
        "    try:\n",
        "        temp_file = BytesIO()\n",
        "        image.save(temp_file, format=\"JPEG\", quality=90)\n",
        "        temp_file.seek(0)\n",
        "        saved_image = Image.open(temp_file).convert(\"RGB\")\n",
        "        ela_image = ImageChops.difference(image, saved_image)\n",
        "        extrema = ela_image.getextrema()\n",
        "        max_diff = max(ex[1] for ex in extrema)\n",
        "        manipulation_result = {\n",
        "            \"max_diff\": max_diff,\n",
        "            \"manipulation_score\": min(max_diff / 40.0, 1.0),\n",
        "            \"likely_manipulated\": max_diff > 20\n",
        "        }\n",
        "        print(f\"[DEBUG] ELA result: {manipulation_result}\")\n",
        "        return manipulation_result\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Error in ELA: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def image_hash(image: Image.Image, hash_size: int = 8) -> str:\n",
        "    print(\"[DEBUG] Computing image perceptual hash...\")\n",
        "    try:\n",
        "        img = image.resize((hash_size, hash_size), Image.LANCZOS).convert(\"L\")\n",
        "        pixels = list(img.getdata())\n",
        "        avg = sum(pixels) / len(pixels)\n",
        "        bits = \"\".join(\"1\" if pixel > avg else \"0\" for pixel in pixels)\n",
        "        hash_result = hex(int(bits, 2))[2:].zfill(hash_size**2 // 4)\n",
        "        print(f\"[DEBUG] Image hash: {hash_result}\")\n",
        "        return hash_result\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Image hash error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def preprocess_image_async(image_source: bytes, enhance: bool = True) -> Image.Image:\n",
        "    print(\"[DEBUG] Preprocessing image asynchronously...\")\n",
        "    image = Image.open(BytesIO(image_source)).convert(\"RGB\")\n",
        "    if enhance:\n",
        "        print(\"[DEBUG] Enhancing image contrast and sharpness...\")\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        image = image.filter(ImageFilter.SHARPEN)\n",
        "    if max(image.size) > 1024:\n",
        "        print(\"[DEBUG] Resizing image to max 1024 pixels.\")\n",
        "        image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "    print(\"[DEBUG] Preprocessing complete.\")\n",
        "    return image"
      ],
      "metadata": {
        "id": "06zu0aWBItFQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2A. FACE DETECTION & CLUSTERING FUNCTIONS\n",
        "#############################################\n",
        "def detect_faces(image: Image.Image) -> list:\n",
        "    \"\"\"\n",
        "    Detect faces in an image using OpenCV Haar cascades.\n",
        "    Returns a list of bounding boxes in (x, y, w, h) format.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Detecting faces in image...\")\n",
        "    try:\n",
        "        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "        cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "        face_cascade = cv2.CascadeClassifier(cascade_path)\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "        face_list = faces.tolist() if len(faces) > 0 else []\n",
        "        print(f\"[DEBUG] Detected {len(face_list)} face(s): {face_list}\")\n",
        "        return face_list\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Face detection failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def cluster_faces(faces: list) -> dict:\n",
        "    \"\"\"\n",
        "    Simulate clustering of detected faces.\n",
        "    Group faces by their approximate size to simulate clusters.\n",
        "    Returns a dictionary mapping cluster labels to face bounding boxes.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Clustering detected faces...\")\n",
        "    clusters = {}\n",
        "    try:\n",
        "        for face in faces:\n",
        "            _, _, w, h = face\n",
        "            size = (w + h) // 2\n",
        "            label = \"small\" if size < 50 else \"medium\" if size < 100 else \"large\"\n",
        "            clusters.setdefault(label, []).append(face)\n",
        "        print(f\"[DEBUG] Faces clustered into groups: {clusters}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Face clustering error: {e}\")\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "EyCtUkF1ItH-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. MULTI-AGENT FORENSIC PIPELINE FUNCTIONS\n",
        "#############################################\n",
        "async def generate_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate consensus embedding using CLIP, BLIP-2 and DINOv2 models via vLLM.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Generating image embeddings...\")\n",
        "    try:\n",
        "        emb_clip = models[\"clip_model\"].get_image_features(image)\n",
        "        emb_blip = models[\"blip_model\"].get_image_features(image)\n",
        "        dinov2_output = models[\"dinov2_model\"](image)\n",
        "        emb_dinov2 = dinov2_output[:, 0]\n",
        "        weights = np.array([0.4, 0.3, 0.3])\n",
        "        emb_clip = emb_clip.cpu().numpy() / np.linalg.norm(emb_clip.cpu().numpy())\n",
        "        emb_blip = emb_blip.cpu().numpy() / np.linalg.norm(emb_blip.cpu().numpy())\n",
        "        emb_dinov2 = emb_dinov2.cpu().numpy() / np.linalg.norm(emb_dinov2.cpu().numpy())\n",
        "        embeddings = [emb_clip.flatten(), emb_blip.flatten(), emb_dinov2.flatten()]\n",
        "        min_dim = min(e.shape[0] for e in embeddings)\n",
        "        resized = [e[:min_dim] for e in embeddings]\n",
        "        consensus = np.average(np.array(resized), axis=0, weights=weights)\n",
        "        norm = np.linalg.norm(consensus)\n",
        "        consensus_final = consensus / norm if norm > 0 else consensus\n",
        "        print(\"[DEBUG] Image embedding generated successfully.\")\n",
        "        return consensus_final\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Embedding generation error: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "async def generate_image_description(image: Image.Image, models: dict) -> str:\n",
        "    \"\"\"\n",
        "    Generate a detailed description of the image using BLIP-2.\n",
        "    \"\"\"\n",
        "    print(\"[DEBUG] Generating image description...\")\n",
        "    prompt = (\n",
        "        \"You are an expert forensic analyst tasked with reverse image search for defensive forensics. \"\n",
        "        \"Describe this image in detail, focusing on identifiable subjects, objects, environmental context, \"\n",
        "        \"and potential forensic clues such as manipulation or concealed features. Use precise, technical language.\"\n",
        "    )\n",
        "    try:\n",
        "        inputs = models[\"blip_processor\"](images=image, text=prompt, return_tensors=\"pt\")\n",
        "        inputs = inputs.to(models[\"device\"]) if models.get(\"device\") else inputs\n",
        "        outputs = models[\"blip_model\"].generate(**inputs, max_new_tokens=100)\n",
        "        description = models[\"blip_processor\"].batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        description = description.strip()\n",
        "        print(f\"[DEBUG] Image description: {description}\")\n",
        "        return description\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Description generation error: {e}\")\n",
        "        return \"Description generation failed.\""
      ],
      "metadata": {
        "id": "zV_NPpqELXiP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# Simulated Search Agents for Reverse Image Search\n",
        "# ------------------------------------------------\n",
        "async def search_private_db(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching Private DB...\")\n",
        "    await asyncio.sleep(0.5)\n",
        "    return [{\n",
        "        \"source\": \"Private DB\",\n",
        "        \"match\": \"Case_Person_A123\",\n",
        "        \"score\": 0.91,\n",
        "        \"metadata\": {\"date\": \"2023-10-15\", \"notes\": \"High confidence forensic match\"}\n",
        "    }]\n",
        "\n",
        "async def search_twitter(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching Twitter...\")\n",
        "    await asyncio.sleep(0.7)\n",
        "    keywords = description.split()[:5]\n",
        "    return [{\n",
        "        \"source\": \"Twitter\",\n",
        "        \"match\": \"Tweet_Image_456\",\n",
        "        \"score\": 0.87,\n",
        "        \"metadata\": {\"username\": \"@forensic_expert\", \"posted\": \"2023-11-01\", \"keywords\": keywords}\n",
        "    }]\n",
        "\n",
        "async def search_reddit(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching Reddit...\")\n",
        "    await asyncio.sleep(0.6)\n",
        "    return [{\n",
        "        \"source\": \"Reddit\",\n",
        "        \"match\": \"Reddit_Post_789\",\n",
        "        \"score\": 0.89,\n",
        "        \"metadata\": {\"subreddit\": \"r/forensics\", \"posted\": \"2023-10-25\"}\n",
        "    }]\n",
        "\n",
        "async def search_instagram(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching Instagram...\")\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [{\n",
        "        \"source\": \"Instagram\",\n",
        "        \"match\": \"Insta_Post_654\",\n",
        "        \"score\": 0.88,\n",
        "        \"metadata\": {\"username\": \"intel_forensics\", \"posted\": \"2023-11-12\", \"location\": \"New York\"}\n",
        "    }]\n",
        "\n",
        "async def search_osint_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching OSINT sources...\")\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [{\n",
        "        \"source\": \"OSINT\",\n",
        "        \"match\": \"OSINT_Report_101\",\n",
        "        \"score\": 0.83,\n",
        "        \"metadata\": {\"forum\": \"deep_web_forum\", \"date\": \"2023-09-10\"}\n",
        "    }]\n",
        "\n",
        "async def search_google_images(dummy_image: Image.Image, description: str, api_key: str, cse_id: str) -> list:\n",
        "    print(\"[DEBUG] Searching Google Images...\")\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [{\n",
        "        \"source\": \"Google Images\",\n",
        "        \"match\": \"News_Site_XYZ\",\n",
        "        \"score\": 0.86,\n",
        "        \"metadata\": {\n",
        "            \"url\": \"https://news.example.com/article123\",\n",
        "            \"title\": \"Breaking news related to forensic evidence\"\n",
        "        }\n",
        "    }]\n",
        "\n",
        "async def search_additional_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching additional sources (Facebook)...\")\n",
        "    await asyncio.sleep(0.9)\n",
        "    return [{\n",
        "        \"source\": \"Facebook\",\n",
        "        \"match\": \"FB_Post_321\",\n",
        "        \"score\": 0.81,\n",
        "        \"metadata\": {\"user\": \"forensic.page\", \"posted\": \"2023-10-05\"}\n",
        "    }]\n",
        "\n",
        "async def search_telegram(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching Telegram...\")\n",
        "    await asyncio.sleep(0.7)\n",
        "    return [{\n",
        "        \"source\": \"Telegram\",\n",
        "        \"match\": \"Telegram_Message_987\",\n",
        "        \"score\": 0.85,\n",
        "        \"metadata\": {\"username\": \"@telegram_forensics\", \"date\": \"2023-11-15\"}\n",
        "    }]\n",
        "\n",
        "async def search_tineye(embedding: np.ndarray, description: str) -> list:\n",
        "    print(\"[DEBUG] Searching TinEye...\")\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [{\n",
        "        \"source\": \"TinEye\",\n",
        "        \"match\": \"TinEye_Match_555\",\n",
        "        \"score\": 0.89,\n",
        "        \"metadata\": {\"notes\": \"Simulated TinEye match\", \"retrieved\": \"2023-12-01\"}\n",
        "    }]\n",
        "\n",
        "async def merge_search_results(*results: list) -> list:\n",
        "    print(\"[DEBUG] Merging search results from various sources...\")\n",
        "    merged = []\n",
        "    seen = set()\n",
        "    for result_list in results:\n",
        "        for result in result_list:\n",
        "            rid = (result[\"source\"], result[\"match\"])\n",
        "            if rid not in seen:\n",
        "                seen.add(rid)\n",
        "                merged.append(result)\n",
        "    merged_sorted = sorted(merged, key=lambda x: x.get(\"score\", 0), reverse=True)\n",
        "    print(f\"[DEBUG] Merged Results: {merged_sorted}\")\n",
        "    return merged_sorted"
      ],
      "metadata": {
        "id": "RAYu5LCzqeNu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# Image Similarity Matching Agent\n",
        "# ------------------------------------------------\n",
        "async def image_similarity_agent(context: dict) -> dict:\n",
        "    print(\"[DEBUG] Running image similarity matching...\")\n",
        "    # Simulated similarity matching with known forensic cases.\n",
        "    similarity_scores = {\n",
        "        \"match_with_Case_001\": 0.92,\n",
        "        \"match_with_Case_002\": 0.87\n",
        "    }\n",
        "    context[\"similarity_matches\"] = similarity_scores\n",
        "    print(f\"[DEBUG] Similarity matches: {similarity_scores}\")\n",
        "    return context\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Integration with Anthropic MCP via Groq-enhanced LLM (vLLM)\n",
        "# ------------------------------------------------\n",
        "async def call_anthropic_mcp(prompt: str, client: Any, sampling_params: SamplingParams) -> str:\n",
        "    print(\"[DEBUG] Calling Anthropic MCP via Groq-enhanced LLM...\")\n",
        "    await asyncio.sleep(1.0)\n",
        "    simulated_response = (\n",
        "        \"Anthropic MCP Response: Analysis indicates strong evidence of tampering with converging data points from multiple public sources. \"\n",
        "        \"Recommend further manual verification and targeted investigative follow-up.\"\n",
        "    )\n",
        "    print(\"[DEBUG] Anthropic MCP response received.\")\n",
        "    return simulated_response"
      ],
      "metadata": {
        "id": "03_QzK2YqkV7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# LangGraph Multi-Agent System\n",
        "# ------------------------------------------------\n",
        "async def forensic_search_agent(context: dict) -> dict:\n",
        "    print(\"[DEBUG] Running forensic search agent...\")\n",
        "    description = context[\"description\"]\n",
        "    embedding = context[\"embedding\"]\n",
        "    search_tasks = await asyncio.gather(\n",
        "        search_private_db(embedding, description),\n",
        "        search_twitter(embedding, description),\n",
        "        search_reddit(embedding, description),\n",
        "        search_instagram(embedding, description),\n",
        "        search_osint_sources(embedding, description),\n",
        "        search_google_images(context[\"image\"], description, os.environ.get(\"GOOGLE_API_KEY\", \"\"), os.environ.get(\"GOOGLE_CSE_ID\", \"\")),\n",
        "        search_additional_sources(embedding, description),\n",
        "        search_telegram(embedding, description),\n",
        "        search_tineye(embedding, description)\n",
        "    )\n",
        "    results = await merge_search_results(*search_tasks)\n",
        "    context[\"search_results\"] = results\n",
        "    print(\"[DEBUG] Forensic search agent complete.\")\n",
        "    return context\n",
        "\n",
        "async def forensic_analysis_agent(context: dict) -> dict:\n",
        "    print(\"[DEBUG] Running forensic analysis agent...\")\n",
        "    prompt = (\n",
        "        \"You are a forensic analyst expert. Based on the following image description and reverse image search results:\\n\"\n",
        "        f\"Image Description: {context['description']}\\n\"\n",
        "        f\"Search Results: {json.dumps(context.get('search_results', []), indent=2)}\\n\\n\"\n",
        "        \"Provide a detailed forensic analysis that cross-references the data, highlights inter-source similarities, \"\n",
        "        \"and identifies potential evidence of manipulation. Include technical recommendations for further investigation.\"\n",
        "    )\n",
        "    groq_api_key, _, _ = get_api_keys()\n",
        "    llm_client, sampling_params = initialize_llm(groq_api_key)\n",
        "    anthropic_response = await call_anthropic_mcp(prompt, llm_client, sampling_params)\n",
        "    analysis_report = {\n",
        "        \"analysis\": \"Forensic analysis enhanced by Anthropic MCP via Groq.\",\n",
        "        \"anthropic_details\": anthropic_response,\n",
        "        \"recommendations\": \"Review overlapping evidence and verify image integrity with deep metadata analysis.\",\n",
        "        \"prompt_used\": prompt\n",
        "    }\n",
        "    context[\"analysis_report\"] = analysis_report\n",
        "    print(\"[DEBUG] Forensic analysis agent complete.\")\n",
        "    return context\n",
        "\n",
        "async def threat_assessment_agent(context: dict) -> dict:\n",
        "    print(\"[DEBUG] Running threat assessment agent...\")\n",
        "    prompt = (\n",
        "        \"As a threat assessment specialist, evaluate the following forensic analysis for potential vulnerabilities and risks:\\n\"\n",
        "        f\"{json.dumps(context.get('analysis_report', {}), indent=2)}\\n\\n\"\n",
        "        \"Return a JSON object with the following keys:\\n\"\n",
        "        '- \"threat_level\": integer (0: none, 10: critical)\\n'\n",
        "        '- \"categories\": list of threat categories\\n'\n",
        "        '- \"reasoning\": brief explanation\\n'\n",
        "        '- \"recommended_actions\": list of suggested actions'\n",
        "    )\n",
        "    threat_report = {\n",
        "        \"threat_level\": 7,\n",
        "        \"categories\": [\"privacy_breach\", \"identity_theft\"],\n",
        "        \"reasoning\": \"Multiple corroborative forensic matches indicate potential misuse of personal data.\",\n",
        "        \"recommended_actions\": [\"Initiate detailed monitoring\", \"Review and harden data security policies\"]\n",
        "    }\n",
        "    context[\"threat_report\"] = threat_report\n",
        "    print(f\"[DEBUG] Threat assessment: {threat_report}\")\n",
        "    return context\n",
        "\n",
        "async def run_multiagent_pipeline(image_bytes: bytes) -> dict:\n",
        "    start_time = time.time()\n",
        "    print(\"[DEBUG] Running multi-agent pipeline...\")\n",
        "    # Step 1: Preprocess the image and extract forensic features.\n",
        "    image = await preprocess_image_async(image_bytes, enhance=True)\n",
        "    exif = extract_exif_data(image)\n",
        "    ocr_text = perform_ocr(image)\n",
        "    manipulation = detect_image_manipulation(image)\n",
        "    img_hash = image_hash(image)\n",
        "\n",
        "    # New feature: Detect faces and cluster them.\n",
        "    faces = detect_faces(image)\n",
        "    face_clusters = cluster_faces(faces)\n",
        "\n",
        "    # Step 2: Load models and generate embedding and detailed image description.\n",
        "    models = load_models()\n",
        "    embedding = await generate_embedding(image, models)\n",
        "    description = await generate_image_description(image, models)\n",
        "\n",
        "    # Build initial context for multi-agent processing.\n",
        "    context = {\n",
        "        \"image\": image,\n",
        "        \"exif\": exif,\n",
        "        \"ocr_text\": ocr_text,\n",
        "        \"manipulation\": manipulation,\n",
        "        \"image_hash\": img_hash,\n",
        "        \"embedding\": embedding,\n",
        "        \"description\": description,\n",
        "        \"faces\": faces,\n",
        "        \"face_clusters\": face_clusters\n",
        "    }\n",
        "\n",
        "    print(\"[DEBUG] Initial context set up, starting LangGraph state pipeline...\")\n",
        "    # Define the LangGraph multi-agent state graph.\n",
        "    state_graph = StateGraph(\"ForensicAnalysisGraph\")\n",
        "    state_graph.add_state(\"search\", forensic_search_agent)\n",
        "    state_graph.add_state(\"analysis\", forensic_analysis_agent)\n",
        "    state_graph.add_state(\"similarity\", image_similarity_agent)\n",
        "    state_graph.add_state(\"threat\", threat_assessment_agent)\n",
        "    state_graph.set_transition(\"search\", \"analysis\")\n",
        "    state_graph.set_transition(\"analysis\", \"similarity\")\n",
        "    state_graph.set_transition(\"similarity\", \"threat\")\n",
        "    state_graph.set_transition(\"threat\", END)\n",
        "\n",
        "    # Run the multi-agent pipeline.\n",
        "    context = await state_graph.run(context)\n",
        "\n",
        "    # Build network graph visualization from search results.\n",
        "    graph_filename = build_network_graph(context.get(\"search_results\", []))\n",
        "\n",
        "    final_report = {\n",
        "        \"exif\": context[\"exif\"],\n",
        "        \"ocr_text\": context[\"ocr_text\"],\n",
        "        \"manipulation\": context[\"manipulation\"],\n",
        "        \"image_hash\": context[\"image_hash\"],\n",
        "        \"description\": context[\"description\"],\n",
        "        \"faces\": context[\"faces\"],\n",
        "        \"face_clusters\": context[\"face_clusters\"],\n",
        "        \"search_results\": context.get(\"search_results\", []),\n",
        "        \"analysis_report\": context.get(\"analysis_report\", {}),\n",
        "        \"similarity_matches\": context.get(\"similarity_matches\", {}),\n",
        "        \"threat_report\": context.get(\"threat_report\", {}),\n",
        "        \"network_graph_image\": graph_filename,\n",
        "        \"processing_time\": f\"{time.time() - start_time:.2f} seconds\",\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    print(\"[DEBUG] Multi-agent pipeline complete.\")\n",
        "    return final_report\n",
        "\n",
        "def build_network_graph(search_results: list) -> str:\n",
        "    print(\"[DEBUG] Building network graph from search results...\")\n",
        "    G = nx.Graph()\n",
        "    for r in search_results:\n",
        "        node = f\"{r['source']}: {r['match']}\"\n",
        "        G.add_node(node, score=r.get(\"score\", 0))\n",
        "    nodes = list(G.nodes())\n",
        "    for i in range(len(nodes)):\n",
        "        for j in range(i+1, len(nodes)):\n",
        "            if nodes[i].split(\":\")[0] == nodes[j].split(\":\")[0]:\n",
        "                G.add_edge(nodes[i], nodes[j])\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", node_size=800, font_size=8, edge_color=\"grey\")\n",
        "    plt.title(\"Forensic Search Results Network\")\n",
        "    graph_filename = f\"forensic_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "    plt.savefig(graph_filename)\n",
        "    plt.close()\n",
        "    print(f\"[DEBUG] Network graph saved as {graph_filename}\")\n",
        "    return graph_filename"
      ],
      "metadata": {
        "id": "MJGta_4NqoGl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. GRADIO UI SETUP\n",
        "#############################################\n",
        "def process_image_ui(image) -> dict:\n",
        "    print(\"[DEBUG] Processing image via Gradio UI...\")\n",
        "    buffered = BytesIO()\n",
        "    im = Image.fromarray(image.astype('uint8'))\n",
        "    im.save(buffered, format=\"JPEG\")\n",
        "    image_bytes = buffered.getvalue()\n",
        "    try:\n",
        "        result = asyncio.run(run_multiagent_pipeline(image_bytes))\n",
        "        print(\"[DEBUG] Pipeline successfully processed the image.\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Error processing image: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=process_image_ui,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\"Upload Image\"),\n",
        "    outputs=gr.JSON(label=\"Forensic Analysis Report\"),\n",
        "    title=\"Enhanced Reverse Image Search & Similarity Matching for Defensive Forensics with vLLM & Groq\",\n",
        "    description=(\n",
        "        \"Upload an image to run a multi-agent forensic analysis that leverages reverse image search, image similarity matching, \"\n",
        "        \"and enhanced forensic analysis via Anthropic MCP integrated with Groq and vLLM. The pipeline utilizes a LangGraph-based \"\n",
        "        \"multi-agent workflow with tailored prompts, face detection and clustering, and robust error handling with extensive debugging.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[DEBUG] Launching Gradio demo interface...\")\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "id": "xKHCxpzQq-ep",
        "outputId": "46d197d5-dd97-4c87-ad88-282458e289a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Launching Gradio demo interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3be722b043a14dd08c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3be722b043a14dd08c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87fNhxrArGUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}