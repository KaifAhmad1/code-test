{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22f591d67c7d4e6fb1a3dc2f0f5b077b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f28c0dc9af4243baa428e34d7b5116ec",
              "IPY_MODEL_204afc138b664aca858429cbcd6bd8d6",
              "IPY_MODEL_d6e717d71118412b92d12686c4a34e1b"
            ],
            "layout": "IPY_MODEL_784794330dd74854aa04a79ef274443d"
          }
        },
        "f28c0dc9af4243baa428e34d7b5116ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d260150d81f549b19d73135840103ba6",
            "placeholder": "​",
            "style": "IPY_MODEL_cee79b6312214a6db49be2052889cfd9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "204afc138b664aca858429cbcd6bd8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ab7442d1c14fba85b27660fe179b0f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_440b8b67838c478ea9d63f9fa831431e",
            "value": 2
          }
        },
        "d6e717d71118412b92d12686c4a34e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_140eab6768cf475cb50b4f251d980bce",
            "placeholder": "​",
            "style": "IPY_MODEL_3ac717da737b4c10ae8b1588c7abeb22",
            "value": " 2/2 [01:25&lt;00:00, 40.48s/it]"
          }
        },
        "784794330dd74854aa04a79ef274443d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d260150d81f549b19d73135840103ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee79b6312214a6db49be2052889cfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ab7442d1c14fba85b27660fe179b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440b8b67838c478ea9d63f9fa831431e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "140eab6768cf475cb50b4f251d980bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ac717da737b4c10ae8b1588c7abeb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Face_Similarity_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reverse Image Search System for Defensive Forensics**"
      ],
      "metadata": {
        "id": "AVEgMKQJFRqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OJzbvTscFMq1"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain langgraph numpy pillow requests vllm aiohttp opencv-python-headless networkx matplotlib nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import nest_asyncio\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import aiohttp\n",
        "import torch\n",
        "\n",
        "# Transformers for image embeddings and description generation\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel\n",
        "\n",
        "# LLM orchestration with Groq and LangChain\n",
        "from groq import Groq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Multi-agent workflow using LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Patch asyncio for environments with an existing event loop\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "y8qfEoZQK2hA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 1. API KEYS & MODEL INITIALIZATION\n",
        "#############################################\n",
        "\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "GOOGLE_CSE_ID = getpass.getpass(\"Enter your Google CSE ID: \")\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")"
      ],
      "metadata": {
        "id": "O1c6PH_HqJBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d79b8a8-1e68-4b4d-894d-8506b89f99e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n",
            "Enter your Google CSE ID: ··········\n",
            "Enter your Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading models...\")\n",
        "\n",
        "# Initialize CLIP model for image embeddings\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Initialize BLIP-2 model for image semantic understanding and description generation\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "blip_model = AutoModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "\n",
        "# Initialize DINOv2 model for robust feature extraction\n",
        "dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "dinov2_processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = clip_model.to(device)\n",
        "blip_model = blip_model.to(device)\n",
        "dinov2_model = dinov2_model.to(device)\n",
        "\n",
        "print(f\"Models loaded on {device}\")\n",
        "\n",
        "# Initialize Groq client and LLM for forensic analysis\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "llama_llm = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Initialize the forensic assistant.\"}],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "F0LqhIJm0774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "22f591d67c7d4e6fb1a3dc2f0f5b077b",
            "f28c0dc9af4243baa428e34d7b5116ec",
            "204afc138b664aca858429cbcd6bd8d6",
            "d6e717d71118412b92d12686c4a34e1b",
            "784794330dd74854aa04a79ef274443d",
            "d260150d81f549b19d73135840103ba6",
            "cee79b6312214a6db49be2052889cfd9",
            "33ab7442d1c14fba85b27660fe179b0f",
            "440b8b67838c478ea9d63f9fa831431e",
            "140eab6768cf475cb50b4f251d980bce",
            "3ac717da737b4c10ae8b1588c7abeb22"
          ]
        },
        "outputId": "64d083d2-6222-477d-fbe2-3d7033653db4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22f591d67c7d4e6fb1a3dc2f0f5b077b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. IMAGE PROCESSING FUNCTIONS\n",
        "#############################################\n",
        "\n",
        "async def fetch_image_async(image_url: str) -> bytes:\n",
        "    \"\"\"Asynchronously fetch an image from a URL.\"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(image_url) as response:\n",
        "            if response.status != 200:\n",
        "                raise ValueError(f\"Failed to fetch image. HTTP status code: {response.status}\")\n",
        "            return await response.read()\n",
        "\n",
        "async def preprocess_image_async(image_url_or_bytes: str or bytes) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Download and preprocess the image asynchronously.\n",
        "    Enhances contrast, sharpens, and returns a cleaned-up RGB PIL image.\n",
        "    \"\"\"\n",
        "    if isinstance(image_url_or_bytes, str):\n",
        "        image_bytes = await fetch_image_async(image_url_or_bytes)\n",
        "    else:\n",
        "        image_bytes = image_url_or_bytes\n",
        "\n",
        "    image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    # Enhance contrast and sharpen the image\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    image = enhancer.enhance(1.5)\n",
        "    image = image.filter(ImageFilter.SHARPEN)\n",
        "    # Resize image if any dimension exceeds 1024 pixels\n",
        "    if max(image.size) > 1024:\n",
        "        image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "    return image\n",
        "\n",
        "def detect_faces(image: Image.Image) -> list:\n",
        "    \"\"\"\n",
        "    Detect faces in the image using Haar Cascade from OpenCV.\n",
        "    Returns a list of cropped face images as PIL Image objects.\n",
        "    \"\"\"\n",
        "    img_cv = np.array(image)\n",
        "    img_cv = img_cv[:, :, ::-1].copy()  # Convert from RGB to BGR format\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    face_images = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_img = image.crop((x, y, x+w, y+h))\n",
        "        face_img = face_img.resize((224, 224), Image.LANCZOS)\n",
        "        face_images.append(face_img)\n",
        "    return face_images"
      ],
      "metadata": {
        "id": "qsEDe3vQeRrq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. EMBEDDING & DESCRIPTION FUNCTIONS\n",
        "#############################################\n",
        "\n",
        "async def generate_clip_embedding(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the CLIP model.\"\"\"\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_blip_embedding(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the BLIP-2 model for semantic representation.\"\"\"\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = blip_model.get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_dinov2_embedding(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the DINOv2 model (CLS token).\"\"\"\n",
        "    inputs = dinov2_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = dinov2_model(**inputs).last_hidden_state[:, 0]\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_image_description(image: Image.Image) -> str:\n",
        "    \"\"\"\n",
        "    Generate a detailed description of the image using BLIP-2.\n",
        "    The description focuses on identifiable people, objects, and locations.\n",
        "    \"\"\"\n",
        "    prompt = \"Describe this image in detail with focus on identifiable people, objects, and locations:\"\n",
        "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = blip_model.generate(**inputs, max_new_tokens=100)\n",
        "    description = blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return description.strip()\n",
        "\n",
        "async def extract_classical_features(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract classical image features using ORB and optionally SIFT.\n",
        "    Returns the set of descriptors chosen based on the number of keypoints.\n",
        "    \"\"\"\n",
        "    image_np = np.array(image)\n",
        "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "    orb = cv2.ORB_create(nfeatures=1000)\n",
        "    keypoints_orb, descriptors_orb = orb.detectAndCompute(gray, None)\n",
        "    try:\n",
        "        sift = cv2.SIFT_create()\n",
        "        keypoints_sift, descriptors_sift = sift.detectAndCompute(gray, None)\n",
        "    except Exception:\n",
        "        descriptors_sift = np.array([])\n",
        "    if descriptors_orb is None and len(descriptors_sift) == 0:\n",
        "        return np.array([])\n",
        "    elif descriptors_orb is None:\n",
        "        return descriptors_sift\n",
        "    elif len(descriptors_sift) == 0:\n",
        "        return descriptors_orb\n",
        "    else:\n",
        "        return descriptors_sift if len(keypoints_sift) > len(keypoints_orb) else descriptors_orb"
      ],
      "metadata": {
        "id": "l1LKcYWci3HU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. IMAGE SIMILARITY FUNCTION\n",
        "#############################################\n",
        "\n",
        "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two image embeddings.\n",
        "    Returns a float between -1 and 1 (1 indicates identical embeddings).\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(embedding1.flatten(), embedding2.flatten())\n",
        "    norm1 = np.linalg.norm(embedding1)\n",
        "    norm2 = np.linalg.norm(embedding2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(dot_product / (norm1 * norm2))"
      ],
      "metadata": {
        "id": "0_ohptUyjCD8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 5. SEARCH ENGINE FUNCTIONS (SIMULATED)\n",
        "#############################################\n",
        "\n",
        "async def search_private_db(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Simulate a search in a private database using image embeddings.\n",
        "    Returns a list of matching entries with metadata and an embedding placeholder.\n",
        "    \"\"\"\n",
        "    await asyncio.sleep(0.5)\n",
        "    return [\n",
        "        {\"source\": \"Private DB\", \"match\": \"Person_123\", \"score\": 0.91,\n",
        "         \"metadata\": {\"date\": \"2023-10-15\"}, \"embedding\": embedding},\n",
        "        {\"source\": \"Private DB\", \"match\": \"Person_456\", \"score\": 0.85,\n",
        "         \"metadata\": {\"date\": \"2023-09-22\"}, \"embedding\": embedding}\n",
        "    ]\n",
        "\n",
        "async def search_twitter(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"\n",
        "    Simulate a Twitter search using keywords from the image description.\n",
        "    Returns matching tweet details with an embedding placeholder.\n",
        "    \"\"\"\n",
        "    await asyncio.sleep(0.7)\n",
        "    keywords = description.split()[:5]\n",
        "    return [\n",
        "        {\"source\": \"Twitter\", \"match\": \"Tweet_Image_456\", \"score\": 0.87,\n",
        "         \"metadata\": {\"username\": \"@user123\", \"posted\": \"2023-11-01\", \"keywords\": keywords},\n",
        "         \"embedding\": embedding}\n",
        "    ]\n",
        "\n",
        "async def search_reddit(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"Simulate a Reddit search and return matching posts with an embedding placeholder.\"\"\"\n",
        "    await asyncio.sleep(0.6)\n",
        "    return [\n",
        "        {\"source\": \"Reddit\", \"match\": \"Reddit_Post_321\", \"score\": 0.89,\n",
        "         \"metadata\": {\"subreddit\": \"r/pics\", \"posted\": \"2023-10-25\"},\n",
        "         \"embedding\": embedding}\n",
        "    ]\n",
        "\n",
        "async def search_instagram(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"Simulate an Instagram search and return matching posts with an embedding placeholder.\"\"\"\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [\n",
        "        {\"source\": \"Instagram\", \"match\": \"Insta_Post_654\", \"score\": 0.88,\n",
        "         \"metadata\": {\"username\": \"user456\", \"posted\": \"2023-11-12\", \"location\": \"New York\"},\n",
        "         \"embedding\": embedding}\n",
        "    ]\n",
        "\n",
        "async def search_osint_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    \"\"\"Simulate an OSINT search across multiple sources, returning matching posts with embedding placeholders.\"\"\"\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [\n",
        "        {\"source\": \"OSINT\", \"match\": \"DarkWeb_Post_999\", \"score\": 0.83,\n",
        "         \"metadata\": {\"forum\": \"anonymous_forum\", \"date\": \"2023-09-10\"},\n",
        "         \"embedding\": embedding},\n",
        "        {\"source\": \"OSINT\", \"match\": \"Telegram_Group_123\", \"score\": 0.79,\n",
        "         \"metadata\": {\"group\": \"public_channel_xyz\", \"date\": \"2023-10-30\"},\n",
        "         \"embedding\": embedding}\n",
        "    ]\n",
        "\n",
        "async def search_tineye(image: Image.Image) -> list:\n",
        "    \"\"\"Simulate a TinEye reverse image search and return results with a None embedding.\"\"\"\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format='JPEG')\n",
        "    img_byte_arr.seek(0)\n",
        "    await asyncio.sleep(1.2)\n",
        "    return [\n",
        "        {\"source\": \"TinEye\", \"match\": \"Website_ABC\", \"score\": 0.92,\n",
        "         \"metadata\": {\"domain\": \"example.com\", \"first_crawled\": \"2023-08-15\"},\n",
        "         \"embedding\": None}\n",
        "    ]\n",
        "\n",
        "async def search_google_images(image: Image.Image, description: str) -> list:\n",
        "    \"\"\"Simulate a Google Images search and return matching results with a None embedding.\"\"\"\n",
        "    search_terms = \" \".join(description.split()[:7])\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [\n",
        "        {\"source\": \"Google Images\", \"match\": \"News_Site_XYZ\", \"score\": 0.86,\n",
        "         \"metadata\": {\"url\": \"https://example-news.com/article123\", \"title\": \"Example article related to the image\"},\n",
        "         \"embedding\": None}\n",
        "    ]\n",
        "\n",
        "async def merge_search_results(*results: list) -> list:\n",
        "    \"\"\"Merge search results from all sources and sort them by their score (descending).\"\"\"\n",
        "    merged = []\n",
        "    for result_list in results:\n",
        "        merged.extend(result_list)\n",
        "    return sorted(merged, key=lambda x: x.get('score', 0), reverse=True)"
      ],
      "metadata": {
        "id": "Dp1fjU8TjS7d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 6. ANALYSIS & THREAT ASSESSMENT FUNCTIONS\n",
        "#############################################\n",
        "\n",
        "async def analyze_results_with_llm(results: list, image_description: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze the reverse image search results and computed similarity scores using an LLM.\n",
        "    Generates a structured forensic assessment report.\n",
        "    \"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "You are a forensic analyst. Analyze the following reverse image search results.\n",
        "\n",
        "IMAGE DESCRIPTION:\n",
        "{image_description}\n",
        "\n",
        "SEARCH RESULTS (in JSON):\n",
        "{search_results}\n",
        "\n",
        "Provide a structured forensic assessment that includes:\n",
        "1. Cross-referencing of entities.\n",
        "2. Temporal and geographic correlations.\n",
        "3. Evaluation of source reliability.\n",
        "4. Insights on image similarity between the current image and search results.\n",
        "5. Overall forensic insights about the image origin.\n",
        "\n",
        "Format your response as a clear multi-section report.\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"image_description\", \"search_results\"],\n",
        "        template=prompt_template\n",
        "    )\n",
        "    chain = LLMChain(llm=llama_llm, prompt=prompt)\n",
        "    response = await chain.arun(\n",
        "        image_description=image_description,\n",
        "        search_results=json.dumps(results, indent=2)\n",
        "    )\n",
        "    return response.strip()\n",
        "\n",
        "async def threat_assessment(analysis: str) -> dict:\n",
        "    \"\"\"\n",
        "    Assess potential threats or forensic concerns based on the analysis.\n",
        "    Returns a JSON object containing threat level, categories, reasoning, and recommended actions.\n",
        "    \"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "Based on the following forensic image analysis, assess potential threats or concerns.\n",
        "\n",
        "{analysis}\n",
        "\n",
        "Return a JSON object with the keys:\n",
        "- \"threat_level\": integer (0-10)\n",
        "- \"categories\": list of strings (e.g., [\"identity_theft\", \"privacy_breach\"])\n",
        "- \"reasoning\": a brief explanation\n",
        "- \"recommended_actions\": a list of recommended actions\n",
        "\n",
        "Return valid JSON only.\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"analysis\"],\n",
        "        template=prompt_template\n",
        "    )\n",
        "    chain = LLMChain(llm=llama_llm, prompt=prompt)\n",
        "    response = await chain.arun(analysis=analysis)\n",
        "    try:\n",
        "        return json.loads(response.strip())\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"threat_level\": 5,\n",
        "            \"categories\": [\"unknown\"],\n",
        "            \"reasoning\": \"LLM response parsing failed. Check the forensic analysis manually.\",\n",
        "            \"recommended_actions\": [\"Review analysis manually.\"]\n",
        "        }\n",
        "\n",
        "async def graph_link_analysis(results: list) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Create a network graph visualization of the search results and similarity scores.\n",
        "    Nodes represent sources, matches, and metadata.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    for result in results:\n",
        "        source = result['source']\n",
        "        match = result['match']\n",
        "        score = result['score']\n",
        "        G.add_node(source, type='source')\n",
        "        G.add_node(match, type='match')\n",
        "        G.add_edge(source, match, weight=score)\n",
        "        if 'metadata' in result:\n",
        "            for key, value in result['metadata'].items():\n",
        "                if isinstance(value, str):\n",
        "                    meta_node = f\"{key}:{value}\"\n",
        "                    G.add_node(meta_node, type='metadata')\n",
        "                    G.add_edge(match, meta_node, weight=1.0)\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    node_colors = []\n",
        "    for node in G.nodes():\n",
        "        node_type = G.nodes[node].get('type', 'unknown')\n",
        "        if node_type == 'source':\n",
        "            node_colors.append('lightblue')\n",
        "        elif node_type == 'match':\n",
        "            node_colors.append('lightgreen')\n",
        "        else:\n",
        "            node_colors.append('lightgray')\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=node_colors,\n",
        "            edge_color='gray', font_size=8, node_size=800, alpha=0.8)\n",
        "    edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in G.edges(data=True) if 'weight' in d}\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
        "    plt.title(\"Graph & Link Analysis of Search Results\")\n",
        "    return plt.gcf()"
      ],
      "metadata": {
        "id": "vTmg5X_njimi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 7. MULTI-AGENT WORKFLOW (LANGGRAPH)\n",
        "#############################################\n",
        "\n",
        "async def build_search_system(image_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Execute an end-to-end multi-agent workflow:\n",
        "    1. Preprocess the image (download, clean, face detection, description generation).\n",
        "    2. Generate image embeddings using CLIP, BLIP-2, DINOv2 and extract classical features.\n",
        "    3. Simulate reverse image searches across multiple sources.\n",
        "    4. Compute image similarity between the current image and the search result embeddings.\n",
        "    5. Analyze results using an LLM for forensic insights and threat assessment.\n",
        "    6. Generate a final forensic report.\n",
        "    \"\"\"\n",
        "    state_schema = {\n",
        "        \"image\": None,\n",
        "        \"image_description\": \"\",\n",
        "        \"faces\": [],\n",
        "        \"clip_embedding\": None,\n",
        "        \"blip_embedding\": None,\n",
        "        \"dinov2_embedding\": None,\n",
        "        \"classical_features\": None,\n",
        "        \"search_results\": {},\n",
        "        \"merged_results\": [],\n",
        "        \"analysis\": \"\",\n",
        "        \"threat_assessment\": {},\n",
        "        \"graph\": None,\n",
        "        \"final_report\": \"\"\n",
        "    }\n",
        "\n",
        "    workflow = StateGraph(state_schema)\n",
        "\n",
        "    # Node 1: Preprocessing\n",
        "    async def preprocess_node(state):\n",
        "        print(\"Preprocessing image...\")\n",
        "        image = await preprocess_image_async(image_url)\n",
        "        faces = detect_faces(image)\n",
        "        description = await generate_image_description(image)\n",
        "        print(\"Image Description (first 100 chars):\", description[:100])\n",
        "        print(f\"Detected {len(faces)} face(s)\")\n",
        "        return {**state,\n",
        "                \"image\": image,\n",
        "                \"faces\": faces,\n",
        "                \"image_description\": description}\n",
        "\n",
        "    # Node 2: Embedding Generation\n",
        "    async def embedding_node(state):\n",
        "        print(\"Generating image embeddings...\")\n",
        "        image = state[\"image\"]\n",
        "        clip_emb, blip_emb, dinov2_emb, classical_features = await asyncio.gather(\n",
        "            generate_clip_embedding(image),\n",
        "            generate_blip_embedding(image),\n",
        "            generate_dinov2_embedding(image),\n",
        "            extract_classical_features(image)\n",
        "        )\n",
        "        return {**state,\n",
        "                \"clip_embedding\": clip_emb,\n",
        "                \"blip_embedding\": blip_emb,\n",
        "                \"dinov2_embedding\": dinov2_emb,\n",
        "                \"classical_features\": classical_features}\n",
        "\n",
        "    # Node 3: Search Engine and Similarity Computation\n",
        "    async def search_node(state):\n",
        "        print(\"Searching across multiple sources...\")\n",
        "        clip_emb = state[\"clip_embedding\"]\n",
        "        description = state[\"image_description\"]\n",
        "        image = state[\"image\"]\n",
        "        private_results, twitter_results, reddit_results, instagram_results, osint_results, tineye_results, google_results = await asyncio.gather(\n",
        "            search_private_db(clip_emb, description),\n",
        "            search_twitter(clip_emb, description),\n",
        "            search_reddit(clip_emb, description),\n",
        "            search_instagram(clip_emb, description),\n",
        "            search_osint_sources(clip_emb, description),\n",
        "            search_tineye(image),\n",
        "            search_google_images(image, description)\n",
        "        )\n",
        "        search_results = {\n",
        "            \"private_db\": private_results,\n",
        "            \"twitter\": twitter_results,\n",
        "            \"reddit\": reddit_results,\n",
        "            \"instagram\": instagram_results,\n",
        "            \"osint\": osint_results,\n",
        "            \"tineye\": tineye_results,\n",
        "            \"google\": google_results\n",
        "        }\n",
        "        merged = await merge_search_results(\n",
        "            private_results, twitter_results, reddit_results, instagram_results,\n",
        "            osint_results, tineye_results, google_results\n",
        "        )\n",
        "        print(f\"Found {len(merged)} merged result(s)\")\n",
        "        # Compute similarity for results that have an embedding (if available)\n",
        "        for result in merged:\n",
        "            if result.get(\"embedding\") is not None:\n",
        "                similarity = compute_cosine_similarity(clip_emb, result[\"embedding\"])\n",
        "                result[\"similarity\"] = similarity\n",
        "            else:\n",
        "                result[\"similarity\"] = None\n",
        "        return {**state,\n",
        "                \"search_results\": search_results,\n",
        "                \"merged_results\": merged}\n",
        "\n",
        "    # Node 4: Analysis and Threat Assessment\n",
        "    async def analysis_node(state):\n",
        "        print(\"Analyzing results with LLM and assessing threat...\")\n",
        "        analysis = await analyze_results_with_llm(state[\"merged_results\"], state[\"image_description\"])\n",
        "        threat_info = await threat_assessment(analysis)\n",
        "        graph_fig = await graph_link_analysis(state[\"merged_results\"])\n",
        "        return {**state,\n",
        "                \"analysis\": analysis,\n",
        "                \"threat_assessment\": threat_info,\n",
        "                \"graph\": graph_fig}\n",
        "\n",
        "    # Node 5: Report Generation\n",
        "    async def report_node(state):\n",
        "        print(\"Generating final forensic report...\")\n",
        "        report = f\"\"\"\n",
        "==================== FORENSIC REPORT ====================\n",
        "\n",
        "Image Description:\n",
        "{state[\"image_description\"]}\n",
        "\n",
        "Key Findings:\n",
        "- Total Merged Matches: {len(state[\"merged_results\"])}\n",
        "- Faces Detected: {len(state[\"faces\"])}\n",
        "- Threat Level: {state[\"threat_assessment\"].get(\"threat_level\", \"Unknown\")}/10\n",
        "\n",
        "Detailed Analysis:\n",
        "{state[\"analysis\"]}\n",
        "\n",
        "Threat Assessment:\n",
        "- Level: {state[\"threat_assessment\"].get(\"threat_level\", \"Unknown\")}/10\n",
        "- Categories: {', '.join(state[\"threat_assessment\"].get(\"categories\", [\"Unknown\"]))}\n",
        "- Reasoning: {state[\"threat_assessment\"].get(\"reasoning\", \"Not available\")}\n",
        "- Recommended Actions: {', '.join(state[\"threat_assessment\"].get(\"recommended_actions\", [\"None specified\"]))}\n",
        "\n",
        "Top 5 Search Results with Similarity Scores:\n",
        "{json.dumps(state[\"merged_results\"][:5], indent=2)}\n",
        "\n",
        "===========================================================\n",
        "        \"\"\"\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    # Build workflow nodes and define flow\n",
        "    workflow.add_node(\"preprocess\", preprocess_node)\n",
        "    workflow.add_node(\"embed\", embedding_node)\n",
        "    workflow.add_node(\"search\", search_node)\n",
        "    workflow.add_node(\"analyze\", analysis_node)\n",
        "    workflow.add_node(\"report\", report_node)\n",
        "\n",
        "    workflow.add_edge(\"preprocess\", \"embed\")\n",
        "    workflow.add_edge(\"embed\", \"search\")\n",
        "    workflow.add_edge(\"search\", \"analyze\")\n",
        "    workflow.add_edge(\"analyze\", \"report\")\n",
        "    workflow.add_edge(\"report\", END)\n",
        "\n",
        "    workflow.set_entry_point(\"preprocess\")\n",
        "\n",
        "    print(\"Starting multi-agent reverse image search workflow...\")\n",
        "    result = await workflow.ainvoke({})\n",
        "    print(\"Workflow completed!\")\n",
        "    return result"
      ],
      "metadata": {
        "id": "SRtNR-fwjtBk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 8. MAIN FUNCTION\n",
        "#############################################\n",
        "\n",
        "async def main(image_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Main function to run the reverse image similarity and forensic analysis system.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        print(f\"Processing image from URL: {image_url}\")\n",
        "        result = await build_search_system(image_url)\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Total Processing Time: {elapsed:.2f} seconds\")\n",
        "        print(result[\"final_report\"])\n",
        "        # To display the generated graph, uncomment the following line:\n",
        "        # plt.show()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(\"Error during processing:\", str(e))\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with a valid image URL for testing in production\n",
        "    test_image_url = \"https://example.com/test_image.jpg\"\n",
        "    asyncio.run(main(test_image_url))"
      ],
      "metadata": {
        "id": "Ohe4NRKMjy11",
        "outputId": "9ed7a95e-009d-4ce3-cd80-0e07059a3ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image from URL: https://example.com/test_image.jpg\n",
            "Error during processing: unhashable type: 'dict'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3f1ZOO-O9Ark"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}