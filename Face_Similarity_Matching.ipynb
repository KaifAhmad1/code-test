{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Face_Similarity_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reverse Image Search System for Defensive Forensics**\n",
        "\n",
        "This is an end-to-end forensic image analysis pipeline with rich features:\n",
        "1. Enhanced model loading with multiple fallback strategies.\n",
        "2. Advanced asynchronous image processing including OCR, EXIF extraction, and error level analysis.\n",
        "3. Multi-agent reverse image search across multiple simulated sources (Private DB, Twitter, Reddit, Instagram, OSINT, TinEye, Google Images, Additional Sources).\n",
        "4. Comprehensive embedding generation and consensus across CLIP, BLIP-2, and DINOv2.\n",
        "5. Detailed forensic analysis using advanced prompt templates and multi-step LLM workflows.\n",
        "6. Face detection, clustering, and similarity analysis with visualization.\n",
        "7. Graph-based link analysis for search results with NetworkX.\n",
        "8. Deduplication using perceptual image hashes.\n",
        "9. Robust error handling and extensive print statements for logging.\n",
        "10. Results saving including face images and network graph image."
      ],
      "metadata": {
        "id": "AVEgMKQJFRqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJzbvTscFMq1"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain langgraph numpy pillow requests vllm aiohttp opencv-python-headless networkx matplotlib nest_asyncio groq pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "import getpass\n",
        "from io import BytesIO\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Union, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ExifTags, ImageChops\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import aiohttp\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "# LLM orchestration with Groq and LangChain\n",
        "from groq import Groq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Multi-agent workflow using LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Patch asyncio for environments like Jupyter or nested loops\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# vLLM for optimizing model inference\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Global flag for OCR availability\n",
        "OCR_AVAILABLE = False\n",
        "try:\n",
        "    import pytesseract\n",
        "    OCR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OCR_AVAILABLE = False"
      ],
      "metadata": {
        "id": "y8qfEoZQK2hA",
        "outputId": "07a505e0-f04a-4e48-eb79-f75616778a9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-19 06:45:28 [__init__.py:256] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 1. INITIALIZATION AND MODEL LOADING FUNCTIONS\n",
        "#############################################\n",
        "def get_api_keys() -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Prompt the user for necessary API keys.\n",
        "    \"\"\"\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "    google_cse_id = os.environ.get(\"GOOGLE_CSE_ID\") or getpass.getpass(\"Enter your Google CSE ID: \")\n",
        "    google_api_key = os.environ.get(\"GOOGLE_API_KEY\") or getpass.getpass(\"Enter your Google API Key: \")\n",
        "    return groq_api_key, google_cse_id, google_api_key\n",
        "\n",
        "\n",
        "def load_models(device: Optional[str] = None) -> dict:\n",
        "    \"\"\"\n",
        "    Load required models with multiple fallback strategies.\n",
        "    \"\"\"\n",
        "    print(\"Loading models...\")\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    models = {}\n",
        "    try:\n",
        "        # Attempt to load with vLLM\n",
        "        clip_model = LLM(\n",
        "            model=\"openai/clip-vit-large-patch14\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"clip_model\"] = clip_model\n",
        "        models[\"clip_processor\"] = None\n",
        "        print(\"CLIP (vLLM) model loaded successfully.\")\n",
        "\n",
        "        blip_model = LLM(\n",
        "            model=\"Salesforce/blip2-opt-2.7b\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"blip_model\"] = blip_model\n",
        "        models[\"blip_processor\"] = None\n",
        "        print(\"BLIP-2 (vLLM) model loaded successfully.\")\n",
        "\n",
        "        dinov2_model = LLM(\n",
        "            model=\"facebook/dinov2-base\",\n",
        "            tensor_parallel_size=1,\n",
        "            gpu_memory_utilization=0.7,\n",
        "            dtype=\"float16\"\n",
        "        )\n",
        "        models[\"dinov2_model\"] = dinov2_model\n",
        "        models[\"dinov2_processor\"] = None\n",
        "        print(\"DINOv2 (vLLM) model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"vLLM loading failed: {e}\")\n",
        "        print(\"Falling back to direct transformer imports...\")\n",
        "        try:\n",
        "            from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel\n",
        "            clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "            models[\"clip_model\"] = clip_model.to(device)\n",
        "            models[\"clip_processor\"] = clip_processor\n",
        "            print(\"CLIP (Direct) model loaded successfully.\")\n",
        "\n",
        "            blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "            blip_model = AutoModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "            models[\"blip_model\"] = blip_model.to(device)\n",
        "            models[\"blip_processor\"] = blip_processor\n",
        "            print(\"BLIP-2 (Direct) model loaded successfully.\")\n",
        "\n",
        "            dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "            dinov2_processor = AutoProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "            models[\"dinov2_model\"] = dinov2_model.to(device)\n",
        "            models[\"dinov2_processor\"] = dinov2_processor\n",
        "            print(\"DINOv2 (Direct) model loaded successfully.\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Direct imports failed: {e2}\")\n",
        "            print(\"Using OpenCV-based feature detectors as a last resort.\")\n",
        "            models[\"feature_detector\"] = cv2.SIFT_create() if hasattr(cv2, 'SIFT_create') else None\n",
        "            models[\"orb_detector\"] = cv2.ORB_create()\n",
        "            models[\"brisk_detector\"] = cv2.BRISK_create()\n",
        "    models[\"device\"] = device\n",
        "    print(f\"Model initialization complete on device: {device}\")\n",
        "    return models\n",
        "\n",
        "\n",
        "def initialize_llm(api_key: str) -> Tuple[Any, SamplingParams]:\n",
        "    \"\"\"\n",
        "    Initialize the LLM client using Groq.\n",
        "    \"\"\"\n",
        "    client = Groq(api_key=api_key)\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return client, sampling_params"
      ],
      "metadata": {
        "id": "O1c6PH_HqJBP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 2. IMAGE PROCESSING FUNCTIONS\n",
        "#############################################\n",
        "async def fetch_image_async(image_url: str) -> bytes:\n",
        "    \"\"\"\n",
        "    Asynchronously fetch an image from a URL.\n",
        "    \"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(image_url) as response:\n",
        "            if response.status != 200:\n",
        "                raise ValueError(f\"Failed to fetch image: {response.status}\")\n",
        "            return await response.read()\n",
        "\n",
        "\n",
        "def load_local_image(image_path: str) -> bytes:\n",
        "    \"\"\"\n",
        "    Load an image from a local file.\n",
        "    \"\"\"\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "async def preprocess_image_async(image_source: Union[str, bytes], enhance: bool = True) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Preprocess image: load, enhance, and resize.\n",
        "    \"\"\"\n",
        "    if isinstance(image_source, str):\n",
        "        if image_source.startswith(\"http\"):\n",
        "            image_bytes = await fetch_image_async(image_source)\n",
        "        else:\n",
        "            image_bytes = load_local_image(image_source)\n",
        "    else:\n",
        "        image_bytes = image_source\n",
        "\n",
        "    image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    if enhance:\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        image = image.filter(ImageFilter.SHARPEN)\n",
        "    if max(image.size) > 1024:\n",
        "        image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "    return image\n",
        "\n",
        "\n",
        "def extract_exif_data(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Extract EXIF metadata from an image.\n",
        "    \"\"\"\n",
        "    exif_data = {}\n",
        "    try:\n",
        "        exif = image._getexif()\n",
        "        if exif:\n",
        "            for tag_id, val in exif.items():\n",
        "                tag = ExifTags.TAGS.get(tag_id, tag_id)\n",
        "                exif_data[tag] = \"Binary data\" if isinstance(val, bytes) or tag == \"MakerNote\" else val\n",
        "    except Exception:\n",
        "        pass\n",
        "    return exif_data\n",
        "\n",
        "\n",
        "def perform_ocr(image: Image.Image) -> str:\n",
        "    \"\"\"\n",
        "    Perform OCR on an image if pytesseract is available.\n",
        "    \"\"\"\n",
        "    if not OCR_AVAILABLE:\n",
        "        return \"OCR not available. Install pytesseract.\"\n",
        "    try:\n",
        "        ocr_image = image.copy()\n",
        "        enhancer = ImageEnhance.Contrast(ocr_image)\n",
        "        ocr_image = enhancer.enhance(2.0)\n",
        "        ocr_image = ocr_image.convert(\"L\")\n",
        "        return pytesseract.image_to_string(ocr_image).strip()\n",
        "    except Exception as e:\n",
        "        return f\"OCR error: {e}\"\n",
        "\n",
        "\n",
        "def detect_image_manipulation(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Detect potential image manipulation using Error Level Analysis (ELA).\n",
        "    \"\"\"\n",
        "    temp_file = BytesIO()\n",
        "    image.save(temp_file, format=\"JPEG\", quality=90)\n",
        "    temp_file.seek(0)\n",
        "    saved_image = Image.open(temp_file).convert(\"RGB\")\n",
        "    ela_image = ImageChops.difference(image, saved_image)\n",
        "    extrema = ela_image.getextrema()\n",
        "    max_diff = max(ex[1] for ex in extrema)\n",
        "    return {\n",
        "        \"max_diff\": max_diff,\n",
        "        \"manipulation_score\": min(max_diff / 40.0, 1.0),\n",
        "        \"likely_manipulated\": max_diff > 20\n",
        "    }\n",
        "\n",
        "\n",
        "def detect_faces(image: Image.Image, cascade_file: Optional[str] = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Detect faces using Haar Cascades from OpenCV.\n",
        "    \"\"\"\n",
        "    img_cv = np.array(image)\n",
        "    img_cv = img_cv[:, :, ::-1].copy()  # Convert RGB to BGR\n",
        "    cascade = cascade_file if cascade_file and os.path.exists(cascade_file) else cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "    face_cascade = cv2.CascadeClassifier(cascade)\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    if len(faces) == 0:\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.05, 3)\n",
        "    face_images = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_img = image.crop((x, y, x+w, y+h)).resize((224, 224), Image.LANCZOS)\n",
        "        face_images.append({\"image\": face_img, \"coords\": (x, y, w, h)})\n",
        "    return face_images\n",
        "\n",
        "\n",
        "def save_faces(faces: List[Dict[str, Any]], output_dir: str = \"faces_output\") -> None:\n",
        "    \"\"\"\n",
        "    Save detected face images to disk.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    for idx, face in enumerate(faces):\n",
        "        face_path = os.path.join(output_dir, f\"face_{timestamp}_{idx+1}.jpg\")\n",
        "        face[\"image\"].save(face_path)\n",
        "    print(f\"Saved {len(faces)} face image(s) in {output_dir}\")\n",
        "\n",
        "\n",
        "def image_hash(image: Image.Image, hash_size: int = 8) -> str:\n",
        "    \"\"\"\n",
        "    Compute a perceptual image hash for deduplication.\n",
        "    \"\"\"\n",
        "    img = image.resize((hash_size, hash_size), Image.LANCZOS).convert(\"L\")\n",
        "    pixels = list(img.getdata())\n",
        "    avg = sum(pixels) / len(pixels)\n",
        "    bits = \"\".join(\"1\" if pixel > avg else \"0\" for pixel in pixels)\n",
        "    return hex(int(bits, 2))[2:].zfill(hash_size**2 // 4)"
      ],
      "metadata": {
        "id": "06zu0aWBItFQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 3. EMBEDDING & DESCRIPTION FUNCTIONS\n",
        "#############################################\n",
        "async def generate_clip_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the CLIP model.\"\"\"\n",
        "    inputs = models[\"clip_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"clip_model\"].get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_blip_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using BLIP-2 model for semantic representation.\"\"\"\n",
        "    inputs = models[\"blip_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"blip_model\"].get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_dinov2_embedding(image: Image.Image, models: dict) -> np.ndarray:\n",
        "    \"\"\"Generate an image embedding using the DINOv2 model (CLS token).\"\"\"\n",
        "    inputs = models[\"dinov2_processor\"](images=image, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"dinov2_model\"](**inputs).last_hidden_state[:, 0]\n",
        "    embedding = outputs.cpu().numpy()\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    return embedding / norm if norm > 0 else embedding\n",
        "\n",
        "async def generate_image_description(image: Image.Image, models: dict, prompt=None) -> str:\n",
        "    \"\"\"\n",
        "    Generate a detailed description of the image using BLIP-2.\n",
        "    Focuses on identifiable people, objects, and locations.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        models: Dictionary with loaded models\n",
        "        prompt: Optional custom prompt\n",
        "    \"\"\"\n",
        "    if prompt is None:\n",
        "        prompt = \"Describe this image in detail with focus on identifiable people, objects, and locations:\"\n",
        "\n",
        "    inputs = models[\"blip_processor\"](images=image, text=prompt, return_tensors=\"pt\").to(models[\"device\"])\n",
        "    with torch.no_grad():\n",
        "        outputs = models[\"blip_model\"].generate(**inputs, max_new_tokens=100)\n",
        "    description = models[\"blip_processor\"].batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return description.strip()\n",
        "\n",
        "async def extract_classical_features(image: Image.Image) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Extract classical image features using ORB and optionally SIFT.\n",
        "    Returns descriptors selected based on the number of keypoints.\n",
        "    \"\"\"\n",
        "    image_np = np.array(image)\n",
        "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Extract ORB features\n",
        "    orb = cv2.ORB_create(nfeatures=1000)\n",
        "    keypoints_orb, descriptors_orb = orb.detectAndCompute(gray, None)\n",
        "\n",
        "    # Extract SIFT features if available\n",
        "    descriptors_sift = None\n",
        "    keypoints_sift = None\n",
        "    try:\n",
        "        sift = cv2.SIFT_create()\n",
        "        keypoints_sift, descriptors_sift = sift.detectAndCompute(gray, None)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Use the better descriptor\n",
        "    if descriptors_orb is None and descriptors_sift is None:\n",
        "        return None, None\n",
        "    elif descriptors_orb is None:\n",
        "        return keypoints_sift, descriptors_sift\n",
        "    elif descriptors_sift is None:\n",
        "        return keypoints_orb, descriptors_orb\n",
        "    else:\n",
        "        if len(keypoints_sift) > len(keypoints_orb):\n",
        "            return keypoints_sift, descriptors_sift\n",
        "        else:\n",
        "            return keypoints_orb, descriptors_orb\n",
        "\n",
        "def get_histogram_features(image: Image.Image) -> dict:\n",
        "    \"\"\"\n",
        "    Extract color histogram features from the image.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with histogram features\n",
        "    \"\"\"\n",
        "    # Convert to NumPy array\n",
        "    img_np = np.array(image)\n",
        "\n",
        "    # Calculate histograms for each channel\n",
        "    hist_r = cv2.calcHist([img_np], [0], None, [256], [0, 256])\n",
        "    hist_g = cv2.calcHist([img_np], [1], None, [256], [0, 256])\n",
        "    hist_b = cv2.calcHist([img_np], [2], None, [256], [0, 256])\n",
        "\n",
        "    # Normalize histograms\n",
        "    hist_r = cv2.normalize(hist_r, hist_r).flatten()\n",
        "    hist_g = cv2.normalize(hist_g, hist_g).flatten()\n",
        "    hist_b = cv2.normalize(hist_b, hist_b).flatten()\n",
        "\n",
        "    return {\n",
        "        \"histogram_r\": hist_r.tolist(),\n",
        "        \"histogram_g\": hist_g.tolist(),\n",
        "        \"histogram_b\": hist_b.tolist()\n",
        "    }\n",
        "\n",
        "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two image embeddings.\n",
        "    Returns a float between -1 and 1 (1 indicates identical embeddings).\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(embedding1.flatten(), embedding2.flatten())\n",
        "    norm1 = np.linalg.norm(embedding1)\n",
        "    norm2 = np.linalg.norm(embedding2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(dot_product / (norm1 * norm2))\n",
        "\n",
        "async def generate_consensus_embedding(clip_emb, blip_emb, dinov2_emb) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate a consensus embedding by averaging the CLIP, BLIP-2, and DINOv2 embeddings.\n",
        "    Weights the embeddings based on their typical performance.\n",
        "    \"\"\"\n",
        "    # Weight the embeddings (these weights can be adjusted)\n",
        "    weights = np.array([0.4, 0.3, 0.3])  # CLIP, BLIP, DINOv2\n",
        "\n",
        "    # Ensure all embeddings are flattened and normalized\n",
        "    embeddings = [\n",
        "        clip_emb.flatten() / np.linalg.norm(clip_emb),\n",
        "        blip_emb.flatten() / np.linalg.norm(blip_emb),\n",
        "        dinov2_emb.flatten() / np.linalg.norm(dinov2_emb)\n",
        "    ]\n",
        "\n",
        "    # Check if dimensions match, otherwise resize\n",
        "    min_dim = min(e.shape[0] for e in embeddings)\n",
        "    resized_embeddings = [e[:min_dim] for e in embeddings]\n",
        "\n",
        "    # Weighted average\n",
        "    consensus = np.average(np.array(resized_embeddings), axis=0, weights=weights)\n",
        "\n",
        "    # Normalize\n",
        "    norm = np.linalg.norm(consensus)\n",
        "    return consensus / norm if norm > 0 else consensus"
      ],
      "metadata": {
        "id": "EyCtUkF1ItH-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 4. SEARCH ENGINE FUNCTIONS (Simulated Async Calls)\n",
        "#############################################\n",
        "async def search_private_db(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.5)\n",
        "    return [\n",
        "        {\"source\": \"Private DB\", \"match\": \"Person_123\", \"score\": 0.91,\n",
        "         \"metadata\": {\"date\": \"2023-10-15\"}, \"embedding\": embedding.tolist()},\n",
        "        {\"source\": \"Private DB\", \"match\": \"Person_456\", \"score\": 0.85,\n",
        "         \"metadata\": {\"date\": \"2023-09-22\"}, \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_twitter(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.7)\n",
        "    keywords = description.split()[:5]\n",
        "    return [\n",
        "        {\"source\": \"Twitter\", \"match\": \"Tweet_Image_456\", \"score\": 0.87,\n",
        "         \"metadata\": {\"username\": \"@user123\", \"posted\": \"2023-11-01\", \"keywords\": keywords},\n",
        "         \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_reddit(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.6)\n",
        "    return [\n",
        "        {\"source\": \"Reddit\", \"match\": \"Reddit_Post_321\", \"score\": 0.89,\n",
        "         \"metadata\": {\"subreddit\": \"r/pics\", \"posted\": \"2023-10-25\"},\n",
        "         \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_instagram(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.8)\n",
        "    return [\n",
        "        {\"source\": \"Instagram\", \"match\": \"Insta_Post_654\", \"score\": 0.88,\n",
        "         \"metadata\": {\"username\": \"user456\", \"posted\": \"2023-11-12\", \"location\": \"New York\"},\n",
        "         \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_osint_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [\n",
        "        {\"source\": \"OSINT\", \"match\": \"DarkWeb_Post_999\", \"score\": 0.83,\n",
        "         \"metadata\": {\"forum\": \"anonymous_forum\", \"date\": \"2023-09-10\"},\n",
        "         \"embedding\": embedding.tolist()},\n",
        "        {\"source\": \"OSINT\", \"match\": \"Telegram_Group_123\", \"score\": 0.79,\n",
        "         \"metadata\": {\"group\": \"public_channel_xyz\", \"date\": \"2023-10-30\"},\n",
        "         \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_tineye(image: Image.Image) -> list:\n",
        "    img_byte_arr = BytesIO()\n",
        "    image.save(img_byte_arr, format=\"JPEG\")\n",
        "    img_byte_arr.seek(0)\n",
        "    await asyncio.sleep(1.2)\n",
        "    return [\n",
        "        {\"source\": \"TinEye\", \"match\": \"Website_ABC\", \"score\": 0.92,\n",
        "         \"metadata\": {\"domain\": \"example.com\", \"first_crawled\": \"2023-08-15\"},\n",
        "         \"embedding\": None}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_google_images(image: Image.Image, description: str, api_key: str, cse_id: str) -> list:\n",
        "    await asyncio.sleep(1.0)\n",
        "    return [\n",
        "        {\"source\": \"Google Images\", \"match\": \"News_Site_XYZ\", \"score\": 0.86,\n",
        "         \"metadata\": {\"url\": \"https://example-news.com/article123\", \"title\": \"Example article related to the image\"},\n",
        "         \"embedding\": None}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def search_additional_sources(embedding: np.ndarray, description: str) -> list:\n",
        "    await asyncio.sleep(0.9)\n",
        "    return [\n",
        "        {\"source\": \"Facebook\", \"match\": \"FB_Post_123\", \"score\": 0.81,\n",
        "         \"metadata\": {\"user\": \"john.doe\", \"posted\": \"2023-10-05\"},\n",
        "         \"embedding\": embedding.tolist()},\n",
        "        {\"source\": \"LinkedIn\", \"match\": \"LinkedIn_Profile_456\", \"score\": 0.78,\n",
        "         \"metadata\": {\"profile\": \"jane-smith\", \"updated\": \"2023-11-10\"},\n",
        "         \"embedding\": embedding.tolist()}\n",
        "    ]\n",
        "\n",
        "\n",
        "async def merge_search_results(*results: list) -> list:\n",
        "    merged = []\n",
        "    seen = set()\n",
        "    for result_list in results:\n",
        "        for result in result_list:\n",
        "            rid = (result[\"source\"], result[\"match\"])\n",
        "            if rid not in seen:\n",
        "                seen.add(rid)\n",
        "                if isinstance(result.get(\"embedding\"), np.ndarray):\n",
        "                    result[\"embedding\"] = result[\"embedding\"].tolist()\n",
        "                merged.append(result)\n",
        "    return sorted(merged, key=lambda x: x.get(\"score\", 0), reverse=True)"
      ],
      "metadata": {
        "id": "zV_NPpqELXiP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 5. ADVANCED FORENSIC ANALYSIS AND PROMPTING\n",
        "#############################################\n",
        "async def analyze_results_with_llm(results: list, image_description: str, client: Any) -> str:\n",
        "    \"\"\"\n",
        "    Run forensic analysis using rich LLM prompts.\n",
        "    \"\"\"\n",
        "    clean_results = []\n",
        "    for r in results:\n",
        "        clean_results.append({k: v for k, v in r.items() if k != \"embedding\"})\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert forensic analyst. Analyze the following reverse image search results.\n",
        "\n",
        "Image Description:\n",
        "{image_description}\n",
        "\n",
        "Search Results (in JSON):\n",
        "{json.dumps(clean_results, indent=2)}\n",
        "\n",
        "Provide a detailed multi-section report addressing:\n",
        "- Cross-referencing of entities and duplication checks.\n",
        "- Temporal and geographic correlations.\n",
        "- Reliability of sources and potential biases.\n",
        "- Image similarity analysis and forensic clues.\n",
        "- Recommendations for further investigation.\n",
        "\n",
        "Return your structured analysis report.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a forensic image analysis expert with multi-agent capabilities.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "async def threat_assessment(analysis: str, client: Any) -> dict:\n",
        "    \"\"\"\n",
        "    Assess potential threats based on forensic analysis using advanced LLM prompts.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Given the forensic analysis below, evaluate the potential threats and vulnerabilities.\n",
        "\n",
        "Forensic Analysis:\n",
        "{analysis}\n",
        "\n",
        "Return a JSON object with:\n",
        "- \"threat_level\": integer from 0 to 10\n",
        "- \"categories\": a list of threat categories (e.g., [\"identity_theft\", \"privacy_breach\"])\n",
        "- \"reasoning\": a brief explanation\n",
        "- \"recommended_actions\": a list of suggested actions\n",
        "\n",
        "Return only valid JSON.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a threat assessment expert specialized in forensic image analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.1\n",
        "    )\n",
        "    try:\n",
        "        txt = response.choices[0].message.content.strip()\n",
        "        json_start = txt.find(\"{\")\n",
        "        json_end = txt.rfind(\"}\") + 1\n",
        "        if json_start >= 0 and json_end > json_start:\n",
        "            return json.loads(txt[json_start:json_end])\n",
        "        raise ValueError(\"JSON not found in output\")\n",
        "    except Exception as e:\n",
        "        print(f\"Threat assessment parsing error: {e}\")\n",
        "        return {\n",
        "            \"threat_level\": 5,\n",
        "            \"categories\": [\"unknown\"],\n",
        "            \"reasoning\": \"Error parsing LLM results, manual review recommended.\",\n",
        "            \"recommended_actions\": [\"Manual analysis\"]\n",
        "        }\n",
        "\n",
        "\n",
        "async def graph_link_analysis(results: list) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Visualize search results as a network graph.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    for r in results:\n",
        "        node = f\"{r['source']}: {r['match']}\"\n",
        "        G.add_node(node, score=r.get(\"score\", 0))\n",
        "    nodes = list(G.nodes())\n",
        "    for i in range(len(nodes)):\n",
        "        for j in range(i+1, len(nodes)):\n",
        "            if nodes[i].split(\":\")[0] == nodes[j].split(\":\")[0]:\n",
        "                G.add_edge(nodes[i], nodes[j])\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", node_size=1500, font_size=10, edge_color=\"grey\")\n",
        "    plt.title(\"Detected Forensic Search Results Network\")\n",
        "    return plt.gcf()"
      ],
      "metadata": {
        "id": "crwgTfxTLeyE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 6. FACE SIMILARITY AND CLUSTERING ANALYSIS\n",
        "#############################################\n",
        "async def compute_face_similarities(faces: List[Dict[str, Any]], models: dict) -> Dict[Tuple[int, int], float]:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between face embeddings using CLIP.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for face in faces:\n",
        "        emb = await generate_clip_embedding(face[\"image\"], models)\n",
        "        embeddings.append(emb)\n",
        "    similarities = {}\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(i+1, len(embeddings)):\n",
        "            sim = float(np.dot(embeddings[i].flatten(), embeddings[j].flatten()) /\n",
        "                        (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]) + 1e-10))\n",
        "            similarities[(i, j)] = sim\n",
        "    return similarities"
      ],
      "metadata": {
        "id": "WJ9wXfZmLnRi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# 7. MAIN PIPELINE AND MULTI-AGENT WORKFLOW\n",
        "#############################################\n",
        "async def main():\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python defensive_forensics.py <image_url_or_local_path>\")\n",
        "        sys.exit(1)\n",
        "    image_source = sys.argv[1]\n",
        "\n",
        "    # Step 1: Initialize API keys and LLM client\n",
        "    groq_api_key, google_cse_id, google_api_key = get_api_keys()\n",
        "    client, sampling_params = initialize_llm(groq_api_key)\n",
        "\n",
        "    # Step 2: Load models\n",
        "    models = load_models()\n",
        "\n",
        "    # Step 3: Preprocess the input image\n",
        "    print(\"Preprocessing image...\")\n",
        "    image = await preprocess_image_async(image_source, enhance=True)\n",
        "    exif = extract_exif_data(image)\n",
        "    print(\"EXIF Data:\", exif)\n",
        "\n",
        "    # Step 4: Detect faces, perform OCR, ELA, and compute perceptual image hash\n",
        "    faces = detect_faces(image)\n",
        "    if faces:\n",
        "        save_faces(faces)\n",
        "    ocr_text = perform_ocr(image)\n",
        "    manipulation = detect_image_manipulation(image)\n",
        "    img_hash = image_hash(image)\n",
        "    print(\"OCR Text:\", ocr_text)\n",
        "    print(\"Manipulation Results:\", manipulation)\n",
        "    print(\"Image Hash:\", img_hash)\n",
        "\n",
        "    # Step 5: Generate embeddings and description\n",
        "    clip_emb = await generate_clip_embedding(image, models)\n",
        "    blip_emb = await generate_blip_embedding(image, models)\n",
        "    dinov2_emb = await generate_dinov2_embedding(image, models)\n",
        "    consensus_emb = await generate_consensus_embedding(clip_emb, blip_emb, dinov2_emb)\n",
        "    description = await generate_image_description(image, models)\n",
        "    print(\"Image Description:\", description)\n",
        "\n",
        "    # Step 6: Execute asynchronous reverse image search tasks\n",
        "    search_tasks = await asyncio.gather(\n",
        "        search_private_db(consensus_emb, description),\n",
        "        search_twitter(consensus_emb, description),\n",
        "        search_reddit(consensus_emb, description),\n",
        "        search_instagram(consensus_emb, description),\n",
        "        search_osint_sources(consensus_emb, description),\n",
        "        search_tineye(image),\n",
        "        search_google_images(image, description, google_api_key, google_cse_id),\n",
        "        search_additional_sources(consensus_emb, description)\n",
        "    )\n",
        "    merged_results = await merge_search_results(*search_tasks)\n",
        "    print(\"Merged Search Results:\\n\", json.dumps(merged_results, indent=2))\n",
        "\n",
        "    # Step 7: Advanced forensic analysis using LLM\n",
        "    analysis_report = await analyze_results_with_llm(merged_results, description, client)\n",
        "    print(\"Forensic Analysis Report:\\n\", analysis_report)\n",
        "\n",
        "    threat_report = await threat_assessment(analysis_report, client)\n",
        "    print(\"Threat Assessment Report:\\n\", json.dumps(threat_report, indent=2))\n",
        "\n",
        "    # Step 8: Face similarity analysis if multiple faces detected\n",
        "    face_similarities = {}\n",
        "    if len(faces) > 1:\n",
        "        face_similarities = await compute_face_similarities(faces, models)\n",
        "        print(\"Face Similarities:\", face_similarities)\n",
        "\n",
        "    # Step 9: Graph visualization of search results\n",
        "    fig = await graph_link_analysis(merged_results)\n",
        "    graph_filename = f\"forensic_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "    fig.savefig(graph_filename)\n",
        "    print(f\"Network graph saved as {graph_filename}\")\n",
        "\n",
        "    # Step 10: Save final forensic report to file\n",
        "    report_file = f\"forensic_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    final_report = {\n",
        "        \"exif\": exif,\n",
        "        \"ocr_text\": ocr_text,\n",
        "        \"manipulation\": manipulation,\n",
        "        \"image_hash\": img_hash,\n",
        "        \"description\": description,\n",
        "        \"search_results\": merged_results,\n",
        "        \"analysis_report\": analysis_report,\n",
        "        \"threat_report\": threat_report,\n",
        "        \"face_similarities\": face_similarities,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    with open(report_file, \"w\") as f:\n",
        "        json.dump(final_report, f, indent=2)\n",
        "    print(f\"Final forensic report saved to {report_file}\")\n",
        "\n",
        "    print(\"Forensic processing complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "5F-BKm7HLt-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FM9y2SASLyaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}