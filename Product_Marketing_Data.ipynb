{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNo2i8ayJkt3aEk3o5EsgVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Product_Marketing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwVUeKoRio56",
        "outputId": "0f056e49-4878-463b-e0bb-de62a5e175bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crawl4ai in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: aiosqlite~=0.20 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.4.0)\n",
            "Requirement already satisfied: litellm>=1.53.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.67.5)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.0.2)\n",
            "Requirement already satisfied: pillow~=10.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (10.4.0)\n",
            "Requirement already satisfied: playwright>=1.49.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.52.0)\n",
            "Requirement already satisfied: python-dotenv~=1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.13.4)\n",
            "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.2)\n",
            "Requirement already satisfied: xxhash~=3.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.5.0)\n",
            "Requirement already satisfied: rank-bm25~=0.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.2.2)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (24.1.0)\n",
            "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.4.6)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.11.3)\n",
            "Requirement already satisfied: pyOpenSSL>=24.3.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (25.0.0)\n",
            "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (13.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.28.1)\n",
            "Requirement already satisfied: fake-useragent>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (8.1.8)\n",
            "Requirement already satisfied: pyperclip>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: chardet>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.2.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.0)\n",
            "Requirement already satisfied: humanize>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.13.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (1.76.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.4.0)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (2.19.1)\n",
            "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.30.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install crawl4ai aiohttp requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Libraries and Configure Settings\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from datetime import datetime\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Directories in Colab\n",
        "BASE_DIR = \"/content/web_scraping_test\"\n",
        "UPLOADS_DIR = os.path.join(BASE_DIR, \"uploads\")\n",
        "OUTPUTS_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
        "BASE_CSV_FILE = os.path.join(OUTPUTS_DIR, \"base_images.csv\")\n",
        "SECONDARY_CSV_FILE = os.path.join(OUTPUTS_DIR, \"secondary_images.csv\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(UPLOADS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "# Serper API key (replace with your key from serper.dev)\n",
        "SERPER_API_KEY = \"95c2797a69b167639c98ab054e8597d752c6fe6d\"\n",
        "SERPER_ENDPOINT = \"https://api.serper.dev/images\"\n",
        "\n",
        "# Headers for requests\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0\", \"Content-Type\": \"application/json\"}"
      ],
      "metadata": {
        "id": "clnm8j7ZjCiP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define Helper Functions\n",
        "# Async function to download and validate an image\n",
        "async def download_image(session, url, save_path):\n",
        "    try:\n",
        "        async with session.get(url, headers=HEADERS, timeout=10) as response:\n",
        "            if response.status == 200:\n",
        "                content = await response.read()\n",
        "                # Validate image content with Pillow\n",
        "                try:\n",
        "                    Image.open(io.BytesIO(content)).verify()\n",
        "                    with open(save_path, 'wb') as f:\n",
        "                        f.write(content)\n",
        "                    print(f\"Downloaded and validated {save_path}\")\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    print(f\"Invalid image content at {url}: {e}\")\n",
        "                    return False\n",
        "            print(f\"Failed to download {url}: Status {response.status}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Function to search images with Serper API\n",
        "def search_images(query, site, image_type, domain):\n",
        "    try:\n",
        "        payload = {\"q\": query, \"gl\": \"us\", \"hl\": \"en\", \"type\": \"images\", \"num\": 5}\n",
        "        response = requests.post(SERPER_ENDPOINT, headers=HEADERS, json=payload)\n",
        "        response.raise_for_status()\n",
        "        images = response.json().get(\"images\", [])\n",
        "        print(f\"Found {len(images)} images for query: {query}\")\n",
        "        return [\n",
        "            {\"url\": item[\"imageUrl\"], \"site\": site, \"type\": image_type, \"domain\": domain}\n",
        "            for item in images if \"imageUrl\" in item and item[\"imageUrl\"].endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Serper error for {query}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Async function to crawl a website with Crawl4AI\n",
        "async def crawl_website(url, site_name, image_type, domain):\n",
        "    results = []\n",
        "    try:\n",
        "        async with AsyncWebCrawler() as crawler:\n",
        "            result = await crawler.arun(url=url, css_selector=\"img[src*='photo']\")\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled {url}\")\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    for idx, img_url in enumerate(result.extracted_content[:3]):\n",
        "                        if not img_url.startswith('http'):\n",
        "                            img_url = f\"https://{site_name}.com{img_url}\"\n",
        "                        if not img_url.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                            continue\n",
        "                        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                        filename = f\"{site_name}_{image_type}_{timestamp}_{idx}.jpg\"\n",
        "                        save_path = os.path.join(UPLOADS_DIR, filename)\n",
        "                        if await download_image(session, img_url, save_path):\n",
        "                            results.append({\n",
        "                                'site': site_name,\n",
        "                                'type': image_type,\n",
        "                                'url': img_url,\n",
        "                                'local_path': save_path,\n",
        "                                'domain': domain\n",
        "                            })\n",
        "            else:\n",
        "                print(f\"Failed to crawl {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling {url}: {e}\")\n",
        "    return results\n",
        "\n",
        "# Function to save results to CSV\n",
        "def save_to_csv(data, csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['site', 'type', 'url', 'local_path', 'domain'])\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "    print(f\"Saved to {csv_file}\")\n",
        "\n",
        "# Cell 4: Main Script\n",
        "async def main():\n",
        "    # Search queries\n",
        "    queries = [\n",
        "        {\"type\": \"base\", \"query\": \"urban background site:unsplash.com\", \"site\": \"unsplash\", \"domain\": \"urban\"},\n",
        "        {\"type\": \"base\", \"query\": \"modern office site:pexels.com\", \"site\": \"pexels\", \"domain\": \"office\"},\n",
        "        {\"type\": \"secondary\", \"query\": \"denim jacket isolated site:pexels.com\", \"site\": \"pexels\", \"domain\": \"clothing\"},\n",
        "        {\"type\": \"secondary\", \"query\": \"smartphone isolated site:unsplash.com\", \"site\": \"unsplash\", \"domain\": \"electronics\"}\n",
        "    ]\n",
        "\n",
        "    # Step 1: Search with Serper API\n",
        "    base_results = []\n",
        "    secondary_results = []\n",
        "    serper_results = []\n",
        "    for q in queries:\n",
        "        images = search_images(q[\"query\"], q[\"site\"], q[\"type\"], q[\"domain\"])\n",
        "        serper_results.extend(images)\n",
        "\n",
        "    # Step 2: Download Serper images\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for idx, item in enumerate(serper_results):\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"{item['site']}_{item['type']}_{timestamp}_{idx}.jpg\"\n",
        "            save_path = os.path.join(UPLOADS_DIR, filename)\n",
        "            if await download_image(session, item['url'], save_path):\n",
        "                result = {\n",
        "                    'site': item['site'],\n",
        "                    'type': item['type'],\n",
        "                    'url': item['url'],\n",
        "                    'local_path': save_path,\n",
        "                    'domain': item['domain']\n",
        "                }\n",
        "                if item['type'] == \"base\" and len(base_results) < 10:\n",
        "                    base_results.append(result)\n",
        "                elif item['type'] == \"secondary\" and len(secondary_results) < 10:\n",
        "                    secondary_results.append(result)\n",
        "\n",
        "    # Step 3: Crawl with Crawl4AI if needed\n",
        "    if len(base_results) < 10 or len(secondary_results) < 10:\n",
        "        crawl_tasks = [\n",
        "            crawl_website(\"https://unsplash.com/s/photos/background\", \"unsplash\", \"base\", \"urban\"),\n",
        "            crawl_website(\"https://pexels.com/search/background/\", \"pexels\", \"base\", \"office\"),\n",
        "            crawl_website(\"https://pexels.com/search/product/\", \"pexels\", \"secondary\", \"clothing\"),\n",
        "            crawl_website(\"https://unsplash.com/s/photos/product\", \"unsplash\", \"secondary\", \"electronics\")\n",
        "        ]\n",
        "        for task_results in await asyncio.gather(*crawl_tasks):\n",
        "            for result in task_results:\n",
        "                if result['type'] == \"base\" and len(base_results) < 10:\n",
        "                    base_results.append(result)\n",
        "                elif result['type'] == \"secondary\" and len(secondary_results) < 10:\n",
        "                    secondary_results.append(result)\n",
        "\n",
        "    # Step 4: Save results to two CSV files\n",
        "    if base_results:\n",
        "        save_to_csv(base_results, BASE_CSV_FILE)\n",
        "    if secondary_results:\n",
        "        save_to_csv(secondary_results, SECONDARY_CSV_FILE)\n",
        "    print(f\"Collected {len(base_results)} base images and {len(secondary_results)} secondary images\")\n",
        "\n",
        "# Run the main function\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw8XwJolju1E",
        "outputId": "3da14a7a-ad08-433f-c076-00bb13b94509"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serper error for urban background site:unsplash.com: 404 Client Error: Not Found for url: https://api.serper.dev/images\n",
            "Serper error for modern office site:pexels.com: 404 Client Error: Not Found for url: https://api.serper.dev/images\n",
            "Serper error for denim jacket isolated site:pexels.com: 404 Client Error: Not Found for url: https://api.serper.dev/images\n",
            "Serper error for smartphone isolated site:unsplash.com: 404 Client Error: Not Found for url: https://api.serper.dev/images\n",
            "Error crawling https://unsplash.com/s/photos/background: BrowserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium-1169/chrome-linux/chrome\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "║ Looks like Playwright was just installed or updated.       ║\n",
            "║ Please run the following command to download new browsers: ║\n",
            "║                                                            ║\n",
            "║     playwright install                                     ║\n",
            "║                                                            ║\n",
            "║ <3 Playwright Team                                         ║\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "Error crawling https://unsplash.com/s/photos/product: BrowserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium-1169/chrome-linux/chrome\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "║ Looks like Playwright was just installed or updated.       ║\n",
            "║ Please run the following command to download new browsers: ║\n",
            "║                                                            ║\n",
            "║     playwright install                                     ║\n",
            "║                                                            ║\n",
            "║ <3 Playwright Team                                         ║\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "Error crawling https://pexels.com/search/background/: BrowserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium-1169/chrome-linux/chrome\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "║ Looks like Playwright was just installed or updated.       ║\n",
            "║ Please run the following command to download new browsers: ║\n",
            "║                                                            ║\n",
            "║     playwright install                                     ║\n",
            "║                                                            ║\n",
            "║ <3 Playwright Team                                         ║\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "Error crawling https://pexels.com/search/product/: BrowserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium-1169/chrome-linux/chrome\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "║ Looks like Playwright was just installed or updated.       ║\n",
            "║ Please run the following command to download new browsers: ║\n",
            "║                                                            ║\n",
            "║     playwright install                                     ║\n",
            "║                                                            ║\n",
            "║ <3 Playwright Team                                         ║\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "Collected 0 base images and 0 secondary images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yInLrxqDlSmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}