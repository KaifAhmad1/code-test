{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfMuF3fI9aq3ADDlLwRjQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "outputId": "c0288022-ade1-470d-e189-78e39d24a05d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU kaggle pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import tarfile\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PublicDeepFakeDownloader:\n",
        "    def __init__(self, base_dir=\"./deepfake_test_data\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.setup_logging()\n",
        "\n",
        "        # Configure session with robust retry strategy\n",
        "        self.session = self.create_robust_session()\n",
        "\n",
        "        # Only publicly available datasets\n",
        "        self.sources = {\n",
        "            'images': {\n",
        "                'Deepfake Detection Challenge Sample': {\n",
        "                    'url': 'https://github.com/selimsef/dfdc_deepfake_challenge/releases/download/0.0.1/example_videos.zip',\n",
        "                    'description': 'Sample videos from the DFDC dataset'\n",
        "                },\n",
        "                'FaceForensics++ Sample': {\n",
        "                    'url': 'https://github.com/ondyari/FaceForensics/blob/master/dataset/sample_videos.zip?raw=true',\n",
        "                    'description': 'Sample videos from FaceForensics++'\n",
        "                }\n",
        "            },\n",
        "            'videos': {\n",
        "                'Celeb-DF Sample': {\n",
        "                    'url': 'https://github.com/yuezunli/celeb-deepfakeforensics/blob/master/sample_videos.zip?raw=true',\n",
        "                    'description': 'Sample videos from Celeb-DF dataset'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Configure logging with detailed formatting\"\"\"\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(self.base_dir / 'download.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def create_robust_session(self):\n",
        "        \"\"\"Create a session with comprehensive retry strategy\"\"\"\n",
        "        session = requests.Session()\n",
        "        retries = Retry(\n",
        "            total=3,\n",
        "            backoff_factor=0.5,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "        )\n",
        "        session.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "        return session\n",
        "\n",
        "    def check_url_availability(self, url):\n",
        "        \"\"\"Verify if URL is accessible\"\"\"\n",
        "        try:\n",
        "            response = self.session.head(url, timeout=10)\n",
        "            return response.status_code == 200\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"URL check failed for {url}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def download_file(self, url, dest_path, desc=\"\"):\n",
        "        \"\"\"Download file with progress tracking\"\"\"\n",
        "        try:\n",
        "            # First check if URL is accessible\n",
        "            if not self.check_url_availability(url):\n",
        "                self.logger.error(f\"URL not accessible: {url}\")\n",
        "                return False\n",
        "\n",
        "            response = self.session.get(url, stream=True, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            temp_path = dest_path.with_suffix('.temp')\n",
        "            with open(temp_path, 'wb') as f, tqdm(\n",
        "                desc=desc,\n",
        "                total=total_size,\n",
        "                unit='iB',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    size = f.write(chunk)\n",
        "                    pbar.update(size)\n",
        "\n",
        "            # Move temporary file to final destination\n",
        "            temp_path.rename(dest_path)\n",
        "\n",
        "            # Extract if it's a compressed file\n",
        "            if dest_path.suffix in ['.zip', '.tar', '.gz']:\n",
        "                self.extract_archive(dest_path)\n",
        "\n",
        "            self.logger.info(f\"Successfully downloaded: {dest_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error downloading {url}: {str(e)}\")\n",
        "            if hasattr(e, 'response'):\n",
        "                self.logger.error(f\"Response status code: {e.response.status_code}\")\n",
        "            return False\n",
        "\n",
        "    def extract_archive(self, archive_path):\n",
        "        \"\"\"Extract downloaded archives\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Extracting {archive_path}\")\n",
        "            extract_path = archive_path.parent / archive_path.stem\n",
        "\n",
        "            if archive_path.suffix == '.zip':\n",
        "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_path)\n",
        "            elif archive_path.suffix in ['.tar', '.gz']:\n",
        "                with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
        "                    tar_ref.extractall(extract_path)\n",
        "\n",
        "            # Remove the archive after successful extraction\n",
        "            archive_path.unlink()\n",
        "            self.logger.info(f\"Successfully extracted to {extract_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting {archive_path}: {str(e)}\")\n",
        "\n",
        "    def download_all(self, skip_existing=True):\n",
        "        \"\"\"Download all public datasets\"\"\"\n",
        "        summary = {media_type: {'success': 0, 'failed': 0, 'skipped': 0}\n",
        "                  for media_type in self.sources.keys()}\n",
        "\n",
        "        for media_type, datasets in self.sources.items():\n",
        "            self.logger.info(f\"\\nProcessing {media_type} datasets...\")\n",
        "\n",
        "            for dataset_name, info in datasets.items():\n",
        "                dest_path = self.base_dir / media_type / f\"{dataset_name}.zip\"\n",
        "\n",
        "                if skip_existing and dest_path.exists():\n",
        "                    self.logger.info(f\"Skipping existing dataset: {dataset_name}\")\n",
        "                    summary[media_type]['skipped'] += 1\n",
        "                    continue\n",
        "\n",
        "                success = self.download_file(\n",
        "                    info['url'],\n",
        "                    dest_path,\n",
        "                    f\"Downloading {dataset_name}\"\n",
        "                )\n",
        "\n",
        "                if success:\n",
        "                    summary[media_type]['success'] += 1\n",
        "                else:\n",
        "                    summary[media_type]['failed'] += 1\n",
        "\n",
        "        self.save_summary(summary)\n",
        "        return summary\n",
        "\n",
        "    def save_summary(self, summary):\n",
        "        \"\"\"Save download summary to JSON\"\"\"\n",
        "        summary_path = self.base_dir / 'download_summary.json'\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'summary': summary\n",
        "            }, f, indent=2)\n",
        "\n",
        "    def generate_report(self, summary):\n",
        "        \"\"\"Generate detailed download report\"\"\"\n",
        "        report = \"Public DeepFake Test Data Download Summary\\n\"\n",
        "        report += \"=\" * 40 + \"\\n\\n\"\n",
        "\n",
        "        total_success = 0\n",
        "        total_failed = 0\n",
        "        total_skipped = 0\n",
        "\n",
        "        for media_type, counts in summary.items():\n",
        "            success = counts['success']\n",
        "            failed = counts['failed']\n",
        "            skipped = counts['skipped']\n",
        "            total = success + failed + skipped\n",
        "\n",
        "            report += f\"{media_type.title()}:\\n\"\n",
        "            report += f\"  - Successfully downloaded: {success}\\n\"\n",
        "            report += f\"  - Failed downloads: {failed}\\n\"\n",
        "            report += f\"  - Skipped (already exists): {skipped}\\n\"\n",
        "            report += f\"  - Total datasets: {total}\\n\\n\"\n",
        "\n",
        "            total_success += success\n",
        "            total_failed += failed\n",
        "            total_skipped += skipped\n",
        "\n",
        "        report += f\"Overall Statistics:\\n\"\n",
        "        report += f\"  - Total successful downloads: {total_success}\\n\"\n",
        "        report += f\"  - Total failed downloads: {total_failed}\\n\"\n",
        "        report += f\"  - Total skipped: {total_skipped}\\n\"\n",
        "        if total_success + total_failed > 0:\n",
        "            success_rate = (total_success/(total_success+total_failed)*100)\n",
        "            report += f\"  - Success rate: {success_rate:.1f}%\\n\\n\"\n",
        "\n",
        "        report += f\"Storage location: {self.base_dir}\\n\"\n",
        "        report += f\"Detailed logs available at: {self.base_dir}/download.log\"\n",
        "\n",
        "        return report"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    downloader = DeepFakeDownloader()\n",
        "    print(\"Starting deepfake test data download...\")\n",
        "    summary = downloader.download_all()\n",
        "    print(\"\\n\" + downloader.generate_report(summary))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8f0_v4lTMwAe",
        "outputId": "cbb1f099-6fdd-4d2b-eac8-a908b074195a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting deepfake test data download...\n",
            "\n",
            "Downloading images test sets...\n",
            "Error downloading https://github.com/yuezunli/celeb-deepfakeforensics/raw/master/test_release.zip: 404 Client Error: Not Found for url: https://github.com/yuezunli/celeb-deepfakeforensics/raw/master/test_release.zip\n",
            "Error downloading https://dfdc-preview-test.s3.amazonaws.com/dfdc_test_set.zip: 404 Client Error: Not Found for url: https://dfdc-preview-test.s3.amazonaws.com/dfdc_test_set.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)'))': /api/get/test_sample.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading https://github.com/ondyari/FaceForensics/raw/master/dataset/test_set.zip: 404 Client Error: Not Found for url: https://github.com/ondyari/FaceForensics/raw/master/dataset/test_set.zip\n",
            "\n",
            "Downloading videos test sets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)'))': /api/get/test_sample.zip\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)'))': /api/get/test_sample.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading https://dfdc.ai/api/get/test_sample.zip: HTTPSConnectionPool(host='dfdc.ai', port=443): Max retries exceeded with url: /api/get/test_sample.zip (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e4d30132a10>: Failed to resolve 'fakeavceleb.com' ([Errno -2] Name or service not known)\")': /download/test_set.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading https://github.com/EndlessSora/DeeperForensics-1.0/raw/master/test.zip: 404 Client Error: Not Found for url: https://github.com/EndlessSora/DeeperForensics-1.0/raw/master/test.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e4d301309d0>: Failed to resolve 'fakeavceleb.com' ([Errno -2] Name or service not known)\")': /download/test_set.zip\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e4d30b15b90>: Failed to resolve 'fakeavceleb.com' ([Errno -2] Name or service not known)\")': /download/test_set.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading https://fakeavceleb.com/download/test_set.zip: HTTPSConnectionPool(host='fakeavceleb.com', port=443): Max retries exceeded with url: /download/test_set.zip (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e4d315058d0>: Failed to resolve 'fakeavceleb.com' ([Errno -2] Name or service not known)\"))\n",
            "\n",
            "Downloading audio test sets...\n",
            "Error downloading https://datashare.ed.ac.uk/bitstream/handle/10283/3336/LA_test.zip: 404 Client Error: 404 for url: https://datashare.ed.ac.uk/bitstream/handle/10283/3336/LA_test.zip\n",
            "Error downloading https://github.com/fakevoice/dataset/raw/main/test_set.zip: 404 Client Error: Not Found for url: https://github.com/fakevoice/dataset/raw/main/test_set.zip\n",
            "\n",
            "DeepFake Test Data Download Summary\n",
            "===================================\n",
            "\n",
            "Images: 0 test sets\n",
            "Videos: 0 test sets\n",
            "Audio: 0 test sets\n",
            "\n",
            "Total test sets downloaded: 0\n",
            "Storage location: deepfake_test_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjQa9UMpRfxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}