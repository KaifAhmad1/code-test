{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c38ef4-9c44-4d7a-ab7b-a7dfa9090907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q datasets pandas pillow tqdm huggingface_hub decord\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import warnings\n",
        "import hashlib\n",
        "import requests\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDataCollector:\n",
        "    def __init__(self, base_dir: str, max_samples: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize the deepfake data collector.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Base directory for storing downloaded data\n",
        "            max_samples: Maximum number of samples per category (real/fake)\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.metadata = []\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validate downloaded media files.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                return False\n",
        "\n",
        "            if media_type == 'video':\n",
        "                # Add error handling for video validation\n",
        "                try:\n",
        "                    with VideoReader(str(file_path)) as vr:\n",
        "                        frame = vr[0]\n",
        "                        return frame is not None\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Video validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            elif media_type == 'image':\n",
        "                from PIL import Image\n",
        "                try:\n",
        "                    with Image.open(file_path) as img:\n",
        "                        img.verify()\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Image validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            elif media_type == 'audio':\n",
        "                import wave\n",
        "                try:\n",
        "                    with wave.open(str(file_path), 'rb') as audio:\n",
        "                        return audio.getnframes() > 0\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Audio validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Validation failed for {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def download_file(self, url: str, save_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Download a file with proper error handling and verification.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Add timeout and headers\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(url, stream=True, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Create parent directories if they don't exist\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Save the file with progress bar\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            block_size = 8192\n",
        "\n",
        "            with open(save_path, 'wb') as f:\n",
        "                with tqdm(total=total_size, unit='iB', unit_scale=True) as pbar:\n",
        "                    for chunk in response.iter_content(chunk_size=block_size):\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Download failed for {url}: {str(e)}\")\n",
        "            if save_path.exists():\n",
        "                save_path.unlink()  # Delete partial downloads\n",
        "            return False\n",
        "\n",
        "    def process_dataset(self, dataset_name: str, category_name: str, file_ext: str, key: str) -> None:\n",
        "        \"\"\"\n",
        "        Process and download a specific dataset.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing {category_name} dataset: {dataset_name}\")\n",
        "\n",
        "        try:\n",
        "            # Add streaming=True and split=\"train\" options\n",
        "            dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "            save_dir = self.base_dir / category_name\n",
        "\n",
        "            for category in ['real', 'fake']:\n",
        "                category_dir = save_dir / category\n",
        "                category_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                sample_count = 0\n",
        "                for sample in dataset:\n",
        "                    try:\n",
        "                        if sample['label'] == (1 if category == 'fake' else 0):\n",
        "                            if sample_count >= self.max_samples:\n",
        "                                break\n",
        "\n",
        "                            file_path = category_dir / f\"{category}_{sample_count:03d}.{file_ext}\"\n",
        "\n",
        "                            success = False\n",
        "                            if isinstance(sample.get(key), dict) and 'bytes' in sample[key]:\n",
        "                                with open(file_path, 'wb') as f:\n",
        "                                    f.write(sample[key]['bytes'])\n",
        "                                success = True\n",
        "                            elif isinstance(sample.get(key), str):\n",
        "                                success = self.download_file(sample[key], file_path)\n",
        "                            else:\n",
        "                                logger.warning(f\"Unsupported data format for key '{key}' in sample\")\n",
        "                                continue\n",
        "\n",
        "                            if success and self.validate_media_file(file_path, category_name):\n",
        "                                self.metadata.append({\n",
        "                                    'modality': category_name,\n",
        "                                    'category': category,\n",
        "                                    'filename': file_path.name,\n",
        "                                    'file_path': str(file_path),\n",
        "                                    'source_dataset': dataset_name,\n",
        "                                    'checksum': self._get_file_hash(file_path)\n",
        "                                })\n",
        "                                sample_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Error processing sample: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing dataset {dataset_name}: {str(e)}\")\n",
        "\n",
        "    def _get_file_hash(self, file_path: Path) -> str:\n",
        "        \"\"\"Calculate SHA-256 hash of a file.\"\"\"\n",
        "        sha256_hash = hashlib.sha256()\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "                sha256_hash.update(byte_block)\n",
        "        return sha256_hash.hexdigest()\n",
        "\n",
        "    def save_metadata(self) -> None:\n",
        "        \"\"\"Save metadata to CSV file.\"\"\"\n",
        "        if self.metadata:\n",
        "            metadata_path = self.base_dir / \"metadata.csv\"\n",
        "            pd.DataFrame(self.metadata).to_csv(metadata_path, index=False)\n",
        "            logger.info(f\"Metadata saved to {metadata_path}\")\n",
        "        else:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "\n",
        "    def collect_datasets(self, dataset_configs: List[Dict]) -> None:\n",
        "        \"\"\"\n",
        "        Collect multiple datasets based on configuration.\n",
        "        \"\"\"\n",
        "        for config in dataset_configs:\n",
        "            self.process_dataset(**config)\n",
        "        self.save_metadata()\n",
        "\n",
        "def main():\n",
        "    # Install required dependencies first\n",
        "    !pip install -q datasets pandas pillow tqdm huggingface_hub decord\n",
        "    !apt-get install -y ffmpeg\n",
        "\n",
        "    # Dataset configurations - Updated with more reliable datasets\n",
        "    dataset_configs = [\n",
        "        {\n",
        "            \"dataset_name\": \"arnabpai/dfdc-videos\",  # Alternative to mkhLlamaLearn/dfdc\n",
        "            \"category_name\": \"videos\",\n",
        "            \"file_ext\": \"mp4\",\n",
        "            \"key\": \"video\"\n",
        "        },\n",
        "        {\n",
        "            \"dataset_name\": \"jonathansmith/image-deepfakes\",  # Alternative to taohu/faceforensics_h5\n",
        "            \"category_name\": \"images\",\n",
        "            \"file_ext\": \"jpg\",\n",
        "            \"key\": \"image\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Initialize collector with Colab-specific path\n",
        "    collector = DeepfakeDataCollector(\n",
        "        base_dir=\"/content/deepfake_dataset\",\n",
        "        max_samples=20\n",
        "    )\n",
        "\n",
        "    # Collect datasets\n",
        "    collector.collect_datasets(dataset_configs)"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e58ea0-8752-4d8b-a5ca-ed5523ac989b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Error processing dataset arnabpai/dfdc-videos: Dataset 'arnabpai/dfdc-videos' doesn't exist on the Hub or cannot be accessed.\n",
            "ERROR:__main__:Error processing dataset jonathansmith/image-deepfakes: Dataset 'jonathansmith/image-deepfakes' doesn't exist on the Hub or cannot be accessed.\n",
            "WARNING:__main__:No metadata to save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}