{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "outputId": "3bdabbf4-e2b1-48e4-90be-511592e9137f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.9/182.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU yt-dlp aiohttp decord pandas pillow soundfile tqdm crawl4ai[all] nest_asyncio playwright"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and apply nest_asyncio (useful in notebooks)\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Third-party libraries\n",
        "import pandas as pd\n",
        "import yt_dlp\n",
        "from tqdm.asyncio import tqdm\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# crawl4ai components\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "\n",
        "#########################################\n",
        "# Setup Directories and Global Variables\n",
        "#########################################\n",
        "BASE_DIR = os.getcwd()\n",
        "SCRAPED_URLS_DIR = os.path.join(BASE_DIR, 'scraped_urls')\n",
        "IMAGES_DIR      = os.path.join(BASE_DIR, 'deepfake_images')\n",
        "VIDEOS_DIR      = os.path.join(BASE_DIR, 'deepfake_videos')\n",
        "AUDIOS_DIR      = os.path.join(BASE_DIR, 'deepfake_audios')\n",
        "METADATA_FILE   = os.path.join(BASE_DIR, 'download_metadata.json')\n",
        "\n",
        "for d in [SCRAPED_URLS_DIR, IMAGES_DIR, VIDEOS_DIR, AUDIOS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Global list to hold download metadata\n",
        "download_metadata = []"
      ],
      "metadata": {
        "id": "QsB0SfhX5hky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 1. Define URL Scraping Functions Using crawl4ai\n",
        "#########################################\n",
        "async def scrape_media_urls(modality, page_url, parse_function, output_filename):\n",
        "    \"\"\"\n",
        "    Scrape media URLs from a given page URL using crawl4ai's AsyncWebCrawler and a CSS extraction strategy.\n",
        "    Saves the scraped URLs as a CSV file and returns a list of unique URLs.\n",
        "    \"\"\"\n",
        "    print(f\"[{modality.upper()}] Scraping URLs from {page_url}\")\n",
        "\n",
        "    # Define the schema for extraction. Adjust this schema based on the actual structure expected.\n",
        "    schema = {\"extracted\": list}\n",
        "\n",
        "    # Pass both parse_function and schema to the extraction strategy.\n",
        "    extraction_strategy = JsonCssExtractionStrategy(\n",
        "        parse_function=parse_function,\n",
        "        schema=schema\n",
        "    )\n",
        "\n",
        "    crawler = AsyncWebCrawler(start_urls=[page_url], extraction_strategy=extraction_strategy)\n",
        "\n",
        "    try:\n",
        "        # Run the crawler. The API is assumed to return a list of results.\n",
        "        results = await crawler.run()\n",
        "    except Exception as e:\n",
        "        print(f\"[{modality.upper()}] Error during crawling {page_url}: {e}\")\n",
        "        results = []\n",
        "\n",
        "    scraped_urls = []\n",
        "    # Assuming each result is a dict that contains the key \"extracted\" with the URLs.\n",
        "    for result in results:\n",
        "        scraped_urls.extend(result.get('extracted', []))\n",
        "\n",
        "    # Remove duplicates and empty values\n",
        "    scraped_urls = list({url for url in scraped_urls if url})\n",
        "\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame({'url': scraped_urls})\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"[{modality.upper()}] Saved {len(scraped_urls)} URLs to {output_filename}\")\n",
        "    return scraped_urls\n",
        "\n",
        "# Define parse functions (adjust CSS selectors based on the actual page structure)\n",
        "def parse_image_links(response):\n",
        "    # Example: extract src attribute from images with class \"deepfake-img\"\n",
        "    return response.css(\"img.deepfake-img::attr(src)\").getall()\n",
        "\n",
        "def parse_video_links(response):\n",
        "    # Example: extract href attribute from anchors with class \"deepfake-video\"\n",
        "    return response.css(\"a.deepfake-video::attr(href)\").getall()\n",
        "\n",
        "def parse_audio_links(response):\n",
        "    # Example: extract src attribute from audio tags with class \"deepfake-audio\"\n",
        "    return response.css(\"audio.deepfake-audio::attr(src)\").getall()"
      ],
      "metadata": {
        "id": "C7HQW_cI_UxZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 2. Define Functions to Load URLs (with Fallbacks)\n",
        "#########################################\n",
        "def load_scraped_urls(modality):\n",
        "    filepath = os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\")\n",
        "    if os.path.exists(filepath):\n",
        "        df = pd.read_csv(filepath)\n",
        "        urls = df['url'].dropna().tolist()\n",
        "        print(f\"[{modality.upper()}] Loaded {len(urls)} URLs from {filepath}\")\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"[{modality.upper()}] File {filepath} not found. Using fallback method.\")\n",
        "        return []\n",
        "\n",
        "# Fallback generators if scraping fails\n",
        "def fallback_image_urls(n=20):\n",
        "    # For images, use a site that returns a new image every time\n",
        "    return [\"https://thispersondoesnotexist.com/image\"] * n\n",
        "\n",
        "def fallback_video_urls(n=20):\n",
        "    # Leave empty so that we trigger the yt-dlp search fallback later\n",
        "    return []\n",
        "\n",
        "def fallback_audio_urls(n=20):\n",
        "    # Leave empty so that we can extract using Playwright later\n",
        "    return []"
      ],
      "metadata": {
        "id": "mH95w7UJ_ZHN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 3. Define Asynchronous Download Functions with Retries & Metadata Logging\n",
        "#########################################\n",
        "async def download_file(session, url, filename, modality, retries=3):\n",
        "    \"\"\"\n",
        "    Download a file (image or audio) asynchronously using aiohttp with simple retry logic.\n",
        "    Saves the file to disk and logs download metadata.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            async with session.get(url) as resp:\n",
        "                if resp.status == 200:\n",
        "                    content = await resp.read()\n",
        "                    with open(filename, \"wb\") as f:\n",
        "                        f.write(content)\n",
        "                    print(f\"[{modality.upper()}] Downloaded: {filename}\")\n",
        "\n",
        "                    # Record metadata\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": modality,\n",
        "                        \"filename\": filename,\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    return\n",
        "                else:\n",
        "                    print(f\"[{modality.upper()}] Attempt {attempt}: Failed to download {url} (Status {resp.status})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{modality.upper()}] Attempt {attempt}: Exception downloading {url}: {e}\")\n",
        "        await asyncio.sleep(1)  # Wait a bit before retrying\n",
        "    print(f\"[{modality.upper()}] Failed to download {url} after {retries} attempts.\")\n",
        "\n",
        "async def download_images(urls):\n",
        "    print(\"[IMAGE] Starting asynchronous image downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(IMAGES_DIR, f\"image_{idx+1:02d}.jpg\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"image\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "async def download_audios(urls):\n",
        "    if not urls:\n",
        "        print(\"[AUDIO] No audio URLs provided for download.\")\n",
        "        return\n",
        "    print(\"[AUDIO] Starting asynchronous audio downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(AUDIOS_DIR, f\"audio_{idx+1:02d}.mp3\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"audio\"))\n",
        "        await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "J5m4qrVW_lbg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 4. Define Synchronous Video Download Function Using yt-dlp\n",
        "#########################################\n",
        "def download_videos(video_urls, num_videos=20):\n",
        "    \"\"\"\n",
        "    Download videos using yt-dlp. If no URLs are provided, use yt-dlp's search fallback.\n",
        "    \"\"\"\n",
        "    if video_urls:\n",
        "        download_list = video_urls[:num_videos]\n",
        "        print(\"[VIDEO] Downloading videos from scraped URLs...\")\n",
        "        for url in download_list:\n",
        "            ydl_opts = {\n",
        "                'format': 'bestvideo+bestaudio/best',\n",
        "                'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "                'merge_output_format': 'mp4',\n",
        "                'quiet': True,\n",
        "                'no_warnings': True,\n",
        "            }\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                try:\n",
        "                    ydl.download([url])\n",
        "                    # Record a basic metadata record\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": \"video\",\n",
        "                        \"filename\": url,  # filename extraction would require additional parsing\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    print(f\"[VIDEO] Downloaded: {url}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[VIDEO] Error downloading {url}: {e}\")\n",
        "    else:\n",
        "        # Fallback using yt-dlp search\n",
        "        search_query = \"ytsearch20:deepfake compilation\"\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',\n",
        "            'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "            'merge_output_format': 'mp4',\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "        }\n",
        "        print(\"[VIDEO] No scraped video URLs found; using yt-dlp search fallback...\")\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            try:\n",
        "                ydl.download([search_query])\n",
        "                print(\"[VIDEO] Video downloads (fallback search) complete.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[VIDEO] Error with yt-dlp search fallback: {e}\")"
      ],
      "metadata": {
        "id": "GSx5aXRrAGLS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 5. Define Audio Extraction Using Playwright as a Fallback\n",
        "#########################################\n",
        "async def extract_audio_links():\n",
        "    \"\"\"\n",
        "    Use Playwright to extract audio URLs from a dynamic page (as a fallback if no scraped audio URLs exist).\n",
        "    Adjust the CSS selectors as needed.\n",
        "    \"\"\"\n",
        "    print(\"[AUDIO] Extracting audio URLs via Playwright fallback...\")\n",
        "    extracted_links = []\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context()\n",
        "            page = await context.new_page()\n",
        "            # Example URL; adjust to a real page with audio samples.\n",
        "            url = \"https://uberduck.ai/explore\"\n",
        "            await page.goto(url)\n",
        "            # Wait for dynamic content to load (adjust timeout as needed)\n",
        "            await page.wait_for_timeout(5000)\n",
        "\n",
        "            # Extract audio elements; adjust the selector as needed.\n",
        "            audio_elements = await page.query_selector_all(\"audio\")\n",
        "            for el in audio_elements:\n",
        "                src = await el.get_attribute(\"src\")\n",
        "                if src:\n",
        "                    extracted_links.append(src)\n",
        "            await browser.close()\n",
        "            print(f\"[AUDIO] Extracted {len(extracted_links)} audio URLs via Playwright.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[AUDIO] Error during Playwright audio extraction: {e}\")\n",
        "    return extracted_links"
      ],
      "metadata": {
        "id": "L8wK9Hu0AKxq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 6. Save Download Metadata to a JSON File\n",
        "#########################################\n",
        "def save_download_metadata(metadata_list, filename=METADATA_FILE):\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(metadata_list, f, indent=4)\n",
        "        print(f\"Download metadata saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving download metadata: {e}\")"
      ],
      "metadata": {
        "id": "DGOb9We5AOSZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 7. Main Pipeline\n",
        "#########################################\n",
        "async def main_pipeline():\n",
        "    # --- Configuration: Define the source pages for each modality ---\n",
        "    pages = {\n",
        "        \"images\": \"https://deepfakesampleimages.com/gallery\",   # Change to actual URL\n",
        "        \"videos\": \"https://deepfakesamplevideos.com/collection\",  # Change to actual URL\n",
        "        \"audio\":  \"https://deepfakesampleaudio.com/samples\"       # Change to actual URL\n",
        "    }\n",
        "\n",
        "    # --- Scrape URLs Using crawl4ai ---\n",
        "    image_urls = await scrape_media_urls(\"image\", pages[\"images\"], parse_image_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, \"image_urls.csv\"))\n",
        "    video_urls = await scrape_media_urls(\"video\", pages[\"videos\"], parse_video_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, \"video_urls.csv\"))\n",
        "    audio_urls = await scrape_media_urls(\"audio\", pages[\"audio\"], parse_audio_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, \"audio_urls.csv\"))\n",
        "\n",
        "    # --- Load URLs (if available); otherwise, fall back ---\n",
        "    if not image_urls:\n",
        "        image_urls = fallback_image_urls(20)\n",
        "    else:\n",
        "        image_urls = image_urls[:20]\n",
        "\n",
        "    if not video_urls:\n",
        "        video_urls = fallback_video_urls(20)\n",
        "    else:\n",
        "        video_urls = video_urls[:20]\n",
        "\n",
        "    if not audio_urls:\n",
        "        audio_urls = fallback_audio_urls(20)\n",
        "    else:\n",
        "        audio_urls = audio_urls[:20]\n",
        "\n",
        "    # --- Download Images Asynchronously ---\n",
        "    await download_images(image_urls)\n",
        "\n",
        "    # --- Download Videos Synchronously via yt-dlp ---\n",
        "    download_videos(video_urls, num_videos=20)\n",
        "\n",
        "    # --- Download Audios ---\n",
        "    # If audio_urls is empty, attempt extraction using Playwright fallback.\n",
        "    if not audio_urls:\n",
        "        audio_urls = await extract_audio_links()\n",
        "    await download_audios(audio_urls)\n",
        "\n",
        "    # --- Save Download Metadata ---\n",
        "    save_download_metadata(download_metadata)\n",
        "    print(\"All downloads complete. Check the respective directories for files.\")\n",
        "\n",
        "# Run the entire pipeline.\n",
        "if __name__ == '__main__':\n",
        "    asyncio.run(main_pipeline())"
      ],
      "metadata": {
        "id": "ZsjX5iYIAQ94",
        "outputId": "424ad649-b672-4c50-d325-3a7a9ff37cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMAGE] Scraping URLs from https://deepfakesampleimages.com/gallery\n",
            "[IMAGE] Error during crawling https://deepfakesampleimages.com/gallery: 'AsyncWebCrawler' object has no attribute 'run'\n",
            "[IMAGE] Saved 0 URLs to /content/scraped_urls/image_urls.csv\n",
            "[VIDEO] Scraping URLs from https://deepfakesamplevideos.com/collection\n",
            "[VIDEO] Error during crawling https://deepfakesamplevideos.com/collection: 'AsyncWebCrawler' object has no attribute 'run'\n",
            "[VIDEO] Saved 0 URLs to /content/scraped_urls/video_urls.csv\n",
            "[AUDIO] Scraping URLs from https://deepfakesampleaudio.com/samples\n",
            "[AUDIO] Error during crawling https://deepfakesampleaudio.com/samples: 'AsyncWebCrawler' object has no attribute 'run'\n",
            "[AUDIO] Saved 0 URLs to /content/scraped_urls/audio_urls.csv\n",
            "[IMAGE] Starting asynchronous image downloads...\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 1: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 2: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Attempt 3: Failed to download https://thispersondoesnotexist.com/image (Status 404)\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[IMAGE] Failed to download https://thispersondoesnotexist.com/image after 3 attempts.\n",
            "[VIDEO] No scraped video URLs found; using yt-dlp search fallback...\n",
            "[VIDEO] Video downloads (fallback search) complete.\n",
            "[AUDIO] Extracting audio URLs via Playwright fallback...\n",
            "[AUDIO] Error during Playwright audio extraction: BrowserType.launch: Executable doesn't exist at /root/.cache/ms-playwright/chromium_headless_shell-1148/chrome-linux/headless_shell\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "║ Looks like Playwright was just installed or updated.       ║\n",
            "║ Please run the following command to download new browsers: ║\n",
            "║                                                            ║\n",
            "║     playwright install                                     ║\n",
            "║                                                            ║\n",
            "║ <3 Playwright Team                                         ║\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "[AUDIO] No audio URLs provided for download.\n",
            "Download metadata saved to /content/download_metadata.json\n",
            "All downloads complete. Check the respective directories for files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YBhGDTrAUfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}