{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6673097-513d-4463-bacb-933906653b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.9/182.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU yt-dlp aiohttp decord pandas pillow soundfile tqdm crawl4ai\n",
        "!pip install -qU crawl4ai[all]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy"
      ],
      "metadata": {
        "id": "QsB0SfhX5hky"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from tqdm.asyncio import tqdm\n",
        "import yt_dlp\n",
        "from playwright.async_api import async_playwright\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# Setup Directories\n",
        "#########################################\n",
        "os.makedirs('scraped_urls', exist_ok=True)\n",
        "os.makedirs('deepfake_images', exist_ok=True)\n",
        "os.makedirs('deepfake_videos', exist_ok=True)\n",
        "os.makedirs('deepfake_audios', exist_ok=True)"
      ],
      "metadata": {
        "id": "EgMfhWd3eKv3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 1. Use Crawl4ai to Scrape Media URLs\n",
        "#########################################\n",
        "# Define source pages for each modality (example URLs - adjust as needed)\n",
        "pages = {\n",
        "    \"images\": \"https://deepfakesampleimages.com/gallery\",     # Example: a gallery page of deepfake images\n",
        "    \"videos\": \"https://deepfakesamplevideos.com/collection\",    # Example: a collection page of deepfake videos\n",
        "    \"audio\":  \"https://deepfakesampleaudio.com/samples\"          # Example: a page listing deepfake audio samples\n",
        "}"
      ],
      "metadata": {
        "id": "yI8RBANp6EqY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parse functions using CSS selectors (adjust selectors based on actual page structure)\n",
        "def parse_image_links(response):\n",
        "    # Extract image URLs from <img class=\"deepfake-img\" src=\"...\">\n",
        "    return response.css(\"img.deepfake-img::attr(src)\").getall()\n",
        "\n",
        "def parse_video_links(response):\n",
        "    # Extract video URLs from <a class=\"deepfake-video\" href=\"...\">\n",
        "    return response.css(\"a.deepfake-video::attr(href)\").getall()\n",
        "\n",
        "def parse_audio_links(response):\n",
        "    # Extract audio URLs from <audio class=\"deepfake-audio\" src=\"...\">\n",
        "    return response.css(\"audio.deepfake-audio::attr(src)\").getall()\n",
        "\n",
        "# Create Crawl4ai tasks for each modality\n",
        "tasks = [\n",
        "    Task(name=\"image_urls\", start_urls=[pages[\"images\"]], parse_function=parse_image_links),\n",
        "    Task(name=\"video_urls\", start_urls=[pages[\"videos\"]], parse_function=parse_video_links),\n",
        "    Task(name=\"audio_urls\", start_urls=[pages[\"audio\"]], parse_function=parse_audio_links)\n",
        "]\n",
        "\n",
        "print(\"Starting Crawl4ai to scrape media URLs...\")\n",
        "crawler = Crawler(tasks=tasks, output_dir=\"scraped_urls\")\n",
        "crawler.run()\n",
        "print(\"Crawl4ai scraping complete.\")"
      ],
      "metadata": {
        "id": "1vIX9xjk6IDf",
        "outputId": "bbd639c6-1e37-4788-c21a-8eb89743c823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Task' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bf6106f2b1d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create Crawl4ai tasks for each modality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m tasks = [\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image_urls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_image_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"video_urls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_video_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"audio_urls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_audio_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Task' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load scraped URLs from CSV files created by Crawl4ai.\n",
        "def load_scraped_urls(modality):\n",
        "    filepath = os.path.join(\"scraped_urls\", f\"{modality}_urls.csv\")\n",
        "    if os.path.exists(filepath):\n",
        "        df = pd.read_csv(filepath)\n",
        "        # Assuming the CSV has a column 'url'\n",
        "        return df['url'].tolist()\n",
        "    else:\n",
        "        print(f\"File {filepath} not found. Using fallback method.\")\n",
        "        return []\n",
        "\n",
        "# Load URLs for each modality\n",
        "image_urls = load_scraped_urls(\"image\")\n",
        "video_urls = load_scraped_urls(\"video\")\n",
        "audio_urls = load_scraped_urls(\"audio\")\n",
        "\n",
        "# Fallbacks if no URLs were scraped:\n",
        "if not image_urls:\n",
        "    # For images, repeatedly use ThisPersonDoesNotExist which returns a new image each call.\n",
        "    image_urls = [\"https://thispersondoesnotexist.com/image\"] * 20\n",
        "else:\n",
        "    image_urls = image_urls[:20]\n",
        "\n",
        "if not video_urls:\n",
        "    # For videos, we'll later use yt-dlp's search fallback.\n",
        "    video_urls = []\n",
        "if not audio_urls:\n",
        "    # For audio, we will later extract using Playwright.\n",
        "    audio_urls = []"
      ],
      "metadata": {
        "id": "GcAxBKXO6Nrx",
        "outputId": "953f71fa-fd87-4c58-f14c-c70b896382a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File scraped_urls/image_urls.csv not found. Using fallback method.\n",
            "File scraped_urls/video_urls.csv not found. Using fallback method.\n",
            "File scraped_urls/audio_urls.csv not found. Using fallback method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 2. Download Images Asynchronously using aiohttp and tqdm\n",
        "#########################################\n",
        "async def download_image(session, url, filename):\n",
        "    try:\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                content = await resp.read()\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(content)\n",
        "                print(f\"Downloaded image: {filename}\")\n",
        "            else:\n",
        "                print(f\"Failed to download image {url}: Status {resp.status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception while downloading image {url}: {e}\")\n",
        "\n",
        "async def download_images(urls):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(\"deepfake_images\", f\"image_{idx+1:02d}.jpg\")\n",
        "            tasks.append(download_image(session, url, filename))\n",
        "        await asyncio.gather(*tasks)\n"
      ],
      "metadata": {
        "id": "PYgEj6xO6TC8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 3. Download Videos Using yt-dlp (Synchronous)\n",
        "#########################################\n",
        "def download_videos(num_videos=20):\n",
        "    if video_urls:\n",
        "        # Download from scraped video URLs\n",
        "        download_list = video_urls[:num_videos]\n",
        "        print(\"Downloading videos from scraped URLs...\")\n",
        "        for url in download_list:\n",
        "            ydl_opts = {\n",
        "                'format': 'bestvideo+bestaudio/best',\n",
        "                'outtmpl': 'deepfake_videos/%(id)s.%(ext)s',\n",
        "                'merge_output_format': 'mp4',\n",
        "                'quiet': False,\n",
        "            }\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                try:\n",
        "                    ydl.download([url])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading video {url}: {e}\")\n",
        "    else:\n",
        "        # Fallback: use yt-dlp search to download 20 videos\n",
        "        search_query = f\"ytsearch20:deepfake compilation\"\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',\n",
        "            'outtmpl': 'deepfake_videos/%(id)s.%(ext)s',\n",
        "            'merge_output_format': 'mp4',\n",
        "            'quiet': False,\n",
        "        }\n",
        "        print(\"Downloading videos using yt-dlp search fallback...\")\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([search_query])\n",
        "    print(\"Video downloads complete.\")"
      ],
      "metadata": {
        "id": "G4ShiaWQ6XMv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 4. Download Audios Asynchronously using Playwright & aiohttp\n",
        "#########################################\n",
        "async def extract_audio_links():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context()\n",
        "        page = await context.new_page()\n",
        "        url = \"https://uberduck.ai/explore\"\n",
        "        await page.goto(url)\n",
        "        await page.wait_for_timeout(5000)  # wait for dynamic content to load\n",
        "\n",
        "        # Adjust selector according to actual structure – here we assume <audio class=\"sample-audio\">\n",
        "        audio_elements = await page.query_selector_all(\"audio.sample-audio\")\n",
        "        links = []\n",
        "        for elem in audio_elements:\n",
        "            src = await elem.get_attribute(\"src\")\n",
        "            if src:\n",
        "                links.append(src)\n",
        "        await browser.close()\n",
        "        print(f\"Extracted {len(links)} audio links from UberDuck.\")\n",
        "        return links[:20]\n",
        "\n",
        "async def download_audio(session, url, filename):\n",
        "    try:\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                content = await resp.read()\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(content)\n",
        "                print(f\"Downloaded audio: {filename}\")\n",
        "            else:\n",
        "                print(f\"Failed to download audio {url}: Status {resp.status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception while downloading audio {url}: {e}\")\n",
        "\n",
        "async def download_audios(urls):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(\"deepfake_audios\", f\"audio_{idx+1:02d}.mp3\")\n",
        "            tasks.append(download_audio(session, url, filename))\n",
        "        await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "GLOeNQPX6cex"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 5. Main Function to Run Entire Pipeline\n",
        "#########################################\n",
        "def main():\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    print(\"Starting image downloads...\")\n",
        "    loop.run_until_complete(download_images(image_urls))\n",
        "\n",
        "    print(\"Starting video downloads...\")\n",
        "    download_videos(20)\n",
        "\n",
        "    # For audio, if scraped audio URLs are empty, extract using Playwright\n",
        "    if not audio_urls:\n",
        "        print(\"No scraped audio URLs found; extracting using Playwright...\")\n",
        "        audio_urls_extracted = loop.run_until_complete(extract_audio_links())\n",
        "    else:\n",
        "        audio_urls_extracted = audio_urls[:20]\n",
        "\n",
        "    print(\"Starting audio downloads...\")\n",
        "    loop.run_until_complete(download_audios(audio_urls_extracted))\n",
        "\n",
        "    print(\"All downloads complete. Check the 'deepfake_images', 'deepfake_videos', and 'deepfake_audios' directories.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "znBmPRM06gjJ",
        "outputId": "8a3017b9-6c50-43cc-e3be-8a842acc2f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting image downloads...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "This event loop is already running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-df6d54247ba9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-df6d54247ba9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting image downloads...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting video downloads...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \"\"\"\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYPxK9GZ61PS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}