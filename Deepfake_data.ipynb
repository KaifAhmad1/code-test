{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d93ff6-1210-49b0-8a9e-e871c9244ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.2/702.2 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.9/182.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU yt-dlp aiohttp decord pandas pillow soundfile tqdm crawl4ai[all] nest_asyncio playwright telethon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import yt_dlp\n",
        "from tqdm.asyncio import tqdm\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# Import crawl4ai components (if available)\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "\n",
        "# Import Telethon for Telegram scraping\n",
        "from telethon import TelegramClient\n",
        "\n",
        "#########################################\n",
        "# Setup Directories and Global Variables\n",
        "#########################################\n",
        "BASE_DIR = os.getcwd()\n",
        "SCRAPED_URLS_DIR = os.path.join(BASE_DIR, 'scraped_urls')\n",
        "IMAGES_DIR      = os.path.join(BASE_DIR, 'deepfake_images')\n",
        "VIDEOS_DIR      = os.path.join(BASE_DIR, 'deepfake_videos')\n",
        "AUDIOS_DIR      = os.path.join(BASE_DIR, 'deepfake_audios')\n",
        "TEXT_DIR        = os.path.join(BASE_DIR, 'deepfake_texts')\n",
        "TELEGRAM_DIR    = os.path.join(BASE_DIR, 'telegram_messages')\n",
        "METADATA_FILE   = os.path.join(BASE_DIR, 'download_metadata.json')\n",
        "\n",
        "for d in [SCRAPED_URLS_DIR, IMAGES_DIR, VIDEOS_DIR, AUDIOS_DIR, TEXT_DIR, TELEGRAM_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "download_metadata = []  # Global metadata list\n",
        "\n",
        "#########################################\n",
        "# Helper: Ensure Playwright Browsers Are Installed\n",
        "#########################################\n",
        "def install_playwright_browsers():\n",
        "    try:\n",
        "        print(\"[SYSTEM] Installing Playwright browsers...\")\n",
        "        subprocess.run([\"playwright\", \"install\"], check=True)\n",
        "        print(\"[SYSTEM] Playwright browsers installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[SYSTEM] Error installing Playwright browsers: {e}\")"
      ],
      "metadata": {
        "id": "QsB0SfhX5hky"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 1. URL Scraping Functions using crawl4ai\n",
        "#########################################\n",
        "async def scrape_media_urls(modality, page_url, parse_function, output_filename):\n",
        "    \"\"\"\n",
        "    Scrape media URLs from a given page URL using crawl4ai's AsyncWebCrawler.\n",
        "    Saves URLs to CSV and returns a unique list.\n",
        "    \"\"\"\n",
        "    print(f\"[{modality.upper()}] Scraping URLs from {page_url}\")\n",
        "    schema = {\"extracted\": list}\n",
        "    extraction_strategy = JsonCssExtractionStrategy(parse_function=parse_function, schema=schema)\n",
        "    crawler = AsyncWebCrawler(start_urls=[page_url], extraction_strategy=extraction_strategy)\n",
        "    try:\n",
        "        results = await crawler.start()\n",
        "    except Exception as e:\n",
        "        print(f\"[{modality.upper()}] Error crawling {page_url}: {e}\")\n",
        "        results = []\n",
        "\n",
        "    scraped_urls = []\n",
        "    for result in results:\n",
        "        scraped_urls.extend(result.get('extracted', []))\n",
        "    scraped_urls = list({url for url in scraped_urls if url})\n",
        "    df = pd.DataFrame({'url': scraped_urls})\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"[{modality.upper()}] Saved {len(scraped_urls)} URLs to {output_filename}\")\n",
        "    return scraped_urls\n",
        "\n",
        "# Standard parse functions:\n",
        "def parse_image_links(response):\n",
        "    return response.css(\"img::attr(src)\").getall()\n",
        "\n",
        "def parse_video_links(response):\n",
        "    return response.css(\"a::attr(href)\").getall() or response.css(\"video::attr(src)\").getall()\n",
        "\n",
        "def parse_audio_links(response):\n",
        "    return response.css(\"audio::attr(src)\").getall()\n",
        "\n",
        "def parse_text_links(response):\n",
        "    return response.css(\"a::attr(href)\").getall()"
      ],
      "metadata": {
        "id": "C7HQW_cI_UxZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 2. Functions to Load URLs (with Fallbacks)\n",
        "#########################################\n",
        "def load_scraped_urls(modality):\n",
        "    filepath = os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\")\n",
        "    if os.path.exists(filepath):\n",
        "        df = pd.read_csv(filepath)\n",
        "        urls = df['url'].dropna().tolist()\n",
        "        print(f\"[{modality.upper()}] Loaded {len(urls)} URLs from {filepath}\")\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"[{modality.upper()}] File {filepath} not found. Using fallback method.\")\n",
        "        return []\n",
        "\n",
        "# Fallback function that avoids general nature images by using a keyword for manipulated media\n",
        "def fallback_image_urls(n=20, seed_keyword=\"deepfake\"):\n",
        "    return [f\"https://picsum.photos/seed/{seed_keyword}{i}/600/400\" for i in range(n)]\n",
        "\n",
        "def fallback_video_urls(n=20):\n",
        "    return []  # Let yt-dlp fallback handle this\n",
        "\n",
        "def fallback_audio_urls(n=20):\n",
        "    return []\n",
        "\n",
        "def fallback_text_urls(n=20):\n",
        "    sample_articles = [\n",
        "        \"https://www.theguardian.com/technology/2024/oct/16/its-not-me-its-just-my-face-the-models-who-found-their-likenesses-had-been-used-in-ai-propaganda\",\n",
        "        \"https://www.wired.com/story/generative-ai-deepfakes/\",\n",
        "        \"https://www.ft.com/content/7f22ce59-1c6c-4d84-bca8-dc539992e286\",\n",
        "        \"https://nypost.com/2024/07/17/lifestyle/how-people-are-being-tricked-by-deepfake-doctor-videos/\",\n",
        "        \"https://www.openfox.com/deepfakes-and-their-impact-on-society/\"\n",
        "    ]\n",
        "    return (sample_articles * ((n // len(sample_articles)) + 1))[:n]"
      ],
      "metadata": {
        "id": "mH95w7UJ_ZHN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 3. Asynchronous Download Functions with Retry Logic\n",
        "#########################################\n",
        "async def download_file(session, url, filename, modality, retries=3):\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            async with session.get(url) as resp:\n",
        "                if resp.status == 200:\n",
        "                    content = await resp.read()\n",
        "                    with open(filename, \"wb\") as f:\n",
        "                        f.write(content)\n",
        "                    print(f\"[{modality.upper()}] Downloaded: {filename}\")\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": modality,\n",
        "                        \"filename\": filename,\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    return\n",
        "                else:\n",
        "                    print(f\"[{modality.upper()}] Attempt {attempt}: Status {resp.status} for {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{modality.upper()}] Attempt {attempt}: Exception for {url}: {e}\")\n",
        "        await asyncio.sleep(1)\n",
        "    print(f\"[{modality.upper()}] Failed to download {url} after {retries} attempts.\")\n",
        "\n",
        "async def download_images(urls):\n",
        "    print(\"[IMAGE] Starting image downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(IMAGES_DIR, f\"deepfake_image_{idx+1:03d}.jpg\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"image\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "async def download_audios(urls):\n",
        "    if not urls:\n",
        "        print(\"[AUDIO] No audio URLs provided for download.\")\n",
        "        return\n",
        "    print(\"[AUDIO] Starting audio downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(AUDIOS_DIR, f\"deepfake_audio_{idx+1:03d}.mp3\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"audio\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "async def download_texts(urls):\n",
        "    if not urls:\n",
        "        print(\"[TEXT] No text URLs provided for download.\")\n",
        "        return\n",
        "    print(\"[TEXT] Starting text downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(TEXT_DIR, f\"deepfake_text_{idx+1:03d}.html\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"text\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "def download_videos(video_urls, num_videos=20):\n",
        "    if video_urls:\n",
        "        download_list = video_urls[:num_videos]\n",
        "        print(\"[VIDEO] Downloading videos from scraped URLs...\")\n",
        "        for url in download_list:\n",
        "            ydl_opts = {\n",
        "                'format': 'bestvideo+bestaudio/best',\n",
        "                'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "                'merge_output_format': 'mp4',\n",
        "                'quiet': True,\n",
        "                'no_warnings': True,\n",
        "            }\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                try:\n",
        "                    ydl.download([url])\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": \"video\",\n",
        "                        \"filename\": url,\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    print(f\"[VIDEO] Downloaded: {url}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[VIDEO] Error downloading {url}: {e}\")\n",
        "    else:\n",
        "        search_query = \"ytsearch20:deepfake compilation\"\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',\n",
        "            'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "            'merge_output_format': 'mp4',\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "        }\n",
        "        print(\"[VIDEO] No scraped video URLs; using yt-dlp search fallback...\")\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            try:\n",
        "                ydl.download([search_query])\n",
        "                print(\"[VIDEO] Fallback video downloads complete.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[VIDEO] yt-dlp search fallback error: {e}\")"
      ],
      "metadata": {
        "id": "J5m4qrVW_lbg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 4. Audio Extraction via Playwright (Fallback)\n",
        "#########################################\n",
        "async def extract_audio_links():\n",
        "    print(\"[AUDIO] Extracting audio URLs via Playwright fallback...\")\n",
        "    extracted_links = []\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context()\n",
        "            page = await context.new_page()\n",
        "            url = \"https://uberduck.ai/explore\"\n",
        "            await page.goto(url)\n",
        "            await page.wait_for_timeout(5000)\n",
        "            audio_elements = await page.query_selector_all(\"audio\")\n",
        "            for el in audio_elements:\n",
        "                src = await el.get_attribute(\"src\")\n",
        "                if src:\n",
        "                    extracted_links.append(src)\n",
        "            await browser.close()\n",
        "            print(f\"[AUDIO] Extracted {len(extracted_links)} audio URLs via Playwright.\")\n",
        "    except Exception as e:\n",
        "        if \"Executable doesn't exist\" in str(e):\n",
        "            install_playwright_browsers()\n",
        "        print(f\"[AUDIO] Playwright extraction error: {e}\")\n",
        "    return extracted_links"
      ],
      "metadata": {
        "id": "GSx5aXRrAGLS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 5. Telegram Scraping Using Telethon\n",
        "#########################################\n",
        "# Updated configuration with actual Telegram channel usernames.\n",
        "# Ensure that you have valid API credentials and that these channels are accessible.\n",
        "TELEGRAM_CONFIG = {\n",
        "    \"api_id\": 123456,               # Replace with your actual Telegram API ID\n",
        "    \"api_hash\": \"YOUR_API_HASH\",    # Replace with your actual Telegram API Hash\n",
        "    \"phone\": \"+111111111111\",        # Replace with your phone number (with country code)\n",
        "    \"channels\": [\n",
        "        \"deepfakesarchive\",       # e.g., t.me/deepfakesarchive\n",
        "        \"deepfakes\",              # e.g., t.me/deepfakes\n",
        "        \"fakemediaforensics\"      # e.g., t.me/fakemediaforensics\n",
        "    ]\n",
        "}\n",
        "\n",
        "async def scrape_telegram_messages(config):\n",
        "    \"\"\"\n",
        "    Scrape messages from a list of Telegram channels using Telethon and save them to a CSV file.\n",
        "    \"\"\"\n",
        "    print(\"[TELEGRAM] Starting Telegram scraping...\")\n",
        "    client = TelegramClient('telegram_session', config[\"api_id\"], config[\"api_hash\"])\n",
        "    await client.start(phone=config[\"phone\"])\n",
        "\n",
        "    all_messages = []\n",
        "    # Iterate over each channel in the provided list\n",
        "    for channel_identifier in config[\"channels\"]:\n",
        "        try:\n",
        "            channel = await client.get_entity(channel_identifier)\n",
        "            print(f\"[TELEGRAM] Scraping channel: {channel_identifier}\")\n",
        "            messages_data = []\n",
        "            async for message in client.iter_messages(channel, limit=100):  # Adjust limit as needed\n",
        "                messages_data.append({\n",
        "                    \"channel\": channel_identifier,\n",
        "                    \"message_id\": message.id,\n",
        "                    \"date\": message.date.isoformat() if message.date else None,\n",
        "                    \"sender_id\": message.sender_id,\n",
        "                    \"text\": message.text\n",
        "                })\n",
        "            print(f\"[TELEGRAM] Retrieved {len(messages_data)} messages from {channel_identifier}\")\n",
        "            all_messages.extend(messages_data)\n",
        "        except Exception as e:\n",
        "            print(f\"[TELEGRAM] Error scraping {channel_identifier}: {e}\")\n",
        "\n",
        "    # Save the aggregated messages to a CSV file\n",
        "    df = pd.DataFrame(all_messages)\n",
        "    telegram_filename = os.path.join(TELEGRAM_DIR, \"telegram_messages.csv\")\n",
        "    df.to_csv(telegram_filename, index=False, encoding='utf-8')\n",
        "    print(f\"[TELEGRAM] Saved {len(all_messages)} messages to {telegram_filename}\")\n",
        "    await client.disconnect()\n",
        "    return all_messages"
      ],
      "metadata": {
        "id": "DGOb9We5AOSZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 6. Save Download Metadata to JSON\n",
        "#########################################\n",
        "def save_download_metadata(metadata_list, filename=METADATA_FILE):\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(metadata_list, f, indent=4)\n",
        "        print(f\"[SYSTEM] Download metadata saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[SYSTEM] Error saving metadata: {e}\")"
      ],
      "metadata": {
        "id": "ZsjX5iYIAQ94"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 7. Main Pipeline with Expanded Modalities (including diverse sources and Telegram channels)\n",
        "#########################################\n",
        "async def main_pipeline():\n",
        "    # Define source pages for various modalities using publicly available forensic datasets.\n",
        "    pages = {\n",
        "        \"politician_manipulated_images\": \"https://github.com/ondyari/FaceForensics\",   # FaceForensics++ (manipulated faces)\n",
        "        \"celebrity_manipulated_videos\": \"https://sites.google.com/view/celebdf\",         # Celeb-DF dataset page\n",
        "        \"human_manipulation_audios\": \"https://www.asvspoof.org/2019/index.html\",         # ASVspoof 2019 dataset page\n",
        "        \"deepfake_voice_detection\": \"https://www.kaggle.com/datasets/search?query=deepfake+voice\",\n",
        "        \"deepfake_video_detection\": \"https://www.kaggle.com/datasets/search?query=deepfake+video+detection\",\n",
        "        \"ai_generated_images\": \"https://thiswaifudoesnotexist.net/\"\n",
        "    }\n",
        "\n",
        "    # Process each modality:\n",
        "    for modality, page_url in pages.items():\n",
        "        if any(key in modality for key in [\"image\", \"photoshop\", \"face\", \"ai_generated\"]):\n",
        "            urls = await scrape_media_urls(modality, page_url, parse_image_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\"))\n",
        "        elif \"video\" in modality:\n",
        "            urls = await scrape_media_urls(modality, page_url, parse_video_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\"))\n",
        "        elif \"audio\" in modality or \"voice\" in modality:\n",
        "            urls = await scrape_media_urls(modality, page_url, parse_audio_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\"))\n",
        "        elif \"text\" in modality:\n",
        "            urls = await scrape_media_urls(modality, page_url, parse_text_links,\n",
        "                                           os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\"))\n",
        "        else:\n",
        "            urls = []\n",
        "\n",
        "        # Use fallback only if scraping yielded no URLs\n",
        "        if not urls:\n",
        "            print(f\"[{modality.upper()}] No scraped URLs; using fallback.\")\n",
        "            if any(key in modality for key in [\"image\", \"photoshop\", \"face\", \"ai_generated\"]):\n",
        "                seed = modality  # the modality itself implies manipulated media\n",
        "                urls = fallback_image_urls(20, seed_keyword=seed)\n",
        "            elif \"video\" in modality:\n",
        "                urls = fallback_video_urls(20)\n",
        "            elif \"audio\" in modality:\n",
        "                urls = fallback_audio_urls(20)\n",
        "            elif \"text\" in modality:\n",
        "                urls = fallback_text_urls(20)\n",
        "        else:\n",
        "            urls = urls[:20]\n",
        "\n",
        "        # Download files based on modality\n",
        "        if any(key in modality for key in [\"image\", \"photoshop\", \"face\", \"ai_generated\"]):\n",
        "            await download_images(urls)\n",
        "        elif \"video\" in modality:\n",
        "            download_videos(urls, num_videos=20)\n",
        "        elif \"audio\" in modality or \"voice\" in modality:\n",
        "            await download_audios(urls)\n",
        "        elif \"text\" in modality:\n",
        "            await download_texts(urls)\n",
        "        else:\n",
        "            print(f\"[{modality.upper()}] Modality not recognized for downloading.\")\n",
        "\n",
        "    # Extra audio extraction via Playwright fallback\n",
        "    extra_audio_urls = await extract_audio_links()\n",
        "    if extra_audio_urls:\n",
        "        await download_audios(extra_audio_urls)\n",
        "\n",
        "    # ----- New Telegram Scraping Step -----\n",
        "    print(\"[TELEGRAM] Scraping Telegram messages from target channels...\")\n",
        "    await scrape_telegram_messages(TELEGRAM_CONFIG)\n",
        "\n",
        "    # Save all download metadata\n",
        "    save_download_metadata(download_metadata)\n",
        "    print(\"[SYSTEM] All downloads complete. Check the respective directories for files.\")\n",
        "\n",
        "#########################################\n",
        "# Entry Point with Fixes for Running Event Loop\n",
        "#########################################\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()\n",
        "except RuntimeError:\n",
        "    loop = None\n",
        "\n",
        "if loop and loop.is_running():\n",
        "    print(\"[SYSTEM] Detected running event loop. Using loop.run_until_complete()...\")\n",
        "    loop.run_until_complete(main_pipeline())\n",
        "else:\n",
        "    asyncio.run(main_pipeline())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "5P7qOKH_fxfQ",
        "outputId": "b5e2b945-e276-4240-a285-356af642659e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYSTEM] Detected running event loop. Using loop.run_until_complete()...\n",
            "[POLITICIAN_MANIPULATED_IMAGES] Scraping URLs from https://github.com/ondyari/FaceForensics\n",
            "[INIT].... → Crawl4AI 0.4.248\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'AsyncWebCrawler' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5a2e18278165>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SYSTEM] Detected running event loop. Using loop.run_until_complete()...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-5a2e18278165>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"photoshop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"face\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ai_generated\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             urls = await scrape_media_urls(modality, page_url, parse_image_links,\n\u001b[0m\u001b[1;32m     19\u001b[0m                                            os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\"))\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"video\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodality\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-14b82c3310b3>\u001b[0m in \u001b[0;36mscrape_media_urls\u001b[0;34m(modality, page_url, parse_function, output_filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscraped_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mscraped_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extracted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mscraped_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscraped_urls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'AsyncWebCrawler' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57umQzSUllfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}