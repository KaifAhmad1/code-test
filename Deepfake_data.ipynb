{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db05faa-8269-4606-8357-1168de955580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q datasets pandas pillow tqdm huggingface_hub decord\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import hashlib\n",
        "import requests\n",
        "from decord import VideoReader\n",
        "from PIL import Image\n",
        "import wave"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDataCollector:\n",
        "    def __init__(self, base_dir: str, max_samples: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize the deepfake data collector.\n",
        "\n",
        "        Args:\n",
        "            base_dir: Base directory for storing downloaded data\n",
        "            max_samples: Maximum number of samples per category (real/fake)\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.metadata = []\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> bool:\n",
        "        \"\"\" Validate media file integrity. \"\"\"\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                return False\n",
        "\n",
        "            if media_type == \"video\":\n",
        "                with VideoReader(str(file_path)) as vr:\n",
        "                    return vr[0] is not None\n",
        "\n",
        "            elif media_type == \"image\":\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                return True\n",
        "\n",
        "            elif media_type == \"audio\":\n",
        "                with wave.open(str(file_path), \"rb\") as audio:\n",
        "                    return audio.getnframes() > 0\n",
        "\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Validation failed for {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def download_file(self, url: str, save_path: Path) -> bool:\n",
        "        \"\"\" Download a file with proper error handling. \"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Download failed for {url}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def process_dataset(self, dataset_name: str, category_name: str, file_ext: str, key: str) -> None:\n",
        "        \"\"\" Process and download a limited number of samples from a dataset. \"\"\"\n",
        "        logger.info(f\"Processing {category_name} dataset: {dataset_name}\")\n",
        "\n",
        "        try:\n",
        "            dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "            save_dir = self.base_dir / category_name\n",
        "            save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            sample_count = 0\n",
        "            for sample in dataset:\n",
        "                if sample_count >= self.max_samples:\n",
        "                    break\n",
        "\n",
        "                file_path = save_dir / f\"{category_name}_{sample_count:03d}.{file_ext}\"\n",
        "                success = False\n",
        "\n",
        "                # Handle both direct bytes and URLs\n",
        "                if isinstance(sample[key], dict) and \"bytes\" in sample[key]:\n",
        "                    with open(file_path, \"wb\") as f:\n",
        "                        f.write(sample[key][\"bytes\"])\n",
        "                    success = True\n",
        "                elif isinstance(sample[key], str):\n",
        "                    success = self.download_file(sample[key], file_path)\n",
        "\n",
        "                if success and self.validate_media_file(file_path, category_name):\n",
        "                    self.metadata.append({\n",
        "                        \"modality\": category_name,\n",
        "                        \"filename\": file_path.name,\n",
        "                        \"file_path\": str(file_path),\n",
        "                        \"source_dataset\": dataset_name,\n",
        "                        \"checksum\": self._get_file_hash(file_path),\n",
        "                    })\n",
        "                    sample_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing dataset {dataset_name}: {str(e)}\")\n",
        "\n",
        "    def _get_file_hash(self, file_path: Path) -> str:\n",
        "        \"\"\" Calculate SHA-256 hash of a file. \"\"\"\n",
        "        sha256_hash = hashlib.sha256()\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "                sha256_hash.update(byte_block)\n",
        "        return sha256_hash.hexdigest()\n",
        "\n",
        "    def save_metadata(self) -> None:\n",
        "        \"\"\" Save metadata to CSV file. \"\"\"\n",
        "        if self.metadata:\n",
        "            metadata_path = self.base_dir / \"metadata.csv\"\n",
        "            pd.DataFrame(self.metadata).to_csv(metadata_path, index=False)\n",
        "            logger.info(f\"Metadata saved to {metadata_path}\")\n",
        "        else:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "\n",
        "    def collect_datasets(self, dataset_configs):\n",
        "        \"\"\" Collect multiple datasets based on configuration. \"\"\"\n",
        "        for config in dataset_configs:\n",
        "            self.process_dataset(**config)\n",
        "        self.save_metadata()"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_configs = [\n",
        "        {\"dataset_name\": \"thenewsupercell/fixed-fakeavceleb\", \"category_name\": \"audio\", \"file_ext\": \"wav\", \"key\": \"audio\"},\n",
        "        {\"dataset_name\": \"mkhLlamaLearn/dfdc\", \"category_name\": \"video\", \"file_ext\": \"mp4\", \"key\": \"video\"},\n",
        "        {\"dataset_name\": \"taohu/faceforensics_h5\", \"category_name\": \"image\", \"file_ext\": \"jpg\", \"key\": \"image\"},\n",
        "    ]\n",
        "\n",
        "    collector = DeepfakeDataCollector(base_dir=Path.home() / \"Downloads\" / \"deepfake_dataset\", max_samples=20)\n",
        "    collector.collect_datasets(dataset_configs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395341ef-bf66-4140-903a-23399e790cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}