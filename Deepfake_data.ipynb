{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9139ff9-f950-41a4-b0e8-fcbb70ad9120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "langchain 0.3.16 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.2 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "%pip install -qU soundfile numpy datasets pandas pillow tqdm huggingface_hub decord"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import hashlib\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from typing import Dict, List, Optional, Tuple"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeMediaCollector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dir: str = \"./deepfake_dataset\",\n",
        "        max_samples: int = 5,\n",
        "        max_retries: int = 3,\n",
        "        timeout: int = 30,\n",
        "        max_workers: int = 4\n",
        "    ):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.max_retries = max_retries\n",
        "        self.timeout = timeout\n",
        "        self.max_workers = max_workers\n",
        "        self.metadata: List[Dict] = []\n",
        "        self.temp_dir = self.base_dir / \"temp\"\n",
        "        self._create_directory_structure()\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create the necessary directory structure with error handling\"\"\"\n",
        "        try:\n",
        "            for dir_type in ['video', 'image', 'audio']:\n",
        "                for category in ['real', 'fake']:\n",
        "                    dir_path = self.base_dir / dir_type / category\n",
        "                    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "            self.temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create directory structure: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def download_file(self, url: str, output_path: Path) -> bool:\n",
        "        \"\"\"Download a single file with retry logic and progress bar\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    async with session.get(url, timeout=self.timeout) as response:\n",
        "                        if response.status != 200:\n",
        "                            logger.warning(f\"Attempt {attempt + 1} failed: HTTP {response.status}\")\n",
        "                            continue\n",
        "\n",
        "                        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "                        with open(output_path, 'wb') as f, tqdm(\n",
        "                            desc=output_path.name,\n",
        "                            total=total_size,\n",
        "                            unit='iB',\n",
        "                            unit_scale=True\n",
        "                        ) as pbar:\n",
        "                            async for chunk in response.content.iter_chunked(8192):\n",
        "                                size = f.write(chunk)\n",
        "                                pbar.update(size)\n",
        "\n",
        "                        return True\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                logger.warning(f\"Timeout on attempt {attempt + 1} for {url}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Download failed on attempt {attempt + 1} for {url}: {str(e)}\")\n",
        "                if output_path.exists():\n",
        "                    output_path.unlink()\n",
        "\n",
        "            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "        return False\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> Tuple[bool, Optional[str]]:\n",
        "        \"\"\"Validate downloaded media files\"\"\"\n",
        "        if not file_path.exists():\n",
        "            return False, \"File does not exist\"\n",
        "\n",
        "        if file_path.stat().st_size == 0:\n",
        "            return False, \"File is empty\"\n",
        "\n",
        "        try:\n",
        "            if media_type == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                if len(vr) == 0:\n",
        "                    return False, \"Video has no frames\"\n",
        "                _ = vr[0].asnumpy()\n",
        "                return True, None\n",
        "\n",
        "            elif media_type == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                    return True, None\n",
        "\n",
        "            elif media_type == 'audio':\n",
        "                data, _ = sf.read(file_path)\n",
        "                if len(data) == 0:\n",
        "                    return False, \"Audio file is empty\"\n",
        "                return True, None\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def add_to_metadata(self, modality: str, category: str, file_path: Path):\n",
        "        \"\"\"Add file metadata to collection\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read()\n",
        "                md5_hash = hashlib.md5(content).hexdigest()\n",
        "                sha256_hash = hashlib.sha256(content).hexdigest()\n",
        "\n",
        "            file_stats = file_path.stat()\n",
        "            metadata_entry = {\n",
        "                'modality': modality,\n",
        "                'category': category,\n",
        "                'filename': file_path.name,\n",
        "                'file_path': str(file_path),\n",
        "                'file_size': file_stats.st_size,\n",
        "                'md5_hash': md5_hash,\n",
        "                'sha256_hash': sha256_hash,\n",
        "                'creation_time': file_stats.st_ctime,\n",
        "                'modification_time': file_stats.st_mtime\n",
        "            }\n",
        "\n",
        "            # Add modality-specific metadata\n",
        "            if modality == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                metadata_entry.update({\n",
        "                    'frame_count': len(vr),\n",
        "                    'width': vr[0].shape[1],\n",
        "                    'height': vr[0].shape[0],\n",
        "                })\n",
        "            elif modality == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    metadata_entry.update({\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format,\n",
        "                    })\n",
        "            elif modality == 'audio':\n",
        "                data, samplerate = sf.read(file_path)\n",
        "                metadata_entry.update({\n",
        "                    'duration': len(data) / samplerate,\n",
        "                    'samplerate': samplerate,\n",
        "                    'channels': data.shape[1] if len(data.shape) > 1 else 1,\n",
        "                })\n",
        "\n",
        "            self.metadata.append(metadata_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to add metadata for {file_path}: {str(e)}\")\n",
        "\n",
        "    async def process_url(self, url: str, category: str, media_type: str):\n",
        "        \"\"\"Process a single URL: download, validate, and add metadata\"\"\"\n",
        "        output_path = self.base_dir / media_type / category / Path(url).name\n",
        "\n",
        "        if await self.download_file(url, output_path):\n",
        "            is_valid, error_msg = self.validate_media_file(output_path, media_type)\n",
        "            if is_valid:\n",
        "                self.add_to_metadata(media_type, category, output_path)\n",
        "            else:\n",
        "                logger.error(f\"Invalid {media_type} file {output_path}: {error_msg}\")\n",
        "                output_path.unlink(missing_ok=True)\n",
        "\n",
        "    async def process_urls(self, urls_dict: Dict[str, List[str]], media_type: str):\n",
        "        \"\"\"Process a batch of URLs for a specific media type\"\"\"\n",
        "        tasks = []\n",
        "        for category, urls in urls_dict.items():\n",
        "            for url in urls[:self.max_samples]:\n",
        "                tasks.append(self.process_url(url, category, media_type))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save metadata and generate summary\"\"\"\n",
        "        if not self.metadata:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            metadata_df = pd.DataFrame(self.metadata)\n",
        "            metadata_df.to_csv(self.base_dir / \"metadata.csv\", index=False)\n",
        "\n",
        "            summary = {\n",
        "                'total_files': len(metadata_df),\n",
        "                'total_size_mb': metadata_df['file_size'].sum() / (1024 * 1024),\n",
        "                'by_modality': metadata_df.groupby('modality')['filename'].count().to_dict(),\n",
        "                'by_category': metadata_df.groupby('category')['filename'].count().to_dict()\n",
        "            }\n",
        "\n",
        "            with open(self.base_dir / \"summary.json\", 'w') as f:\n",
        "                json.dump(summary, f, indent=2)\n",
        "\n",
        "            logger.info(f\"Metadata and summary saved to {self.base_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metadata: {str(e)}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files\"\"\"\n",
        "        try:\n",
        "            if self.temp_dir.exists():\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "            logger.info(\"Cleanup completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cleanup failed: {str(e)}\")\n",
        "\n",
        "async def run_collector(collector: DeepfakeMediaCollector, urls: Dict[str, Dict[str, List[str]]]):\n",
        "    \"\"\"Run the collector with proper async handling\"\"\"\n",
        "    for media_type, urls_dict in urls.items():\n",
        "        await collector.process_urls(urls_dict, media_type)\n",
        "    collector.save_metadata()\n",
        "    collector.cleanup()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that handles both Jupyter notebook and regular Python environments\n",
        "    \"\"\"\n",
        "    # Example usage\n",
        "    urls = {\n",
        "        'video': {\n",
        "            'real': [\n",
        "                'https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/real/000_003.mp4',\n",
        "                'https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/real/001_009.mp4',\n",
        "                'https://paperswithcode.com/datasets?task=deepfake-detection',\n",
        "                'https://github.com/Daisy-Zhang/Awesome-Deepfakes-Detection',\n",
        "                'https://www.researchgate.net/publication/382316749_Video_and_Audio_Deepfake_Datasets_and_Open_Issues_in_Deepfake_Technology_Being_Ahead_of_the_Curve',\n",
        "                'https://github.com/Daisy-Zhang/Awesome-Deepfakes',\n",
        "                'https://github.com/DASH-Lab/FakeAVCeleb',\n",
        "                'https://www.kaggle.com/competitions/deepfake-detection-challenge',\n",
        "                'https://www.kaggle.com/datasets/abdallamohamed312/in-the-wild-audio-deepfake'\n",
        "            ],\n",
        "            'fake': [\n",
        "                'https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/fake/000_003.mp4',\n",
        "                'https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/fake/001_009.mp4'\n",
        "            ]\n",
        "        },\n",
        "        'image': {\n",
        "            'real': [\n",
        "                'https://github.com/danmohami/celeb-df/raw/master/dataset/images/real/000001.png',\n",
        "                'https://github.com/danmohami/celeb-df/raw/master/dataset/images/real/000002.png'\n",
        "            ],\n",
        "            'fake': [\n",
        "                'https://github.com/danmohami/celeb-df/raw/master/dataset/images/fake/000001.png',\n",
        "                'https://github.com/danmohami/celeb-df/raw/master/dataset/images/fake/000002.png'\n",
        "            ]\n",
        "        },\n",
        "        'audio': {\n",
        "            'real': [\n",
        "                'https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/real/000001.wav',\n",
        "                'https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/real/000002.wav'\n",
        "            ],\n",
        "            'fake': [\n",
        "                'https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/fake/000001.wav',\n",
        "                'https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/fake/000002.wav'\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    collector = DeepfakeMediaCollector()\n",
        "\n",
        "    try:\n",
        "        # Try to get the current event loop\n",
        "        loop = asyncio.get_event_loop()\n",
        "        if loop.is_running():\n",
        "            # We're in a Jupyter notebook with a running event loop\n",
        "            # Use asyncio.ensure_future() instead\n",
        "            asyncio.ensure_future(run_collector(collector, urls))\n",
        "        else:\n",
        "            # We're in a regular Python environment\n",
        "            loop.run_until_complete(run_collector(collector, urls))\n",
        "    except RuntimeError:\n",
        "        # No event loop exists yet\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(run_collector(collector, urls))\n",
        "    finally:\n",
        "        try:\n",
        "            loop.close()\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Jupyter notebook usage, you can also use this helper function\n",
        "async def run_in_notebook(urls_dict):\n",
        "    \"\"\"\n",
        "    Helper function for running the collector in a Jupyter notebook\n",
        "    Usage:\n",
        "    await run_in_notebook(urls_dict)\n",
        "    \"\"\"\n",
        "    collector = DeepfakeMediaCollector()\n",
        "    await run_collector(collector, urls_dict)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb596ecd-59f1-4b1e-b411-6e22e513763c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Attempt 1 failed: HTTP 403\n",
            "WARNING:__main__:Attempt 2 failed: HTTP 403\n",
            "WARNING:__main__:Attempt 3 failed: HTTP 403\n",
            "WARNING:__main__:Attempt 1 failed: HTTP 404\n",
            "WARNING:__main__:Attempt 1 failed: HTTP 404\n",
            "WARNING:__main__:Attempt 2 failed: HTTP 404\n",
            "WARNING:__main__:Attempt 3 failed: HTTP 404\n",
            "WARNING:__main__:Attempt 1 failed: HTTP 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}