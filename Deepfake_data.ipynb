{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbe1e83-2f6a-476b-9eb1-8b7d677f52dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "langchain 0.3.16 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.2 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "%pip install -qU soundfile numpy datasets pandas pillow tqdm huggingface_hub decord"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import hashlib\n",
        "import requests\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import gdown\n",
        "import json"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeMediaCollector:\n",
        "    def __init__(self, base_dir: str, max_samples: int = 20):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.metadata = []\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def download_file(self, url: str, save_path: Path, gdrive: bool = False) -> bool:\n",
        "        try:\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            if gdrive:\n",
        "                return gdown.download(url, str(save_path), quiet=False)\n",
        "            else:\n",
        "                with requests.get(url, stream=True) as response:\n",
        "                    response.raise_for_status()\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "                    with open(save_path, 'wb') as f, tqdm(\n",
        "                        desc=save_path.name,\n",
        "                        total=total_size,\n",
        "                        unit='iB',\n",
        "                        unit_scale=True\n",
        "                    ) as pbar:\n",
        "                        for chunk in response.iter_content(chunk_size=8192):\n",
        "                            size = f.write(chunk)\n",
        "                            pbar.update(size)\n",
        "                return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Download failed for {url}: {str(e)}\")\n",
        "            if save_path.exists():\n",
        "                save_path.unlink()\n",
        "            return False\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> bool:\n",
        "        try:\n",
        "            if not file_path.exists():\n",
        "                return False\n",
        "\n",
        "            if media_type == 'video':\n",
        "                try:\n",
        "                    with VideoReader(str(file_path)) as vr:\n",
        "                        return vr[0] is not None\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Video validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            elif media_type == 'image':\n",
        "                try:\n",
        "                    with Image.open(file_path) as img:\n",
        "                        img.verify()\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Image validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            elif media_type == 'audio':\n",
        "                try:\n",
        "                    data, samplerate = sf.read(file_path)\n",
        "                    return len(data) > 0 and samplerate > 0\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Audio validation failed: {str(e)}\")\n",
        "                    return False\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Validation failed for {file_path}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def process_ff_dataset(self, source_dir: Path):\n",
        "        \"\"\"Process FaceForensics++ dataset structure\"\"\"\n",
        "        logger.info(\"Processing FaceForensics++ dataset\")\n",
        "\n",
        "        categories = {\n",
        "            'original': 'real',\n",
        "            'DeepFakeDetection': 'fake',\n",
        "            'Face2Face': 'fake',\n",
        "            'FaceSwap': 'fake',\n",
        "            'Deepfakes': 'fake',\n",
        "            'NeuralTextures': 'fake'\n",
        "        }\n",
        "\n",
        "        for src_category, dst_category in categories.items():\n",
        "            src_path = source_dir / src_category / 'c23' / 'videos'\n",
        "            if not src_path.exists():\n",
        "                continue\n",
        "\n",
        "            for video_file in src_path.glob('*.mp4'):\n",
        "                if len([x for x in self.metadata if x['category'] == dst_category]) >= self.max_samples:\n",
        "                    break\n",
        "\n",
        "                dst_path = self.base_dir / 'video' / dst_category / video_file.name\n",
        "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                try:\n",
        "                    if not dst_path.exists():\n",
        "                        dst_path.write_bytes(video_file.read_bytes())\n",
        "\n",
        "                    if self.validate_media_file(dst_path, 'video'):\n",
        "                        self.metadata.append({\n",
        "                            'modality': 'video',\n",
        "                            'category': dst_category,\n",
        "                            'filename': dst_path.name,\n",
        "                            'file_path': str(dst_path),\n",
        "                            'source': 'FaceForensics++',\n",
        "                            'manipulation': src_category\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing {video_file}: {str(e)}\")\n",
        "\n",
        "    def process_celeb_df(self, source_dir: Path):\n",
        "        \"\"\"Process Celeb-DF dataset structure\"\"\"\n",
        "        logger.info(\"Processing Celeb-DF dataset\")\n",
        "\n",
        "        real_path = source_dir / 'Celeb-real'\n",
        "        fake_path = source_dir / 'Celeb-synthesis'\n",
        "\n",
        "        for category, path in [('real', real_path), ('fake', fake_path)]:\n",
        "            if not path.exists():\n",
        "                continue\n",
        "\n",
        "            for video_file in path.glob('*.mp4'):\n",
        "                if len([x for x in self.metadata if x['category'] == category]) >= self.max_samples:\n",
        "                    break\n",
        "\n",
        "                dst_path = self.base_dir / 'video' / category / video_file.name\n",
        "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                try:\n",
        "                    if not dst_path.exists():\n",
        "                        dst_path.write_bytes(video_file.read_bytes())\n",
        "\n",
        "                    if self.validate_media_file(dst_path, 'video'):\n",
        "                        self.metadata.append({\n",
        "                            'modality': 'video',\n",
        "                            'category': category,\n",
        "                            'filename': dst_path.name,\n",
        "                            'file_path': str(dst_path),\n",
        "                            'source': 'Celeb-DF',\n",
        "                            'manipulation': 'None' if category == 'real' else 'face_swap'\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing {video_file}: {str(e)}\")\n",
        "\n",
        "    def process_wav2lip(self, source_dir: Path):\n",
        "        \"\"\"Process Wav2Lip dataset structure\"\"\"\n",
        "        logger.info(\"Processing Wav2Lip dataset\")\n",
        "\n",
        "        categories = {\n",
        "            'real_videos': 'real',\n",
        "            'fake_videos': 'fake'\n",
        "        }\n",
        "\n",
        "        for src_category, dst_category in categories.items():\n",
        "            src_path = source_dir / src_category\n",
        "            if not src_path.exists():\n",
        "                continue\n",
        "\n",
        "            for video_file in src_path.glob('*.mp4'):\n",
        "                if len([x for x in self.metadata if x['category'] == dst_category]) >= self.max_samples:\n",
        "                    break\n",
        "\n",
        "                dst_path = self.base_dir / 'video' / dst_category / video_file.name\n",
        "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                try:\n",
        "                    if not dst_path.exists():\n",
        "                        dst_path.write_bytes(video_file.read_bytes())\n",
        "\n",
        "                    if self.validate_media_file(dst_path, 'video'):\n",
        "                        self.metadata.append({\n",
        "                            'modality': 'video',\n",
        "                            'category': dst_category,\n",
        "                            'filename': dst_path.name,\n",
        "                            'file_path': str(dst_path),\n",
        "                            'source': 'Wav2Lip',\n",
        "                            'manipulation': 'None' if dst_category == 'real' else 'lip_sync'\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing {video_file}: {str(e)}\")\n",
        "\n",
        "    def save_metadata(self) -> None:\n",
        "        if self.metadata:\n",
        "            metadata_path = self.base_dir / \"metadata.csv\"\n",
        "            pd.DataFrame(self.metadata).to_csv(metadata_path, index=False)\n",
        "            logger.info(f\"Metadata saved to {metadata_path}\")\n",
        "        else:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "\n",
        "def main():\n",
        "    # Initialize collector\n",
        "    collector = DeepfakeMediaCollector(\n",
        "        base_dir=\"./deepfake_dataset\",\n",
        "        max_samples=20\n",
        "    )\n",
        "\n",
        "    # Process datasets from local directories (if available)\n",
        "    datasets = {\n",
        "        'faceforensics': Path('/path/to/faceforensics'),\n",
        "        'celeb_df': Path('/path/to/celeb-df'),\n",
        "        'wav2lip': Path('/path/to/wav2lip')\n",
        "    }\n",
        "\n",
        "    for dataset_name, dataset_path in datasets.items():\n",
        "        if dataset_path.exists():\n",
        "            if dataset_name == 'faceforensics':\n",
        "                collector.process_ff_dataset(dataset_path)\n",
        "            elif dataset_name == 'celeb_df':\n",
        "                collector.process_celeb_df(dataset_path)\n",
        "            elif dataset_name == 'wav2lip':\n",
        "                collector.process_wav2lip(dataset_path)\n",
        "\n",
        "    # Save metadata\n",
        "    collector.save_metadata()"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfcdf29-14c9-4207-a03e-d32e49c322cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:No metadata to save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}