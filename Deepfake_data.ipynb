{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d577bc-9b97-4d73-f0f3-5885490b3cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q yt-dlp aiohttp decord pandas pillow soundfile tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import hashlib\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Import yt_dlp for video downloads\n",
        "import yt_dlp"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeMediaCollector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dir: str = \"./deepfake_dataset\",\n",
        "        max_samples: int = 20,  # Download 20 samples per category\n",
        "        max_retries: int = 3,\n",
        "        timeout: int = 30,\n",
        "        max_workers: int = 4  # Controls concurrent downloads\n",
        "    ):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.max_retries = max_retries\n",
        "        self.timeout = timeout\n",
        "        self.max_workers = max_workers\n",
        "        self.metadata: List[Dict] = []\n",
        "        self.temp_dir = self.base_dir / \"temp\"\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.semaphore = asyncio.Semaphore(self.max_workers)\n",
        "        self._create_directory_structure()\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create the necessary directory structure with error handling.\"\"\"\n",
        "        try:\n",
        "            for dir_type in ['video', 'image', 'audio']:\n",
        "                for category in ['real', 'fake']:\n",
        "                    (self.base_dir / dir_type / category).mkdir(parents=True, exist_ok=True)\n",
        "            self.temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create directory structure: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def init_session(self):\n",
        "        \"\"\"Initialize a single aiohttp session for all downloads.\"\"\"\n",
        "        if self.session is None:\n",
        "            self.session = aiohttp.ClientSession()\n",
        "\n",
        "    async def download_file(self, url: str, output_path: Path) -> bool:\n",
        "        \"\"\"Download a single file with retry logic using the shared session.\"\"\"\n",
        "        # Skip download if file already exists and is nonempty\n",
        "        if output_path.exists() and output_path.stat().st_size > 0:\n",
        "            logger.info(f\"File already exists, skipping: {output_path}\")\n",
        "            return True\n",
        "\n",
        "        async with self.semaphore:\n",
        "            for attempt in range(self.max_retries):\n",
        "                try:\n",
        "                    async with self.session.get(url, timeout=self.timeout) as response:\n",
        "                        if response.status != 200:\n",
        "                            logger.warning(f\"Attempt {attempt + 1}: HTTP {response.status} for {url}\")\n",
        "                            continue\n",
        "\n",
        "                        total_size = int(response.headers.get('content-length', 0))\n",
        "                        with open(output_path, 'wb') as f, tqdm(\n",
        "                            desc=output_path.name,\n",
        "                            total=total_size,\n",
        "                            unit='iB',\n",
        "                            unit_scale=True\n",
        "                        ) as pbar:\n",
        "                            async for chunk in response.content.iter_chunked(8192):\n",
        "                                size = f.write(chunk)\n",
        "                                pbar.update(size)\n",
        "                        return True\n",
        "\n",
        "                except asyncio.TimeoutError:\n",
        "                    logger.warning(f\"Timeout on attempt {attempt + 1} for {url}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Download failed on attempt {attempt + 1} for {url}: {str(e)}\")\n",
        "                    if output_path.exists():\n",
        "                        output_path.unlink()\n",
        "                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "            return False\n",
        "\n",
        "    async def download_with_yt_dlp(self, url: str, output_dir: Path) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Download a video using yt-dlp.\n",
        "        The output directory is specified, and a hook captures the final filename.\n",
        "        \"\"\"\n",
        "        downloaded_file = None\n",
        "\n",
        "        def hook(d):\n",
        "            nonlocal downloaded_file\n",
        "            if d.get('status') == 'finished':\n",
        "                downloaded_file = d.get('filename')\n",
        "\n",
        "        ydl_opts = {\n",
        "            'outtmpl': str(output_dir / '%(title)s.%(ext)s'),\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "            'progress_hooks': [hook],\n",
        "        }\n",
        "        try:\n",
        "            # Run yt-dlp download in a separate thread.\n",
        "            await asyncio.to_thread(lambda: yt_dlp.YoutubeDL(ydl_opts).download([url]))\n",
        "            return downloaded_file\n",
        "        except Exception as e:\n",
        "            logger.error(f\"yt-dlp download failed for {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> Tuple[bool, Optional[str]]:\n",
        "        \"\"\"Validate downloaded media files.\"\"\"\n",
        "        if not file_path.exists():\n",
        "            return False, \"File does not exist\"\n",
        "        if file_path.stat().st_size == 0:\n",
        "            return False, \"File is empty\"\n",
        "        try:\n",
        "            if media_type == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                if len(vr) == 0:\n",
        "                    return False, \"Video has no frames\"\n",
        "                _ = vr[0].asnumpy()\n",
        "                return True, None\n",
        "            elif media_type == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                return True, None\n",
        "            elif media_type == 'audio':\n",
        "                data, _ = sf.read(file_path)\n",
        "                if len(data) == 0:\n",
        "                    return False, \"Audio file is empty\"\n",
        "                return True, None\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def add_to_metadata(self, modality: str, category: str, file_path: Path):\n",
        "        \"\"\"Add file metadata to collection.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read()\n",
        "                md5_hash = hashlib.md5(content).hexdigest()\n",
        "                sha256_hash = hashlib.sha256(content).hexdigest()\n",
        "            file_stats = file_path.stat()\n",
        "            metadata_entry = {\n",
        "                'modality': modality,\n",
        "                'category': category,\n",
        "                'filename': file_path.name,\n",
        "                'file_path': str(file_path),\n",
        "                'file_size': file_stats.st_size,\n",
        "                'md5_hash': md5_hash,\n",
        "                'sha256_hash': sha256_hash,\n",
        "                'creation_time': file_stats.st_ctime,\n",
        "                'modification_time': file_stats.st_mtime\n",
        "            }\n",
        "            # Add modality-specific metadata.\n",
        "            if modality == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                metadata_entry.update({\n",
        "                    'frame_count': len(vr),\n",
        "                    'width': vr[0].shape[1],\n",
        "                    'height': vr[0].shape[0],\n",
        "                })\n",
        "            elif modality == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    metadata_entry.update({\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format,\n",
        "                    })\n",
        "            elif modality == 'audio':\n",
        "                data, samplerate = sf.read(file_path)\n",
        "                metadata_entry.update({\n",
        "                    'duration': len(data) / samplerate,\n",
        "                    'samplerate': samplerate,\n",
        "                    'channels': data.shape[1] if len(data.shape) > 1 else 1,\n",
        "                })\n",
        "            self.metadata.append(metadata_entry)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to add metadata for {file_path}: {str(e)}\")\n",
        "\n",
        "    async def process_url(self, url: str, category: str, media_type: str):\n",
        "        \"\"\"\n",
        "        Process a single URL: download (using yt-dlp for supported sites),\n",
        "        validate, and add metadata.\n",
        "        \"\"\"\n",
        "        if any(domain in url for domain in ['youtube.com', 'youtu.be']):\n",
        "            output_dir = self.base_dir / media_type / category\n",
        "            output_dir.mkdir(parents=True, exist_ok=True)\n",
        "            logger.info(f\"Using yt-dlp to download: {url}\")\n",
        "            downloaded_file = await self.download_with_yt_dlp(url, output_dir)\n",
        "            if downloaded_file:\n",
        "                output_path = Path(downloaded_file)\n",
        "                is_valid, error_msg = self.validate_media_file(output_path, media_type)\n",
        "                if is_valid:\n",
        "                    self.add_to_metadata(media_type, category, output_path)\n",
        "                else:\n",
        "                    logger.error(f\"Invalid {media_type} file {output_path}: {error_msg}\")\n",
        "                    output_path.unlink(missing_ok=True)\n",
        "            else:\n",
        "                logger.error(f\"Failed to download with yt-dlp: {url}\")\n",
        "        else:\n",
        "            output_path = self.base_dir / media_type / category / Path(url).name\n",
        "            if await self.download_file(url, output_path):\n",
        "                is_valid, error_msg = self.validate_media_file(output_path, media_type)\n",
        "                if is_valid:\n",
        "                    self.add_to_metadata(media_type, category, output_path)\n",
        "                else:\n",
        "                    logger.error(f\"Invalid {media_type} file {output_path}: {error_msg}\")\n",
        "                    output_path.unlink(missing_ok=True)\n",
        "\n",
        "    async def process_urls(self, urls_dict: Dict[str, List[str]], media_type: str):\n",
        "        \"\"\"Process a batch of URLs for a specific media type.\"\"\"\n",
        "        tasks = [self.process_url(url, category, media_type)\n",
        "                 for category, urls in urls_dict.items()\n",
        "                 for url in urls[:self.max_samples]]\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save metadata and generate summary.\"\"\"\n",
        "        if not self.metadata:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "            return\n",
        "        try:\n",
        "            metadata_df = pd.DataFrame(self.metadata)\n",
        "            metadata_df.to_csv(self.base_dir / \"metadata.csv\", index=False)\n",
        "            summary = {\n",
        "                'total_files': len(metadata_df),\n",
        "                'total_size_mb': metadata_df['file_size'].sum() / (1024 * 1024),\n",
        "                'by_modality': metadata_df.groupby('modality')['filename'].count().to_dict(),\n",
        "                'by_category': metadata_df.groupby('category')['filename'].count().to_dict()\n",
        "            }\n",
        "            with open(self.base_dir / \"summary.json\", 'w') as f:\n",
        "                json.dump(summary, f, indent=2)\n",
        "            logger.info(f\"Metadata and summary saved to {self.base_dir}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metadata: {str(e)}\")\n",
        "\n",
        "    async def close_session(self):\n",
        "        \"\"\"Properly close the aiohttp session.\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def cleanup(self):\n",
        "        \"\"\"Clean up temporary files and close session.\"\"\"\n",
        "        try:\n",
        "            if self.temp_dir.exists():\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "            logger.info(\"Cleanup completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cleanup failed: {str(e)}\")\n",
        "        await self.close_session()\n",
        "\n",
        "\n",
        "async def run_collector(collector: DeepfakeMediaCollector, urls: Dict[str, Dict[str, List[str]]]):\n",
        "    \"\"\"Run the collector with proper async handling.\"\"\"\n",
        "    await collector.init_session()\n",
        "    # Process each modality separately.\n",
        "    await collector.process_urls(urls, 'video')\n",
        "    await collector.process_urls(urls, 'image')\n",
        "    await collector.process_urls(urls, 'audio')\n",
        "    collector.save_metadata()\n",
        "    await collector.cleanup()"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function handling both Jupyter notebook and regular Python environments.\n",
        "    Generates 20 URLs for each modality/category.\n",
        "    \"\"\"\n",
        "    # Generate video URLs for FaceForensics++.\n",
        "    real_video_urls = [f\"https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/real/000_{i:03d}.mp4\" for i in range(1, 21)]\n",
        "    fake_video_urls = [f\"https://github.com/ondyari/FaceForensics/raw/master/dataset/videos/fake/000_{i:03d}.mp4\" for i in range(1, 21)]\n",
        "\n",
        "    # Generate image URLs for Celeb-DF samples.\n",
        "    real_image_urls = [f\"https://github.com/danmohami/celeb-df/raw/master/dataset/images/real/{i:06d}.png\" for i in range(1, 21)]\n",
        "    fake_image_urls = [f\"https://github.com/danmohami/celeb-df/raw/master/dataset/images/fake/{i:06d}.png\" for i in range(1, 21)]\n",
        "\n",
        "    # Generate audio URLs for FakeAVCeleb.\n",
        "    real_audio_urls = [f\"https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/real/{i:06d}.wav\" for i in range(1, 21)]\n",
        "    fake_audio_urls = [f\"https://github.com/DariusAf/FakeAVCeleb/raw/master/dataset/audio/fake/{i:06d}.wav\" for i in range(1, 21)]\n",
        "\n",
        "    urls = {\n",
        "        'video': {\n",
        "            'real': real_video_urls,\n",
        "            'fake': fake_video_urls\n",
        "        },\n",
        "        'image': {\n",
        "            'real': real_image_urls,\n",
        "            'fake': fake_image_urls\n",
        "        },\n",
        "        'audio': {\n",
        "            'real': real_audio_urls,\n",
        "            'fake': fake_audio_urls\n",
        "        }\n",
        "    }\n",
        "\n",
        "    collector = DeepfakeMediaCollector()\n",
        "\n",
        "    try:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        if loop.is_running():\n",
        "            asyncio.ensure_future(run_collector(collector, urls))\n",
        "        else:\n",
        "            loop.run_until_complete(run_collector(collector, urls))\n",
        "    except RuntimeError:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(run_collector(collector, urls))\n",
        "    finally:\n",
        "        try:\n",
        "            loop.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# For Jupyter Notebook usage\n",
        "async def run_in_notebook(urls_dict):\n",
        "    \"\"\"\n",
        "    Helper function for running the collector in a Jupyter notebook.\n",
        "    Usage:\n",
        "      await run_in_notebook(urls_dict)\n",
        "    \"\"\"\n",
        "    collector = DeepfakeMediaCollector()\n",
        "    await run_collector(collector, urls_dict)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5ff0cf-5a62-493d-bc0d-84bb4f9ca34c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-2' coro=<run_collector() done, defined at <ipython-input-4-c3a4b3dd810e>:243> exception=TypeError(\"unhashable type: 'slice'\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-c3a4b3dd810e>\", line 247, in run_collector\n",
            "    await collector.process_urls(urls, 'video')\n",
            "  File \"<ipython-input-4-c3a4b3dd810e>\", line 202, in process_urls\n",
            "    tasks = [self.process_url(url, category, media_type)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-4-c3a4b3dd810e>\", line 204, in <listcomp>\n",
            "    for url in urls[:self.max_samples]]\n",
            "               ~~~~^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: unhashable type: 'slice'\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7bad997f7090>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}