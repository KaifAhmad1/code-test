{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5b3ac0-6d94-48a2-8bfe-9cddd284a208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.2/702.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.9/182.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU yt-dlp aiohttp decord pandas pillow soundfile tqdm crawl4ai[all] nest_asyncio playwright telethon feedparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import yt_dlp\n",
        "from tqdm.asyncio import tqdm\n",
        "from playwright.async_api import async_playwright\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "# Import Crawl4AI components\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "\n",
        "#########################################\n",
        "# Setup Directories and Global Variables\n",
        "#########################################\n",
        "BASE_DIR = os.getcwd()\n",
        "SCRAPED_URLS_DIR = os.path.join(BASE_DIR, 'scraped_urls')\n",
        "IMAGES_DIR = os.path.join(BASE_DIR, 'deepfake_images')\n",
        "VIDEOS_DIR = os.path.join(BASE_DIR, 'deepfake_videos')\n",
        "AUDIOS_DIR = os.path.join(BASE_DIR, 'deepfake_audios')\n",
        "METADATA_FILE = os.path.join(BASE_DIR, 'download_metadata.json')\n",
        "\n",
        "# Create only the needed directories\n",
        "for d in [SCRAPED_URLS_DIR, IMAGES_DIR, VIDEOS_DIR, AUDIOS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "download_metadata = []\n",
        "\n",
        "#########################################\n",
        "# Helper: Ensure Playwright Browsers Are Installed\n",
        "#########################################\n",
        "def install_playwright_browsers():\n",
        "    try:\n",
        "        print(\"[SYSTEM] Installing Playwright browsers...\")\n",
        "        subprocess.run([\"playwright\", \"install\"], check=True)\n",
        "        print(\"[SYSTEM] Playwright browsers installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[SYSTEM] Error installing Playwright browsers: {e}\")"
      ],
      "metadata": {
        "id": "p4NIw0Qe2VrD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 1. URL Scraping Functions using Crawl4AI\n",
        "#########################################\n",
        "async def scrape_media_urls(modality, page_url, parse_function, output_filename):\n",
        "    \"\"\"\n",
        "    Scrape media URLs from a given page URL using Crawl4AI’s AsyncWebCrawler.\n",
        "    Saves URLs to CSV and returns a unique list.\n",
        "    \"\"\"\n",
        "    print(f\"[{modality.upper()}] Scraping URLs from {page_url}\")\n",
        "    schema = {\"extracted\": list}\n",
        "    extraction_strategy = JsonCssExtractionStrategy(parse_function=parse_function, schema=schema)\n",
        "\n",
        "    try:\n",
        "        async with AsyncWebCrawler(extraction_strategy=extraction_strategy) as crawler:\n",
        "            result = await crawler.arun(page_url, config=CrawlerRunConfig())\n",
        "    except Exception as e:\n",
        "        print(f\"[{modality.upper()}] Error crawling {page_url}: {e}\")\n",
        "        result = None\n",
        "\n",
        "    scraped_urls = []\n",
        "    if result and result.extracted_content:\n",
        "        try:\n",
        "            data = json.loads(result.extracted_content)\n",
        "            scraped_urls = data.get('extracted', [])\n",
        "        except Exception as e:\n",
        "            print(f\"[{modality.upper()}] Error parsing extracted content: {e}\")\n",
        "    scraped_urls = list({url for url in scraped_urls if url})\n",
        "    df = pd.DataFrame({'url': scraped_urls})\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"[{modality.upper()}] Saved {len(scraped_urls)} URLs to {output_filename}\")\n",
        "    return scraped_urls\n",
        "\n",
        "def parse_image_links(response):\n",
        "    return response.css(\"img.media-image::attr(src)\").getall()\n",
        "\n",
        "def parse_video_links(response):\n",
        "    return response.css(\"video source::attr(src)\").getall()\n",
        "\n",
        "def parse_audio_links(response):\n",
        "    return response.css(\"audio::attr(src)\").getall()"
      ],
      "metadata": {
        "id": "_a_rLt3M2Vtm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 2. Functions to Load URLs (with Fallbacks)\n",
        "#########################################\n",
        "def load_scraped_urls(modality):\n",
        "    filepath = os.path.join(SCRAPED_URLS_DIR, f\"{modality}_urls.csv\")\n",
        "    if os.path.exists(filepath):\n",
        "        df = pd.read_csv(filepath)\n",
        "        urls = df['url'].dropna().tolist()\n",
        "        print(f\"[{modality.upper()}] Loaded {len(urls)} URLs from {filepath}\")\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"[{modality.upper()}] File {filepath} not found. Using fallback method.\")\n",
        "        return []\n",
        "\n",
        "def fallback_image_urls(n=20, seed_keyword=\"deepfake\"):\n",
        "    return [f\"https://picsum.photos/seed/{seed_keyword}{i}/600/400\" for i in range(n)]\n",
        "\n",
        "def fallback_video_urls(n=20):\n",
        "    return []  # Let yt-dlp search fallback handle video search if needed\n",
        "\n",
        "def fallback_audio_urls(n=20):\n",
        "    return []  # Return an empty list; our extraction fallback will be used"
      ],
      "metadata": {
        "id": "ASRpBuoU2VwM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 3. Asynchronous Download Functions with Retry Logic\n",
        "#########################################\n",
        "async def download_file(session, url, filename, modality, retries=3):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            async with session.get(url, headers=headers) as resp:\n",
        "                if resp.status == 200:\n",
        "                    content = await resp.read()\n",
        "                    with open(filename, \"wb\") as f:\n",
        "                        f.write(content)\n",
        "                    print(f\"[{modality.upper()}] Downloaded: {filename}\")\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": modality,\n",
        "                        \"filename\": filename,\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    return\n",
        "                else:\n",
        "                    print(f\"[{modality.upper()}] Attempt {attempt}: Status {resp.status} for {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{modality.upper()}] Attempt {attempt}: Exception for {url}: {e}\")\n",
        "        await asyncio.sleep(1)\n",
        "    print(f\"[{modality.upper()}] Failed to download {url} after {retries} attempts.\")\n",
        "\n",
        "async def download_images(urls):\n",
        "    print(\"[IMAGE] Starting image downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(IMAGES_DIR, f\"deepfake_image_{idx+1:03d}.jpg\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"image\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "async def download_audios(urls):\n",
        "    if not urls:\n",
        "        print(\"[AUDIO] No audio URLs provided for download.\")\n",
        "        return\n",
        "    print(\"[AUDIO] Starting audio downloads...\")\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for idx, url in enumerate(urls):\n",
        "            filename = os.path.join(AUDIOS_DIR, f\"deepfake_audio_{idx+1:03d}.mp3\")\n",
        "            tasks.append(download_file(session, url, filename, modality=\"audio\"))\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "def download_videos(video_urls, num_videos=20):\n",
        "    if video_urls:\n",
        "        download_list = video_urls[:num_videos]\n",
        "        print(\"[VIDEO] Downloading videos from scraped URLs...\")\n",
        "        for url in download_list:\n",
        "            ydl_opts = {\n",
        "                'format': 'bestvideo+bestaudio/best',\n",
        "                'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "                'merge_output_format': 'mp4',\n",
        "                'quiet': True,\n",
        "                'no_warnings': True,\n",
        "            }\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                try:\n",
        "                    ydl.download([url])\n",
        "                    download_metadata.append({\n",
        "                        \"modality\": \"video\",\n",
        "                        \"filename\": url,\n",
        "                        \"url\": url,\n",
        "                        \"download_time\": datetime.now().isoformat()\n",
        "                    })\n",
        "                    print(f\"[VIDEO] Downloaded: {url}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[VIDEO] Error downloading {url}: {e}\")\n",
        "    else:\n",
        "        search_query = \"ytsearch20:deepfake compilation\"\n",
        "        ydl_opts = {\n",
        "            'format': 'bestvideo+bestaudio/best',\n",
        "            'outtmpl': os.path.join(VIDEOS_DIR, '%(id)s.%(ext)s'),\n",
        "            'merge_output_format': 'mp4',\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "        }\n",
        "        print(\"[VIDEO] No scraped video URLs; using yt-dlp search fallback...\")\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            try:\n",
        "                ydl.download([search_query])\n",
        "                print(\"[VIDEO] Fallback video downloads complete.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[VIDEO] yt-dlp search fallback error: {e}\")"
      ],
      "metadata": {
        "id": "k7PioT0e2Vym"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 4. Audio Extraction via Playwright (Fallback)\n",
        "#########################################\n",
        "async def extract_audio_links():\n",
        "    print(\"[AUDIO] Extracting audio URLs via Playwright fallback...\")\n",
        "    extracted_links = []\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context()\n",
        "            page = await context.new_page()\n",
        "            url = \"https://uberduck.ai/explore\"\n",
        "            await page.goto(url)\n",
        "            await page.wait_for_selector(\"audio\", state=\"visible\")\n",
        "            audio_elements = await page.query_selector_all(\"audio\")\n",
        "            for el in audio_elements:\n",
        "                src = await el.get_attribute(\"src\")\n",
        "                if src:\n",
        "                    extracted_links.append(src)\n",
        "            await browser.close()\n",
        "            print(f\"[AUDIO] Extracted {len(extracted_links)} audio URLs via Playwright.\")\n",
        "    except Exception as e:\n",
        "        if \"Executable doesn't exist\" in str(e):\n",
        "            install_playwright_browsers()\n",
        "        print(f\"[AUDIO] Playwright extraction error: {e}\")\n",
        "    return extracted_links"
      ],
      "metadata": {
        "id": "y7zLVR5s2V01"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 5. Save Download Metadata to JSON\n",
        "#########################################\n",
        "def save_download_metadata(metadata_list, filename=METADATA_FILE):\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(metadata_list, f, indent=4)\n",
        "        print(f\"[SYSTEM] Download metadata saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[SYSTEM] Error saving metadata: {e}\")"
      ],
      "metadata": {
        "id": "b2K6w4QH2V3b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 6. Main Pipeline with Expanded Deepfake Sources\n",
        "#########################################\n",
        "async def main_pipeline():\n",
        "    # VIDEO SOURCES: Each tuple is (Dataset name, URL)\n",
        "    video_sources = [\n",
        "        (\"UADFV\", \"https://ai.facebook.com/datasets/dfdc/\"),\n",
        "        (\"Deepfake-TIMIT\", \"https://github.com/ondyari/FaceForensics\"),\n",
        "        (\"DFFD\", \"https://github.com/EndlessSora/DeeperForensics-1.0\"),\n",
        "        (\"Celeb-DF\", \"https://github.com/cypw/Celeb-DF\"),\n",
        "        (\"DFDC\", \"https://ai.facebook.com/datasets/dfdc/\"),\n",
        "        (\"FaceForensics++\", \"https://github.com/ondyari/FaceForensics\"),\n",
        "        (\"FFIW-10K\", \"https://github.com/ondyari/FaceForensics\"),\n",
        "        (\"DeeperForensics-1.0\", \"https://github.com/EndlessSora/DeeperForensics-1.0\"),\n",
        "        (\"WildDeepfake\", \"https://github.com/deepfakeinthewild/deepfake-in-the-wild\"),\n",
        "        (\"ForgeryNet\", \"https://github.com/duxingdong/ForgeryNet\"),\n",
        "        (\"AV-Deepfake1M\", \"https://github.com/ControlNet/AV-Deepfake1M\"),\n",
        "        (\"DeepFake MNIST+\", \"https://github.com/yourplaceholder/DeepFakeMNISTplus\"),\n",
        "        (\"VideoSham\", \"https://github.com/adobe-research/VideoSham-dataset\"),\n",
        "        (\"RWDF-23\", \"https://rwdf23.example.com\")  # Placeholder URL; replace with actual link if available\n",
        "    ]\n",
        "\n",
        "    # IMAGE SOURCES: Each tuple is (Dataset name, URL)\n",
        "    image_sources = [\n",
        "        (\"DFFD_Image\", \"https://github.com/ondyari/FaceForensics\"),\n",
        "        (\"iFakeFaceDB\", \"https://github.com/YourRepository/iFakeFaceDB\"),  # Replace with actual URL if available\n",
        "        (\"100kFaces\", \"https://thiswaifudoesnotexist.net/\"),\n",
        "        (\"FFHQ\", \"https://github.com/NVlabs/ffhq-dataset\"),\n",
        "        (\"ForgeryNet_Image\", \"https://github.com/duxingdong/ForgeryNet\"),\n",
        "        (\"DeepFakeRealImages\", \"https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\"),\n",
        "        (\"DeepfakeDetection_Images\", \"https://github.com/yourrepository/deepfake-detection-images\")  # Placeholder\n",
        "    ]\n",
        "\n",
        "    # AUDIO SOURCES: Each tuple is (Dataset name, URL)\n",
        "    audio_sources = [\n",
        "        (\"WaveFake\", \"https://paperswithcode.com/dataset/wavefake\"),\n",
        "        (\"DEEP-VOICE\", \"https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition\"),\n",
        "        (\"InTheWild_Audio\", \"https://www.kaggle.com/datasets/abdallamohamed312/in-the-wild-audio-deepfake\"),\n",
        "        (\"Codecfake\", \"https://arxiv.org/abs/2405.04880\"),\n",
        "        (\"CrossDomain_ADD\", \"https://arxiv.org/abs/2404.04904\"),\n",
        "        (\"DeepFake-Audio-Rangers_Arabic\", \"https://huggingface.co/datasets/DeepFake-Audio-Rangers/Arabic_Audio_Deepfake\"),\n",
        "        (\"SONICS\", \"https://paperswithcode.com/dataset/sonics\"),\n",
        "        (\"ASVspoof\", \"https://www.asvspoof.org/\"),\n",
        "        (\"ADD2022\", \"https://example.com/add2022\")  # Placeholder URL; update if available\n",
        "    ]\n",
        "\n",
        "    #############################\n",
        "    # Process Video Sources\n",
        "    #############################\n",
        "    for name, url in video_sources:\n",
        "        print(f\"\\n[VIDEO SOURCE: {name}]\")\n",
        "        output_file = os.path.join(SCRAPED_URLS_DIR, f\"video_{name}_urls.csv\")\n",
        "        urls = await scrape_media_urls(name, url, parse_video_links, output_file)\n",
        "        if urls:\n",
        "            urls = urls[:20]\n",
        "        else:\n",
        "            print(f\"[{name.upper()}] No URLs scraped; using fallback.\")\n",
        "            urls = fallback_video_urls(20)\n",
        "        download_videos(urls, num_videos=20)\n",
        "\n",
        "    #############################\n",
        "    # Process Image Sources\n",
        "    #############################\n",
        "    for name, url in image_sources:\n",
        "        print(f\"\\n[IMAGE SOURCE: {name}]\")\n",
        "        output_file = os.path.join(SCRAPED_URLS_DIR, f\"image_{name}_urls.csv\")\n",
        "        urls = await scrape_media_urls(name, url, parse_image_links, output_file)\n",
        "        if urls:\n",
        "            urls = urls[:20]\n",
        "        else:\n",
        "            print(f\"[{name.upper()}] No URLs scraped; using fallback.\")\n",
        "            urls = fallback_image_urls(20, seed_keyword=name)\n",
        "        await download_images(urls)\n",
        "\n",
        "    #############################\n",
        "    # Process Audio Sources\n",
        "    #############################\n",
        "    all_audio_urls = []\n",
        "    for name, url in audio_sources:\n",
        "        print(f\"\\n[AUDIO SOURCE: {name}]\")\n",
        "        output_file = os.path.join(SCRAPED_URLS_DIR, f\"audio_{name}_urls.csv\")\n",
        "        urls = await scrape_media_urls(name, url, parse_audio_links, output_file)\n",
        "        if urls:\n",
        "            print(f\"[{name.upper()}] Scraped {len(urls)} audio URLs.\")\n",
        "            all_audio_urls.extend(urls)\n",
        "        else:\n",
        "            print(f\"[{name.upper()}] No URLs scraped from page {url}.\")\n",
        "    # Remove duplicates and limit to 50 audio URLs.\n",
        "    all_audio_urls = list({u for u in all_audio_urls if u})[:50]\n",
        "    # If no audio URLs were found, use fallback extraction.\n",
        "    if not all_audio_urls:\n",
        "        print(\"[AUDIO] No audio URLs scraped from all sources; using Playwright extraction fallback.\")\n",
        "        all_audio_urls = await extract_audio_links()\n",
        "    await download_audios(all_audio_urls)\n",
        "\n",
        "    #############################\n",
        "    # Save all download metadata\n",
        "    #############################\n",
        "    save_download_metadata(download_metadata)\n",
        "    print(\"[SYSTEM] All downloads complete. Check the respective directories for files.\")"
      ],
      "metadata": {
        "id": "JIsL0V9D2V6D"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 7. Entry Point with Event Loop Handling\n",
        "#########################################\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()\n",
        "except RuntimeError:\n",
        "    loop = None\n",
        "\n",
        "if loop and loop.is_running():\n",
        "    print(\"[SYSTEM] Detected running event loop. Using loop.run_until_complete()...\")\n",
        "    loop.run_until_complete(main_pipeline())\n",
        "else:\n",
        "    asyncio.run(main_pipeline())"
      ],
      "metadata": {
        "id": "1mK7e1ew2V74",
        "outputId": "614fef2e-f315-445d-a58b-3e537e56467a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYSTEM] Detected running event loop. Using loop.run_until_complete()...\n",
            "\n",
            "[VIDEO SOURCE: UADFV]\n",
            "[UADFV] Scraping URLs from https://ai.facebook.com/datasets/dfdc/\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://ai.facebook.com/datasets/dfdc/... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://ai.facebook.com/datasets/dfdc/... | Status: True | Total: 0.16s\n",
            "[UADFV] Saved 0 URLs to /content/scraped_urls/video_UADFV_urls.csv\n",
            "[UADFV] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: Deepfake-TIMIT]\n",
            "[DEEPFAKE-TIMIT] Scraping URLs from https://github.com/ondyari/FaceForensics\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/ondyari/FaceForensics... | Status: True | Time: 0.03s\n",
            "[COMPLETE] ● https://github.com/ondyari/FaceForensics... | Status: True | Total: 0.22s\n",
            "[DEEPFAKE-TIMIT] Saved 0 URLs to /content/scraped_urls/video_Deepfake-TIMIT_urls.csv\n",
            "[DEEPFAKE-TIMIT] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: DFFD]\n",
            "[DFFD] Scraping URLs from https://github.com/EndlessSora/DeeperForensics-1.0\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/EndlessSora/DeeperForensics-1.0... | Status: True | Time: 0.03s\n",
            "[COMPLETE] ● https://github.com/EndlessSora/DeeperForensics-1.0... | Status: True | Total: 0.21s\n",
            "[DFFD] Saved 0 URLs to /content/scraped_urls/video_DFFD_urls.csv\n",
            "[DFFD] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: Celeb-DF]\n",
            "[CELEB-DF] Scraping URLs from https://github.com/cypw/Celeb-DF\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/cypw/Celeb-DF... | Status: True | Time: 0.03s\n",
            "[COMPLETE] ● https://github.com/cypw/Celeb-DF... | Status: True | Total: 0.21s\n",
            "[CELEB-DF] Saved 0 URLs to /content/scraped_urls/video_Celeb-DF_urls.csv\n",
            "[CELEB-DF] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: DFDC]\n",
            "[DFDC] Scraping URLs from https://ai.facebook.com/datasets/dfdc/\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://ai.facebook.com/datasets/dfdc/... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://ai.facebook.com/datasets/dfdc/... | Status: True | Total: 0.16s\n",
            "[DFDC] Saved 0 URLs to /content/scraped_urls/video_DFDC_urls.csv\n",
            "[DFDC] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: FaceForensics++]\n",
            "[FACEFORENSICS++] Scraping URLs from https://github.com/ondyari/FaceForensics\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/ondyari/FaceForensics... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/ondyari/FaceForensics... | Status: True | Total: 0.17s\n",
            "[FACEFORENSICS++] Saved 0 URLs to /content/scraped_urls/video_FaceForensics++_urls.csv\n",
            "[FACEFORENSICS++] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: FFIW-10K]\n",
            "[FFIW-10K] Scraping URLs from https://github.com/ondyari/FaceForensics\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/ondyari/FaceForensics... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/ondyari/FaceForensics... | Status: True | Total: 0.18s\n",
            "[FFIW-10K] Saved 0 URLs to /content/scraped_urls/video_FFIW-10K_urls.csv\n",
            "[FFIW-10K] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: DeeperForensics-1.0]\n",
            "[DEEPERFORENSICS-1.0] Scraping URLs from https://github.com/EndlessSora/DeeperForensics-1.0\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/EndlessSora/DeeperForensics-1.0... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/EndlessSora/DeeperForensics-1.0... | Status: True | Total: 0.18s\n",
            "[DEEPERFORENSICS-1.0] Saved 0 URLs to /content/scraped_urls/video_DeeperForensics-1.0_urls.csv\n",
            "[DEEPERFORENSICS-1.0] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: WildDeepfake]\n",
            "[WILDDEEPFAKE] Scraping URLs from https://github.com/deepfakeinthewild/deepfake-in-the-wild\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/deepfakeinthewild/deepfake-in-t... | Status: True | Time: 0.03s\n",
            "[COMPLETE] ● https://github.com/deepfakeinthewild/deepfake-in-t... | Status: True | Total: 0.25s\n",
            "[WILDDEEPFAKE] Saved 0 URLs to /content/scraped_urls/video_WildDeepfake_urls.csv\n",
            "[WILDDEEPFAKE] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: ForgeryNet]\n",
            "[FORGERYNET] Scraping URLs from https://github.com/duxingdong/ForgeryNet\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/duxingdong/ForgeryNet... | Status: True | Time: 0.03s\n",
            "[COMPLETE] ● https://github.com/duxingdong/ForgeryNet... | Status: True | Total: 0.22s\n",
            "[FORGERYNET] Saved 0 URLs to /content/scraped_urls/video_ForgeryNet_urls.csv\n",
            "[FORGERYNET] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: AV-Deepfake1M]\n",
            "[AV-DEEPFAKE1M] Scraping URLs from https://github.com/ControlNet/AV-Deepfake1M\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/ControlNet/AV-Deepfake1M... | Status: True | Time: 0.02s\n",
            "[COMPLETE] ● https://github.com/ControlNet/AV-Deepfake1M... | Status: True | Total: 0.22s\n",
            "[AV-DEEPFAKE1M] Saved 0 URLs to /content/scraped_urls/video_AV-Deepfake1M_urls.csv\n",
            "[AV-DEEPFAKE1M] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: DeepFake MNIST+]\n",
            "[DEEPFAKE MNIST+] Scraping URLs from https://github.com/yourplaceholder/DeepFakeMNISTplus\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/yourplaceholder/DeepFakeMNISTpl... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/yourplaceholder/DeepFakeMNISTpl... | Status: True | Total: 0.21s\n",
            "[DEEPFAKE MNIST+] Saved 0 URLs to /content/scraped_urls/video_DeepFake MNIST+_urls.csv\n",
            "[DEEPFAKE MNIST+] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: VideoSham]\n",
            "[VIDEOSHAM] Scraping URLs from https://github.com/adobe-research/VideoSham-dataset\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/adobe-research/VideoSham-datase... | Status: True | Time: 0.05s\n",
            "[COMPLETE] ● https://github.com/adobe-research/VideoSham-datase... | Status: True | Total: 0.21s\n",
            "[VIDEOSHAM] Saved 0 URLs to /content/scraped_urls/video_VideoSham_urls.csv\n",
            "[VIDEOSHAM] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[VIDEO SOURCE: RWDF-23]\n",
            "[RWDF-23] Scraping URLs from https://rwdf23.example.com\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[ERROR]... × https://rwdf23.example.com... | Error: \n",
            "┌───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
            "│ × Unexpected error in _crawl_web at line 1354 in _crawl_web (../usr/local/lib/python3.11/dist-                        │\n",
            "│ packages/crawl4ai/async_crawler_strategy.py):                                                                         │\n",
            "│   Error: Failed on navigating ACS-GOTO:                                                                               │\n",
            "│   Page.goto: net::ERR_NAME_NOT_RESOLVED at https://rwdf23.example.com/                                                │\n",
            "│   Call log:                                                                                                           │\n",
            "│   - navigating to \"https://rwdf23.example.com/\", waiting until \"domcontentloaded\"                                     │\n",
            "│                                                                                                                       │\n",
            "│                                                                                                                       │\n",
            "│   Code context:                                                                                                       │\n",
            "│   1349                       response = await page.goto(                                                              │\n",
            "│   1350                           url, wait_until=config.wait_until, timeout=config.page_timeout                       │\n",
            "│   1351                       )                                                                                        │\n",
            "│   1352                       redirected_url = page.url                                                                │\n",
            "│   1353                   except Error as e:                                                                           │\n",
            "│   1354 →                     raise RuntimeError(f\"Failed on navigating ACS-GOTO:\\n{str(e)}\")                          │\n",
            "│   1355                                                                                                                │\n",
            "│   1356                   await self.execute_hook(                                                                     │\n",
            "│   1357                       \"after_goto\", page, context=context, url=url, response=response, config=config           │\n",
            "│   1358                   )                                                                                            │\n",
            "│   1359                                                                                                                │\n",
            "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "[RWDF-23] Saved 0 URLs to /content/scraped_urls/video_RWDF-23_urls.csv\n",
            "[RWDF-23] No URLs scraped; using fallback.\n",
            "[VIDEO] No scraped video URLs; using yt-dlp search fallback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: unable to download video data: HTTP Error 403: Forbidden\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] yt-dlp search fallback error: ERROR: unable to download video data: HTTP Error 403: Forbidden\n",
            "\n",
            "[IMAGE SOURCE: DFFD_Image]\n",
            "[DFFD_IMAGE] Scraping URLs from https://github.com/ondyari/FaceForensics\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/ondyari/FaceForensics... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/ondyari/FaceForensics... | Status: True | Total: 0.21s\n",
            "[DFFD_IMAGE] Saved 0 URLs to /content/scraped_urls/image_DFFD_Image_urls.csv\n",
            "[DFFD_IMAGE] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "\n",
            "[IMAGE SOURCE: iFakeFaceDB]\n",
            "[IFAKEFACEDB] Scraping URLs from https://github.com/YourRepository/iFakeFaceDB\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/YourRepository/iFakeFaceDB... | Status: True | Time: 0.02s\n",
            "[COMPLETE] ● https://github.com/YourRepository/iFakeFaceDB... | Status: True | Total: 0.19s\n",
            "[IFAKEFACEDB] Saved 0 URLs to /content/scraped_urls/image_iFakeFaceDB_urls.csv\n",
            "[IFAKEFACEDB] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "\n",
            "[IMAGE SOURCE: 100kFaces]\n",
            "[100KFACES] Scraping URLs from https://thiswaifudoesnotexist.net/\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://thiswaifudoesnotexist.net/... | Status: True | Time: 0.02s\n",
            "[COMPLETE] ● https://thiswaifudoesnotexist.net/... | Status: True | Total: 0.21s\n",
            "[100KFACES] Saved 0 URLs to /content/scraped_urls/image_100kFaces_urls.csv\n",
            "[100KFACES] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Attempt 1: Status 503 for https://picsum.photos/seed/100kFaces1/600/400\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "\n",
            "[IMAGE SOURCE: FFHQ]\n",
            "[FFHQ] Scraping URLs from https://github.com/NVlabs/ffhq-dataset\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/NVlabs/ffhq-dataset... | Status: True | Time: 0.02s\n",
            "[COMPLETE] ● https://github.com/NVlabs/ffhq-dataset... | Status: True | Total: 0.21s\n",
            "[FFHQ] Saved 0 URLs to /content/scraped_urls/image_FFHQ_urls.csv\n",
            "[FFHQ] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Attempt 1: Status 503 for https://picsum.photos/seed/FFHQ14/600/400\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "\n",
            "[IMAGE SOURCE: ForgeryNet_Image]\n",
            "[FORGERYNET_IMAGE] Scraping URLs from https://github.com/duxingdong/ForgeryNet\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/duxingdong/ForgeryNet... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ● https://github.com/duxingdong/ForgeryNet... | Status: True | Total: 0.27s\n",
            "[FORGERYNET_IMAGE] Saved 0 URLs to /content/scraped_urls/image_ForgeryNet_Image_urls.csv\n",
            "[FORGERYNET_IMAGE] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "\n",
            "[IMAGE SOURCE: DeepFakeRealImages]\n",
            "[DEEPFAKEREALIMAGES] Scraping URLs from https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://www.kaggle.com/datasets/manjilkarki/deepfa... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://www.kaggle.com/datasets/manjilkarki/deepfa... | Status: True | Total: 0.21s\n",
            "[DEEPFAKEREALIMAGES] Saved 0 URLs to /content/scraped_urls/image_DeepFakeRealImages_urls.csv\n",
            "[DEEPFAKEREALIMAGES] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Attempt 1: Status 503 for https://picsum.photos/seed/DeepFakeRealImages15/600/400\n",
            "[IMAGE] Attempt 1: Status 503 for https://picsum.photos/seed/DeepFakeRealImages13/600/400\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "\n",
            "[IMAGE SOURCE: DeepfakeDetection_Images]\n",
            "[DEEPFAKEDETECTION_IMAGES] Scraping URLs from https://github.com/yourrepository/deepfake-detection-images\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://github.com/yourrepository/deepfake-detecti... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://github.com/yourrepository/deepfake-detecti... | Status: True | Total: 0.23s\n",
            "[DEEPFAKEDETECTION_IMAGES] Saved 0 URLs to /content/scraped_urls/image_DeepfakeDetection_Images_urls.csv\n",
            "[DEEPFAKEDETECTION_IMAGES] No URLs scraped; using fallback.\n",
            "[IMAGE] Starting image downloads...\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_003.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_014.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_016.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_009.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_007.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_006.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_002.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_008.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_019.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_004.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_013.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_011.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_001.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_005.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_020.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_015.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_017.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_018.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_010.jpg\n",
            "[IMAGE] Downloaded: /content/deepfake_images/deepfake_image_012.jpg\n",
            "\n",
            "[AUDIO SOURCE: WaveFake]\n",
            "[WAVEFAKE] Scraping URLs from https://paperswithcode.com/dataset/wavefake\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://paperswithcode.com/dataset/wavefake... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ● https://paperswithcode.com/dataset/wavefake... | Status: True | Total: 0.28s\n",
            "[WAVEFAKE] Saved 0 URLs to /content/scraped_urls/audio_WaveFake_urls.csv\n",
            "[WAVEFAKE] No URLs scraped from page https://paperswithcode.com/dataset/wavefake.\n",
            "\n",
            "[AUDIO SOURCE: DEEP-VOICE]\n",
            "[DEEP-VOICE] Scraping URLs from https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://www.kaggle.com/datasets/birdy654/deep-voic... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://www.kaggle.com/datasets/birdy654/deep-voic... | Status: True | Total: 0.22s\n",
            "[DEEP-VOICE] Saved 0 URLs to /content/scraped_urls/audio_DEEP-VOICE_urls.csv\n",
            "[DEEP-VOICE] No URLs scraped from page https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition.\n",
            "\n",
            "[AUDIO SOURCE: InTheWild_Audio]\n",
            "[INTHEWILD_AUDIO] Scraping URLs from https://www.kaggle.com/datasets/abdallamohamed312/in-the-wild-audio-deepfake\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://www.kaggle.com/datasets/abdallamohamed312/... | Status: True | Time: 0.00s\n",
            "[COMPLETE] ● https://www.kaggle.com/datasets/abdallamohamed312/... | Status: True | Total: 0.23s\n",
            "[INTHEWILD_AUDIO] Saved 0 URLs to /content/scraped_urls/audio_InTheWild_Audio_urls.csv\n",
            "[INTHEWILD_AUDIO] No URLs scraped from page https://www.kaggle.com/datasets/abdallamohamed312/in-the-wild-audio-deepfake.\n",
            "\n",
            "[AUDIO SOURCE: Codecfake]\n",
            "[CODECFAKE] Scraping URLs from https://arxiv.org/abs/2405.04880\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://arxiv.org/abs/2405.04880... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://arxiv.org/abs/2405.04880... | Status: True | Total: 0.21s\n",
            "[CODECFAKE] Saved 0 URLs to /content/scraped_urls/audio_Codecfake_urls.csv\n",
            "[CODECFAKE] No URLs scraped from page https://arxiv.org/abs/2405.04880.\n",
            "\n",
            "[AUDIO SOURCE: CrossDomain_ADD]\n",
            "[CROSSDOMAIN_ADD] Scraping URLs from https://arxiv.org/abs/2404.04904\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://arxiv.org/abs/2404.04904... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://arxiv.org/abs/2404.04904... | Status: True | Total: 0.26s\n",
            "[CROSSDOMAIN_ADD] Saved 0 URLs to /content/scraped_urls/audio_CrossDomain_ADD_urls.csv\n",
            "[CROSSDOMAIN_ADD] No URLs scraped from page https://arxiv.org/abs/2404.04904.\n",
            "\n",
            "[AUDIO SOURCE: DeepFake-Audio-Rangers_Arabic]\n",
            "[DEEPFAKE-AUDIO-RANGERS_ARABIC] Scraping URLs from https://huggingface.co/datasets/DeepFake-Audio-Rangers/Arabic_Audio_Deepfake\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://huggingface.co/datasets/DeepFake-Audio-Ran... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://huggingface.co/datasets/DeepFake-Audio-Ran... | Status: True | Total: 0.25s\n",
            "[DEEPFAKE-AUDIO-RANGERS_ARABIC] Saved 0 URLs to /content/scraped_urls/audio_DeepFake-Audio-Rangers_Arabic_urls.csv\n",
            "[DEEPFAKE-AUDIO-RANGERS_ARABIC] No URLs scraped from page https://huggingface.co/datasets/DeepFake-Audio-Rangers/Arabic_Audio_Deepfake.\n",
            "\n",
            "[AUDIO SOURCE: SONICS]\n",
            "[SONICS] Scraping URLs from https://paperswithcode.com/dataset/sonics\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://paperswithcode.com/dataset/sonics... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ● https://paperswithcode.com/dataset/sonics... | Status: True | Total: 0.27s\n",
            "[SONICS] Saved 0 URLs to /content/scraped_urls/audio_SONICS_urls.csv\n",
            "[SONICS] No URLs scraped from page https://paperswithcode.com/dataset/sonics.\n",
            "\n",
            "[AUDIO SOURCE: ASVspoof]\n",
            "[ASVSPOOF] Scraping URLs from https://www.asvspoof.org/\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://www.asvspoof.org/... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://www.asvspoof.org/... | Status: True | Total: 0.25s\n",
            "[ASVSPOOF] Saved 0 URLs to /content/scraped_urls/audio_ASVspoof_urls.csv\n",
            "[ASVSPOOF] No URLs scraped from page https://www.asvspoof.org/.\n",
            "\n",
            "[AUDIO SOURCE: ADD2022]\n",
            "[ADD2022] Scraping URLs from https://example.com/add2022\n",
            "[INIT].... → Crawl4AI 0.4.248\n",
            "[FETCH]... ↓ https://example.com/add2022... | Status: True | Time: 0.01s\n",
            "[COMPLETE] ● https://example.com/add2022... | Status: True | Total: 0.22s\n",
            "[ADD2022] Saved 0 URLs to /content/scraped_urls/audio_ADD2022_urls.csv\n",
            "[ADD2022] No URLs scraped from page https://example.com/add2022.\n",
            "[AUDIO] No audio URLs scraped from all sources; using Playwright extraction fallback.\n",
            "[AUDIO] Extracting audio URLs via Playwright fallback...\n",
            "[AUDIO] Playwright extraction error: Page.wait_for_selector: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "  - waiting for locator(\"audio\") to be visible\n",
            "\n",
            "[AUDIO] No audio URLs provided for download.\n",
            "[SYSTEM] Download metadata saved to /content/download_metadata.json\n",
            "[SYSTEM] All downloads complete. Check the respective directories for files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvJi_BiD2V_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}