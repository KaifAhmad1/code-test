{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3adQdF8KJWyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9139ff9-f950-41a4-b0e8-fcbb70ad9120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "langchain 0.3.16 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.2 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "%pip install -qU soundfile numpy datasets pandas pillow tqdm huggingface_hub decord"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import hashlib\n",
        "import requests\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeMediaCollector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dir: str = \"./deepfake_dataset\",\n",
        "        max_samples: int = 20,\n",
        "        max_retries: int = 3,\n",
        "        timeout: int = 30,\n",
        "        max_workers: int = 4\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the DeepfakeMediaCollector with improved configuration options.\n",
        "\n",
        "        Args:\n",
        "            base_dir (str): Base directory for storing downloaded files\n",
        "            max_samples (int): Maximum number of samples to collect per category\n",
        "            max_retries (int): Maximum number of download retry attempts\n",
        "            timeout (int): Download timeout in seconds\n",
        "            max_workers (int): Maximum number of concurrent download workers\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.max_retries = max_retries\n",
        "        self.timeout = timeout\n",
        "        self.max_workers = max_workers\n",
        "        self.metadata: List[Dict] = []\n",
        "        self.temp_dir = self.base_dir / \"temp\"\n",
        "\n",
        "        # Create directories with error handling\n",
        "        self._create_directory_structure()\n",
        "\n",
        "        # Initialize session for connection pooling\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create the necessary directory structure with error handling\"\"\"\n",
        "        try:\n",
        "            for dir_type in ['video', 'image', 'audio']:\n",
        "                for category in ['real', 'fake']:\n",
        "                    dir_path = self.base_dir / dir_type / category\n",
        "                    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "            self.temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create directory structure: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def download_with_progress(self, url: str, output_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Download file with progress bar using aiohttp for better async support.\n",
        "\n",
        "        Args:\n",
        "            url (str): URL to download from\n",
        "            output_path (Path): Where to save the file\n",
        "\n",
        "        Returns:\n",
        "            bool: True if download successful, False otherwise\n",
        "        \"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    async with session.get(url, timeout=self.timeout) as response:\n",
        "                        if response.status != 200:\n",
        "                            logger.warning(f\"Attempt {attempt + 1} failed: HTTP {response.status}\")\n",
        "                            continue\n",
        "\n",
        "                        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "                        with open(output_path, 'wb') as f, tqdm(\n",
        "                            desc=output_path.name,\n",
        "                            total=total_size,\n",
        "                            unit='iB',\n",
        "                            unit_scale=True\n",
        "                        ) as pbar:\n",
        "                            async for chunk in response.content.iter_chunked(8192):\n",
        "                                size = f.write(chunk)\n",
        "                                pbar.update(size)\n",
        "\n",
        "                        return True\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                logger.warning(f\"Timeout on attempt {attempt + 1} for {url}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Download failed on attempt {attempt + 1} for {url}: {str(e)}\")\n",
        "                if output_path.exists():\n",
        "                    output_path.unlink()\n",
        "\n",
        "            # Add exponential backoff between retries\n",
        "            await asyncio.sleep(2 ** attempt)\n",
        "\n",
        "        return False\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> Tuple[bool, Optional[str]]:\n",
        "        \"\"\"\n",
        "        Validate media files with improved error handling and metadata extraction.\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to media file\n",
        "            media_type (str): Type of media ('video', 'image', or 'audio')\n",
        "\n",
        "        Returns:\n",
        "            Tuple[bool, Optional[str]]: (is_valid, error_message)\n",
        "        \"\"\"\n",
        "        if not file_path.exists():\n",
        "            return False, \"File does not exist\"\n",
        "\n",
        "        if file_path.stat().st_size == 0:\n",
        "            return False, \"File is empty\"\n",
        "\n",
        "        try:\n",
        "            if media_type == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                frame_count = len(vr)\n",
        "                if frame_count == 0:\n",
        "                    return False, \"Video has no frames\"\n",
        "                # Test first frame access\n",
        "                _ = vr[0].asnumpy()\n",
        "                return True, None\n",
        "\n",
        "            elif media_type == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                    # Extract basic image metadata\n",
        "                    return True, None\n",
        "\n",
        "            elif media_type == 'audio':\n",
        "                data, samplerate = sf.read(file_path)\n",
        "                if len(data) == 0:\n",
        "                    return False, \"Audio file is empty\"\n",
        "                return True, None\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def add_to_metadata(self, modality: str, category: str, file_path: Path):\n",
        "        \"\"\"Add entry to metadata with enhanced file analysis\"\"\"\n",
        "        try:\n",
        "            # Calculate both MD5 and SHA256 hashes\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read()\n",
        "                md5_hash = hashlib.md5(content).hexdigest()\n",
        "                sha256_hash = hashlib.sha256(content).hexdigest()\n",
        "\n",
        "            file_stats = file_path.stat()\n",
        "\n",
        "            metadata_entry = {\n",
        "                'modality': modality,\n",
        "                'category': category,\n",
        "                'filename': file_path.name,\n",
        "                'file_path': str(file_path),\n",
        "                'file_size': file_stats.st_size,\n",
        "                'md5_hash': md5_hash,\n",
        "                'sha256_hash': sha256_hash,\n",
        "                'creation_time': file_stats.st_ctime,\n",
        "                'modification_time': file_stats.st_mtime,\n",
        "                'manipulation': 'None' if category == 'real' else 'unknown'\n",
        "            }\n",
        "\n",
        "            # Add modality-specific metadata\n",
        "            if modality == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                metadata_entry.update({\n",
        "                    'frame_count': len(vr),\n",
        "                    'width': vr[0].shape[1],\n",
        "                    'height': vr[0].shape[0],\n",
        "                })\n",
        "            elif modality == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    metadata_entry.update({\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format,\n",
        "                    })\n",
        "            elif modality == 'audio':\n",
        "                data, samplerate = sf.read(file_path)\n",
        "                metadata_entry.update({\n",
        "                    'duration': len(data) / samplerate,\n",
        "                    'samplerate': samplerate,\n",
        "                    'channels': data.shape[1] if len(data.shape) > 1 else 1,\n",
        "                })\n",
        "\n",
        "            self.metadata.append(metadata_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to add metadata for {file_path}: {str(e)}\")\n",
        "\n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save metadata with enhanced reporting\"\"\"\n",
        "        if not self.metadata:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Save detailed CSV\n",
        "            metadata_df = pd.DataFrame(self.metadata)\n",
        "            metadata_path = self.base_dir / \"metadata.csv\"\n",
        "            metadata_df.to_csv(metadata_path, index=False)\n",
        "\n",
        "            # Generate comprehensive summary\n",
        "            summary = {\n",
        "                'total_files': len(metadata_df),\n",
        "                'total_size_mb': metadata_df['file_size'].sum() / (1024 * 1024),\n",
        "                'by_modality': metadata_df.groupby('modality').agg({\n",
        "                    'filename': 'count',\n",
        "                    'file_size': ['sum', 'mean', 'min', 'max']\n",
        "                }).round(2),\n",
        "                'by_category': metadata_df.groupby('category').agg({\n",
        "                    'filename': 'count',\n",
        "                    'file_size': ['sum', 'mean']\n",
        "                }).round(2)\n",
        "            }\n",
        "\n",
        "            # Save detailed summary report\n",
        "            summary_path = self.base_dir / \"summary.txt\"\n",
        "            with open(summary_path, 'w') as f:\n",
        "                f.write(\"Dataset Summary Report\\n\")\n",
        "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                f.write(\"Overall Statistics:\\n\")\n",
        "                f.write(f\"Total files: {summary['total_files']}\\n\")\n",
        "                f.write(f\"Total size: {summary['total_size_mb']:.2f} MB\\n\\n\")\n",
        "\n",
        "                f.write(\"By Modality:\\n\")\n",
        "                f.write(str(summary['by_modality']))\n",
        "                f.write(\"\\n\\nBy Category:\\n\")\n",
        "                f.write(str(summary['by_category']))\n",
        "\n",
        "            # Save JSON version for programmatic access\n",
        "            with open(self.base_dir / \"summary.json\", 'w') as f:\n",
        "                json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "            logger.info(f\"Metadata and summaries saved to {self.base_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metadata: {str(e)}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files and resources\"\"\"\n",
        "        try:\n",
        "            if self.temp_dir.exists():\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "\n",
        "            # Close the requests session\n",
        "            self.session.close()\n",
        "\n",
        "            logger.info(\"Cleanup completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cleanup failed: {str(e)}\")\n",
        "\n",
        "    async def async_download_all(self):\n",
        "        \"\"\"Download all samples asynchronously\"\"\"\n",
        "        tasks = []\n",
        "\n",
        "        for url_dict in [self.video_urls, self.image_urls, self.audio_urls]:\n",
        "            for category, urls in url_dict.items():\n",
        "                for url in urls:\n",
        "                    output_path = self._get_output_path(url, category)\n",
        "                    tasks.append(self.download_with_progress(url, output_path))\n",
        "\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        return results\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        collector = DeepfakeMediaCollector()\n",
        "\n",
        "        # Run async downloads\n",
        "        asyncio.run(collector.async_download_all())\n",
        "\n",
        "        # Save metadata and cleanup\n",
        "        collector.save_metadata()\n",
        "        collector.cleanup()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in main: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "8846eff0-883c-49ec-8ed9-54b79c884a9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:An error occurred in main: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-c01e4cb2b9e9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Run async downloads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_download_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Save metadata and cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw1aBSV6NLMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}