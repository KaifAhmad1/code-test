{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Deepfake and Manipulated Media Analysis Data Download**"
      ],
      "metadata": {
        "id": "V5er949VRqvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3adQdF8KJWyE"
      },
      "outputs": [],
      "source": [
        "!pip install -q yt-dlp aiohttp decord pandas pillow soundfile tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import decord\n",
        "from decord import VideoReader\n",
        "import hashlib\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Import yt_dlp for video downloads\n",
        "import yt_dlp"
      ],
      "metadata": {
        "id": "E5SKXUPiJobX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "pxb5lV48LQyB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeMediaCollector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dir: str = \"./deepfake_dataset\",\n",
        "        max_samples: int = 20,  # Download 20 samples per category\n",
        "        max_retries: int = 3,\n",
        "        timeout: int = 30,\n",
        "        max_workers: int = 4  # Controls concurrent downloads\n",
        "    ):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.max_samples = max_samples\n",
        "        self.max_retries = max_retries\n",
        "        self.timeout = timeout\n",
        "        self.max_workers = max_workers\n",
        "        self.metadata: List[Dict] = []\n",
        "        self.temp_dir = self.base_dir / \"temp\"\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.semaphore = asyncio.Semaphore(self.max_workers)\n",
        "        self._create_directory_structure()\n",
        "\n",
        "    def _create_directory_structure(self):\n",
        "        \"\"\"Create the necessary directory structure with error handling.\"\"\"\n",
        "        try:\n",
        "            for dir_type in ['video', 'image', 'audio']:\n",
        "                for category in ['real', 'fake']:\n",
        "                    (self.base_dir / dir_type / category).mkdir(parents=True, exist_ok=True)\n",
        "            self.temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create directory structure: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def init_session(self):\n",
        "        \"\"\"Initialize a single aiohttp session for all downloads.\"\"\"\n",
        "        if self.session is None:\n",
        "            self.session = aiohttp.ClientSession()\n",
        "\n",
        "    async def download_file(self, url: str, output_path: Path) -> bool:\n",
        "        \"\"\"Download a single file with retry logic using the shared session.\"\"\"\n",
        "        # Skip download if file already exists and is nonempty\n",
        "        if output_path.exists() and output_path.stat().st_size > 0:\n",
        "            logger.info(f\"File already exists, skipping: {output_path}\")\n",
        "            return True\n",
        "\n",
        "        async with self.semaphore:\n",
        "            for attempt in range(self.max_retries):\n",
        "                try:\n",
        "                    async with self.session.get(url, timeout=self.timeout) as response:\n",
        "                        if response.status != 200:\n",
        "                            logger.warning(f\"Attempt {attempt + 1}: HTTP {response.status} for {url}\")\n",
        "                            continue\n",
        "\n",
        "                        total_size = int(response.headers.get('content-length', 0))\n",
        "                        with open(output_path, 'wb') as f, tqdm(\n",
        "                            desc=output_path.name,\n",
        "                            total=total_size,\n",
        "                            unit='iB',\n",
        "                            unit_scale=True\n",
        "                        ) as pbar:\n",
        "                            async for chunk in response.content.iter_chunked(8192):\n",
        "                                size = f.write(chunk)\n",
        "                                pbar.update(size)\n",
        "                        return True\n",
        "\n",
        "                except asyncio.TimeoutError:\n",
        "                    logger.warning(f\"Timeout on attempt {attempt + 1} for {url}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Download failed on attempt {attempt + 1} for {url}: {str(e)}\")\n",
        "                    if output_path.exists():\n",
        "                        output_path.unlink()\n",
        "                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "            return False\n",
        "\n",
        "    async def download_with_yt_dlp(self, url: str, output_dir: Path) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Download a video using yt-dlp.\n",
        "        The output directory is specified, and a hook captures the final filename.\n",
        "        \"\"\"\n",
        "        downloaded_file = None\n",
        "\n",
        "        def hook(d):\n",
        "            nonlocal downloaded_file\n",
        "            if d.get('status') == 'finished':\n",
        "                downloaded_file = d.get('filename')\n",
        "\n",
        "        ydl_opts = {\n",
        "            'outtmpl': str(output_dir / '%(title)s.%(ext)s'),\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "            'progress_hooks': [hook],\n",
        "        }\n",
        "        try:\n",
        "            # Run yt-dlp download in a separate thread.\n",
        "            await asyncio.to_thread(lambda: yt_dlp.YoutubeDL(ydl_opts).download([url]))\n",
        "            return downloaded_file\n",
        "        except Exception as e:\n",
        "            logger.error(f\"yt-dlp download failed for {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def validate_media_file(self, file_path: Path, media_type: str) -> Tuple[bool, Optional[str]]:\n",
        "        \"\"\"Validate downloaded media files.\"\"\"\n",
        "        if not file_path.exists():\n",
        "            return False, \"File does not exist\"\n",
        "        if file_path.stat().st_size == 0:\n",
        "            return False, \"File is empty\"\n",
        "        try:\n",
        "            if media_type == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                if len(vr) == 0:\n",
        "                    return False, \"Video has no frames\"\n",
        "                _ = vr[0].asnumpy()\n",
        "                return True, None\n",
        "            elif media_type == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                return True, None\n",
        "            elif media_type == 'audio':\n",
        "                data, _ = sf.read(file_path)\n",
        "                if len(data) == 0:\n",
        "                    return False, \"Audio file is empty\"\n",
        "                return True, None\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "    def add_to_metadata(self, modality: str, category: str, file_path: Path):\n",
        "        \"\"\"Add file metadata to collection.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                content = f.read()\n",
        "                md5_hash = hashlib.md5(content).hexdigest()\n",
        "                sha256_hash = hashlib.sha256(content).hexdigest()\n",
        "            file_stats = file_path.stat()\n",
        "            metadata_entry = {\n",
        "                'modality': modality,\n",
        "                'category': category,\n",
        "                'filename': file_path.name,\n",
        "                'file_path': str(file_path),\n",
        "                'file_size': file_stats.st_size,\n",
        "                'md5_hash': md5_hash,\n",
        "                'sha256_hash': sha256_hash,\n",
        "                'creation_time': file_stats.st_ctime,\n",
        "                'modification_time': file_stats.st_mtime\n",
        "            }\n",
        "            # Add modality-specific metadata.\n",
        "            if modality == 'video':\n",
        "                vr = VideoReader(str(file_path))\n",
        "                metadata_entry.update({\n",
        "                    'frame_count': len(vr),\n",
        "                    'width': vr[0].shape[1],\n",
        "                    'height': vr[0].shape[0],\n",
        "                })\n",
        "            elif modality == 'image':\n",
        "                with Image.open(file_path) as img:\n",
        "                    metadata_entry.update({\n",
        "                        'width': img.width,\n",
        "                        'height': img.height,\n",
        "                        'mode': img.mode,\n",
        "                        'format': img.format,\n",
        "                    })\n",
        "            elif modality == 'audio':\n",
        "                data, samplerate = sf.read(file_path)\n",
        "                metadata_entry.update({\n",
        "                    'duration': len(data) / samplerate,\n",
        "                    'samplerate': samplerate,\n",
        "                    'channels': data.shape[1] if len(data.shape) > 1 else 1,\n",
        "                })\n",
        "            self.metadata.append(metadata_entry)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to add metadata for {file_path}: {str(e)}\")\n",
        "\n",
        "    async def process_url(self, url: str, category: str, media_type: str):\n",
        "        \"\"\"\n",
        "        Process a single URL: download (using yt-dlp for supported sites),\n",
        "        validate, and add metadata.\n",
        "        \"\"\"\n",
        "        if any(domain in url for domain in ['youtube.com', 'youtu.be']):\n",
        "            output_dir = self.base_dir / media_type / category\n",
        "            output_dir.mkdir(parents=True, exist_ok=True)\n",
        "            logger.info(f\"Using yt-dlp to download: {url}\")\n",
        "            downloaded_file = await self.download_with_yt_dlp(url, output_dir)\n",
        "            if downloaded_file:\n",
        "                output_path = Path(downloaded_file)\n",
        "                is_valid, error_msg = self.validate_media_file(output_path, media_type)\n",
        "                if is_valid:\n",
        "                    self.add_to_metadata(media_type, category, output_path)\n",
        "                else:\n",
        "                    logger.error(f\"Invalid {media_type} file {output_path}: {error_msg}\")\n",
        "                    output_path.unlink(missing_ok=True)\n",
        "            else:\n",
        "                logger.error(f\"Failed to download with yt-dlp: {url}\")\n",
        "        else:\n",
        "            output_path = self.base_dir / media_type / category / Path(url).name\n",
        "            if await self.download_file(url, output_path):\n",
        "                is_valid, error_msg = self.validate_media_file(output_path, media_type)\n",
        "                if is_valid:\n",
        "                    self.add_to_metadata(media_type, category, output_path)\n",
        "                else:\n",
        "                    logger.error(f\"Invalid {media_type} file {output_path}: {error_msg}\")\n",
        "                    output_path.unlink(missing_ok=True)\n",
        "\n",
        "    async def process_urls(self, urls_dict: Dict[str, List[str]], media_type: str):\n",
        "        \"\"\"Process a batch of URLs for a specific media type.\"\"\"\n",
        "        tasks = [\n",
        "            self.process_url(url, category, media_type)\n",
        "            for category, urls in urls_dict.items()\n",
        "            for url in urls[:self.max_samples]\n",
        "        ]\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save metadata and generate summary.\"\"\"\n",
        "        if not self.metadata:\n",
        "            logger.warning(\"No metadata to save\")\n",
        "            return\n",
        "        try:\n",
        "            metadata_df = pd.DataFrame(self.metadata)\n",
        "            metadata_df.to_csv(self.base_dir / \"metadata.csv\", index=False)\n",
        "            summary = {\n",
        "                'total_files': len(metadata_df),\n",
        "                'total_size_mb': metadata_df['file_size'].sum() / (1024 * 1024),\n",
        "                'by_modality': metadata_df.groupby('modality')['filename'].count().to_dict(),\n",
        "                'by_category': metadata_df.groupby('category')['filename'].count().to_dict()\n",
        "            }\n",
        "            with open(self.base_dir / \"summary.json\", 'w') as f:\n",
        "                json.dump(summary, f, indent=2)\n",
        "            logger.info(f\"Metadata and summary saved to {self.base_dir}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metadata: {str(e)}\")\n",
        "\n",
        "    async def close_session(self):\n",
        "        \"\"\"Properly close the aiohttp session.\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def cleanup(self):\n",
        "        \"\"\"Clean up temporary files and close session.\"\"\"\n",
        "        try:\n",
        "            if self.temp_dir.exists():\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "            logger.info(\"Cleanup completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cleanup failed: {str(e)}\")\n",
        "        await self.close_session()\n",
        "\n",
        "async def run_collector(collector: DeepfakeMediaCollector, urls: Dict[str, Dict[str, List[str]]]):\n",
        "    \"\"\"Run the collector with proper async handling.\"\"\"\n",
        "    await collector.init_session()\n",
        "    # Process each modality separately by passing only the corresponding dictionary.\n",
        "    await collector.process_urls(urls['video'], 'video')\n",
        "    await collector.process_urls(urls['image'], 'image')\n",
        "    await collector.process_urls(urls['audio'], 'audio')\n",
        "    collector.save_metadata()\n",
        "    await collector.cleanup()"
      ],
      "metadata": {
        "id": "v8dnHHHBYRQo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_urls():\n",
        "    \"\"\"\n",
        "    Initialize URLs for the DeepfakeMediaCollector using YouTube videos and other sources.\n",
        "    Returns a dictionary containing URLs for video, image, and audio samples.\n",
        "    \"\"\"\n",
        "\n",
        "    # Real videos (verified authentic content from official channels)\n",
        "    real_video_urls = [\n",
        "        # Music Videos\n",
        "        \"https://www.youtube.com/watch?v=9bZkp7q19f0\",  # PSY - Gangnam Style\n",
        "        \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",  # Rick Astley - Never Gonna Give You Up\n",
        "        \"https://www.youtube.com/watch?v=kJQP7kiw5Fk\",  # Luis Fonsi - Despacito\n",
        "        # Sports Highlights\n",
        "        \"https://www.youtube.com/watch?v=jofNR_WkoCE\",  # Official NFL Highlights\n",
        "        \"https://www.youtube.com/watch?v=ZnXA0PoEE6Y\",  # NBA Top Plays\n",
        "        # Nature/Documentary\n",
        "        \"https://www.youtube.com/watch?v=K1Y6PchDYfw\",  # National Geographic\n",
        "        \"https://www.youtube.com/watch?v=B1wOK9yGUYM\",  # BBC Earth\n",
        "        # Tech Reviews\n",
        "        \"https://www.youtube.com/watch?v=mW6hFttt_KE\",  # MKBHD Tech Review\n",
        "        \"https://www.youtube.com/watch?v=8jD6F1F4sds\",  # Linus Tech Tips\n",
        "        # Cooking Videos\n",
        "        \"https://www.youtube.com/watch?v=PUP7U5vTMM0\"   # Gordon Ramsay Cooking\n",
        "    ]\n",
        "\n",
        "    # Known deepfake videos (clearly labeled as AI-generated/deepfake content)\n",
        "    fake_video_urls = [\n",
        "        # Celebrity Deepfakes\n",
        "        \"https://www.youtube.com/watch?v=cQ54GDm1eL0\",  # Morgan Freeman Deepfake\n",
        "        \"https://www.youtube.com/watch?v=bPhUhypV27w\",  # Tom Cruise Deepfake\n",
        "        \"https://www.youtube.com/watch?v=oxXpB9pSETo\",  # Biden Deepfake\n",
        "        # AI Generated Content\n",
        "        \"https://www.youtube.com/watch?v=o-YBDTqX_ZU\",  # AI News Anchor\n",
        "        \"https://www.youtube.com/watch?v=AmUC4m6w1wo\",  # AI Generated Speech\n",
        "        # Educational Deepfake Examples\n",
        "        \"https://www.youtube.com/watch?v=C8FO0P2a3dA\",  # Deepfake Detection\n",
        "        \"https://www.youtube.com/watch?v=T76bK2t2r8g\",  # AI Voice Demo\n",
        "        # Synthetic Media Demos\n",
        "        \"https://www.youtube.com/watch?v=u_sJ3HgKZpk\",  # AI Dance Generation\n",
        "        \"https://www.youtube.com/watch?v=6h_BARSvBGw\",  # Synthetic Media Example\n",
        "        \"https://www.youtube.com/watch?v=MVaMv6VzRyk\"   # AI Video Generation\n",
        "    ]\n",
        "\n",
        "    # Real images from verified sources\n",
        "    real_image_urls = [\n",
        "        # Portrait Photography\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/2/23/Official_Presidential_portrait_of_Barack_Obama.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/d/d3/Albert_Einstein_Head.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/1/16/Official_Portrait_of_President_Reagan_1981.jpg\",\n",
        "        # Nature Photography\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/e/e3/Magnificent_CM_Leung.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/3/36/Hopetoun_falls.jpg\",\n",
        "        # Photojournalism\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/5/5b/January_6_Electoral_College_Vote_Count.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Celebrations_on_VJ_Day_in_Hawaii.jpg\",\n",
        "        # Sports Photography\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/8/8c/2021_US_Open_Tennis.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/b/b3/2016_Summer_Olympics_opening_ceremony_1035321-olimpiadas_abertura-4066.jpg\",\n",
        "        # Architecture Photography\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/a/a5/Eiffel_Tower_March_2014.jpg\"\n",
        "    ]\n",
        "\n",
        "    # AI-generated images\n",
        "    fake_image_urls = [\n",
        "        # ThisPersonDoesNotExist\n",
        "        \"https://thispersondoesnotexist.xyz/img/1.jpg\",\n",
        "        \"https://thispersondoesnotexist.xyz/img/2.jpg\",\n",
        "        \"https://thispersondoesnotexist.xyz/img/3.jpg\",\n",
        "        \"https://thispersondoesnotexist.xyz/img/4.jpg\",\n",
        "        \"https://thispersondoesnotexist.xyz/img/5.jpg\",\n",
        "        # Generated Art\n",
        "        \"https://storage.googleapis.com/ai_generated_images/art1.jpg\",\n",
        "        \"https://storage.googleapis.com/ai_generated_images/art2.jpg\",\n",
        "        \"https://storage.googleapis.com/ai_generated_images/art3.jpg\",\n",
        "        \"https://storage.googleapis.com/ai_generated_images/art4.jpg\",\n",
        "        \"https://storage.googleapis.com/ai_generated_images/art5.jpg\"\n",
        "    ]\n",
        "\n",
        "    # Real audio samples\n",
        "    real_audio_urls = [\n",
        "        # Speech Recordings\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/speech.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/introduction.wav\",\n",
        "        # Music Samples\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/BabyElephantWalk60.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/CantinaBand60.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/PinkPanther30.wav\",\n",
        "        # Nature Sounds\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/birds.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/ocean.wav\",\n",
        "        # Instrument Samples\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/piano.wav\",\n",
        "        \"https://www2.cs.uic.edu/~i101/SoundFiles/guitar.wav\"\n",
        "    ]\n",
        "\n",
        "    # Synthetic audio samples\n",
        "    fake_audio_urls = [\n",
        "        # AI Generated Speech\n",
        "        \"https://storage.googleapis.com/synthetic_speech/speech1.wav\",\n",
        "        \"https://storage.googleapis.com/synthetic_speech/speech2.wav\",\n",
        "        \"https://storage.googleapis.com/synthetic_speech/speech3.wav\",\n",
        "        \"https://storage.googleapis.com/synthetic_speech/speech4.wav\",\n",
        "        \"https://storage.googleapis.com/synthetic_speech/speech5.wav\",\n",
        "        # AI Music\n",
        "        \"https://storage.googleapis.com/ai_music/song1.wav\",\n",
        "        \"https://storage.googleapis.com/ai_music/song2.wav\",\n",
        "        \"https://storage.googleapis.com/ai_music/song3.wav\",\n",
        "        # Voice Cloning\n",
        "        \"https://storage.googleapis.com/voice_cloning/clone1.wav\",\n",
        "        \"https://storage.googleapis.com/voice_cloning/clone2.wav\"\n",
        "    ]\n",
        "\n",
        "    # Combine all URLs into the required dictionary structure\n",
        "    urls = {\n",
        "        'video': {\n",
        "            'real': real_video_urls,\n",
        "            'fake': fake_video_urls\n",
        "        },\n",
        "        'image': {\n",
        "            'real': real_image_urls,\n",
        "            'fake': fake_image_urls\n",
        "        },\n",
        "        'audio': {\n",
        "            'real': real_audio_urls,\n",
        "            'fake': fake_audio_urls\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return urls\n",
        "\n",
        "def main():\n",
        "    # Initialize the collector with expanded dataset settings\n",
        "    collector = DeepfakeMediaCollector(\n",
        "        max_samples=10,\n",
        "        base_dir=\"./expanded_deepfake_dataset\",\n",
        "        timeout=180,\n",
        "        max_workers=6\n",
        "    )\n",
        "\n",
        "    # Initialize URLs\n",
        "    urls = initialize_urls()\n",
        "\n",
        "    # Configure logging for YouTube downloads\n",
        "    logging.getLogger('yt_dlp').setLevel(logging.WARNING)\n",
        "\n",
        "    try:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        if loop.is_running():\n",
        "            asyncio.ensure_future(run_collector(collector, urls))\n",
        "        else:\n",
        "            loop.run_until_complete(run_collector(collector, urls))\n",
        "    except RuntimeError:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(run_collector(collector, urls))\n",
        "    finally:\n",
        "        try:\n",
        "            loop.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o-JlIK1KUdP0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgMfhWd3eKv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}