{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxAEPP5tbGUCgaZvCz3TfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Masonry_AI_Researcher_Mohd_Kaif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqS4l0j-hri",
        "outputId": "759b1aa7-1116-4fbe-dc8c-f115ccfef268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.4/58.4 kB 4.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.3/140.3 kB 6.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.7/147.7 kB 8.7 MB/s eta 0:00:00\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -qU langchain langchain-groq langchain-community crawl4ai requests beautifulsoup4 googlesearch-python nest-asyncio tavily-python langgraph pyOpenSSL==24.2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "from typing import List, Dict, Any, TypedDict, Annotated\n",
        "from datetime import datetime\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import nest_asyncio\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# Apply nest_asyncio for compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set API keys (replace with your actual keys)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPER_API_KEY\"] = \"95c2797a69b167639c98ab054e8597d752c6fe6d\""
      ],
      "metadata": {
        "id": "Dpc3ipok-8n8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Agent State\n",
        "class AgentState(TypedDict):\n",
        "    query: str\n",
        "    session_id: str\n",
        "    query_analysis: Dict[str, Any]\n",
        "    search_results: List[Dict[str, str]]\n",
        "    crawled_contents: List[Dict[str, Any]]\n",
        "    analyzed_contents: List[Dict[str, Any]]\n",
        "    final_report: Dict[str, Any]\n",
        "    chat_history: List[Annotated[HumanMessage, AIMessage]]\n",
        "\n",
        "# Initialize Groq LLMs\n",
        "def init_llms() -> Dict[str, ChatGroq]:\n",
        "    \"\"\"Initialize Groq LLMs for summarization and analysis.\"\"\"\n",
        "    return {\n",
        "        'summary': ChatGroq(model_name=\"llama3-70b-8192\", temperature=0.7),\n",
        "        'analysis': ChatGroq(model_name=\"llama3-70b-8192\", temperature=0.5)  # Replaced deprecated mixtral-8x7b-32768\n",
        "    }\n",
        "\n",
        "# Initialize HTTP session\n",
        "def init_session():\n",
        "    \"\"\"Initialize a requests session with default headers.\"\"\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\"User-Agent\": \"Mozilla/5.0 (compatible; ResearchBot/1.0)\"})\n",
        "    return session"
      ],
      "metadata": {
        "id": "OCGfnd6c_Ul9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tools\n",
        "@tool\n",
        "def search_google(query: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Perform a Google search using googlesearch-python.\"\"\"\n",
        "    print(f\"Searching Google for: {query}\")\n",
        "    try:\n",
        "        from googlesearch import search\n",
        "        results = [{'url': url, 'title': '', 'snippet': ''} for url in search(query, num_results=5)]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Google search error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def search_tavily(query: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Perform a search using Tavily API.\"\"\"\n",
        "    print(f\"Searching Tavily for: {query}\")\n",
        "    try:\n",
        "        from tavily import TavilyClient\n",
        "        tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "        results = tavily.search(query, max_results=5)\n",
        "        return [{'url': r['url'], 'title': r['title'], 'snippet': r['content']} for r in results['results']]\n",
        "    except Exception as e:\n",
        "        print(f\"Tavily search error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def search_serper(query: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Perform a search using Google Serper API.\"\"\"\n",
        "    print(f\"Searching Serper for: {query}\")\n",
        "    try:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        headers = {\n",
        "            \"X-API-KEY\": os.environ[\"SERPER_API_KEY\"],\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\"q\": query, \"num\": 5}\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json().get('organic', [])\n",
        "        return [{'url': r['link'], 'title': r.get('title', ''), 'snippet': r.get('snippet', '')} for r in results]\n",
        "    except Exception as e:\n",
        "        print(f\"Serper search error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "async def crawl_url(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Crawl a URL using Crawl4AI with advanced features.\"\"\"\n",
        "    print(f\"Crawling URL: {url}\")\n",
        "    try:\n",
        "        async with AsyncWebCrawler() as crawler:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                params={\n",
        "                    \"return_format\": \"markdown\",\n",
        "                    \"proxy_enabled\": False,\n",
        "                    \"request_timeout\": 30,\n",
        "                    \"limit\": 1,\n",
        "                    \"css_selector\": \"article, main, .content\",\n",
        "                    \"js_execution\": True,\n",
        "                    \"extract_hidden\": True\n",
        "                }\n",
        "            )\n",
        "            if result.success:\n",
        "                return {\n",
        "                    'url': url,\n",
        "                    'title': result.title or url,\n",
        "                    'content': result.markdown[:2000],\n",
        "                    'success': True\n",
        "                }\n",
        "    except Exception as e:\n",
        "        print(f\"Error crawling {url}: {str(e)}\")\n",
        "    return {'url': url, 'title': '', 'content': '', 'success': False}\n",
        "\n",
        "@tool\n",
        "async def extract_content_fallback(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Fallback content extraction using BeautifulSoup.\"\"\"\n",
        "    print(f\"Extracting content (fallback) from: {url}\")\n",
        "    session = init_session()\n",
        "    try:\n",
        "        response = await asyncio.get_event_loop().run_in_executor(None, lambda: session.get(url, timeout=10))\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)[:2000]\n",
        "        title = soup.title.string if soup.title else url\n",
        "        return {'url': url, 'title': title, 'content': text, 'success': True}\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting content from {url}: {str(e)}\")\n",
        "        return {'url': url, 'title': '', 'content': '', 'success': False}"
      ],
      "metadata": {
        "id": "O9-EEO1e_8VM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent Functions\n",
        "def query_analysis(state: AgentState) -> AgentState:\n",
        "    \"\"\"Analyze the query to extract search terms and context.\"\"\"\n",
        "    llm = init_llms()['analysis']\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Analyze the query: '{query}'.\\n\"\n",
        "        \"Return a JSON object with:\\n\"\n",
        "        \"- 'search_terms': Main keywords for search.\\n\"\n",
        "        \"- 'context': Brief context or intent of the query.\\n\"\n",
        "        \"- 'complexity': 'simple' or 'complex' based on query structure.\\n\"\n",
        "        \"- 'info_type': 'facts', 'news', 'opinions', or 'mixed'.\"\n",
        "    )\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "    try:\n",
        "        result = chain.invoke({\"query\": state['query']})\n",
        "        print(f\"Query analysis: {result}\")\n",
        "        state['query_analysis'] = result\n",
        "    except Exception as e:\n",
        "        print(f\"Query analysis error: {str(e)}\")\n",
        "        state['query_analysis'] = {\n",
        "            'search_terms': state['query'],\n",
        "            'context': 'General query',\n",
        "            'complexity': 'simple',\n",
        "            'info_type': 'mixed'\n",
        "        }\n",
        "    return state\n",
        "\n",
        "def search(state: AgentState) -> AgentState:\n",
        "    \"\"\"Execute web searches using multiple tools.\"\"\"\n",
        "    llm = init_llms()['analysis']\n",
        "    tools = [search_google, search_tavily, search_serper]\n",
        "\n",
        "    # Define a ReAct-compatible prompt\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are a search agent. Answer the following query using the provided tools.\\n\\n\"\n",
        "        \"Query: {input}\\n\\n\"\n",
        "        \"Tools available: {tool_names}\\n\\n\"\n",
        "        \"Tool descriptions:\\n{tools}\\n\\n\"\n",
        "        \"Use the tools to find relevant URLs for the query. Return a list of URLs with titles and snippets.\\n\\n\"\n",
        "        \"Agent scratchpad:\\n{agent_scratchpad}\"\n",
        "    )\n",
        "\n",
        "    # Create the ReAct agent\n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    try:\n",
        "        # Let the agent decide which tools to use\n",
        "        result = executor.invoke({\"input\": state['query_analysis']['search_terms']})\n",
        "        search_results = result.get('output', []) if isinstance(result, dict) else []\n",
        "\n",
        "        # Ensure search_results is a list of dictionaries\n",
        "        if isinstance(search_results, list) and all(isinstance(r, dict) for r in search_results):\n",
        "            state['search_results'] = search_results\n",
        "        else:\n",
        "            state['search_results'] = [{'url': r, 'title': '', 'snippet': ''} for r in search_results]\n",
        "\n",
        "        # Deduplicate URLs\n",
        "        seen_urls = set()\n",
        "        state['search_results'] = [r for r in state['search_results'] if r['url'] not in seen_urls and not seen_urls.add(r['url'])]\n",
        "        print(f\"Search results: {len(state['search_results'])} unique URLs found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {str(e)}\")\n",
        "        state['search_results'] = []\n",
        "    return state\n",
        "\n",
        "async def crawl(state: AgentState) -> AgentState:\n",
        "    \"\"\"Crawl URLs to extract content.\"\"\"\n",
        "    contents = await asyncio.gather(*[crawl_url.invoke(r['url']) for r in state['search_results'][:5]], return_exceptions=True)\n",
        "    for i, content in enumerate(contents):\n",
        "        if isinstance(content, Exception) or not content['success']:\n",
        "            print(f\"Retrying crawl for {state['search_results'][i]['url']} with fallback\")\n",
        "            contents[i] = await extract_content_fallback.invoke(state['search_results'][i]['url'])\n",
        "    state['crawled_contents'] = contents\n",
        "    print(f\"Crawled {len([c for c in contents if c['success']])} URLs successfully\")\n",
        "    return state\n",
        "\n",
        "def analyze_content(state: AgentState) -> AgentState:\n",
        "    \"\"\"Analyze crawled content for relevance and key information.\"\"\"\n",
        "    llm = init_llms()['summary']\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Given the query: '{query}' and the following content:\\n\"\n",
        "        \"{content}\\n\\n\"\n",
        "        \"Return a JSON object with:\\n\"\n",
        "        \"- 'url': Source URL.\\n\"\n",
        "        \"- 'summary': Brief summary (50-100 words).\\n\"\n",
        "        \"- 'relevance': Score from 0 to 1 (1 being most relevant).\\n\"\n",
        "        \"- 'key_points': List of 2-3 key points.\"\n",
        "    )\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "    results = []\n",
        "    for content in state['crawled_contents']:\n",
        "        if content['success']:\n",
        "            try:\n",
        "                result = chain.invoke({\"query\": state['query'], \"content\": content['content'][:1000]})\n",
        "                result['url'] = content['url']\n",
        "                results.append(result)\n",
        "                print(f\"Analyzed content from {content['url']}: {result['summary']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing content from {content['url']}: {str(e)}\")\n",
        "    state['analyzed_contents'] = results\n",
        "    return state\n",
        "\n",
        "async def synthesize(state: AgentState) -> AgentState:\n",
        "    \"\"\"Synthesize analyzed content into a final report.\"\"\"\n",
        "    llm = init_llms()['summary']\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Query: {query}\\n\"\n",
        "        \"Analyzed Content: {content}\\n\\n\"\n",
        "        \"Synthesize the information into a JSON report with:\\n\"\n",
        "        \"- 'query': Original query.\\n\"\n",
        "        \"- 'timestamp': Current timestamp.\\n\"\n",
        "        \"- 'summary': Comprehensive summary (150-200 words).\\n\"\n",
        "        \"- 'sources': List of URLs and their summaries.\\n\"\n",
        "        \"- 'confidence': Overall confidence score (0 to 1).\\n\"\n",
        "        \"- 'recommendations': List of 1-2 follow-up actions.\"\n",
        "    )\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "    content_str = json.dumps(state['analyzed_contents'], indent=2)\n",
        "    try:\n",
        "        report = chain.invoke({\"query\": state['query'], \"content\": content_str})\n",
        "        report['timestamp'] = datetime.utcnow().isoformat()\n",
        "        print(f\"Synthesized report for query: {state['query']}\")\n",
        "        state['final_report'] = report\n",
        "    except Exception as e:\n",
        "        print(f\"Synthesis error: {str(e)}\")\n",
        "        state['final_report'] = {\n",
        "            'query': state['query'],\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "            'summary': 'Error synthesizing information.',\n",
        "            'sources': [],\n",
        "            'confidence': 0.0,\n",
        "            'recommendations': []\n",
        "        }\n",
        "    return state"
      ],
      "metadata": {
        "id": "DbDF_42jAlqP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Workflow\n",
        "def build_graph() -> StateGraph:\n",
        "    \"\"\"Build the LangGraph workflow.\"\"\"\n",
        "    graph = StateGraph(AgentState)\n",
        "\n",
        "    # Add nodes with unique names\n",
        "    graph.add_node(\"query_analyzer\", query_analysis)\n",
        "    graph.add_node(\"searcher\", search)\n",
        "    graph.add_node(\"crawler\", crawl)\n",
        "    graph.add_node(\"content_analyzer\", analyze_content)\n",
        "    graph.add_node(\"synthesizer\", synthesize)\n",
        "\n",
        "    # Define edges\n",
        "    graph.set_entry_point(\"query_analyzer\")\n",
        "    graph.add_edge(\"query_analyzer\", \"searcher\")\n",
        "    graph.add_edge(\"searcher\", \"crawler\")\n",
        "    graph.add_edge(\"crawler\", \"content_analyzer\")\n",
        "    graph.add_edge(\"content_analyzer\", \"synthesizer\")\n",
        "    graph.add_edge(\"synthesizer\", END)\n",
        "\n",
        "    return graph.compile()\n",
        "\n",
        "# Save Report\n",
        "def save_report(report: Dict[str, Any], filename: str = \"research_report.json\"):\n",
        "    \"\"\"Save the research report to a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        print(f\"Report saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving report to {filename}: {str(e)}\")"
      ],
      "metadata": {
        "id": "1ERJBE2gAwal"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Research Function\n",
        "async def research(query: str, session_id: str = \"default\") -> Dict[str, Any]:\n",
        "    \"\"\"Execute the research pipeline.\"\"\"\n",
        "    print(f\"Starting research for query: {query}\")\n",
        "    graph = build_graph()\n",
        "    state = AgentState(\n",
        "        query=query,\n",
        "        session_id=session_id,\n",
        "        query_analysis={},\n",
        "        search_results=[],\n",
        "        crawled_contents=[],\n",
        "        analyzed_contents=[],\n",
        "        final_report={},\n",
        "        chat_history=[]\n",
        "    )\n",
        "    final_state = await graph.ainvoke(state)\n",
        "    return final_state['final_report']\n",
        "\n",
        "# Test the Agent\n",
        "async def test_research_agent():\n",
        "    \"\"\"Test the research pipeline with sample queries.\"\"\"\n",
        "    test_queries = [\n",
        "        \"What are the latest advancements in quantum computing as of 2025?\",\n",
        "        \"Compare the economic impacts of AI adoption in the US vs. China.\"\n",
        "    ]\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nTesting query: {query}\")\n",
        "        result = await research(query, session_id=f\"test_{query.replace(' ', '_')}\")\n",
        "        print(json.dumps(result, indent=2))\n",
        "        save_report(result, f\"research_report_{query.replace(' ', '_')}.json\")\n",
        "\n",
        "# Run tests\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(test_research_agent())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZJqtgBTRi3B",
        "outputId": "049fc296-ba75-4016-cbe1-abe9a7d33445"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing query: What are the latest advancements in quantum computing as of 2025?\n",
            "Starting research for query: What are the latest advancements in quantum computing as of 2025?\n",
            "Query analysis: {'search_terms': ['latest advancements', 'quantum computing', '2025'], 'context': 'The user is looking for recent developments and updates in the field of quantum computing as of the year 2025.', 'complexity': 'simple', 'info_type': 'facts'}\n",
            "\n",
            "\n",
            "> Entering new AgentExecutor chain...\n",
            "Search error: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `To answer the query, I'll use a combination of the available tools to find relevant URLs. Here's my approach:\n",
            "\n",
            "```python\n",
            "def search_agent(query):\n",
            "    results = []\n",
            "    \n",
            "    # Search Google using googlesearch-python\n",
            "    google_results = search_google(' '.join(query))\n",
            "    for result in google_results:\n",
            "        results.append({\n",
            "            'title': result['title'],\n",
            "            'url': result['link'],\n",
            "            'snippet': result['description']\n",
            "        })\n",
            "    \n",
            "    # Search Tavily API\n",
            "    tavily_results = search_tavily(' '.join(query))\n",
            "    for result in tavily_results:\n",
            "        results.append({\n",
            "            'title': result['title'],\n",
            "            'url': result['url'],\n",
            "            'snippet': result['snippet']\n",
            "        })\n",
            "    \n",
            "    # Search Google Serper API\n",
            "    serper_results = search_serper(' '.join(query))\n",
            "    for result in serper_results:\n",
            "        results.append({\n",
            "            'title': result['title'],\n",
            "            'url': result['link'],\n",
            "            'snippet': result['snippet']\n",
            "        })\n",
            "    \n",
            "    return results\n",
            "\n",
            "query_results = search_agent(['latest advancements', 'quantum computing', '2025'])\n",
            "for result in query_results:\n",
            "    print(f\"Title: {result['title']}\")\n",
            "    print(f\"URL: {result['url']}\")\n",
            "    print(f\"Snippet: {result['snippet']}\")\n",
            "    print(\"---\")\n",
            "```\n",
            "\n",
            "In this code, I define a `search_agent` function that takes the query as input and returns a list of results. I use each of the available tools to search for relevant URLs and extract the title, URL, and snippet from each result. The results are then combined into a single list and returned.\n",
            "\n",
            "Finally, I call the `search_agent` function with the query and print out the results, including the title, URL, and snippet for each result.\n",
            "\n",
            "Note that you'll need to implement the `search_google`, `search_tavily`, and `search_serper` functions according to the tool descriptions.`\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Crawled 0 URLs successfully\n",
            "Synthesized report for query: What are the latest advancements in quantum computing as of 2025?\n",
            "{\n",
            "  \"query\": \"What are the latest advancements in quantum computing as of 2025?\",\n",
            "  \"timestamp\": \"2025-04-23T16:40:57.164259\",\n",
            "  \"summary\": \"As of 2025, quantum computing has witnessed significant advancements in hardware, software, and applications. Major tech companies like Google, IBM, and Microsoft have made notable progress in developing more powerful and stable quantum processors. Google's Bristlecone processor has demonstrated low error rates, while IBM's Eagle processor has achieved a quantum volume of 32. Microsoft has also made strides in developing a topological quantum computer. Furthermore, advancements in quantum software have enabled the development of more complex algorithms and simulations. Applications of quantum computing have expanded to fields like chemistry, materials science, and cryptography. However, challenges persist, including the need for better error correction and the development of more robust quantum algorithms.\",\n",
            "  \"sources\": [\n",
            "    {\n",
            "      \"url\": \"https://www.google.com/quantum/bristlecone/\",\n",
            "      \"summary\": \"Google's Bristlecone processor achieves low error rates\"\n",
            "    },\n",
            "    {\n",
            "      \"url\": \"https://www.ibm.com/quantum-computing/eagle-processor/\",\n",
            "      \"summary\": \"IBM's Eagle processor achieves a quantum volume of 32\"\n",
            "    },\n",
            "    {\n",
            "      \"url\": \"https://www.microsoft.com/quantum/topological-quantum-computing\",\n",
            "      \"summary\": \"Microsoft develops topological quantum computer\"\n",
            "    }\n",
            "  ],\n",
            "  \"confidence\": 0.8,\n",
            "  \"recommendations\": [\n",
            "    \"Stay updated on the latest advancements in quantum computing through reputable sources like Nature, Science, and IEEE Spectrum.\",\n",
            "    \"Explore online courses and tutorials on quantum computing to deepen your understanding of the subject.\"\n",
            "  ]\n",
            "}\n",
            "Report saved to research_report_What_are_the_latest_advancements_in_quantum_computing_as_of_2025?.json\n",
            "\n",
            "Testing query: Compare the economic impacts of AI adoption in the US vs. China.\n",
            "Starting research for query: Compare the economic impacts of AI adoption in the US vs. China.\n",
            "Query analysis: {'search_terms': ['AI adoption', 'economic impacts', 'US', 'China'], 'context': 'The query is seeking to compare the economic effects of adopting Artificial Intelligence (AI) in two major economies, the United States and China.', 'complexity': 'complex', 'info_type': 'facts'}\n",
            "\n",
            "\n",
            "> Entering new AgentExecutor chain...\n",
            "Search error: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `I'll use the available tools to find relevant URLs for the query. Here's the code to perform the search:\n",
            "```\n",
            "results = []\n",
            "\n",
            "# Search using Google\n",
            "google_results = search_google(' '.join(['AI adoption', 'economic impacts', 'US', 'China']))\n",
            "for result in google_results:\n",
            "    results.append({\n",
            "        'title': result['title'],\n",
            "        'url': result['link'],\n",
            "        'snippet': result['description']\n",
            "    })\n",
            "\n",
            "# Search using Tavily\n",
            "tavily_results = search_tavily(' '.join(['AI adoption', 'economic impacts', 'US', 'China']))\n",
            "for result in tavily_results:\n",
            "    results.append({\n",
            "        'title': result['title'],\n",
            "        'url': result['url'],\n",
            "        'snippet': result['description']\n",
            "    })\n",
            "\n",
            "# Search using Serper\n",
            "serper_results = search_serper(' '.join(['AI adoption', 'economic impacts', 'US', 'China']))\n",
            "for result in serper_results:\n",
            "    results.append({\n",
            "        'title': result['title'],\n",
            "        'url': result['link'],\n",
            "        'snippet': result['snippet']\n",
            "    })\n",
            "\n",
            "print(results)\n",
            "```\n",
            "This will return a list of URLs with titles and snippets relevant to the query. Note that the structure of the results may vary depending on the tool used, so I've standardized the output to include `title`, `url`, and `snippet` for each result.\n",
            "\n",
            "Let me know if you'd like me to refine the search or filter the results in any way!`\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
            "Crawled 0 URLs successfully\n",
            "Synthesized report for query: Compare the economic impacts of AI adoption in the US vs. China.\n",
            "{\n",
            "  \"query\": \"Compare the economic impacts of AI adoption in the US vs. China\",\n",
            "  \"timestamp\": \"2025-04-23T16:41:02.654509\",\n",
            "  \"summary\": \"The economic impacts of AI adoption in the US and China differ significantly. While both countries are investing heavily in AI, the US has a more established AI industry, with a stronger focus on innovation and entrepreneurship. China, on the other hand, is leveraging AI to drive economic growth and reduce labor costs. A report by the McKinsey Global Institute estimates that AI could add 1.2% to 2.2% to the US GDP by 2030, while in China, AI could add 2.6% to 3.4% to GDP. The US has a strong lead in AI research and development, with top universities and tech companies driving innovation. China, however, is catching up quickly, with heavy government investment in AI and a large talent pool. The two countries' approaches to AI adoption will have significant implications for their economies, industries, and workforces.\",\n",
            "  \"sources\": [\n",
            "    {\n",
            "      \"url\": \"https://www.mckinsey.com/featured-insights/artificial-intelligence/the-future-of-work-in-a-rapidly-changing-world\",\n",
            "      \"summary\": \"McKinsey report on the future of work and AI adoption\"\n",
            "    },\n",
            "    {\n",
            "      \"url\": \"https://www.brookings.edu/research/ai-and-the-future-of-work-in-china/\",\n",
            "      \"summary\": \"Brookings Institution report on AI and the future of work in China\"\n",
            "    },\n",
            "    {\n",
            "      \"url\": \"https://www.wsj.com/articles/china-is-catching-up-to-u-s-in-ai-research-11563044400\",\n",
            "      \"summary\": \"Wall Street Journal article on China catching up to the US in AI research\"\n",
            "    }\n",
            "  ],\n",
            "  \"confidence\": 0.8,\n",
            "  \"recommendations\": [\n",
            "    \"Analyze the impact of AI on specific industries in the US and China, such as healthcare and finance\",\n",
            "    \"Investigate the potential risks and challenges associated with AI adoption in both countries\"\n",
            "  ]\n",
            "}\n",
            "Report saved to research_report_Compare_the_economic_impacts_of_AI_adoption_in_the_US_vs._China..json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eziYXu3Rnkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}