{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "15rtGXUXXidW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fa7776-ea1b-4232-b207-9a6d88cebf69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m829.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.6/119.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.6/866.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q apify-client langchain langchain-community langchain-groq  transformers scrapegraphai\n",
        "!pip install -q sentence-transformers requests beautifulsoup4 ratelimit  pyLDAvis faiss-cpu crewai crewai_tools exa exa_py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from exa_py import Exa\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import BaseTool, BrowserbaseLoadTool, EXASearchTool, FirecrawlScrapeWebsiteTool\n",
        "from scrapegraphai.graphs import SmartScraperGraph\n",
        "from scrapegraphai.utils import prettify_exec_info\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and API Keys\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "EXA_API_KEY = \"d0f01fc2-c757-4c06-83f3-e2fd21361bab\"\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "6BZnOZ8nB6Q3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Tools and Tasks**\n",
        "Define functions for real-time data fetching, Twitter scraping, news fetching, CVE data fetching, Exa.ai integration, and advanced data analysis.\n"
      ],
      "metadata": {
        "id": "RKATj2bwCXT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraper(BaseTool):\n",
        "    name: str = \"Web Scraper\"\n",
        "    description: str = \"Scrapes content from a given URL\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=10, period=60)\n",
        "    def run(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
        "        results = []\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(self._scrape_url, url) for url in urls]\n",
        "            for future in as_completed(futures):\n",
        "                results.append(future.result())\n",
        "        return results\n",
        "\n",
        "    def _scrape_url(self, url: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "class TwitterScraper(BaseTool):\n",
        "    name: str = \"Twitter Scraper\"\n",
        "    description: str = \"Fetches tweets based on a given query\"\n",
        "\n",
        "    def run(self, query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "        actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "        try:\n",
        "            run = apify_client.actor(\"apify/twitter-scraper\").call(run_input=actor_input)\n",
        "            dataset_id = run[\"defaultDatasetId\"]\n",
        "            items = apify_client.dataset(dataset_id).list_items().items\n",
        "            return items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class NewsFetcher(BaseTool):\n",
        "    name: str = \"News Fetcher\"\n",
        "    description: str = \"Fetches news articles based on a given query\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=10, period=60)\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        url = \"https://newsapi.org/v2/everything\"\n",
        "        params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "        try:\n",
        "            response = session.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get(\"articles\", [])\n",
        "            return articles\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVEDataFetcher(BaseTool):\n",
        "    name: str = \"CVE Data Fetcher\"\n",
        "    description: str = \"Fetches latest CVE data\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=5, period=60)\n",
        "    def run(self) -> List[Dict[str, Any]]:\n",
        "        url = \"https://cve.circl.lu/api/last\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            cve_items = response.json()\n",
        "            return cve_items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class ExaResearcher(BaseTool):\n",
        "    name: str = \"Exa Researcher\"\n",
        "    description: str = \"Performs research using Exa.ai\"\n",
        "\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            exa_client = Exa(api_key=EXA_API_KEY)\n",
        "            results = exa_client.search(query, max_results=max_results)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Exa.ai research: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVESeverityAnalyzer(BaseTool):\n",
        "    name: str = \"CVE Severity Analyzer\"\n",
        "    description: str = \"Analyzes the severity of a CVE based on its description\"\n",
        "\n",
        "    def run(self, cve_description: str) -> str:\n",
        "        severity_keywords = [\"critical\", \"high\", \"medium\", \"low\"]\n",
        "        severity = \"unknown\"\n",
        "        for keyword in severity_keywords:\n",
        "            if keyword in cve_description.lower():\n",
        "                severity = keyword\n",
        "                break\n",
        "        return f\"The CVE severity is {severity}.\"\n",
        "\n",
        "class IOCExtractor(BaseTool):\n",
        "    name: str = \"IOC Extractor\"\n",
        "    description: str = \"Extracts Indicators of Compromise from given text\"\n",
        "\n",
        "    def run(self, text: str) -> List[str]:\n",
        "        iocs = []\n",
        "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
        "        domain_pattern = r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
        "\n",
        "        iocs.extend(re.findall(ip_pattern, text))\n",
        "        iocs.extend(re.findall(domain_pattern, text))\n",
        "        return list(set(iocs))\n",
        "\n",
        "class TrendAnalyzer(BaseTool):\n",
        "    name: str = \"Trend Analyzer\"\n",
        "    description: str = \"Analyzes trends in cybersecurity data\"\n",
        "\n",
        "    def run(self, data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "        keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\"]\n",
        "        keyword_counts = {keyword: 0 for keyword in keywords}\n",
        "\n",
        "        for item in data:\n",
        "            text = item.get('text', '').lower()\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    keyword_counts[keyword] += 1\n",
        "\n",
        "        trending_topics = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "        return f\"Trend analysis for the timeframe {timeframe} shows increasing threats in: \" + \", \".join([f\"{topic} ({count} mentions)\" for topic, count in trending_topics])\n",
        "\n",
        "class SentimentAnalyzer(BaseTool):\n",
        "    name: str = \"Sentiment Analyzer\"\n",
        "    description: str = \"Analyzes the sentiment of given text\"\n",
        "\n",
        "    def run(self, text: str) -> str:\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        if sentiment > 0.1:\n",
        "            return \"Positive sentiment\"\n",
        "        elif sentiment < -0.1:\n",
        "            return \"Negative sentiment\"\n",
        "        else:\n",
        "            return \"Neutral sentiment\""
      ],
      "metadata": {
        "id": "vrGfZox7B6Tl"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Agents and Tasks**\n",
        "Define the agents with specific roles and goals, and assign the necessary tools."
      ],
      "metadata": {
        "id": "61vqRkBtDHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")\n",
        "\n",
        "# Define agents\n",
        "def create_agents() -> Dict[str, Agent]:\n",
        "    tools = [\n",
        "        WebScraper(),\n",
        "        TwitterScraper(),\n",
        "        NewsFetcher(),\n",
        "        CVEDataFetcher(),\n",
        "        ExaResearcher(),\n",
        "        CVESeverityAnalyzer(),\n",
        "        IOCExtractor(),\n",
        "        TrendAnalyzer(),\n",
        "        SentimentAnalyzer(),\n",
        "        DuckDuckGoSearchRun(),\n",
        "        BrowserbaseLoadTool(),\n",
        "        EXASearchTool(),\n",
        "        FirecrawlScrapeWebsiteTool()\n",
        "    ]\n",
        "\n",
        "    researcher = Agent(\n",
        "        role=\"Cybersecurity Researcher\",\n",
        "        goal=\"Gather and provide relevant cybersecurity information\",\n",
        "        backstory=\"You are an expert cybersecurity researcher with years of experience in threat intelligence.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    analyst = Agent(\n",
        "        role=\"Threat Analyst\",\n",
        "        goal=\"Analyze cybersecurity data and provide insights\",\n",
        "        backstory=\"You are a skilled threat analyst specializing in identifying and assessing cyber threats.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    advisor = Agent(\n",
        "        role=\"Security Advisor\",\n",
        "        goal=\"Provide recommendations based on cybersecurity analysis\",\n",
        "        backstory=\"You are a seasoned security advisor with a track record of helping organizations improve their security posture.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    threat_hunter = Agent(\n",
        "        role=\"Threat Hunter\",\n",
        "        goal=\"Proactively search for hidden threats and IOCs\",\n",
        "        backstory=\"You are an experienced threat hunter known for uncovering sophisticated cyber threats.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    incident_responder = Agent(\n",
        "        role=\"Incident Responder\",\n",
        "        goal=\"Provide guidance on handling cybersecurity incidents\",\n",
        "        backstory=\"You are a quick-thinking incident responder with expertise in containing and mitigating cyber attacks.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"researcher\": researcher,\n",
        "        \"analyst\": analyst,\n",
        "        \"advisor\": advisor,\n",
        "        \"threat_hunter\": threat_hunter,\n",
        "        \"incident_responder\": incident_responder\n",
        "    }\n",
        "\n",
        "# Define tasks\n",
        "def create_tasks(agents: Dict[str, Agent], query: str) -> List[Task]:\n",
        "    researcher_task = Task(\n",
        "        description=f\"Research the latest information related to: {query}\",\n",
        "        agent=agents[\"researcher\"]\n",
        "    )\n",
        "\n",
        "    analyst_task = Task(\n",
        "        description=f\"Analyze the gathered information and identify key insights related to: {query}\",\n",
        "        agent=agents[\"analyst\"]\n",
        "    )\n",
        "\n",
        "    advisor_task = Task(\n",
        "        description=f\"Provide security recommendations based on the analysis of: {query}\",\n",
        "        agent=agents[\"advisor\"]\n",
        "    )\n",
        "\n",
        "    threat_hunter_task = Task(\n",
        "        description=f\"Hunt for potential threats and IOCs related to: {query}\",\n",
        "        agent=agents[\"threat_hunter\"]\n",
        "    )\n",
        "\n",
        "    incident_responder_task = Task(\n",
        "        description=f\"Suggest incident response steps if {query} is a potential security incident\",\n",
        "        agent=agents[\"incident_responder\"]\n",
        "    )\n",
        "\n",
        "    return [researcher_task, analyst_task, advisor_task, threat_hunter_task, incident_responder_task]"
      ],
      "metadata": {
        "id": "YdWZKkJ6B6Wr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Form the Crew**\n",
        "Organize the agents into a Crew and define the process."
      ],
      "metadata": {
        "id": "VCoDMLcxDT8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create crew\n",
        "def create_crew(agents: Dict[str, Agent]) -> Crew:\n",
        "    return Crew(\n",
        "        agents=list(agents.values()),\n",
        "        tasks=[],\n",
        "        process=Process.sequential\n",
        "    )"
      ],
      "metadata": {
        "id": "8SoGgZIOB6ZN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Implement Data Collection**\n",
        "Define functions to collect and curate data from various sources.\n"
      ],
      "metadata": {
        "id": "MLIiuXx7D2dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collection and curation functions\n",
        "def collect_data():\n",
        "    websites = [\n",
        "        \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "        \"https://attack.mitre.org/\",\n",
        "        \"https://www.darkreading.com/\",\n",
        "        \"https://threatpost.com/\",\n",
        "        \"https://krebsonsecurity.com/\",\n",
        "        \"https://www.bleepingcomputer.com/\",\n",
        "        \"https://www.zdnet.com/topic/security/\",\n",
        "        \"https://www.securityweek.com/\",\n",
        "        \"https://www.sans.org/newsletters/newsbites/\",\n",
        "        \"https://www.cyberscoop.com/\",\n",
        "        \"https://www.csoonline.com/\",\n",
        "        \"https://www.infosecurity-magazine.com/\",\n",
        "        \"https://www.wired.com/category/security/\",\n",
        "        \"https://www.schneier.com/\",\n",
        "        \"https://www.theregister.com/security/\",\n",
        "        \"https://thehackernews.com/\",\n",
        "        \"https://www.cyberdefensemagazine.com/\",\n",
        "        \"https://www.fireeye.com/blog.html\",\n",
        "        \"https://unit42.paloaltonetworks.com/\",\n",
        "        \"https://www.microsoft.com/security/blog/\",\n",
        "        \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "        \"https://nakedsecurity.sophos.com/\",\n",
        "        \"https://www.recordedfuture.com/blog/\",\n",
        "        \"https://www.cybersecurity-insiders.com/\",\n",
        "        \"https://www.malwarebytes.com/blog/\"\n",
        "    ]\n",
        "\n",
        "    # Use SmartScraperGraph for web scraping\n",
        "    graph_config = {\n",
        "        \"llm\": {\n",
        "            \"model\": \"groq/gemma-7b-it\",\n",
        "            \"api_key\": GROQ_API_KEY,\n",
        "            \"temperature\": 0\n",
        "        },\n",
        "        \"headless\": False\n",
        "    }\n",
        "\n",
        "    scraped_data = []\n",
        "    for url in websites:\n",
        "        smart_scraper_graph = SmartScraperGraph(\n",
        "            prompt=\"Extract all relevant information from the page.\",\n",
        "            source=url,\n",
        "            config=graph_config\n",
        "        )\n",
        "        result = smart_scraper_graph.run()\n",
        "        scraped_data.append({\n",
        "            \"url\": url,\n",
        "            \"text\": result,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "    tweets = TwitterScraper().run(\"cybersecurity\")\n",
        "    news = NewsFetcher().run(\"cybersecurity\")\n",
        "    cve_data = CVEDataFetcher().run()\n",
        "    exa_research = ExaResearcher().run(\"cybersecurity\")\n",
        "\n",
        "    return {\n",
        "        \"scraped_data\": scraped_data,\n",
        "        \"tweets\": tweets,\n",
        "        \"news\": news,\n",
        "        \"cve_data\": cve_data,\n",
        "        \"exa_research\": exa_research\n",
        "    }\n",
        "\n",
        "def curate_data(data):\n",
        "    curated_data = []\n",
        "\n",
        "    for page in data[\"scraped_data\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in data[\"tweets\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in data[\"news\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in data[\"cve_data\"]:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for research in data[\"exa_research\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Exa.ai\",\n",
        "            \"title\": research.get(\"title\"),\n",
        "            \"abstract\": research.get(\"abstract\"),\n",
        "            \"url\": research.get(\"url\"),\n",
        "            \"timestamp\": research.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    return curated_data\n",
        "\n",
        "def store_in_vector_db(curated_data):\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=item[\"text\"], metadata=item) for item in curated_data]\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(\"vector_store\")\n",
        "\n",
        "def load_vector_store():\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    return FAISS.load_local(\"vector_store\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "hnxIrUBHB6bx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect and curate data\n",
        "raw_data = collect_data()\n",
        "curated_data = curate_data(raw_data)\n",
        "store_in_vector_db(curated_data)\n",
        "vector_store = load_vector_store()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "JOlC50jZsH3h",
        "outputId": "91242776-3807-4944-f2ad-b5e5127aa2e8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Embedding Model missing or not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-95b45a870f6a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Collect and curate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcurated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstore_in_vector_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-b5a56b6678f8>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mscraped_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwebsites\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         smart_scraper_graph = SmartScraperGraph(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extract all relevant information from the page.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapegraphai/graphs/smart_scraper_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prompt, source, config, schema)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"url\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"local_dir\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapegraphai/graphs/abstract_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prompt, config, source, schema)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         self.embedder_model = self._create_default_embedder(llm_config=config[\"llm\"]) if \"embeddings\" not in config else self._create_embedder(\n\u001b[0m\u001b[1;32m     80\u001b[0m             config[\"embeddings\"])\n\u001b[1;32m     81\u001b[0m         self.verbose = False if config is None else config.get(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapegraphai/graphs/abstract_graph.py\u001b[0m in \u001b[0;36m_create_default_embedder\u001b[0;34m(self, llm_config)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mBedrockEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding Model missing or not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Embedding Model missing or not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3FKvZPOg--9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}