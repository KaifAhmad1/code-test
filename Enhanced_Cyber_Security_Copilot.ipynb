{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "15rtGXUXXidW"
      },
      "outputs": [],
      "source": [
        "!pip install -q apify-client langchain langchain-community langchain-groq transformers duckduckgo-search firecrawl-py\n",
        "!pip install -q sentence-transformers requests beautifulsoup4 ratelimit pyLDAvis faiss-cpu crewai crewai_tools exa exa_py langchain-exa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from exa_py import Exa\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import BaseTool, BrowserbaseLoadTool, EXASearchTool, FirecrawlScrapeWebsiteTool\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants and API Keys\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "EXA_API_KEY = \"d0f01fc2-c757-4c06-83f3-e2fd21361bab\"\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = \"fc-1b759fd6d7cd4784bc8f8819060228cd\"\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Tools and Tasks**\n",
        "Define functions for real-time data fetching, Twitter scraping, news fetching, CVE data fetching, Exa.ai integration, and advanced data analysis.\n"
      ],
      "metadata": {
        "id": "RKATj2bwCXT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tools and Tasks\n",
        "class WebScraper:\n",
        "    def run(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
        "        results = []\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(self._scrape_url, url) for url in urls]\n",
        "            for future in as_completed(futures):\n",
        "                results.append(future.result())\n",
        "        return results\n",
        "\n",
        "    def _scrape_url(self, url: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "class TwitterScraper:\n",
        "    def run(self, query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "        actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "        try:\n",
        "            run = apify_client.actor(\"apify/twitter-scraper\").call(run_input=actor_input)\n",
        "            dataset_id = run[\"defaultDatasetId\"]\n",
        "            items = apify_client.dataset(dataset_id).list_items().items\n",
        "            return items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class NewsFetcher:\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        url = \"https://newsapi.org/v2/everything\"\n",
        "        params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "        try:\n",
        "            response = session.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get(\"articles\", [])\n",
        "            return articles\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVEDataFetcher:\n",
        "    def run(self) -> List[Dict[str, Any]]:\n",
        "        url = \"https://cve.circl.lu/api/last\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            cve_items = response.json()\n",
        "            return cve_items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class ExaResearcher:\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            exa_client = Exa(api_key=EXA_API_KEY)\n",
        "            results = exa_client.search(query, max_results=max_results)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Exa.ai research: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVESeverityAnalyzer:\n",
        "    def run(self, cve_description: str) -> str:\n",
        "        severity_keywords = [\"critical\", \"high\", \"medium\", \"low\"]\n",
        "        severity = \"unknown\"\n",
        "        for keyword in severity_keywords:\n",
        "            if keyword in cve_description.lower():\n",
        "                severity = keyword\n",
        "                break\n",
        "        return f\"The CVE severity is {severity}.\"\n",
        "\n",
        "class IOCExtractor:\n",
        "    def run(self, text: str) -> List[str]:\n",
        "        iocs = []\n",
        "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
        "        domain_pattern = r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
        "\n",
        "        iocs.extend(re.findall(ip_pattern, text))\n",
        "        iocs.extend(re.findall(domain_pattern, text))\n",
        "        return list(set(iocs))\n",
        "\n",
        "class TrendAnalyzer:\n",
        "    def run(self, data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "        keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\"]\n",
        "        keyword_counts = {keyword: 0 for keyword in keywords}\n",
        "\n",
        "        for item in data:\n",
        "            text = item.get('text', '').lower()\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    keyword_counts[keyword] += 1\n",
        "\n",
        "        trending_topics = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "        return f\"Trend analysis for the timeframe {timeframe} shows increasing threats in: \" + \", \".join([f\"{topic} ({count} mentions)\" for topic, count in trending_topics])\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def run(self, text: str) -> str:\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        if sentiment > 0.1:\n",
        "            return \"Positive sentiment\"\n",
        "        elif sentiment < -0.1:\n",
        "            return \"Negative sentiment\"\n",
        "        else:\n",
        "            return \"Neutral sentiment\""
      ],
      "metadata": {
        "id": "vrGfZox7B6Tl"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Exa Toools**"
      ],
      "metadata": {
        "id": "oYGxEYL1Fk34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Exa Tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "@tool\n",
        "def search(query: str):\n",
        "    \"\"\"Search for a webpage based on the query.\"\"\"\n",
        "    return exa.search(f\"{query}\", use_autoprompt=True, num_results=5)\n",
        "\n",
        "@tool\n",
        "def find_similar(url: str):\n",
        "    \"\"\"Search for webpages similar to a given URL.\n",
        "    The url passed in should be a URL returned from `search`.\n",
        "    \"\"\"\n",
        "    return exa.find_similar(url, num_results=5)\n",
        "\n",
        "@tool\n",
        "def get_contents(ids: list[str]):\n",
        "    \"\"\"Get the contents of a webpage.\n",
        "    The ids passed in should be a list of ids returned from `search`.\n",
        "    \"\"\"\n",
        "    return exa.get_contents(ids)\n",
        "\n",
        "tools = [search, get_contents, find_similar]"
      ],
      "metadata": {
        "id": "Y9IKtgzgFlS9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Implement Data Collection**\n",
        "Define functions to collect and curate data from various sources.\n"
      ],
      "metadata": {
        "id": "MLIiuXx7D2dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection and Curation Functions\n",
        "def collect_data():\n",
        "    websites = [\n",
        "        \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "        \"https://attack.mitre.org/\",\n",
        "        \"https://www.darkreading.com/\",\n",
        "        \"https://threatpost.com/\",\n",
        "        \"https://krebsonsecurity.com/\",\n",
        "        \"https://www.bleepingcomputer.com/\",\n",
        "        \"https://www.zdnet.com/topic/security/\",\n",
        "        \"https://www.securityweek.com/\",\n",
        "        \"https://www.sans.org/newsletters/newsbites/\",\n",
        "        \"https://www.cyberscoop.com/\",\n",
        "        \"https://www.csoonline.com/\",\n",
        "        \"https://www.infosecurity-magazine.com/\",\n",
        "        \"https://www.wired.com/category/security/\",\n",
        "        \"https://www.schneier.com/\",\n",
        "        \"https://www.theregister.com/security/\",\n",
        "        \"https://thehackernews.com/\",\n",
        "        \"https://www.cyberdefensemagazine.com/\",\n",
        "        \"https://www.fireeye.com/blog.html\",\n",
        "        \"https://unit42.paloaltonetworks.com/\",\n",
        "        \"https://www.microsoft.com/security/blog/\",\n",
        "        \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "        \"https://nakedsecurity.sophos.com/\",\n",
        "        \"https://www.recordedfuture.com/blog/\",\n",
        "        \"https://www.cybersecurity-insiders.com/\",\n",
        "        \"https://www.malwarebytes.com/blog/\"\n",
        "    ]\n",
        "\n",
        "    scraped_data = WebScraper().run(websites)\n",
        "    tweets = TwitterScraper().run(\"cybersecurity\")\n",
        "    news = NewsFetcher().run(\"cybersecurity\")\n",
        "    cve_data = CVEDataFetcher().run()\n",
        "    exa_research = ExaResearcher().run(\"cybersecurity\")\n",
        "\n",
        "    return {\n",
        "        \"scraped_data\": scraped_data,\n",
        "        \"tweets\": tweets,\n",
        "        \"news\": news,\n",
        "        \"cve_data\": cve_data,\n",
        "        \"exa_research\": exa_research\n",
        "    }\n",
        "\n",
        "def curate_data(data):\n",
        "    curated_data = []\n",
        "\n",
        "    for page in data[\"scraped_data\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in data[\"tweets\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in data[\"news\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in data[\"cve_data\"]:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for research in data[\"exa_research\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Exa.ai\",\n",
        "            \"title\": research.get(\"title\"),\n",
        "            \"abstract\": research.get(\"abstract\"),\n",
        "            \"url\": research.get(\"url\"),\n",
        "            \"timestamp\": research.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    return curated_data\n",
        "\n",
        "raw_data = collect_data()\n",
        "curated_data = curate_data(raw_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUvzrzXSqsl",
        "outputId": "4ce6701b-31b3-4521-ae36-a1e20d439c65"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error fetching tweets: Actor with this name was not found\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity&language=en&pageSize=50&sortBy=publishedAt\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "ERROR:__main__:Error fetching CVE data: HTTPSConnectionPool(host='cve.circl.lu', port=443): Max retries exceeded with url: /api/last (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\"))\n",
            "ERROR:__main__:Error fetching Exa.ai research: Exa.search() got an unexpected keyword argument 'max_results'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stroing Data in Vector DB**"
      ],
      "metadata": {
        "id": "ESI2NSr4TLQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_in_vector_db(curated_data):\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=item[\"text\"], metadata=item) for item in curated_data]\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(\"vector_store\")\n",
        "\n",
        "def load_vector_store():\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    return FAISS.load_local(\"vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "store_in_vector_db(curated_data)\n",
        "vector_store = load_vector_store()"
      ],
      "metadata": {
        "id": "3N1bluNlSqvW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize LLMs, Agents and their Tasks**\n",
        "Define the agents with specific roles and goals, and assign the necessary tools."
      ],
      "metadata": {
        "id": "61vqRkBtDHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLMs and Agents\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")\n",
        "\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define Tools\n",
        "def create_tools(vector_store):\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"WebScraper\",\n",
        "            func=WebScraper().run,\n",
        "            description=\"Scrapes web content from a list of URLs\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"TwitterScraper\",\n",
        "            func=TwitterScraper().run,\n",
        "            description=\"Scrapes Twitter content\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"NewsFetcher\",\n",
        "            func=NewsFetcher().run,\n",
        "            description=\"Fetches cybersecurity news articles\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"CVEDataFetcher\",\n",
        "            func=CVEDataFetcher().run,\n",
        "            description=\"Fetches CVE data\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"ExaResearcher\",\n",
        "            func=ExaResearcher().run,\n",
        "            description=\"Performs Exa research\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"IOCExtractor\",\n",
        "            func=IOCExtractor().run,\n",
        "            description=\"Extracts Indicators of Compromise from text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"TrendAnalyzer\",\n",
        "            func=TrendAnalyzer().run,\n",
        "            description=\"Analyzes trends in cybersecurity data\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"SentimentAnalyzer\",\n",
        "            func=SentimentAnalyzer().run,\n",
        "            description=\"Analyzes sentiment of text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"VectorDBSearch\",\n",
        "            func=vector_store.similarity_search,\n",
        "            description=\"Searches the vector database for relevant information\"\n",
        "        ),\n",
        "        DuckDuckGoSearchRun(),\n",
        "    ]\n",
        "\n",
        "# Define Agents\n",
        "def create_agent(role: str, goal: str, backstory: str, tools: List[Tool]):\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", f\"You are a {role}. Your goal is to {goal}. {backstory}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"human\", \"Take into account the previous conversation: {chat_history}\"),\n",
        "        (\"human\", \"Human: {human_input}\"),\n",
        "    ])\n",
        "\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    agent = LLMSingleActionAgent(\n",
        "        llm_chain=llm_chain,\n",
        "        output_parser=None,\n",
        "        stop=[\"\\nHuman:\"],\n",
        "        allowed_tools=[tool.name for tool in tools]\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    return AgentExecutor.from_agent_and_tools(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        verbose=True,\n",
        "        memory=memory\n",
        "    )"
      ],
      "metadata": {
        "id": "YdWZKkJ6B6Wr"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Form the Crew**"
      ],
      "metadata": {
        "id": "PDI9GlJ4T6Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Agents\n",
        "def create_agents(tools: List[Tool]):\n",
        "    return {\n",
        "        \"researcher\": create_agent(\n",
        "            role=\"Cybersecurity Researcher\",\n",
        "            goal=\"Gather and provide relevant cybersecurity information\",\n",
        "            backstory=\"You are an expert cybersecurity researcher with years of experience in threat intelligence.\",\n",
        "            tools=tools\n",
        "        ),\n",
        "        \"analyst\": create_agent(\n",
        "            role=\"Threat Analyst\",\n",
        "            goal=\"Analyze cybersecurity data and provide insights\",\n",
        "            backstory=\"You are a skilled threat analyst specializing in identifying and assessing cyber threats.\",\n",
        "            tools=tools\n",
        "        ),\n",
        "        \"advisor\": create_agent(\n",
        "            role=\"Security Advisor\",\n",
        "            goal=\"Provide recommendations based on cybersecurity analysis\",\n",
        "            backstory=\"You are a seasoned security advisor with a track record of helping organizations improve their security posture.\",\n",
        "            tools=tools\n",
        "        ),\n",
        "        \"threat_hunter\": create_agent(\n",
        "            role=\"Threat Hunter\",\n",
        "            goal=\"Proactively search for hidden threats and IOCs\",\n",
        "            backstory=\"You are an experienced threat hunter known for uncovering sophisticated cyber threats.\",\n",
        "            tools=tools\n",
        "        ),\n",
        "        \"incident_responder\": create_agent(\n",
        "            role=\"Incident Responder\",\n",
        "            goal=\"Provide guidance on handling cybersecurity incidents\",\n",
        "            backstory=\"You are a quick-thinking incident responder with expertise in containing and mitigating cyber attacks.\",\n",
        "            tools=tools\n",
        "        ),\n",
        "    }"
      ],
      "metadata": {
        "id": "e58-epqc1_f9"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define prompt templates**"
      ],
      "metadata": {
        "id": "2xuWndBhUEi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "detailed_prompt_templates = {\n",
        "    \"vulnerability_assessment\": \"Assess the vulnerability {cve_id} in {system}. Provide detailed information including potential impacts and mitigation steps.\",\n",
        "    \"security_recommendation\": \"Provide a security recommendation for mitigating {threat}. Include preventive measures and best practices.\",\n",
        "    \"incident_details\": \"List all details on {sector} security incidents in {location}.\",\n",
        "    \"ransomware_attacks\": \"List all ransomware attacks targeting the {industry} industry in the last {timeframe}.\",\n",
        "    \"recent_incidents\": \"Provide recent incidents related to {ransomware_gang} Ransomware.\"\n",
        "}"
      ],
      "metadata": {
        "id": "keGQulgYUD2J"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Process queries**"
      ],
      "metadata": {
        "id": "bqbN9BuXUSNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process queries\n",
        "def process_query(query: str, agents: Dict[str, AgentExecutor], max_steps: int = 5) -> str:\n",
        "    current_agent_name = \"researcher\"\n",
        "    responses = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        current_agent = agents[current_agent_name]\n",
        "        response = current_agent.run(query)\n",
        "        responses.append(f\"{current_agent_name.capitalize()}: {response}\")\n",
        "\n",
        "        # Simple logic to switch agents based on keywords\n",
        "        if \"analyze\" in query.lower() or \"insight\" in query.lower():\n",
        "            current_agent_name = \"analyst\"\n",
        "        elif \"recommend\" in query.lower() or \"advise\" in query.lower():\n",
        "            current_agent_name = \"advisor\"\n",
        "        elif \"hunt\" in query.lower() or \"ioc\" in query.lower():\n",
        "            current_agent_name = \"threat_hunter\"\n",
        "        elif \"incident\" in query.lower() or \"respond\" in query.lower():\n",
        "            current_agent_name = \"incident_responder\"\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return \"\\n\".join(responses)\n",
        "\n",
        "# Main execution flow\n",
        "def main():\n",
        "    logger.info(\"Starting cybersecurity analysis system...\")\n",
        "\n",
        "    try:\n",
        "        # Collect and curate data\n",
        "        logger.info(\"Collecting and curating data...\")\n",
        "        raw_data = collect_data()\n",
        "        curated_data = curate_data(raw_data)\n",
        "\n",
        "        # Store data in vector database\n",
        "        logger.info(\"Storing data in vector database...\")\n",
        "        store_in_vector_db(curated_data)\n",
        "\n",
        "        # Load vector store\n",
        "        logger.info(\"Loading vector store...\")\n",
        "        vector_store = load_vector_store()\n",
        "\n",
        "        # Create tools and agents\n",
        "        logger.info(\"Creating tools and agents...\")\n",
        "        tools = create_tools(vector_store)\n",
        "        agents = create_agents(tools)\n",
        "\n",
        "        # Command-line interface\n",
        "        while True:\n",
        "            query = input(\"Enter your cybersecurity query (or 'exit' to quit): \")\n",
        "            if query.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            logger.info(f\"Processing query: {query}\")\n",
        "            response = process_query(query, agents)\n",
        "            print(\"\\nResponse:\")\n",
        "            print(response)\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qf1ocoG61_jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa489784-c144-4531-e87e-4fd3b73193c6"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error fetching tweets: Actor with this name was not found\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity&language=en&pageSize=50&sortBy=publishedAt\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "ERROR:__main__:Error fetching CVE data: HTTPSConnectionPool(host='cve.circl.lu', port=443): Max retries exceeded with url: /api/last (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\"))\n",
            "ERROR:__main__:Error fetching Exa.ai research: Exa.search() got an unexpected keyword argument 'max_results'\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMSingleActionAgent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n",
            "ERROR:__main__:An error occurred: 1 validation error for LLMSingleActionAgent\n",
            "output_parser\n",
            "  none is not an allowed value (type=type_error.none.not_allowed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbyonTFTbyv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}