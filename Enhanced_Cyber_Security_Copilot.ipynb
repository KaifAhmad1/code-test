{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "15rtGXUXXidW"
      },
      "outputs": [],
      "source": [
        "!pip install -q apify-client langchain langchain-community langchain-groq networkx pyvis spacy transformers pandas\n",
        "!pip install -q sentence-transformers requests beautifulsoup4 ratelimit langgraph pyLDAvis faiss-cpu crewai crewai_tools exa exa_py matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from exa_py import Exa\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import BaseTool\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and API Keys\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "EXA_API_KEY = \"your_exa_api_key\"\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "6BZnOZ8nB6Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Tools and Tasks**\n",
        "Define functions for real-time data fetching, Twitter scraping, news fetching, CVE data fetching, Exa.ai integration, and advanced data analysis.\n"
      ],
      "metadata": {
        "id": "RKATj2bwCXT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom tools\n",
        "class WebScraper(BaseTool):\n",
        "    name = \"Web Scraper\"\n",
        "    description = \"Scrapes content from a given URL\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=10, period=60)\n",
        "    def run(self, url: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "class TwitterScraper(BaseTool):\n",
        "    name = \"Twitter Scraper\"\n",
        "    description = \"Fetches tweets based on a given query\"\n",
        "\n",
        "    def run(self, query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "        actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "        try:\n",
        "            run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "            dataset_id = run[\"defaultDatasetId\"]\n",
        "            items = apify_client.dataset(dataset_id).list_items().items\n",
        "            return items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class NewsFetcher(BaseTool):\n",
        "    name = \"News Fetcher\"\n",
        "    description = \"Fetches news articles based on a given query\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=10, period=60)\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        url = \"https://newsapi.org/v2/everything\"\n",
        "        params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "        try:\n",
        "            response = session.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get(\"articles\", [])\n",
        "            return articles\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVEDataFetcher(BaseTool):\n",
        "    name = \"CVE Data Fetcher\"\n",
        "    description = \"Fetches latest CVE data\"\n",
        "\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=5, period=60)\n",
        "    def run(self) -> List[Dict[str, Any]]:\n",
        "        url = \"https://cve.circl.lu/api/last\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            cve_items = response.json()\n",
        "            return cve_items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class ExaResearcher(BaseTool):\n",
        "    name = \"Exa Researcher\"\n",
        "    description = \"Performs research using Exa.ai\"\n",
        "\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            results = exa_client.search(query, max_results=max_results)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Exa.ai research: {str(e))\")\n",
        "            return []\n",
        "\n",
        "class CVESeverityAnalyzer(BaseTool):\n",
        "    name = \"CVE Severity Analyzer\"\n",
        "    description = \"Analyzes the severity of a CVE based on its description\"\n",
        "\n",
        "    def run(self, cve_description: str) -> str:\n",
        "        severity_keywords = [\"critical\", \"high\", \"medium\", \"low\"]\n",
        "        severity = \"unknown\"\n",
        "        for keyword in severity_keywords:\n",
        "            if keyword in cve_description.lower():\n",
        "                severity = keyword\n",
        "                break\n",
        "        return f\"The CVE severity is {severity}.\"\n",
        "\n",
        "class IOCExtractor(BaseTool):\n",
        "    name = \"IOC Extractor\"\n",
        "    description = \"Extracts Indicators of Compromise from given text\"\n",
        "\n",
        "    def run(self, text: str) -> List[str]:\n",
        "        # This is a simplified version. In a real-world scenario, you'd use more sophisticated regex patterns or ML models.\n",
        "        iocs = []\n",
        "        # Example patterns (oversimplified for demonstration)\n",
        "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
        "        domain_pattern = r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
        "\n",
        "        iocs.extend(re.findall(ip_pattern, text))\n",
        "        iocs.extend(re.findall(domain_pattern, text))\n",
        "        return list(set(iocs))  # Remove duplicates\n",
        "\n",
        "class TrendAnalyzer(BaseTool):\n",
        "    name = \"Trend Analyzer\"\n",
        "    description = \"Analyzes trends in cybersecurity data\"\n",
        "\n",
        "    def run(self, data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "        # This is a simplified version. In a real-world scenario, you'd implement more sophisticated trend analysis.\n",
        "        keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\"]\n",
        "        keyword_counts = {keyword: 0 for keyword in keywords}\n",
        "\n",
        "        for item in data:\n",
        "            text = item.get('text', '').lower()\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    keyword_counts[keyword] += 1\n",
        "\n",
        "        trending_topics = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "        return f\"Trend analysis for the timeframe {timeframe} shows increasing threats in: \" + \", \".join([f\"{topic} ({count} mentions)\" for topic, count in trending_topics])\n",
        "\n",
        "class SentimentAnalyzer(BaseTool):\n",
        "    name = \"Sentiment Analyzer\"\n",
        "    description = \"Analyzes the sentiment of given text\"\n",
        "\n",
        "    def run(self, text: str) -> str:\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        if sentiment > 0.1:\n",
        "            return \"Positive sentiment\"\n",
        "        elif sentiment < -0.1:\n",
        "            return \"Negative sentiment\"\n",
        "        else:\n",
        "            return \"Neutral sentiment\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vrGfZox7B6Tl",
        "outputId": "3edba885-5cd4-4182-c68e-5b8608138fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Function must have a docstring",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aca3a6c7b917>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai_tools/tools/base_tool.py\u001b[0m in \u001b[0;36mtool\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_make_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_make_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai_tools/tools/base_tool.py\u001b[0m in \u001b[0;36m_make_tool\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_make_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseTool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Function must have a docstring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__annotations__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Function must have type annotations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Function must have a docstring"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Create Agents**\n",
        "Define the agents with specific roles and goals, and assign the necessary tools."
      ],
      "metadata": {
        "id": "61vqRkBtDHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agents\n",
        "def create_agents() -> Dict[str, Agent]:\n",
        "    tools = [\n",
        "        WebScraper(),\n",
        "        TwitterScraper(),\n",
        "        NewsFetcher(),\n",
        "        CVEDataFetcher(),\n",
        "        ExaResearcher(),\n",
        "        CVESeverityAnalyzer(),\n",
        "        IOCExtractor(),\n",
        "        TrendAnalyzer(),\n",
        "        SentimentAnalyzer(),\n",
        "        DuckDuckGoSearchRun()\n",
        "    ]\n",
        "\n",
        "    researcher = Agent(\n",
        "        role=\"Cybersecurity Researcher\",\n",
        "        goal=\"Gather and provide relevant cybersecurity information\",\n",
        "        backstory=\"You are an expert cybersecurity researcher with years of experience in threat intelligence.\",\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    analyst = Agent(\n",
        "        role=\"Threat Analyst\",\n",
        "        goal=\"Analyze cybersecurity data and provide insights\",\n",
        "        backstory=\"You are a skilled threat analyst specializing in identifying and assessing cyber threats.\",\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    advisor = Agent(\n",
        "        role=\"Security Advisor\",\n",
        "        goal=\"Provide recommendations based on cybersecurity analysis\",\n",
        "        backstory=\"You are a seasoned security advisor with a track record of helping organizations improve their security posture.\",\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    threat_hunter = Agent(\n",
        "        role=\"Threat Hunter\",\n",
        "        goal=\"Proactively search for hidden threats and IOCs\",\n",
        "        backstory=\"You are an experienced threat hunter known for uncovering sophisticated cyber threats.\",\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    incident_responder = Agent(\n",
        "        role=\"Incident Responder\",\n",
        "        goal=\"Provide guidance on handling cybersecurity incidents\",\n",
        "        backstory=\"You are a quick-thinking incident responder with expertise in containing and mitigating cyber attacks.\",\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"researcher\": researcher,\n",
        "        \"analyst\": analyst,\n",
        "        \"advisor\": advisor,\n",
        "        \"threat_hunter\": threat_hunter,\n",
        "        \"incident_responder\": incident_responder\n",
        "    }"
      ],
      "metadata": {
        "id": "YdWZKkJ6B6Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "na-1MJ-UOtnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tasks\n",
        "def create_tasks(agents: Dict[str, Agent], query: str) -> List[Task]:\n",
        "    researcher_task = Task(\n",
        "        description=f\"Research the latest information related to: {query}\",\n",
        "        agent=agents[\"researcher\"]\n",
        "    )\n",
        "\n",
        "    analyst_task = Task(\n",
        "        description=f\"Analyze the gathered information and identify key insights related to: {query}\",\n",
        "        agent=agents[\"analyst\"]\n",
        "    )\n",
        "\n",
        "    advisor_task = Task(\n",
        "        description=f\"Provide security recommendations based on the analysis of: {query}\",\n",
        "        agent=agents[\"advisor\"]\n",
        "    )\n",
        "\n",
        "    threat_hunter_task = Task(\n",
        "        description=f\"Hunt for potential threats and IOCs related to: {query}\",\n",
        "        agent=agents[\"threat_hunter\"]\n",
        "    )\n",
        "\n",
        "    incident_responder_task = Task(\n",
        "        description=f\"Suggest incident response steps if {query} is a potential security incident\",\n",
        "        agent=agents[\"incident_responder\"]\n",
        "    )\n",
        "\n",
        "    return [researcher_task, analyst_task, advisor_task, threat_hunter_task, incident_responder_task]\n"
      ],
      "metadata": {
        "id": "mJe6nTjTOuT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Form the Crew**\n",
        "Organize the agents into a Crew and define the process."
      ],
      "metadata": {
        "id": "VCoDMLcxDT8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Crew, Process\n",
        "\n",
        "def create_crew(agents: Dict[str, Agent]) -> Crew:\n",
        "    return Crew(\n",
        "        agents=list(agents.values()),\n",
        "        process=Process.sequential\n",
        "    )"
      ],
      "metadata": {
        "id": "8SoGgZIOB6ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Implement Data Collection**\n",
        "Define functions to collect and curate data from various sources.\n"
      ],
      "metadata": {
        "id": "MLIiuXx7D2dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data():\n",
        "    websites = [\n",
        "            \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "            \"https://attack.mitre.org/\",\n",
        "            \"https://www.darkreading.com/\",\n",
        "            \"https://threatpost.com/\",\n",
        "            \"https://krebsonsecurity.com/\",\n",
        "            \"https://www.bleepingcomputer.com/\",\n",
        "            \"https://www.zdnet.com/topic/security/\",\n",
        "            \"https://www.securityweek.com/\",\n",
        "            \"https://www.sans.org/newsletters/newsbites/\",\n",
        "            \"https://www.cyberscoop.com/\",\n",
        "            \"https://www.csoonline.com/\",\n",
        "            \"https://www.infosecurity-magazine.com/\",\n",
        "            \"https://www.wired.com/category/security/\",\n",
        "            \"https://www.schneier.com/\",\n",
        "            \"https://www.theregister.com/security/\",\n",
        "            \"https://thehackernews.com/\",\n",
        "            \"https://www.cyberdefensemagazine.com/\",\n",
        "            \"https://www.fireeye.com/blog.html\",\n",
        "            \"https://unit42.paloaltonetworks.com/\",\n",
        "            \"https://www.microsoft.com/security/blog/\",\n",
        "            \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "            \"https://nakedsecurity.sophos.com/\",\n",
        "            \"https://www.recordedfuture.com/blog/\",\n",
        "            \"https://www.cybersecurity-insiders.com/\",\n",
        "            \"https://www.malwarebytes.com/blog/\",\n",
        "    ]\n",
        "\n",
        "    scraped_data = scrape_websites(websites)\n",
        "    tweets = fetch_tweets(\"cybersecurity\")\n",
        "    news = fetch_news(\"cybersecurity\")\n",
        "    cve_data = fetch_cve_data()\n",
        "    exa_research = fetch_exa_research(\"cybersecurity\")\n",
        "\n",
        "    return {\n",
        "        \"scraped_data\": scraped_data,\n",
        "        \"tweets\": tweets,\n",
        "        \"news\": news,\n",
        "        \"cve_data\": cve_data,\n",
        "        \"exa_research\": exa_research\n",
        "    }"
      ],
      "metadata": {
        "id": "hnxIrUBHB6bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Curate and Store Data**\n",
        "Aggregate and store the curated data in a vector database."
      ],
      "metadata": {
        "id": "SSWupiFSEsHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def curate_data(scraped_data, tweets, news, cve_data, exa_research):\n",
        "    curated_data = []\n",
        "\n",
        "    for page in scraped_data:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in tweets:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in news:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in cve_data:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for research in exa_research:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Exa.ai\",\n",
        "            \"title\": research.get(\"title\"),\n",
        "            \"abstract\": research.get(\"abstract\"),\n",
        "            \"url\": research.get(\"url\"),\n",
        "            \"timestamp\": research.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    return curated_data\n",
        "\n",
        "def store_in_vector_db(curated_data):\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=item[\"text\"], metadata=item) for item in curated_data]\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(\"vector_store\")\n",
        "\n",
        "def load_vector_store():\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    return FAISS.load_local(\"vector_store\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "kDSH5ccYB6eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **LLM Intialization with Groq**\n"
      ],
      "metadata": {
        "id": "S48zdzU-GknU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "5ClBbMDOGr2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Run the Multi-Agent System**\n",
        "Implement the main function to run the multi-agent system, process queries, and provide responses."
      ],
      "metadata": {
        "id": "ugas2ASzFaNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query: str, agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    current_agent_name = \"researcher\"\n",
        "    responses = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        current_agent = agents[current_agent_name]\n",
        "        response = current_agent.execute(query)\n",
        "        responses.append(f\"{current_agent_name.capitalize()}: {response}\")\n",
        "\n",
        "        next_agent_name = \"researcher\"  # Logic to select next agent based on response\n",
        "        if next_agent_name == current_agent_name or step == max_steps - 1:\n",
        "            break\n",
        "        current_agent_name = next_agent_name\n",
        "\n",
        "    return \"\\n\\n\".join(responses)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        vector_store = load_vector_store()\n",
        "        agents = create_agents(vector_store)\n",
        "        crew = create_crew(agents)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        queries = [\n",
        "            \"Assess the vulnerability CVE-2024-12345 in Windows Server.\",\n",
        "            \"Provide a security recommendation for mitigating phishing attacks.\",\n",
        "            \"List all details on BFSI security incidents in India.\",\n",
        "            \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "            \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "            \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            logger.info(f\"Processing query: {query}\")\n",
        "            result = process_query(query, agents)\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(f\"Response:\\n{result}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OkACkeOZB6g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Enhanced Prompt Templates and Hallucination-Free Queries**\n",
        "Use detailed prompt templates for specific queries to ensure accurate responses and avoid hallucinations."
      ],
      "metadata": {
        "id": "QnN4-ER7FuUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "detailed_prompt_templates = {\n",
        "    \"vulnerability_assessment\": ChatPromptTemplate(\n",
        "        input_variables=[\"cve_id\", \"system\"],\n",
        "        template=\"Assess the vulnerability {cve_id} in {system}. Provide detailed information including potential impacts and mitigation steps.\"\n",
        "    ),\n",
        "    \"security_recommendation\": ChatPromptTemplate(\n",
        "        input_variables=[\"threat\"],\n",
        "        template=\"Provide a security recommendation for mitigating {threat}. Include preventive measures and best practices.\"\n",
        "    ),\n",
        "    \"incident_details\": ChatPromptTemplate(\n",
        "        input_variables=[\"sector\", \"location\"],\n",
        "        template=\"List all details on {sector} security incidents in {location}.\"\n",
        "    ),\n",
        "    \"ransomware_attacks\": ChatPromptTemplate(\n",
        "        input_variables=[\"industry\", \"timeframe\"],\n",
        "        template=\"List all ransomware attacks targeting the {industry} industry in the last {timeframe}.\"\n",
        "    ),\n",
        "    \"recent_incidents\": ChatPromptTemplate(\n",
        "        input_variables=[\"ransomware_gang\"],\n",
        "        template=\"Provide recent incidents related to {ransomware_gang} Ransomware.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def process_query_with_templates(query_type: str, input_variables: Dict[str, str], agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    prompt_template = detailed_prompt_templates.get(query_type)\n",
        "    if not prompt_template:\n",
        "        return \"Invalid query type.\"\n",
        "\n",
        "    query = prompt_template.format(input_variables)\n",
        "    return process_query(query, agents, max_steps)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        vector_store = load_vector_store()\n",
        "        agents = create_agents(vector_store)\n",
        "        crew = create_crew(agents)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        queries = [\n",
        "            (\"vulnerability_assessment\", {\"cve_id\": \"CVE-2024-12345\", \"system\": \"Windows Server\"}),\n",
        "            (\"security_recommendation\", {\"threat\": \"phishing attacks\"}),\n",
        "            (\"incident_details\", {\"sector\": \"BFSI\", \"location\": \"India\"}),\n",
        "            (\"ransomware_attacks\", {\"industry\": \"healthcare\", \"timeframe\": \"7 days\"}),\n",
        "            (\"recent_incidents\", {\"ransomware_gang\": \"Lockbit\"}),\n",
        "            (\"recent_incidents\", {\"ransomware_gang\": \"BlackBasta\"})\n",
        "        ]\n",
        "\n",
        "        for query_type, input_variables in queries:\n",
        "            logger.info(f\"Processing query: {query_type} with inputs: {input_variables}\")\n",
        "            result = process_query_with_templates(query_type, input_variables, agents)\n",
        "            print(f\"\\nQuery: {query_type}\")\n",
        "            print(f\"Response:\\n{result}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "k95zdNV2B6j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "on8MtrC7B6mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JuXwbAvQB6o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3xKCEq1B6rS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}