{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "15rtGXUXXidW"
      },
      "outputs": [],
      "source": [
        "!pip install -q apify-client langchain langchain-community langchain-groq networkx pyvis spacy transformers pandas\n",
        "!pip install -q sentence-transformers requests beautifulsoup4 ratelimit langgraph pyLDAvis faiss-cpu crewai crewai_tools exa exa_py matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from apify_client import ApifyClient\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from crewai_tools import tool\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from textblob import TextBlob\n",
        "from exa_py import Exa\n",
        "import requests\n",
        "from typing import List, Dict, Any\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and API Keys\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "EXA_API_KEY = \"your_exa_api_key\"\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "6BZnOZ8nB6Q3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Tools and Tasks**\n",
        "Define functions for real-time data fetching, Twitter scraping, news fetching, CVE data fetching, Exa.ai integration, and advanced data analysis.\n"
      ],
      "metadata": {
        "id": "RKATj2bwCXT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def scrape_website(url: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        response = session.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "@tool\n",
        "def fetch_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def fetch_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "    try:\n",
        "        response = session.get(url, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching news: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def fetch_cve_data() -> List[Dict[str, Any]]:\n",
        "    url = \"https://cve.circl.lu/api/last\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        cve_items = response.json()\n",
        "        return cve_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def fetch_exa_research(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    exa_client = ExaClient(api_key=EXA_API_KEY)\n",
        "    try:\n",
        "        results = exa_client.search(query, max_results=max_results)\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching Exa.ai research: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def analyze_cve_severity(cve_description: str) -> str:\n",
        "    severity_keywords = [\"critical\", \"high\", \"medium\", \"low\"]\n",
        "    severity = \"unknown\"\n",
        "    for keyword in severity_keywords:\n",
        "        if keyword in cve_description.lower():\n",
        "            severity = keyword\n",
        "            break\n",
        "    return f\"The CVE severity is {severity}.\"\n",
        "\n",
        "@tool\n",
        "def extract_iocs(text: str) -> List[str]:\n",
        "    iocs = [\"example.com\", \"192.168.1.1\"]\n",
        "    return [ioc for ioc in iocs if ioc in text]\n",
        "\n",
        "@tool\n",
        "def trend_analysis(data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "    return f\"Trend analysis for the timeframe {timeframe} shows increasing threats.\"\n",
        "\n",
        "@tool\n",
        "def sentiment_analysis(text: str) -> str:\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return \"Positive sentiment\"\n",
        "    elif sentiment < 0:\n",
        "        return \"Negative sentiment\"\n",
        "    else:\n",
        "        return \"Neutral sentiment\"\n",
        "\n",
        "@tool\n",
        "def topic_modeling(texts: List[str], num_topics: int = 5) -> List[str]:\n",
        "    topics = [\"Cybersecurity\", \"Threats\", \"Vulnerabilities\", \"Attacks\", \"Defense\"]\n",
        "    return topics[:num_topics]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vrGfZox7B6Tl",
        "outputId": "3edba885-5cd4-4182-c68e-5b8608138fd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Function must have a docstring",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aca3a6c7b917>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai_tools/tools/base_tool.py\u001b[0m in \u001b[0;36mtool\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_make_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_make_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai_tools/tools/base_tool.py\u001b[0m in \u001b[0;36m_make_tool\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_make_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseTool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Function must have a docstring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__annotations__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Function must have type annotations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Function must have a docstring"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Create Agents**\n",
        "Define the agents with specific roles and goals, and assign the necessary tools."
      ],
      "metadata": {
        "id": "61vqRkBtDHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent\n",
        "\n",
        "def create_agents(vector_store) -> Dict[str, Agent]:\n",
        "    researcher = Agent(\n",
        "        role=\"Researcher\",\n",
        "        goal=\"Gather and provide relevant information\",\n",
        "        tools=[scrape_website, fetch_tweets, fetch_news, fetch_cve_data, fetch_exa_research, analyze_cve_severity, extract_iocs, trend_analysis, sentiment_analysis, topic_modeling]\n",
        "    )\n",
        "\n",
        "    analyst = Agent(\n",
        "        role=\"Analyst\",\n",
        "        goal=\"Analyze data and provide insights\",\n",
        "        tools=[scrape_website, fetch_tweets, fetch_news, fetch_cve_data, fetch_exa_research, analyze_cve_severity, extract_iocs, trend_analysis, sentiment_analysis, topic_modeling]\n",
        "    )\n",
        "\n",
        "    advisor = Agent(\n",
        "        role=\"Advisor\",\n",
        "        goal=\"Give recommendations based on analysis\",\n",
        "        tools=[scrape_website, fetch_tweets, fetch_news, fetch_cve_data, fetch_exa_research, analyze_cve_severity, extract_iocs, trend_analysis, sentiment_analysis, topic_modeling]\n",
        "    )\n",
        "\n",
        "    threat_hunter = Agent(\n",
        "        role=\"Threat Hunter\",\n",
        "        goal=\"Identify potential threats and IOCs\",\n",
        "        tools=[scrape_website, fetch_tweets, fetch_news, fetch_cve_data, fetch_exa_research, analyze_cve_severity, extract_iocs, trend_analysis, sentiment_analysis, topic_modeling]\n",
        "    )\n",
        "\n",
        "    incident_responder = Agent(\n",
        "        role=\"Incident Responder\",\n",
        "        goal=\"Provide guidance on handling security incidents\",\n",
        "        tools=[scrape_website, fetch_tweets, fetch_news, fetch_cve_data, fetch_exa_research, analyze_cve_severity, extract_iocs, trend_analysis, sentiment_analysis, topic_modeling]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"researcher\": researcher,\n",
        "        \"analyst\": analyst,\n",
        "        \"advisor\": advisor,\n",
        "        \"threat_hunter\": threat_hunter,\n",
        "        \"incident_responder\": incident_responder\n",
        "    }"
      ],
      "metadata": {
        "id": "YdWZKkJ6B6Wr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Form the Crew**\n",
        "Organize the agents into a Crew and define the process."
      ],
      "metadata": {
        "id": "VCoDMLcxDT8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Crew, Process\n",
        "\n",
        "def create_crew(agents: Dict[str, Agent]) -> Crew:\n",
        "    return Crew(\n",
        "        agents=list(agents.values()),\n",
        "        process=Process.sequential\n",
        "    )"
      ],
      "metadata": {
        "id": "8SoGgZIOB6ZN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Implement Data Collection**\n",
        "Define functions to collect and curate data from various sources.\n"
      ],
      "metadata": {
        "id": "MLIiuXx7D2dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data():\n",
        "    websites = [\n",
        "            \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "            \"https://attack.mitre.org/\",\n",
        "            \"https://www.darkreading.com/\",\n",
        "            \"https://threatpost.com/\",\n",
        "            \"https://krebsonsecurity.com/\",\n",
        "            \"https://www.bleepingcomputer.com/\",\n",
        "            \"https://www.zdnet.com/topic/security/\",\n",
        "            \"https://www.securityweek.com/\",\n",
        "            \"https://www.sans.org/newsletters/newsbites/\",\n",
        "            \"https://www.cyberscoop.com/\",\n",
        "            \"https://www.csoonline.com/\",\n",
        "            \"https://www.infosecurity-magazine.com/\",\n",
        "            \"https://www.wired.com/category/security/\",\n",
        "            \"https://www.schneier.com/\",\n",
        "            \"https://www.theregister.com/security/\",\n",
        "            \"https://thehackernews.com/\",\n",
        "            \"https://www.cyberdefensemagazine.com/\",\n",
        "            \"https://www.fireeye.com/blog.html\",\n",
        "            \"https://unit42.paloaltonetworks.com/\",\n",
        "            \"https://www.microsoft.com/security/blog/\",\n",
        "            \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "            \"https://nakedsecurity.sophos.com/\",\n",
        "            \"https://www.recordedfuture.com/blog/\",\n",
        "            \"https://www.cybersecurity-insiders.com/\",\n",
        "            \"https://www.malwarebytes.com/blog/\",\n",
        "    ]\n",
        "\n",
        "    scraped_data = scrape_websites(websites)\n",
        "    tweets = fetch_tweets(\"cybersecurity\")\n",
        "    news = fetch_news(\"cybersecurity\")\n",
        "    cve_data = fetch_cve_data()\n",
        "    exa_research = fetch_exa_research(\"cybersecurity\")\n",
        "\n",
        "    return {\n",
        "        \"scraped_data\": scraped_data,\n",
        "        \"tweets\": tweets,\n",
        "        \"news\": news,\n",
        "        \"cve_data\": cve_data,\n",
        "        \"exa_research\": exa_research\n",
        "    }"
      ],
      "metadata": {
        "id": "hnxIrUBHB6bx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Curate and Store Data**\n",
        "Aggregate and store the curated data in a vector database."
      ],
      "metadata": {
        "id": "SSWupiFSEsHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def curate_data(scraped_data, tweets, news, cve_data, exa_research):\n",
        "    curated_data = []\n",
        "\n",
        "    for page in scraped_data:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in tweets:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in news:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in cve_data:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for research in exa_research:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Exa.ai\",\n",
        "            \"title\": research.get(\"title\"),\n",
        "            \"abstract\": research.get(\"abstract\"),\n",
        "            \"url\": research.get(\"url\"),\n",
        "            \"timestamp\": research.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    return curated_data\n",
        "\n",
        "def store_in_vector_db(curated_data):\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=item[\"text\"], metadata=item) for item in curated_data]\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(\"vector_store\")\n",
        "\n",
        "def load_vector_store():\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    return FAISS.load_local(\"vector_store\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "kDSH5ccYB6eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **LLM Intialization with Groq**\n"
      ],
      "metadata": {
        "id": "S48zdzU-GknU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "5ClBbMDOGr2K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Run the Multi-Agent System**\n",
        "Implement the main function to run the multi-agent system, process queries, and provide responses."
      ],
      "metadata": {
        "id": "ugas2ASzFaNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query: str, agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    current_agent_name = \"researcher\"\n",
        "    responses = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        current_agent = agents[current_agent_name]\n",
        "        response = current_agent.execute(query)\n",
        "        responses.append(f\"{current_agent_name.capitalize()}: {response}\")\n",
        "\n",
        "        next_agent_name = \"researcher\"  # Logic to select next agent based on response\n",
        "        if next_agent_name == current_agent_name or step == max_steps - 1:\n",
        "            break\n",
        "        current_agent_name = next_agent_name\n",
        "\n",
        "    return \"\\n\\n\".join(responses)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        vector_store = load_vector_store()\n",
        "        agents = create_agents(vector_store)\n",
        "        crew = create_crew(agents)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        queries = [\n",
        "            \"Assess the vulnerability CVE-2024-12345 in Windows Server.\",\n",
        "            \"Provide a security recommendation for mitigating phishing attacks.\",\n",
        "            \"List all details on BFSI security incidents in India.\",\n",
        "            \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "            \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "            \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            logger.info(f\"Processing query: {query}\")\n",
        "            result = process_query(query, agents)\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(f\"Response:\\n{result}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OkACkeOZB6g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Enhanced Prompt Templates and Hallucination-Free Queries**\n",
        "Use detailed prompt templates for specific queries to ensure accurate responses and avoid hallucinations."
      ],
      "metadata": {
        "id": "QnN4-ER7FuUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "detailed_prompt_templates = {\n",
        "    \"vulnerability_assessment\": ChatPromptTemplate(\n",
        "        input_variables=[\"cve_id\", \"system\"],\n",
        "        template=\"Assess the vulnerability {cve_id} in {system}. Provide detailed information including potential impacts and mitigation steps.\"\n",
        "    ),\n",
        "    \"security_recommendation\": ChatPromptTemplate(\n",
        "        input_variables=[\"threat\"],\n",
        "        template=\"Provide a security recommendation for mitigating {threat}. Include preventive measures and best practices.\"\n",
        "    ),\n",
        "    \"incident_details\": ChatPromptTemplate(\n",
        "        input_variables=[\"sector\", \"location\"],\n",
        "        template=\"List all details on {sector} security incidents in {location}.\"\n",
        "    ),\n",
        "    \"ransomware_attacks\": ChatPromptTemplate(\n",
        "        input_variables=[\"industry\", \"timeframe\"],\n",
        "        template=\"List all ransomware attacks targeting the {industry} industry in the last {timeframe}.\"\n",
        "    ),\n",
        "    \"recent_incidents\": ChatPromptTemplate(\n",
        "        input_variables=[\"ransomware_gang\"],\n",
        "        template=\"Provide recent incidents related to {ransomware_gang} Ransomware.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def process_query_with_templates(query_type: str, input_variables: Dict[str, str], agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    prompt_template = detailed_prompt_templates.get(query_type)\n",
        "    if not prompt_template:\n",
        "        return \"Invalid query type.\"\n",
        "\n",
        "    query = prompt_template.format(input_variables)\n",
        "    return process_query(query, agents, max_steps)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        vector_store = load_vector_store()\n",
        "        agents = create_agents(vector_store)\n",
        "        crew = create_crew(agents)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        queries = [\n",
        "            (\"vulnerability_assessment\", {\"cve_id\": \"CVE-2024-12345\", \"system\": \"Windows Server\"}),\n",
        "            (\"security_recommendation\", {\"threat\": \"phishing attacks\"}),\n",
        "            (\"incident_details\", {\"sector\": \"BFSI\", \"location\": \"India\"}),\n",
        "            (\"ransomware_attacks\", {\"industry\": \"healthcare\", \"timeframe\": \"7 days\"}),\n",
        "            (\"recent_incidents\", {\"ransomware_gang\": \"Lockbit\"}),\n",
        "            (\"recent_incidents\", {\"ransomware_gang\": \"BlackBasta\"})\n",
        "        ]\n",
        "\n",
        "        for query_type, input_variables in queries:\n",
        "            logger.info(f\"Processing query: {query_type} with inputs: {input_variables}\")\n",
        "            result = process_query_with_templates(query_type, input_variables, agents)\n",
        "            print(f\"\\nQuery: {query_type}\")\n",
        "            print(f\"Response:\\n{result}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred in the main function: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "k95zdNV2B6j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "on8MtrC7B6mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JuXwbAvQB6o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3xKCEq1B6rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLx7_NH4B6tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSoJ0jfmB6xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76t_FdexB60c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkKhqVR_B63M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kC4HDfP6B66R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQaaKAMVB69a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}