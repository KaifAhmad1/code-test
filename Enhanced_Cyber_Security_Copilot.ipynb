{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15rtGXUXXidW",
        "outputId": "20f10e16-03f8-4f16-f0ac-4095daa23976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q apify-client langchain langchain-community langchain-groq networkx pyvis spacy transformers pandas\n",
        "%pip install -q sentence-transformers requests beautifulsoup4 ratelimit langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Annotated, TypedDict\n",
        "import logging\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from langchain.agents import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "WEBSITES = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "    \"https://krebsonsecurity.com/\",\n",
        "    \"https://www.bleepingcomputer.com/\",\n",
        "    \"https://www.zdnet.com/topic/security/\",\n",
        "    \"https://www.securityweek.com/\",\n",
        "    \"https://www.sans.org/newsletters/newsbites/\",\n",
        "    \"https://www.cyberscoop.com/\",\n",
        "    \"https://www.csoonline.com/\",\n",
        "    \"https://www.infosecurity-magazine.com/\",\n",
        "    \"https://www.wired.com/category/security/\",\n",
        "    \"https://www.schneier.com/\",\n",
        "    \"https://www.theregister.com/security/\",\n",
        "    \"https://thehackernews.com/\",\n",
        "    \"https://www.cyberdefensemagazine.com/\",\n",
        "    \"https://www.fireeye.com/blog.html\",\n",
        "    \"https://unit42.paloaltonetworks.com/\",\n",
        "    \"https://www.microsoft.com/security/blog/\",\n",
        "    \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "    \"https://nakedsecurity.sophos.com/\",\n",
        "    \"https://www.recordedfuture.com/blog/\",\n",
        "    \"https://www.cybersecurity-insiders.com/\",\n",
        "    \"https://www.malwarebytes.com/blog/\",\n",
        "]\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts.xml\",\n",
        "    \"https://krebsonsecurity.com/feed/\",\n",
        "    \"https://threatpost.com/feed/\",\n",
        "    \"https://www.darkreading.com/rss_simple.asp\"\n",
        "]"
      ],
      "metadata": {
        "id": "DBpAjqmvihA8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "RRi1CmpSYlfI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rate-limited GET request\n",
        "@sleep_and_retry\n",
        "@limits(calls=15, period=1)  # 5 calls per second\n",
        "def rate_limited_get(url: str, **kwargs) -> requests.Response:\n",
        "    return session.get(url, timeout=10, **kwargs)"
      ],
      "metadata": {
        "id": "Go2qxs8tYmib"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Website scraping using Apify actor\n",
        "def scrape_website_with_apify(url: str) -> Dict[str, Any]:\n",
        "    logger.info(f\"Scraping {url} with Apify...\")\n",
        "    try:\n",
        "        actor_input = {\n",
        "            \"url\": url,\n",
        "            \"proxyConfiguration\": {\"useApifyProxy\": True}\n",
        "        }\n",
        "        run = apify_client.actor(\"apify/website-content-crawler\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        if items:\n",
        "            return {\"url\": url, \"text\": items[0].get(\"text\", \"\"), \"timestamp\": datetime.now().isoformat()}\n",
        "        else:\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": \"No content found\"}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url} with Apify: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "# Website scraping\n",
        "def scrape_website(url: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        response = rate_limited_get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "def scrape_websites(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_url = {executor.submit(scrape_website, url): url for url in urls}\n",
        "        results = [future.result() for future in as_completed(future_to_url)]\n",
        "    logger.info(f\"Successfully scraped {len(results)} pages.\")\n",
        "    return results\n",
        "\n",
        "# Fetch tweets\n",
        "def fetch_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"searchTerms\": [query],\n",
        "        \"maxTweets\": max_tweets,\n",
        "        \"languageCode\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch news articles\n",
        "def fetch_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching news for query: {query}\")\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"language\": \"en\",\n",
        "        \"pageSize\": max_results,\n",
        "        \"apiKey\": NEWS_API_KEY,\n",
        "        \"sortBy\": \"publishedAt\"\n",
        "    }\n",
        "    try:\n",
        "        response = rate_limited_get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "        logger.info(f\"Fetched {len(articles)} news articles.\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching news: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Scrape Reddit\n",
        "def scrape_reddit(query: str, max_results: int = 100) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Scraping Reddit for: {query}\")\n",
        "    actor_input = {\n",
        "        \"searchTerms\": [query],\n",
        "        \"maxPosts\": max_results\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"comchat/reddit-api-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} Reddit posts.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping Reddit: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch CVE data\n",
        "def fetch_cve_data() -> List[Dict[str, Any]]:\n",
        "    logger.info(\"Fetching CVE data\")\n",
        "    url = \"https://cve.circl.lu/api/last\"\n",
        "    try:\n",
        "        response = rate_limited_get(url)\n",
        "        response.raise_for_status()\n",
        "        cve_items = response.json()\n",
        "        logger.info(f\"Fetched {len(cve_items)} CVE items.\")\n",
        "        return cve_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch Google News articles\n",
        "def fetch_google_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching Google News for: {query}\")\n",
        "    actor_input = {\n",
        "        \"queries\": query,\n",
        "        \"maxPagesPerQuery\": max_results\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"lhotanova/google-news-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} Google News articles.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching Google News: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch Bing search results\n",
        "def fetch_bing_search(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching Bing search results for: {query}\")\n",
        "    actor_input = {\n",
        "        \"queries\": query,\n",
        "        \"maxPagesPerQuery\": max_results\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"curious_coder/bing-search-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} Bing search results.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching Bing search results: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch LinkedIn posts\n",
        "def fetch_linkedin_posts(query: str, max_posts: int = 100) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching LinkedIn posts for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"searchTerms\": [query],\n",
        "        \"maxPosts\": max_posts,\n",
        "        \"languageCode\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"curious_coder/linkedin-post-search-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} LinkedIn posts.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching LinkedIn posts: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Fetch RSS feeds\n",
        "def fetch_rss_feeds(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching RSS feeds from {len(urls)} URLs\")\n",
        "    run_input = {\n",
        "        \"startUrls\": urls,\n",
        "        \"maxItems\": 50\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"jupri/rss-xml-scraper\").call(run_input=run_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} RSS feed items.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feeds: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Kn1O2gnk8P3R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Curate data from various sources\n",
        "def curate_data(website_data, tweets, news, reddit_posts, cve_data, google_news, bing_results, linkedin_posts, rss_feeds):\n",
        "    curated_data = []\n",
        "\n",
        "    # Process and curate data from websites\n",
        "    for page in website_data:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from Twitter\n",
        "    for tweet in tweets:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from news articles\n",
        "    for article in news:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from Reddit posts\n",
        "    for post in reddit_posts:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Reddit\",\n",
        "            \"url\": post.get(\"url\"),\n",
        "            \"title\": post.get(\"title\"),\n",
        "            \"selftext\": post.get(\"selftext\"),\n",
        "            \"timestamp\": post.get(\"created_utc\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from CVE data\n",
        "    for cve in cve_data:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from Google News articles\n",
        "    for article in google_news:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Google News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from Bing search results\n",
        "    for item in bing_results:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Bing\",\n",
        "            \"url\": item.get(\"url\"),\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"snippet\": item.get(\"snippet\"),\n",
        "            \"timestamp\": item.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from LinkedIn posts\n",
        "    for post in linkedin_posts:\n",
        "        curated_data.append({\n",
        "            \"source\": \"LinkedIn\",\n",
        "            \"text\": post.get(\"text\"),\n",
        "            \"user\": post.get(\"user\"),\n",
        "            \"timestamp\": post.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    # Process and curate data from RSS feeds\n",
        "    for feed in rss_feeds:\n",
        "        curated_data.append({\n",
        "            \"source\": \"RSS\",\n",
        "            \"url\": feed.get(\"link\"),\n",
        "            \"title\": feed.get(\"title\"),\n",
        "            \"description\": feed.get(\"description\"),\n",
        "            \"timestamp\": feed.get(\"pubDate\")\n",
        "        })\n",
        "\n",
        "    return curated_data"
      ],
      "metadata": {
        "id": "kBQ_qsgy8P5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define tags and queries\n",
        "tags = [\n",
        "    \"malware\", \"ransomware\", \"threat\", \"cybersecurity\", \"phishing\",\n",
        "    \"data breach\", \"DDoS attack\", \"APT\", \"zero-day\", \"exploit\",\n",
        "    \"vulnerability\", \"incident response\", \"threat intelligence\",\n",
        "    \"SIEM\", \"EDR\", \"XDR\", \"cloud security\", \"IoT security\",\n",
        "    \"AI security\", \"blockchain security\", \"cryptography\",\n",
        "    \"network security\", \"application security\", \"DevSecOps\",\n",
        "    \"container security\", \"Kubernetes security\", \"SOAR\",\n",
        "    \"threat hunting\", \"OSINT\", \"penetration testing\",\n",
        "    \"red teaming\", \"blue teaming\", \"purple teaming\",\n",
        "    \"cyber insurance\", \"compliance\", \"GDPR\", \"HIPAA\",\n",
        "    \"PCI DSS\", \"NIST\", \"ISO 27001\", \"zero trust\",\n",
        "    \"passwordless\", \"biometrics\", \"MFA\", \"IAM\", \"PAM\",\n",
        "    \"cyber resilience\", \"cyber hygiene\", \"security awareness\",\n",
        "    \"social engineering\", \"insider threat\", \"supply chain attack\",\n",
        "    \"quantum computing\", \"post-quantum cryptography\", \"5G security\",\n",
        "    \"OT security\", \"ICS security\", \"SCADA security\", \"mobile security\",\n",
        "    \"endpoint security\", \"email security\", \"web security\",\n",
        "    \"API security\", \"CASB\", \"CWPP\", \"CSPM\", \"CNAPP\",\n",
        "    \"cyber warfare\", \"cyber espionage\", \"hacktivism\", \"cyber terrorism\",\n",
        "    \"cyber crime\", \"dark web\", \"threat actor\", \"nation-state attack\",\n",
        "    \"latest cybersecurity incidents\", \"recent cyber attacks\", \"real-time threats\",\n",
        "    \"emerging vulnerabilities\", \"critical infrastructure security\", \"cyber defense\",\n",
        "    \"cybersecurity trends\", \"cybersecurity news\", \"cybersecurity alerts\",\n",
        "    \"cybersecurity updates\", \"cybersecurity bulletins\", \"cybersecurity advisories\",\n",
        "    \"cybersecurity reports\", \"cybersecurity analysis\", \"cybersecurity research\"\n",
        "]\n",
        "\n",
        "queries = [\n",
        "    \"cybersecurity threats\",\n",
        "    \"vulnerability assessment\",\n",
        "    \"latest security updates\",\n",
        "    \"List all details on {{BFSI}} security incidents in {{India}}.\",\n",
        "    \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\",\n",
        "    \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\",\n",
        "    \"Recent data breaches\",\n",
        "    \"Latest phishing campaigns\",\n",
        "    \"Real-time cybersecurity alerts\",\n",
        "    \"Emerging cyber threats\",\n",
        "    \"Critical infrastructure security incidents\",\n",
        "    \"Recent DDoS attacks\",\n",
        "    \"Latest zero-day vulnerabilities\",\n",
        "    \"Recent APT activities\",\n",
        "    \"Latest cybersecurity news\",\n",
        "    \"Recent cybersecurity trends\",\n",
        "    \"Latest cybersecurity advisories\",\n",
        "    \"Recent cybersecurity bulletins\",\n",
        "    \"Latest cybersecurity reports\",\n",
        "    \"Recent cybersecurity research\",\n",
        "    \"Latest cybersecurity analysis\"\n",
        "]\n",
        "\n",
        "# Main function to orchestrate the process\n",
        "def main():\n",
        "    for query in queries:\n",
        "        website_data = scrape_websites(WEBSITES)\n",
        "        tweets = fetch_tweets(query)\n",
        "        news = fetch_news(query)\n",
        "        reddit_posts = scrape_reddit(query)\n",
        "        cve_data = fetch_cve_data()\n",
        "        google_news = fetch_google_news(query)\n",
        "        bing_results = fetch_bing_search(query)\n",
        "        linkedin_posts = fetch_linkedin_posts(query)\n",
        "        rss_feeds = fetch_rss_feeds(RSS_FEEDS)\n",
        "\n",
        "        curated_data = curate_data(\n",
        "            website_data, tweets, news, reddit_posts, cve_data,\n",
        "            google_news, bing_results, linkedin_posts, rss_feeds\n",
        "        )\n",
        "\n",
        "        # Print only the first instance of curated data for demonstration\n",
        "        if curated_data:\n",
        "            print(json.dumps(curated_data[0], indent=2))\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7vNZGXIq8P8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f81d7c6-8eed-4ac3-9e33-87412751342f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity+threats&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error scraping Reddit: Input is not valid: Field input.searchList is required\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "ERROR:__main__:Error fetching CVE data: HTTPSConnectionPool(host='cve.circl.lu', port=443): Max retries exceeded with url: /api/last (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\"))\n",
            "ERROR:__main__:Error fetching Google News: You must rent a paid Actor in order to run it.\n",
            "ERROR:__main__:Error fetching Bing search results: You must rent a paid Actor in order to run it.\n",
            "ERROR:__main__:Error fetching LinkedIn posts: You must rent a paid Actor in order to run it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"Website\",\n",
            "  \"url\": \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
            "  \"text\": \"Cybersecurity Alerts & Advisories | CISA Skip to main content An official website of the United States government Here\\u2019s how you know Here\\u2019s how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock ( Lock A locked padlock ) or https:// means you\\u2019ve safely connected to the .gov website. Share sensitive information only on official, secure websites. Free Cyber Services #protect2024 Secure Our World Shields Up Report A Cyber Issue Search Menu Close Topics Topics Cybersecurity Best Practices Cyber Threats and Advisories Critical Infrastructure Security and Resilience Election Security Emergency Communications Industrial Control Systems Information and Communications Technology Supply Chain Security Partnerships and Collaboration Physical Security Risk Management How can we help? Government Educational Institutions Industry State, Local, Tribal, and Territorial Individuals and Families Small and Medium Businesses Find Help Locally Faith-Based Community Executives High-Risk Communities Spotlight Resources & Tools Resources & Tools All Resources & Tools Services Programs Resources Training Groups News & Events News & Events News Events Cybersecurity Alerts & Advisories Directives Request a CISA Speaker Congressional Testimony CISA Conferences CISA Live! Careers Careers Benefits & Perks HireVue Applicant Reasonable Accommodations Process Hiring Resume & Application Tips Students & Recent Graduates Veteran and Military Spouses Work @ CISA About About Culture Divisions & Offices Regions Leadership Doing Business with CISA Site Links Reporting Employee and Contractor Misconduct CISA GitHub CISA Central 2023 Year In Review Contact Us Free Cyber Services #protect2024 Secure Our World Shields Up Report A Cyber Issue Breadcrumb Home News & Events Share: Filters What are you looking for? Text Search (optional) Sort by (optional) Release Date Last Revised Title Relevance Vendor Leave this field blank (optional) Advisory Type Alert Analysis Report (-) Cybersecurity Advisory ICS Advisory ICS Medical Advisory ICS Alert Audience Educational Institutions Federal Government Industry Small and Medium Businesses State, Local, Tribal, and Territorial Government Co-Sealers and Partners Cyber National Mission Force Federal Bureau of Investigation Federal Civilian Executive Branch Agencies Five Eyes International Multi-State Information Sharing and Analysis Center National Security Agency United States Cyber Command MITRE ATT&CK TTP Brute Force (T1110) Collection (TA0009) Command and Control (TA0011) Credential Access (TA0006) Defense Evasion (TA0005) Discovery (TA0007) Execution (TA0002) Exfiltration (TA0010) Impact (TA0040) Initial Access (TA0001) Lateral Movement (TA0008) Persistence (TA0003) Phishing for Information (T1598) Privilege Escalation (TA0004) Reconnaissance (TA0043) Resource Development (TA0042) Nation-State Actor China Iran North Korea Russia Release Year 2024 2023 2022 2021 2020 2019 2018 Sector Communications Sector Critical Manufacturing Sector Defense Industrial Base Sector Emergency Services Sector Energy Sector Financial Services Sector Food and Agriculture Sector Government Services and Facilities Sector Healthcare and Public Health Sector Information Technology Sector Transportation Systems Sector Water and Wastewater Systems Topic Nation-State Cyber Actors Multifactor Authentication Incident Detection, Response, and Prevention Information Sharing Organizations and Cyber Safety Malware, Phishing, and Ransomware Securing Networks Cybersecurity Best Practices Critical Infrastructure Security and Resilience Information and Communications Technology Supply Chain Security Cyber Threats and Advisories Vendor No result Reset Cybersecurity Alerts & Advisories View Cybersecurity Advisories Only View Advisory Definitions Filters: (-) Cybersecurity Advisory Clear all filters Jul 25, 2024 Cybersecurity Advisory | AA24-207A North Korea Cyber Group Conducts Global Espionage Campaign to Advance Regime\\u2019s Military and Nuclear Programs Jul 11, 2024 Cybersecurity Advisory | AA24-193A CISA Red Team\\u2019s Operations Against a Federal Civilian Executive Branch Organization Highlights the Necessity of Defense-in-Depth Jul 08, 2024 Cybersecurity Advisory | AA24-190A People\\u2019s Republic of China (PRC) Ministry of State Security APT40 Tradecraft in Action May 10, 2024 Cybersecurity Advisory | AA24-131A #StopRansomware: Black Basta Apr 18, 2024 Cybersecurity Advisory | AA24-109A #StopRansomware: Akira Ransomware Feb 29, 2024 Cybersecurity Advisory | AA24-060B Threat Actors Exploit Multiple Vulnerabilities in Ivanti Connect Secure and Policy Secure Gateways Feb 29, 2024 Cybersecurity Advisory | AA24-060A #StopRansomware: Phobos Ransomware Feb 26, 2024 Cybersecurity Advisory | AA24-057A SVR Cyber Actors Adapt Tactics for Initial Cloud Access Feb 15, 2024 Cybersecurity Advisory | AA24-046A Threat Actor Leverages Compromised Account of Former Employee to Access State Government Organization Feb 07, 2024 Cybersecurity Advisory | AA24-038A PRC State-Sponsored Actors Compromise and Maintain Persistent Access to U.S. Critical Infrastructure Currently on page 1 Page 2 Page 3 Page 4 Page 5 Page 6 Page 7 Page 8 Page 9 \\u2026 Go to next page Next Go to last page Last Advisory Definitions Cybersecurity Advisory: In-depth reports covering a specific cybersecurity issue, often including threat actor tactics, techniques, and procedures; indicators of compromise; and mitigations. Alert: Concise summaries covering cybersecurity topics, such as mitigations that vendors have published for vulnerabilities in their products. ICS Advisory: Concise summaries covering industrial control system (ICS) cybersecurity topics, primarily focused on mitigations that ICS vendors have published for vulnerabilities in their products. ICS Medical Advisory: Concise summaries covering ICS medical cybersecurity topics, primarily focused on mitigations that ICS medical vendors have published for vulnerabilities in their products. Analysis Report: In-depth analysis of a new or evolving cyber threat, including technical details and remediations. Return to top Topics Spotlight Resources & Tools News & Events Careers About Cybersecurity & Infrastructure Security Agency Facebook Twitter LinkedIn YouTube Instagram RSS CISA Central 1-844-Say-CISA SayCISA@cisa.gov DHS Seal CISA.gov An official website of the U.S. Department of Homeland Security About CISA Budget and Performance DHS.gov Equal Opportunity & Accessibility FOIA Requests No FEAR Act Office of Inspector General Privacy Policy Subscribe The White House USA.gov Website Feedback\",\n",
            "  \"timestamp\": \"2024-07-30T12:53:32.480969\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced process_scraped_data function\n",
        "def process_scraped_data(data: List[Dict[str, Any]]) -> List[str]:\n",
        "    processed_texts = []\n",
        "    for item in data:\n",
        "        if item[\"source\"] == \"Website\":\n",
        "            processed_texts.append(f\"Website Content ({item['url']}): {item['text']}\")\n",
        "        elif item[\"source\"] in [\"Twitter\", \"LinkedIn\"]:\n",
        "            processed_texts.append(f\"{item['source']} Post: {item['text']} - Posted by {item['user']} at {item['timestamp']}\")\n",
        "        elif item[\"source\"] in [\"News\", \"Google News\", \"RSS\"]:\n",
        "            processed_texts.append(f\"{item['source']} Article: {item['title']} - {item['description']} (Published: {item['timestamp']})\")\n",
        "        elif item[\"source\"] == \"Reddit\":\n",
        "            processed_texts.append(f\"Reddit Post: {item['title']} - {item['selftext']} (Posted: {item['timestamp']})\")\n",
        "        elif item[\"source\"] == \"CVE\":\n",
        "            processed_texts.append(f\"CVE: {item['cve_id']} - {item['description']} (Published: {item['timestamp']})\")\n",
        "        elif item[\"source\"] == \"Bing\":\n",
        "            processed_texts.append(f\"Bing Search Result: {item['title']} - {item['snippet']} (Indexed: {item['timestamp']})\")\n",
        "\n",
        "    return processed_texts"
      ],
      "metadata": {
        "id": "khJ82mt-lMpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")\n",
        "\n",
        "# Define system and human messages\n",
        "system_message = \"\"\"You are an expert cybersecurity analyst with extensive knowledge in threat analysis,\n",
        "vulnerability assessment, and security recommendations. Provide detailed, precise, and actionable insights.\n",
        "Always consider the latest threat intelligence and best practices in your analysis.\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"human\", \"{text}\")])"
      ],
      "metadata": {
        "id": "pKja0_jKGtbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from pyvis.network import Network\n",
        "import networkx as nx\n",
        "\n",
        "# Configure logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def add_node(self, node, **attrs):\n",
        "        self.graph.add_node(node, **attrs)\n",
        "\n",
        "    def add_edge(self, u, v, **attrs):\n",
        "        self.graph.add_edge(u, v, **attrs)\n",
        "\n",
        "    def visualize(self, output_file):\n",
        "        net = Network(notebook=True)\n",
        "        for node in self.graph.nodes(data=True):\n",
        "            net.add_node(node[0], title=node[1].get('title', node[0]))\n",
        "        for edge in self.graph.edges(data=True):\n",
        "            net.add_edge(edge[0], edge[1], title=edge[2].get('relation', ''))\n",
        "        net.show(output_file)\n",
        "        logger.info(f\"Knowledge graph visualized at {output_file}\")\n",
        "\n",
        "# Initialize knowledge graph\n",
        "kg = KnowledgeGraph()\n",
        "# Visualize the graph\n",
        "kg.visualize(\"knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "oYPEmb9ekJxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create vector store\n",
        "def create_vector_store(texts: List[str]) -> FAISS:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    docs = text_splitter.create_documents(texts)\n",
        "    vector_store = FAISS.from_documents(docs, embeddings)\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "HUPEeUM2Gtgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced cybersecurity analysis tools\n",
        "def analyze_cve_severity(cve_description: str) -> str:\n",
        "    severity_keywords = {\n",
        "        \"critical\": 10, \"high\": 7, \"medium\": 5, \"low\": 3,\n",
        "        \"remote code execution\": 9, \"privilege escalation\": 8,\n",
        "        \"denial of service\": 6, \"information disclosure\": 4\n",
        "    }\n",
        "\n",
        "    description_lower = cve_description.lower()\n",
        "    max_severity = max(score for keyword, score in severity_keywords.items() if keyword in description_lower)\n",
        "\n",
        "    if max_severity >= 9:\n",
        "        return f\"Critical (Score: {max_severity})\"\n",
        "    elif max_severity >= 7:\n",
        "        return f\"High (Score: {max_severity})\"\n",
        "    elif max_severity >= 4:\n",
        "        return f\"Medium (Score: {max_severity})\"\n",
        "    else:\n",
        "        return f\"Low (Score: {max_severity})\"\n",
        "\n",
        "def extract_iocs(text: str) -> Dict[str, List[str]]:\n",
        "    iocs = {\n",
        "        \"ip_addresses\": [],\n",
        "        \"domains\": [],\n",
        "        \"hashes\": [],\n",
        "        \"urls\": []\n",
        "    }\n",
        "\n",
        "    # Regular expressions for different IOC types\n",
        "    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n",
        "    domain_pattern = r'\\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,}\\b'\n",
        "    hash_pattern = r'\\b[a-fA-F0-9]{32,64}\\b'\n",
        "    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
        "\n",
        "    iocs[\"ip_addresses\"] = re.findall(ip_pattern, text)\n",
        "    iocs[\"domains\"] = re.findall(domain_pattern, text)\n",
        "    iocs[\"hashes\"] = re.findall(hash_pattern, text)\n",
        "    iocs[\"urls\"] = re.findall(url_pattern, text)\n",
        "\n",
        "    return iocs\n",
        "\n",
        "def trend_analysis(data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "    keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\", \"supply chain attack\", \"cloud security\", \"insider threat\"]\n",
        "    timeframe_days = {\"week\": 7, \"month\": 30, \"3months\": 90}\n",
        "\n",
        "    if timeframe not in timeframe_days:\n",
        "        return \"Invalid timeframe. Please use 'week', 'month', or '3months'.\"\n",
        "\n",
        "    cutoff_date = datetime.now() - timedelta(days=timeframe_days[timeframe])\n",
        "    recent_data = [item for item in data if datetime.fromisoformat(item['timestamp']) > cutoff_date]\n",
        "\n",
        "    keyword_counts = {keyword: sum(1 for item in recent_data if keyword in (item.get('text', '') + item.get('title', '') + item.get('description', '')).lower()) for keyword in keywords}\n",
        "\n",
        "    sorted_trends = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    trend_report = f\"Top cybersecurity trends in the last {timeframe}:\\n\"\n",
        "    trend_report += \"\\n\".join(f\"- {keyword.capitalize()}: {count} mentions\" for keyword, count in sorted_trends)\n",
        "\n",
        "    # Generate a bar plot of the trends\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=[item[0] for item in sorted_trends], y=[item[1] for item in sorted_trends])\n",
        "    plt.title(f\"Cybersecurity Trends - Last {timeframe}\")\n",
        "    plt.xlabel(\"Keywords\")\n",
        "    plt.ylabel(\"Mention Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"trend_analysis_{timeframe}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    trend_report += f\"\\n\\nA bar plot of the trends has been saved as 'trend_analysis_{timeframe}.png'.\"\n",
        "\n",
        "    return trend_report\n",
        "\n",
        "# New function: Sentiment analysis\n",
        "def sentiment_analysis(text: str) -> Dict[str, Any]:\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment\n",
        "    return {\n",
        "        \"polarity\": sentiment.polarity,\n",
        "        \"subjectivity\": sentiment.subjectivity,\n",
        "        \"sentiment\": \"positive\" if sentiment.polarity > 0 else \"negative\" if sentiment.polarity < 0 else \"neutral\"\n",
        "    }\n",
        "\n",
        "# New function: Topic modeling\n",
        "def topic_modeling(texts: List[str], num_topics: int = 5) -> str:\n",
        "    # Preprocess the texts\n",
        "    texts = [re.sub(r'\\s+', ' ', text.lower()) for text in texts]\n",
        "    texts = [re.sub(r'[^\\w\\s]', '', text) for text in texts]\n",
        "\n",
        "    # Create a dictionary and corpus\n",
        "    dictionary = corpora.Dictionary([text.split() for text in texts])\n",
        "    corpus = [dictionary.doc2bow(text.split()) for text in texts]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "    # Prepare the visualization\n",
        "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "    pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
        "\n",
        "    # Generate a summary of the topics\n",
        "    topics_summary = \"Topic Modeling Results:\\n\\n\"\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        topics_summary += f\"Topic {idx}: {topic}\\n\"\n",
        "\n",
        "    topics_summary += \"\\nAn interactive visualization of the topics has been saved as 'lda_visualization.html'.\"\n",
        "\n",
        "    return topics_summary"
      ],
      "metadata": {
        "id": "_JyK354IHSkh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the agent's tools\n",
        "def define_tools(vector_store: FAISS, scraped_data: List[Dict[str, Any]]) -> List[Tool]:\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"Search\",\n",
        "            func=lambda q: vector_store.similarity_search(q, k=3),\n",
        "            description=\"Useful for searching information in the knowledge base\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Summarize\",\n",
        "            func=lambda q: llm.predict(f\"Summarize the following text:\\n{q}\"),\n",
        "            description=\"Useful for summarizing long pieces of text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Analyze CVE Severity\",\n",
        "            func=analyze_cve_severity,\n",
        "            description=\"Analyzes the severity of a CVE based on its description\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Extract IOCs\",\n",
        "            func=extract_iocs,\n",
        "            description=\"Extracts potential Indicators of Compromise (IOCs) from text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Trend Analysis\",\n",
        "            func=lambda timeframe: trend_analysis(scraped_data, timeframe),\n",
        "            description=\"Analyzes cybersecurity trends over a given timeframe (week, month, or 3months)\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Sentiment Analysis\",\n",
        "            func=sentiment_analysis,\n",
        "            description=\"Analyzes the sentiment of a given text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Topic Modeling\",\n",
        "            func=lambda texts: topic_modeling(texts, num_topics=5),\n",
        "            description=\"Performs topic modeling on a collection of texts\"\n",
        "        )\n",
        "    ]"
      ],
      "metadata": {
        "id": "PtAMTD4DHSmp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agent types\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict[str, str]], \"The messages in the conversation\"]\n",
        "    current_agent: Annotated[str, \"The current agent processing the message\"]\n",
        "    scratchpad: Annotated[List[Dict[str, str]], \"The agent's scratchpad\"]\n",
        "\n",
        "# Define agent nodes\n",
        "def create_agent_node(role: str, system_message: str):\n",
        "    def agent_function(state: AgentState, tools: List[Tool]):\n",
        "        messages = state['messages']\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_message),\n",
        "            (\"human\", \"{input}\"),\n",
        "            (\"human\", \"Thought: {agent_scratchpad}\")\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt)\n",
        "        result = chain.run(input=messages[-1]['content'], agent_scratchpad=state['scratchpad'])\n",
        "        return {**state, \"messages\": messages + [{\"role\": \"assistant\", \"content\": result}]}\n",
        "    return agent_function\n",
        "\n",
        "researcher_agent = create_agent_node(\n",
        "    \"researcher\",\n",
        "    \"You are a cybersecurity researcher. Your role is to gather and analyze information using the provided tools.\"\n",
        ")\n",
        "\n",
        "analyst_agent = create_agent_node(\n",
        "    \"analyst\",\n",
        "    \"You are a cybersecurity analyst. Your role is to interpret data and provide insights based on the information gathered.\"\n",
        ")\n",
        "\n",
        "advisor_agent = create_agent_node(\n",
        "    \"advisor\",\n",
        "    \"You are a cybersecurity advisor. Your role is to provide recommendations and action plans based on the analysis.\"\n",
        ")\n",
        "\n",
        "threat_hunter_agent = create_agent_node(\n",
        "    \"threat_hunter\",\n",
        "    \"You are a threat hunter. Your role is to proactively search for hidden threats and advanced persistent threats (APTs) in the data. Use IOC extraction and analysis tools to identify potential compromises.\"\n",
        ")\n",
        "\n",
        "incident_responder_agent = create_agent_node(\n",
        "    \"incident_responder\",\n",
        "    \"You are an incident responder. Your role is to analyze potential security incidents, provide immediate mitigation steps, and develop longer-term remediation plans.\"\n",
        ")\n",
        "\n",
        "# Define the agent selection function\n",
        "def select_next_agent(state: AgentState):\n",
        "    last_message = state['messages'][-1]['content'].lower()\n",
        "    if \"cve\" in last_message or \"vulnerability\" in last_message:\n",
        "        return \"analyst\"\n",
        "    elif \"recommend\" in last_message or \"mitigat\" in last_message:\n",
        "        return \"advisor\"\n",
        "    elif \"incident\" in last_message or \"attack\" in last_message:\n",
        "        return \"incident_responder\"\n",
        "    elif \"ransomware\" in last_message or \"threat\" in last_message:\n",
        "        return \"threat_hunter\"\n",
        "    else:\n",
        "        return \"researcher\""
      ],
      "metadata": {
        "id": "31lZdXA4HSo9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the multi-agent system\n",
        "def create_multi_agent_system(tools: List[Tool]):\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add agent nodes\n",
        "    workflow.add_node(\"researcher\", researcher_agent)\n",
        "    workflow.add_node(\"analyst\", analyst_agent)\n",
        "    workflow.add_node(\"advisor\", advisor_agent)\n",
        "    workflow.add_node(\"threat_hunter\", threat_hunter_agent)\n",
        "    workflow.add_node(\"incident_responder\", incident_responder_agent)\n",
        "\n",
        "    # Add edges\n",
        "    for node in [\"researcher\", \"analyst\", \"advisor\", \"threat_hunter\", \"incident_responder\"]:\n",
        "        workflow.add_edge(node, select_next_agent)\n",
        "\n",
        "    # Set the entrypoint\n",
        "    workflow.set_entry_point(\"researcher\")\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "# Enhanced main function\n",
        "def main(scraped_data: List[Dict[str, Any]]):\n",
        "    try:\n",
        "        processed_texts = process_scraped_data(scraped_data)\n",
        "        vector_store = create_vector_store(processed_texts)\n",
        "        tools = define_tools(vector_store, scraped_data)\n",
        "        multi_agent_system = create_multi_agent_system(tools)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        queries = [\n",
        "            \"Assess the vulnerability CVE-2024-12345 in Windows Server.\",\n",
        "            \"Provide a security recommendation for mitigating phishing attacks.\",\n",
        "            \"List all details on BFSI security incidents in India.\",\n",
        "            \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "            \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "            \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            with get_openai_callback() as cb:\n",
        "                initial_state = AgentState(\n",
        "                    messages=[{\"role\": \"human\", \"content\": query}],\n",
        "                    current_agent=\"researcher\",\n",
        "                    scratchpad=[]\n",
        "                )\n",
        "                final_state = multi_agent_system.invoke(initial_state)\n",
        "\n",
        "                # Process and display the final response\n",
        "                final_response = final_state['messages'][-1]['content']\n",
        "                print(f\"Response: {final_response}\")\n",
        "\n",
        "                # Update knowledge graph based on the response\n",
        "                update_knowledge_graph([{\"source\": \"User Query\", \"title\": query}])\n",
        "\n",
        "                logger.info(f\"Tokens used: {cb.total_tokens}\")\n",
        "                logger.info(f\"Cost of query: ${cb.total_cost:.4f}\")\n",
        "\n",
        "        # Generate final report\n",
        "        generate_final_report(scraped_data, processed_texts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "u3BNMjIUHSrk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load your scraped data here\n",
        "        with open('scraped_data.json', 'r') as f:\n",
        "            scraped_data = json.load(f)\n",
        "        main(scraped_data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"scraped_data.json file not found. Please ensure the file exists in the current directory.\")\n",
        "    except json.JSONDecodeError:\n",
        "        logger.error(\"Error decoding JSON from scraped_data.json. Please ensure the file contains valid JSON.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "EGWogoz9HSu8",
        "outputId": "8cddfa45-ffab-497f-a5cb-8213af173dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:scraped_data.json file not found. Please ensure the file exists in the current directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZwdAe6kGtmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfEdjs9_8QDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f17Lg6l58QHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}