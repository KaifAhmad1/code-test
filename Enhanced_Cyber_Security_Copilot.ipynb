{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "15rtGXUXXidW"
      },
      "outputs": [],
      "source": [
        "!pip install -q apify-client langchain langchain-community langchain-groq transformers duckduckgo-search firecrawl-py\n",
        "!pip install -q sentence-transformers requests beautifulsoup4 ratelimit pyLDAvis faiss-cpu crewai crewai_tools exa exa_py langchain-exa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from exa_py import Exa\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import BaseTool, BrowserbaseLoadTool, EXASearchTool, FirecrawlScrapeWebsiteTool\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants and API Keys\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "EXA_API_KEY = \"d0f01fc2-c757-4c06-83f3-e2fd21361bab\"\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = \"fc-1b759fd6d7cd4784bc8f8819060228cd\"\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define Tools and Tasks**\n",
        "Define functions for real-time data fetching, Twitter scraping, news fetching, CVE data fetching, Exa.ai integration, and advanced data analysis.\n"
      ],
      "metadata": {
        "id": "RKATj2bwCXT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tools and Tasks\n",
        "class WebScraper:\n",
        "    def run(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
        "        results = []\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(self._scrape_url, url) for url in urls]\n",
        "            for future in as_completed(futures):\n",
        "                results.append(future.result())\n",
        "        return results\n",
        "\n",
        "    def _scrape_url(self, url: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "class TwitterScraper:\n",
        "    def run(self, query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "        actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "        try:\n",
        "            run = apify_client.actor(\"apify/twitter-scraper\").call(run_input=actor_input)\n",
        "            dataset_id = run[\"defaultDatasetId\"]\n",
        "            items = apify_client.dataset(dataset_id).list_items().items\n",
        "            return items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class NewsFetcher:\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        url = \"https://newsapi.org/v2/everything\"\n",
        "        params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "        try:\n",
        "            response = session.get(url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get(\"articles\", [])\n",
        "            return articles\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVEDataFetcher:\n",
        "    def run(self) -> List[Dict[str, Any]]:\n",
        "        url = \"https://cve.circl.lu/api/last\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            cve_items = response.json()\n",
        "            return cve_items\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class ExaResearcher:\n",
        "    def run(self, query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            exa_client = Exa(api_key=EXA_API_KEY)\n",
        "            results = exa_client.search(query, max_results=max_results)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Exa.ai research: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class CVESeverityAnalyzer:\n",
        "    def run(self, cve_description: str) -> str:\n",
        "        severity_keywords = [\"critical\", \"high\", \"medium\", \"low\"]\n",
        "        severity = \"unknown\"\n",
        "        for keyword in severity_keywords:\n",
        "            if keyword in cve_description.lower():\n",
        "                severity = keyword\n",
        "                break\n",
        "        return f\"The CVE severity is {severity}.\"\n",
        "\n",
        "class IOCExtractor:\n",
        "    def run(self, text: str) -> List[str]:\n",
        "        iocs = []\n",
        "        ip_pattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
        "        domain_pattern = r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
        "\n",
        "        iocs.extend(re.findall(ip_pattern, text))\n",
        "        iocs.extend(re.findall(domain_pattern, text))\n",
        "        return list(set(iocs))\n",
        "\n",
        "class TrendAnalyzer:\n",
        "    def run(self, data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "        keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\"]\n",
        "        keyword_counts = {keyword: 0 for keyword in keywords}\n",
        "\n",
        "        for item in data:\n",
        "            text = item.get('text', '').lower()\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    keyword_counts[keyword] += 1\n",
        "\n",
        "        trending_topics = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "        return f\"Trend analysis for the timeframe {timeframe} shows increasing threats in: \" + \", \".join([f\"{topic} ({count} mentions)\" for topic, count in trending_topics])\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def run(self, text: str) -> str:\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        if sentiment > 0.1:\n",
        "            return \"Positive sentiment\"\n",
        "        elif sentiment < -0.1:\n",
        "            return \"Negative sentiment\"\n",
        "        else:\n",
        "            return \"Neutral sentiment\""
      ],
      "metadata": {
        "id": "vrGfZox7B6Tl"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Exa Toools**"
      ],
      "metadata": {
        "id": "oYGxEYL1Fk34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Exa Tools\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "@tool\n",
        "def search(query: str):\n",
        "    \"\"\"Search for a webpage based on the query.\"\"\"\n",
        "    return exa.search(f\"{query}\", use_autoprompt=True, num_results=5)\n",
        "\n",
        "@tool\n",
        "def find_similar(url: str):\n",
        "    \"\"\"Search for webpages similar to a given URL.\n",
        "    The url passed in should be a URL returned from `search`.\n",
        "    \"\"\"\n",
        "    return exa.find_similar(url, num_results=5)\n",
        "\n",
        "@tool\n",
        "def get_contents(ids: list[str]):\n",
        "    \"\"\"Get the contents of a webpage.\n",
        "    The ids passed in should be a list of ids returned from `search`.\n",
        "    \"\"\"\n",
        "    return exa.get_contents(ids)\n",
        "\n",
        "tools = [search, get_contents, find_similar]"
      ],
      "metadata": {
        "id": "Y9IKtgzgFlS9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Implement Data Collection**\n",
        "Define functions to collect and curate data from various sources.\n"
      ],
      "metadata": {
        "id": "MLIiuXx7D2dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection and Curation Functions\n",
        "def collect_data():\n",
        "    websites = [\n",
        "        \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "        \"https://attack.mitre.org/\",\n",
        "        \"https://www.darkreading.com/\",\n",
        "        \"https://threatpost.com/\",\n",
        "        \"https://krebsonsecurity.com/\",\n",
        "        \"https://www.bleepingcomputer.com/\",\n",
        "        \"https://www.zdnet.com/topic/security/\",\n",
        "        \"https://www.securityweek.com/\",\n",
        "        \"https://www.sans.org/newsletters/newsbites/\",\n",
        "        \"https://www.cyberscoop.com/\",\n",
        "        \"https://www.csoonline.com/\",\n",
        "        \"https://www.infosecurity-magazine.com/\",\n",
        "        \"https://www.wired.com/category/security/\",\n",
        "        \"https://www.schneier.com/\",\n",
        "        \"https://www.theregister.com/security/\",\n",
        "        \"https://thehackernews.com/\",\n",
        "        \"https://www.cyberdefensemagazine.com/\",\n",
        "        \"https://www.fireeye.com/blog.html\",\n",
        "        \"https://unit42.paloaltonetworks.com/\",\n",
        "        \"https://www.microsoft.com/security/blog/\",\n",
        "        \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "        \"https://nakedsecurity.sophos.com/\",\n",
        "        \"https://www.recordedfuture.com/blog/\",\n",
        "        \"https://www.cybersecurity-insiders.com/\",\n",
        "        \"https://www.malwarebytes.com/blog/\"\n",
        "    ]\n",
        "\n",
        "    scraped_data = WebScraper().run(websites)\n",
        "    tweets = TwitterScraper().run(\"cybersecurity\")\n",
        "    news = NewsFetcher().run(\"cybersecurity\")\n",
        "    cve_data = CVEDataFetcher().run()\n",
        "    exa_research = ExaResearcher().run(\"cybersecurity\")\n",
        "\n",
        "    return {\n",
        "        \"scraped_data\": scraped_data,\n",
        "        \"tweets\": tweets,\n",
        "        \"news\": news,\n",
        "        \"cve_data\": cve_data,\n",
        "        \"exa_research\": exa_research\n",
        "    }\n",
        "\n",
        "def curate_data(data):\n",
        "    curated_data = []\n",
        "\n",
        "    for page in data[\"scraped_data\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in data[\"tweets\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in data[\"news\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in data[\"cve_data\"]:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for research in data[\"exa_research\"]:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Exa.ai\",\n",
        "            \"title\": research.get(\"title\"),\n",
        "            \"abstract\": research.get(\"abstract\"),\n",
        "            \"url\": research.get(\"url\"),\n",
        "            \"timestamp\": research.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    return curated_data\n",
        "\n",
        "raw_data = collect_data()\n",
        "curated_data = curate_data(raw_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUvzrzXSqsl",
        "outputId": "4ce6701b-31b3-4521-ae36-a1e20d439c65"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error fetching tweets: Actor with this name was not found\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity&language=en&pageSize=50&sortBy=publishedAt\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\")': /api/last\n",
            "ERROR:__main__:Error fetching CVE data: HTTPSConnectionPool(host='cve.circl.lu', port=443): Max retries exceeded with url: /api/last (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=30)\"))\n",
            "ERROR:__main__:Error fetching Exa.ai research: Exa.search() got an unexpected keyword argument 'max_results'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stroing Data in Vector DB**"
      ],
      "metadata": {
        "id": "ESI2NSr4TLQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_in_vector_db(curated_data):\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "    documents = [Document(page_content=item[\"text\"], metadata=item) for item in curated_data]\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(\"vector_store\")\n",
        "\n",
        "def load_vector_store():\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"BAAI/bge-small-en\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    return FAISS.load_local(\"vector_store\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "store_in_vector_db(curated_data)\n",
        "vector_store = load_vector_store()"
      ],
      "metadata": {
        "id": "3N1bluNlSqvW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Initialize LLMs, Agents and their Tasks**\n",
        "Define the agents with specific roles and goals, and assign the necessary tools."
      ],
      "metadata": {
        "id": "61vqRkBtDHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLMs and Agents\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")\n",
        "\n",
        "def create_agents() -> Dict[str, Agent]:\n",
        "    tools = [\n",
        "        {\"name\": \"WebScraper\", \"tool\": WebScraper()},\n",
        "        {\"name\": \"TwitterScraper\", \"tool\": TwitterScraper()},\n",
        "        {\"name\": \"NewsFetcher\", \"tool\": NewsFetcher()},\n",
        "        {\"name\": \"CVEDataFetcher\", \"tool\": CVEDataFetcher()},\n",
        "        {\"name\": \"ExaResearcher\", \"tool\": ExaResearcher()},\n",
        "        {\"name\": \"CVESeverityAnalyzer\", \"tool\": CVESeverityAnalyzer()},\n",
        "        {\"name\": \"IOCExtractor\", \"tool\": IOCExtractor()},\n",
        "        {\"name\": \"TrendAnalyzer\", \"tool\": TrendAnalyzer()},\n",
        "        {\"name\": \"SentimentAnalyzer\", \"tool\": SentimentAnalyzer()},\n",
        "        {\"name\": \"DuckDuckGoSearchRun\", \"tool\": DuckDuckGoSearchRun()},\n",
        "        {\"name\": \"EXASearchTool\", \"tool\": EXASearchTool()},\n",
        "        {\"name\": \"FirecrawlScrapeWebsiteTool\", \"tool\": FirecrawlScrapeWebsiteTool()},\n",
        "        {\"name\": \"search\", \"tool\": search},\n",
        "        {\"name\": \"get_contents\", \"tool\": get_contents},\n",
        "        {\"name\": \"find_similar\", \"tool\": find_similar}\n",
        "    ]\n",
        "\n",
        "    researcher = Agent(\n",
        "        role=\"Cybersecurity Researcher\",\n",
        "        goal=\"Gather and provide relevant cybersecurity information\",\n",
        "        backstory=\"You are an expert cybersecurity researcher with years of experience in threat intelligence.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    analyst = Agent(\n",
        "        role=\"Threat Analyst\",\n",
        "        goal=\"Analyze cybersecurity data and provide insights\",\n",
        "        backstory=\"You are a skilled threat analyst specializing in identifying and assessing cyber threats.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    advisor = Agent(\n",
        "        role=\"Security Advisor\",\n",
        "        goal=\"Provide recommendations based on cybersecurity analysis\",\n",
        "        backstory=\"You are a seasoned security advisor with a track record of helping organizations improve their security posture.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    threat_hunter = Agent(\n",
        "        role=\"Threat Hunter\",\n",
        "        goal=\"Proactively search for hidden threats and IOCs\",\n",
        "        backstory=\"You are an experienced threat hunter known for uncovering sophisticated cyber threats.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    incident_responder = Agent(\n",
        "        role=\"Incident Responder\",\n",
        "        goal=\"Provide guidance on handling cybersecurity incidents\",\n",
        "        backstory=\"You are a quick-thinking incident responder with expertise in containing and mitigating cyber attacks.\",\n",
        "        tools=tools,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"researcher\": researcher,\n",
        "        \"analyst\": analyst,\n",
        "        \"advisor\": advisor,\n",
        "        \"threat_hunter\": threat_hunter,\n",
        "        \"incident_responder\": incident_responder\n",
        "    }\n",
        "\n",
        "# Define tasks\n",
        "def create_tasks(agents: Dict[str, Agent], query: str) -> List[Task]:\n",
        "    researcher_task = Task(\n",
        "        description=f\"Research the latest information related to: {query}\",\n",
        "        agent=agents[\"researcher\"],\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    analyst_task = Task(\n",
        "        description=f\"Analyze the gathered information and identify key insights related to: {query}\",\n",
        "        agent=agents[\"analyst\"],\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    advisor_task = Task(\n",
        "        description=f\"Provide security recommendations based on the analysis of: {query}\",\n",
        "        agent=agents[\"advisor\"],\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    threat_hunter_task = Task(\n",
        "        description=f\"Hunt for potential threats and IOCs related to: {query}\",\n",
        "        agent=agents[\"threat_hunter\"],\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    incident_responder_task = Task(\n",
        "        description=f\"Suggest incident response steps if {query} is a potential security incident\",\n",
        "        agent=agents[\"incident_responder\"],\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    return [researcher_task, analyst_task, advisor_task, threat_hunter_task, incident_responder_task]"
      ],
      "metadata": {
        "id": "YdWZKkJ6B6Wr"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Form the Crew**"
      ],
      "metadata": {
        "id": "PDI9GlJ4T6Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Form the Crew\n",
        "# Create crew\n",
        "def create_crew(agents: Dict[str, Agent]) -> Crew:\n",
        "    return Crew(\n",
        "        agents=list(agents.values()),\n",
        "        tasks=[],\n",
        "        process=Process.sequential\n",
        "    )"
      ],
      "metadata": {
        "id": "e58-epqc1_f9"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Define prompt templates**"
      ],
      "metadata": {
        "id": "2xuWndBhUEi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "detailed_prompt_templates = {\n",
        "    \"vulnerability_assessment\": ChatPromptTemplate.from_template(\n",
        "        \"Assess the vulnerability {cve_id} in {system}. Provide detailed information including potential impacts and mitigation steps.\"\n",
        "    ),\n",
        "    \"security_recommendation\": ChatPromptTemplate.from_template(\n",
        "        \"Provide a security recommendation for mitigating {threat}. Include preventive measures and best practices.\"\n",
        "    ),\n",
        "    \"incident_details\": ChatPromptTemplate.from_template(\n",
        "        \"List all details on {sector} security incidents in {location}.\"\n",
        "    ),\n",
        "    \"ransomware_attacks\": ChatPromptTemplate.from_template(\n",
        "        \"List all ransomware attacks targeting the {industry} industry in the last {timeframe}.\"\n",
        "    ),\n",
        "    \"recent_incidents\": ChatPromptTemplate.from_template(\n",
        "        \"Provide recent incidents related to {ransomware_gang} Ransomware.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def process_query_with_templates(query_type: str, input_variables: Dict[str, str], agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    prompt_template = detailed_prompt_templates.get(query_type)\n",
        "    if not prompt_template:\n",
        "        return \"Invalid query type.\"\n",
        "\n",
        "    query = prompt_template.format(**input_variables)\n",
        "    return process_query(query, agents, max_steps)\n",
        "\n",
        "def process_query(query: str, agents: Dict[str, Agent], max_steps: int = 5) -> str:\n",
        "    current_agent_name = \"researcher\"\n",
        "    responses = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        current_agent = agents[current_agent_name]\n",
        "        task = Task(description=query, agent=current_agent)\n",
        "        response = current_agent.execute_task(task)\n",
        "        responses.append(f\"{current_agent_name.capitalize()}: {response}\")\n",
        "\n",
        "        # Simple logic to select next agent based on keywords in the response\n",
        "        if \"analyze\" in response.lower():\n",
        "            next_agent_name = \"analyst\"\n",
        "        elif \"recommend\" in response.lower():\n",
        "            next_agent_name = \"advisor\"\n",
        "        elif \"threat\" in response.lower():\n",
        "            next_agent_name = \"threat_hunter\"\n",
        "        elif \"incident\" in response.lower():\n",
        "            next_agent_name = \"incident_responder\"\n",
        "        else:\n",
        "            next_agent_name = \"researcher\"\n",
        "\n",
        "        if next_agent_name == current_agent_name or step == max_steps - 1:\n",
        "            break\n",
        "        current_agent_name = next_agent_name\n",
        "\n",
        "    return \"\\n\\n\".join(responses)"
      ],
      "metadata": {
        "id": "keGQulgYUD2J"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Process queries**"
      ],
      "metadata": {
        "id": "bqbN9BuXUSNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process queries\n",
        "queries = [\n",
        "    \"Assess the vulnerability CVE-2024-12345 in Windows Server.\",\n",
        "    \"Provide a security recommendation for mitigating phishing attacks.\",\n",
        "    \"List all details on BFSI security incidents in India.\",\n",
        "    \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "    \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "    \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "]\n",
        "\n",
        "# Create agents and crew\n",
        "agents = create_agents()\n",
        "crew = create_crew(agents)\n",
        "\n",
        "# Parse and process queries\n",
        "for query in queries:\n",
        "    if \"Assess the vulnerability\" in query:\n",
        "        query_type = \"vulnerability_assessment\"\n",
        "        input_variables = {\"cve_id\": query.split(\" \")[4], \"system\": query.split(\" \")[6].rstrip(\".\")}\n",
        "    elif \"Provide a security recommendation\" in query:\n",
        "        query_type = \"security_recommendation\"\n",
        "        input_variables = {\"threat\": query.split(\" \")[5]}\n",
        "    elif \"List all details on\" in query:\n",
        "        query_type = \"incident_details\"\n",
        "        input_variables = {\"sector\": query.split(\" \")[5], \"location\": query.split(\" \")[8]}\n",
        "    elif \"List all ransomware attacks\" in query:\n",
        "        query_type = \"ransomware_attacks\"\n",
        "        input_variables = {\"industry\": query.split(\" \")[6], \"timeframe\": query.split(\" \")[10]}\n",
        "    elif \"Provide recent incidents related to\" in query:\n",
        "        query_type = \"recent_incidents\"\n",
        "        input_variables = {\"ransomware_gang\": query.split(\" \")[5]}\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    logger.info(f\"Processing query: {query_type} with inputs: {input_variables}\")\n",
        "    result = process_query_with_templates(query_type, input_variables, agents)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"Response:\\n{result}\")"
      ],
      "metadata": {
        "id": "qf1ocoG61_jZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "884e542b-d060-4561-ca55-d956f79cb1a6"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for Agent\n  Value error, 15 validation errors for CrewAgentExecutor\ntools -> 0\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 1\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 2\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 3\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 4\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 5\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 6\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 7\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 8\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 9\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 10\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 11\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 12\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 13\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 14\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error) [type=value_error, input_value={'role': 'Cybersecurity R...ecretStr('**********'))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-1fdef3351cbc>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create agents and crew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcrew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_crew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-17a66610383f>\u001b[0m in \u001b[0;36mcreate_agents\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     researcher = Agent(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Cybersecurity Researcher\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gather and provide relevant cybersecurity information\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_ops_agent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/agent_builder/base_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmodel_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# The following line sets a flag that we use to determine when `__init__` gets overridden by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Agent\n  Value error, 15 validation errors for CrewAgentExecutor\ntools -> 0\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 1\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 2\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 3\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 4\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 5\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 6\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 7\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 8\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 9\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 10\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 11\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 12\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 13\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)\ntools -> 14\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error) [type=value_error, input_value={'role': 'Cybersecurity R...ecretStr('**********'))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbyonTFTbyv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}