{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "spYxHOdiLSLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "15rtGXUXXidW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ae9d5a-987f-4de7-bb4c-dd8c4d521e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q apify-client langchain langchain-community langchain-groq networkx pyvis spacy transformers pandas\n",
        "%pip install -q sentence-transformers requests beautifulsoup4 ratelimit langgraph pyLDAvis faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up Logging**"
      ],
      "metadata": {
        "id": "GaMr-G20LYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Annotated, TypedDict\n",
        "import logging\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from bs4 import BeautifulSoup\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from langchain.agents import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constants and API Keys**"
      ],
      "metadata": {
        "id": "iVlCvJI5LdC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "APIFY_API_KEY = \"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\"\n",
        "NEWS_API_KEY = os.getenv(\"c50f733b00e34575a7c203c38cd97391\")\n",
        "GROQ_API_KEY = \"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\"\n",
        "WEBSITES = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "    \"https://krebsonsecurity.com/\",\n",
        "    \"https://www.bleepingcomputer.com/\",\n",
        "    \"https://www.zdnet.com/topic/security/\",\n",
        "    \"https://www.securityweek.com/\",\n",
        "    \"https://www.sans.org/newsletters/newsbites/\",\n",
        "    \"https://www.cyberscoop.com/\",\n",
        "    \"https://www.csoonline.com/\",\n",
        "    \"https://www.infosecurity-magazine.com/\",\n",
        "    \"https://www.wired.com/category/security/\",\n",
        "    \"https://www.schneier.com/\",\n",
        "    \"https://www.theregister.com/security/\",\n",
        "    \"https://thehackernews.com/\",\n",
        "    \"https://www.cyberdefensemagazine.com/\",\n",
        "    \"https://www.fireeye.com/blog.html\",\n",
        "    \"https://unit42.paloaltonetworks.com/\",\n",
        "    \"https://www.microsoft.com/security/blog/\",\n",
        "    \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "    \"https://nakedsecurity.sophos.com/\",\n",
        "    \"https://www.recordedfuture.com/blog/\",\n",
        "    \"https://www.cybersecurity-insiders.com/\",\n",
        "    \"https://www.malwarebytes.com/blog/\",\n",
        "]\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts.xml\",\n",
        "    \"https://krebsonsecurity.com/feed/\",\n",
        "    \"https://threatpost.com/feed/\",\n",
        "    \"https://www.darkreading.com/rss_simple.asp\"\n",
        "]"
      ],
      "metadata": {
        "id": "DBpAjqmvihA8",
        "outputId": "aa3f23b9-03f2-45dd-b38f-4c698dcdfc27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Apify Client and Configure Requests Session**"
      ],
      "metadata": {
        "id": "9KdndjFALx_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(APIFY_API_KEY)\n",
        "\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "RRi1CmpSYlfI",
        "outputId": "46d6d13e-85c6-450b-d323-382c85a46a71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rate-Limited GET Request**"
      ],
      "metadata": {
        "id": "X3TDutU_L6Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rate-limited GET request\n",
        "@sleep_and_retry\n",
        "@limits(calls=15, period=1)  # 5 calls per second\n",
        "def rate_limited_get(url: str, **kwargs) -> requests.Response:\n",
        "    return session.get(url, timeout=10, **kwargs)"
      ],
      "metadata": {
        "id": "Go2qxs8tYmib",
        "outputId": "5d5e5ef9-bc3a-4fb2-b1de-ae43c5c63694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Website Scraping Functions and Fetch Data Functions**"
      ],
      "metadata": {
        "id": "856H_kbyMDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_website_with_apify(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Scrape a website using Apify.\"\"\"\n",
        "    logger.info(f\"Scraping {url} with Apify...\")\n",
        "    try:\n",
        "        actor_input = {\"url\": url, \"proxyConfiguration\": {\"useApifyProxy\": True}}\n",
        "        run = apify_client.actor(\"apify/website-content-crawler\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        if items:\n",
        "            return {\"url\": url, \"text\": items[0].get(\"text\", \"\"), \"timestamp\": datetime.now().isoformat()}\n",
        "        else:\n",
        "            return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": \"No content found\"}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url} with Apify: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "def scrape_website(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Scrape a website using BeautifulSoup.\"\"\"\n",
        "    try:\n",
        "        response = rate_limited_get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "def scrape_websites(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Scrape multiple websites concurrently.\"\"\"\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_url = {executor.submit(scrape_website, url): url for url in urls}\n",
        "        results = [future.result() for future in as_completed(future_to_url)]\n",
        "    logger.info(f\"Successfully scraped {len(results)} pages.\")\n",
        "    return results\n",
        "\n",
        "def fetch_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch tweets using Apify's Twitter scraper.\"\"\"\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\"searchTerms\": [query], \"maxTweets\": max_tweets, \"languageCode\": \"en\"}\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch news articles using NewsAPI.\"\"\"\n",
        "    logger.info(f\"Fetching news for query: {query}\")\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\"q\": query, \"language\": \"en\", \"pageSize\": max_results, \"apiKey\": NEWS_API_KEY, \"sortBy\": \"publishedAt\"}\n",
        "    try:\n",
        "        response = rate_limited_get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "        logger.info(f\"Fetched {len(articles)} news articles.\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching news: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_cve_data() -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch CVE data from CIRCL API.\"\"\"\n",
        "    logger.info(\"Fetching CVE data\")\n",
        "    url = \"https://cve.circl.lu/api/last\"\n",
        "    try:\n",
        "        response = rate_limited_get(url)\n",
        "        response.raise_for_status()\n",
        "        cve_items = response.json()\n",
        "        logger.info(f\"Fetched {len(cve_items)} CVE items.\")\n",
        "        return cve_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_rss_feeds(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch RSS feeds using Apify's RSS scraper.\"\"\"\n",
        "    logger.info(f\"Fetching RSS feeds from {len(urls)} URLs\")\n",
        "    run_input = {\"startUrls\": urls, \"maxItems\": 50}\n",
        "    try:\n",
        "        run = apify_client.actor(\"jupri/rss-xml-scraper\").call(run_input=run_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} RSS feed items.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feeds: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Kn1O2gnk8P3R",
        "outputId": "fcdff4fc-f79f-4484-8144-0fbbf0186097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curate Data Function**"
      ],
      "metadata": {
        "id": "Y1IOdXQ7MNA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def curate_data(website_data, tweets, news, cve_data, rss_feeds):\n",
        "    \"\"\"Curate data from various sources.\"\"\"\n",
        "    curated_data = []\n",
        "\n",
        "    for page in website_data:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Website\",\n",
        "            \"url\": page.get(\"url\"),\n",
        "            \"text\": page.get(\"text\"),\n",
        "            \"timestamp\": page.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for tweet in tweets:\n",
        "        curated_data.append({\n",
        "            \"source\": \"Twitter\",\n",
        "            \"text\": tweet.get(\"text\"),\n",
        "            \"user\": tweet.get(\"user\"),\n",
        "            \"timestamp\": tweet.get(\"timestamp\")\n",
        "        })\n",
        "\n",
        "    for article in news:\n",
        "        curated_data.append({\n",
        "            \"source\": \"News\",\n",
        "            \"url\": article.get(\"url\"),\n",
        "            \"title\": article.get(\"title\"),\n",
        "            \"description\": article.get(\"description\"),\n",
        "            \"timestamp\": article.get(\"publishedAt\")\n",
        "        })\n",
        "\n",
        "    for cve in cve_data:\n",
        "        cve_meta = cve.get(\"cve\", {}).get(\"CVE_data_meta\", {})\n",
        "        description_data = cve.get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])\n",
        "        curated_data.append({\n",
        "            \"source\": \"CVE\",\n",
        "            \"cve_id\": cve_meta.get(\"ID\"),\n",
        "            \"description\": description_data[0].get(\"value\"),\n",
        "            \"timestamp\": cve.get(\"publishedDate\")\n",
        "        })\n",
        "\n",
        "    for feed in rss_feeds:\n",
        "        curated_data.append({\n",
        "            \"source\": \"RSS\",\n",
        "            \"url\": feed.get(\"link\"),\n",
        "            \"title\": feed.get(\"title\"),\n",
        "            \"description\": feed.get(\"description\"),\n",
        "            \"timestamp\": feed.get(\"pubDate\")\n",
        "        })\n",
        "\n",
        "    return curated_data"
      ],
      "metadata": {
        "id": "kBQ_qsgy8P5z",
        "outputId": "de56288f-4fce-4014-f66f-f4e1331c7ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Scraped Data**"
      ],
      "metadata": {
        "id": "XvV7jCfHMcnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_item(item: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Preprocess a single data item.\"\"\"\n",
        "    processed_item = {\n",
        "        \"source\": item[\"source\"],\n",
        "        \"content\": \"\",\n",
        "        \"timestamp\": item.get(\"timestamp\", \"\"),\n",
        "        \"keywords\": [],\n",
        "        \"sentiment\": 0\n",
        "    }\n",
        "\n",
        "    if item[\"source\"] == \"Website\":\n",
        "        processed_item[\"content\"] = item.get(\"text\", \"\")[:500]  # Truncate to first 500 characters\n",
        "    elif item[\"source\"] == \"Twitter\":\n",
        "        processed_item[\"content\"] = item.get(\"text\", \"\")\n",
        "    elif item[\"source\"] in [\"News\", \"RSS\"]:\n",
        "        processed_item[\"content\"] = f\"{item.get('title', '')} - {item.get('description', '')}\"\n",
        "    elif item[\"source\"] == \"CVE\":\n",
        "        processed_item[\"content\"] = f\"{item.get('cve_id', '')} - {item.get('description', '')}\"\n",
        "\n",
        "    processed_item[\"keywords\"] = extract_keywords(processed_item[\"content\"])\n",
        "    processed_item[\"sentiment\"] = perform_sentiment_analysis(processed_item[\"content\"])\n",
        "\n",
        "    return processed_item\n",
        "\n",
        "def extract_keywords(text: str, top_n: int = 5) -> List[str]:\n",
        "    \"\"\"Extract top keywords from text.\"\"\"\n",
        "    words = text.lower().split()\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if len(word) > 3:  # Ignore short words\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    return sorted(word_freq, key=word_freq.get, reverse=True)[:top_n]\n",
        "\n",
        "def perform_sentiment_analysis(text: str) -> float:\n",
        "    \"\"\"Perform sentiment analysis on text.\"\"\"\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "def process_curated_data(curated_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Process all curated data items.\"\"\"\n",
        "    return [preprocess_item(item) for item in curated_data]\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "def store_in_vector_db(processed_data: List[Dict[str, Any]], file_path: str = \"vector_store\") -> None:\n",
        "    \"\"\"Store processed data in a vector database.\"\"\"\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "    # Convert the processed data to Document objects\n",
        "    documents = [\n",
        "        Document(page_content=item[\"content\"], metadata=item)\n",
        "        for item in processed_data\n",
        "    ]\n",
        "\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    vector_store.save_local(file_path)\n",
        "\n",
        "def store_in_kg(processed_data: List[Dict[str, Any]], file_path: str = \"knowledge_graph.json\") -> nx.Graph:\n",
        "    \"\"\"Store processed data in a knowledge graph.\"\"\"\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for item in processed_data:\n",
        "        source = item[\"source\"]\n",
        "        content = item[\"content\"][:50]  # Use first 50 chars as node identifier\n",
        "        keywords = item[\"keywords\"]\n",
        "\n",
        "        graph.add_node(source, type=\"source\")\n",
        "        graph.add_node(content, type=\"content\", full_content=item[\"content\"], sentiment=item[\"sentiment\"])\n",
        "        graph.add_edge(source, content)\n",
        "\n",
        "        for keyword in keywords:\n",
        "            graph.add_node(keyword, type=\"keyword\")\n",
        "            graph.add_edge(content, keyword)\n",
        "\n",
        "    # Save the graph as JSON for easier inspection\n",
        "    data = nx.node_link_data(graph)\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "    return graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAjbWkvXCegh",
        "outputId": "2a2a7c88-c150-4f8f-b620-36309718cefe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Define queries for cybersecurity-related data\n",
        "    queries = [\n",
        "        \"cybersecurity\", \"BFSI security incidents\", \"ransomware attacks healthcare\",\n",
        "        \"Lockbit Ransomware\", \"BlackBasta Ransomware\", \"latest cybersecurity incidents\",\n",
        "        \"cyber threats\", \"data breaches\", \"malware\", \"phishing attacks\",\n",
        "        \"network security\", \"cloud security\", \"cyber defense\", \"cybercrime\",\n",
        "        \"information security\", \"vulnerability management\", \"threat intelligence\",\n",
        "        \"incident response\", \"security awareness\", \"cybersecurity trends\"\n",
        "    ]\n",
        "\n",
        "    # Gather data from various sources\n",
        "    website_data = scrape_websites(WEBSITES)\n",
        "\n",
        "    tweets = []\n",
        "    for query in queries:\n",
        "        tweets.extend(fetch_tweets(query))\n",
        "\n",
        "    news_articles = []\n",
        "    for query in queries:\n",
        "        news_articles.extend(fetch_news(query))\n",
        "\n",
        "    cve_data = fetch_cve_data()\n",
        "    rss_data = fetch_rss_feeds(RSS_FEEDS)\n",
        "\n",
        "    # Curate all collected data\n",
        "    curated_data = curate_data(website_data, tweets, news_articles, cve_data, rss_data)\n",
        "\n",
        "    # Process and analyze the curated data\n",
        "    processed_data = process_curated_data(curated_data)\n",
        "\n",
        "    # Store data in vector database and knowledge graph\n",
        "    store_in_vector_db(processed_data)\n",
        "    graph = store_in_kg(processed_data)\n",
        "\n",
        "    logger.info(f\"Processed {len(processed_data)} items.\")\n",
        "    logger.info(f\"Knowledge graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bp60w2HQHIwq",
        "outputId": "e95b7144-7085-49d3-ed4f-13ea92a83955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=BFSI+security+incidents&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=ransomware+attacks+healthcare&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=Lockbit+Ransomware&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=BlackBasta+Ransomware&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=latest+cybersecurity+incidents&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cyber+threats&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=data+breaches&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=malware&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=phishing+attacks&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=network+security&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cloud+security&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cyber+defense&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybercrime&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=information+security&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=vulnerability+management&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=threat+intelligence&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=incident+response&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=security+awareness&language=en&pageSize=50&sortBy=publishedAt\n",
            "ERROR:__main__:Error fetching news: 401 Client Error: Unauthorized for url: https://newsapi.org/v2/everything?q=cybersecurity+trends&language=en&pageSize=50&sortBy=publishedAt\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\")': /api/last\n",
            "ERROR:__main__:Error fetching CVE data: HTTPSConnectionPool(host='cve.circl.lu', port=443): Max retries exceeded with url: /api/last (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='cve.circl.lu', port=443): Read timed out. (read timeout=10)\"))\n",
            "/usr/local/lib/python3.10/dist-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  other = LooseVersion(other)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize HuggingFace Embeddings and LLM**"
      ],
      "metadata": {
        "id": "6H_z1sczMl5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")\n",
        "\n",
        "# Define system and human messages\n",
        "system_message = \"\"\"You are an expert cybersecurity analyst with extensive knowledge in threat analysis,\n",
        "vulnerability assessment, and security recommendations. Provide detailed, precise, and actionable insights.\n",
        "Always consider the latest threat intelligence and best practices in your analysis.\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"human\", \"{text}\")])"
      ],
      "metadata": {
        "id": "khJ82mt-lMpn",
        "outputId": "6a270a6f-6c46-4bed-cc4c-7dfd78bbd9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced KnowledgeGraph Class**"
      ],
      "metadata": {
        "id": "OeMQJ74uM6sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def add_node(self, node, **attrs):\n",
        "        self.graph.add_node(node, **attrs)\n",
        "\n",
        "    def add_edge(self, u, v, **attrs):\n",
        "        self.graph.add_edge(u, v, **attrs)\n",
        "\n",
        "    def visualize(self, output_file):\n",
        "        net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
        "        net.set_options(\"\"\"\n",
        "        var options = {\n",
        "          \"nodes\": {\n",
        "            \"shape\": \"dot\",\n",
        "            \"size\": 16,\n",
        "            \"font\": {\n",
        "              \"size\": 12\n",
        "            }\n",
        "          },\n",
        "          \"edges\": {\n",
        "            \"color\": {\n",
        "              \"inherit\": true\n",
        "            },\n",
        "            \"smooth\": {\n",
        "              \"type\": \"continuous\"\n",
        "            }\n",
        "          },\n",
        "          \"physics\": {\n",
        "            \"forceAtlas2Based\": {\n",
        "              \"gravitationalConstant\": -50,\n",
        "              \"centralGravity\": 0.01,\n",
        "              \"springLength\": 230,\n",
        "              \"springConstant\": 0.18\n",
        "            },\n",
        "            \"maxVelocity\": 50,\n",
        "            \"solver\": \"forceAtlas2Based\",\n",
        "            \"timestep\": 0.22,\n",
        "            \"stabilization\": {\n",
        "              \"iterations\": 150\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "        \"\"\")\n",
        "\n",
        "        for node in self.graph.nodes(data=True):\n",
        "            node_id = node[0]\n",
        "            node_attrs = node[1]\n",
        "            title = node_attrs.get('title', node_id)\n",
        "            color = node_attrs.get('color', '#00ff00')\n",
        "            shape = node_attrs.get('shape', 'dot')\n",
        "            size = node_attrs.get('size', 16)\n",
        "            net.add_node(node_id, label=title, color=color, shape=shape, size=size)\n",
        "\n",
        "        for edge in self.graph.edges(data=True):\n",
        "            source = edge[0]\n",
        "            target = edge[1]\n",
        "            edge_attrs = edge[2]\n",
        "            relation = edge_attrs.get('relation', '')\n",
        "            color = edge_attrs.get('color', '#ffffff')\n",
        "            net.add_edge(source, target, title=relation, color=color)\n",
        "\n",
        "        net.show(output_file)\n",
        "        logger.info(f\"Knowledge graph visualized at {output_file}\")\n",
        "\n",
        "# Initialize knowledge graph\n",
        "kg = KnowledgeGraph()\n",
        "# Visualize the graph\n",
        "kg.visualize(\"enhanced_knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "pKja0_jKGtbc",
        "outputId": "6b0525c9-8763-465b-c8cc-df7da06a9923",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "enhanced_knowledge_graph.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Store**"
      ],
      "metadata": {
        "id": "T9v_dy7QNDYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Cybersecurity Analysis Tools**"
      ],
      "metadata": {
        "id": "2NTyYzh1NG9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced cybersecurity analysis tools\n",
        "def analyze_cve_severity(cve_description: str) -> str:\n",
        "    severity_keywords = {\n",
        "        \"critical\": 10, \"high\": 7, \"medium\": 5, \"low\": 3,\n",
        "        \"remote code execution\": 9, \"privilege escalation\": 8,\n",
        "        \"denial of service\": 6, \"information disclosure\": 4\n",
        "    }\n",
        "\n",
        "    description_lower = cve_description.lower()\n",
        "    max_severity = max(score for keyword, score in severity_keywords.items() if keyword in description_lower)\n",
        "\n",
        "    if max_severity >= 9:\n",
        "        return f\"Critical (Score: {max_severity})\"\n",
        "    elif max_severity >= 7:\n",
        "        return f\"High (Score: {max_severity})\"\n",
        "    elif max_severity >= 4:\n",
        "        return f\"Medium (Score: {max_severity})\"\n",
        "    else:\n",
        "        return f\"Low (Score: {max_severity})\"\n",
        "\n",
        "def extract_iocs(text: str) -> Dict[str, List[str]]:\n",
        "    iocs = {\n",
        "        \"ip_addresses\": [],\n",
        "        \"domains\": [],\n",
        "        \"hashes\": [],\n",
        "        \"urls\": []\n",
        "    }\n",
        "\n",
        "    # Regular expressions for different IOC types\n",
        "    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n",
        "    domain_pattern = r'\\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,}\\b'\n",
        "    hash_pattern = r'\\b[a-fA-F0-9]{32,64}\\b'\n",
        "    url_pattern = r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
        "\n",
        "    iocs[\"ip_addresses\"] = re.findall(ip_pattern, text)\n",
        "    iocs[\"domains\"] = re.findall(domain_pattern, text)\n",
        "    iocs[\"hashes\"] = re.findall(hash_pattern, text)\n",
        "    iocs[\"urls\"] = re.findall(url_pattern, text)\n",
        "\n",
        "    return iocs\n",
        "\n",
        "def trend_analysis(data: List[Dict[str, Any]], timeframe: str) -> str:\n",
        "    keywords = [\"ransomware\", \"phishing\", \"data breach\", \"malware\", \"zero-day\", \"supply chain attack\", \"cloud security\", \"insider threat\"]\n",
        "    timeframe_days = {\"week\": 7, \"month\": 30, \"3months\": 90}\n",
        "\n",
        "    if timeframe not in timeframe_days:\n",
        "        return \"Invalid timeframe. Please use 'week', 'month', or '3months'.\"\n",
        "\n",
        "    cutoff_date = datetime.now() - timedelta(days=timeframe_days[timeframe])\n",
        "    recent_data = [item for item in data if datetime.fromisoformat(item['timestamp']) > cutoff_date]\n",
        "\n",
        "    keyword_counts = {keyword: sum(1 for item in recent_data if keyword in (item.get('text', '') + item.get('title', '') + item.get('description', '')).lower()) for keyword in keywords}\n",
        "\n",
        "    sorted_trends = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    trend_report = f\"Top cybersecurity trends in the last {timeframe}:\\n\"\n",
        "    trend_report += \"\\n\".join(f\"- {keyword.capitalize()}: {count} mentions\" for keyword, count in sorted_trends)\n",
        "\n",
        "    # Generate a bar plot of the trends\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=[item[0] for item in sorted_trends], y=[item[1] for item in sorted_trends])\n",
        "    plt.title(f\"Cybersecurity Trends - Last {timeframe}\")\n",
        "    plt.xlabel(\"Keywords\")\n",
        "    plt.ylabel(\"Mention Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"trend_analysis_{timeframe}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    trend_report += f\"\\n\\nA bar plot of the trends has been saved as 'trend_analysis_{timeframe}.png'.\"\n",
        "\n",
        "    return trend_report\n",
        "\n",
        "# New function: Sentiment analysis\n",
        "def sentiment_analysis(text: str) -> Dict[str, Any]:\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment\n",
        "    return {\n",
        "        \"polarity\": sentiment.polarity,\n",
        "        \"subjectivity\": sentiment.subjectivity,\n",
        "        \"sentiment\": \"positive\" if sentiment.polarity > 0 else \"negative\" if sentiment.polarity < 0 else \"neutral\"\n",
        "    }\n",
        "\n",
        "# New function: Topic modeling\n",
        "def topic_modeling(texts: List[str], num_topics: int = 5) -> str:\n",
        "    # Preprocess the texts\n",
        "    texts = [re.sub(r'\\s+', ' ', text.lower()) for text in texts]\n",
        "    texts = [re.sub(r'[^\\w\\s]', '', text) for text in texts]\n",
        "\n",
        "    # Create a dictionary and corpus\n",
        "    dictionary = corpora.Dictionary([text.split() for text in texts])\n",
        "    corpus = [dictionary.doc2bow(text.split()) for text in texts]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "    # Prepare the visualization\n",
        "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "    pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
        "\n",
        "    # Generate a summary of the topics\n",
        "    topics_summary = \"Topic Modeling Results:\\n\\n\"\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        topics_summary += f\"Topic {idx}: {topic}\\n\"\n",
        "\n",
        "    topics_summary += \"\\nAn interactive visualization of the topics has been saved as 'lda_visualization.html'.\"\n",
        "\n",
        "    return topics_summary"
      ],
      "metadata": {
        "id": "L_Ze8936jnpW",
        "outputId": "b9cb81a3-85ae-43c8-b226-c35dcc47ea15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tools**"
      ],
      "metadata": {
        "id": "HE-8jwgZNnZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the agent's tools\n",
        "def define_tools(vector_store: FAISS, scraped_data: List[Dict[str, Any]]) -> List[Tool]:\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"Search\",\n",
        "            func=lambda q: vector_store.similarity_search(q, k=3),\n",
        "            description=\"Useful for searching information in the knowledge base\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Summarize\",\n",
        "            func=lambda q: llm.predict(f\"Summarize the following text:\\n{q}\"),\n",
        "            description=\"Useful for summarizing long pieces of text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Analyze CVE Severity\",\n",
        "            func=analyze_cve_severity,\n",
        "            description=\"Analyzes the severity of a CVE based on its description\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Extract IOCs\",\n",
        "            func=extract_iocs,\n",
        "            description=\"Extracts potential Indicators of Compromise (IOCs) from text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Trend Analysis\",\n",
        "            func=lambda timeframe: trend_analysis(scraped_data, timeframe),\n",
        "            description=\"Analyzes cybersecurity trends over a given timeframe (week, month, or 3months)\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Sentiment Analysis\",\n",
        "            func=sentiment_analysis,\n",
        "            description=\"Analyzes the sentiment of a given text\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"Topic Modeling\",\n",
        "            func=lambda texts: topic_modeling(texts, num_topics=5),\n",
        "            description=\"Performs topic modeling on a collection of texts\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "# Test agent tools\n",
        "tools = define_tools(vector_store, scraped_data)\n",
        "logger.info(f\"Tools defined: {tools}\")"
      ],
      "metadata": {
        "id": "diiZSbMQjnsE",
        "outputId": "39f572fa-3fbb-4975-e47c-33781389911e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Agent Nodes and Multi-Agent System**"
      ],
      "metadata": {
        "id": "b7u49wc7NveX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agent types\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict[str, str]], \"The messages in the conversation\"]\n",
        "    current_agent: Annotated[str, \"The current agent processing the message\"]\n",
        "    scratchpad: Annotated[List[Dict[str, str]], \"The agent's scratchpad\"]\n",
        "\n",
        "# Define agent nodes\n",
        "def create_agent_node(role: str, system_message: str):\n",
        "    def agent_function(state: AgentState, tools: List[Tool]):\n",
        "        messages = state['messages']\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_message),\n",
        "            (\"human\", \"{input}\"),\n",
        "            (\"human\", \"Thought: {agent_scratchpad}\")\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt)\n",
        "        result = chain.run(input=messages[-1]['content'], agent_scratchpad=state['scratchpad'])\n",
        "        return {**state, \"messages\": messages + [{\"role\": \"assistant\", \"content\": result}]}\n",
        "    return agent_function\n",
        "\n",
        "researcher_agent = create_agent_node(\n",
        "    \"researcher\",\n",
        "    \"You are a cybersecurity researcher. Your role is to gather and analyze information using the provided tools.\"\n",
        ")\n",
        "\n",
        "analyst_agent = create_agent_node(\n",
        "    \"analyst\",\n",
        "    \"You are a cybersecurity analyst. Your role is to interpret data and provide insights based on the information gathered.\"\n",
        ")\n",
        "\n",
        "advisor_agent = create_agent_node(\n",
        "    \"advisor\",\n",
        "    \"You are a cybersecurity advisor. Your role is to provide recommendations and action plans based on the analysis.\"\n",
        ")\n",
        "\n",
        "threat_hunter_agent = create_agent_node(\n",
        "    \"threat_hunter\",\n",
        "    \"You are a threat hunter. Your role is to proactively search for hidden threats and advanced persistent threats (APTs) in the data. Use IOC extraction and analysis tools to identify potential compromises.\"\n",
        ")\n",
        "\n",
        "incident_responder_agent = create_agent_node(\n",
        "    \"incident_responder\",\n",
        "    \"You are an incident responder. Your role is to analyze potential security incidents, provide immediate mitigation steps, and develop longer-term remediation plans.\"\n",
        ")\n",
        "\n",
        "# Define the agent selection function\n",
        "def select_next_agent(state: AgentState):\n",
        "    last_message = state['messages'][-1]['content'].lower()\n",
        "    if \"cve\" in last_message or \"vulnerability\" in last_message:\n",
        "        return \"analyst\"\n",
        "    elif \"recommend\" in last_message or \"mitigat\" in last_message:\n",
        "        return \"advisor\"\n",
        "    elif \"incident\" in last_message or \"attack\" in last_message:\n",
        "        return \"incident_responder\"\n",
        "    elif \"ransomware\" in last_message or \"threat\" in last_message:\n",
        "        return \"threat_hunter\"\n",
        "    else:\n",
        "        return \"researcher\"\n",
        "\n",
        "# Create the multi-agent system\n",
        "def create_multi_agent_system(tools: List[Tool]):\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add agent nodes\n",
        "    workflow.add_node(\"researcher\", researcher_agent)\n",
        "    workflow.add_node(\"analyst\", analyst_agent)\n",
        "    workflow.add_node(\"advisor\", advisor_agent)\n",
        "    workflow.add_node(\"threat_hunter\", threat_hunter_agent)\n",
        "    workflow.add_node(\"incident_responder\", incident_responder_agent)\n",
        "\n",
        "    # Add edges\n",
        "    for node in [\"researcher\", \"analyst\", \"advisor\", \"threat_hunter\", \"incident_responder\"]:\n",
        "        workflow.add_edge(node, select_next_agent)\n",
        "\n",
        "    # Set the entrypoint\n",
        "    workflow.set_entry_point(\"researcher\")\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()\n",
        "\n",
        "# Test multi-agent system\n",
        "multi_agent_system = create_multi_agent_system(tools)\n",
        "logger.info(f\"Multi-agent system created\")"
      ],
      "metadata": {
        "id": "71Xa_JiYnfJJ",
        "outputId": "9cf22171-887f-44e1-9230-9f0b6080c193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function**"
      ],
      "metadata": {
        "id": "8r_8TXZQN32c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced main function\n",
        "def main(scraped_data: List[Dict[str, Any]]):\n",
        "    try:\n",
        "        processed_texts = process_scraped_data(scraped_data)\n",
        "        vector_store = create_vector_store(processed_texts)\n",
        "        tools = define_tools(vector_store, scraped_data)\n",
        "        multi_agent_system = create_multi_agent_system(tools)\n",
        "\n",
        "        logger.info(\"Enhanced Cybersecurity Multi-Agent system initialized successfully.\")\n",
        "\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        queries = [\n",
        "            \"Assess the vulnerability CVE-2024-12345 in Windows Server.\",\n",
        "            \"Provide a security recommendation for mitigating phishing attacks.\",\n",
        "            \"List all details on BFSI security incidents in India.\",\n",
        "            \"List all ransomware attacks targeting the healthcare industry in the last 7 days.\",\n",
        "            \"Provide recent incidents related to Lockbit Ransomware gang.\",\n",
        "            \"Provide recent incidents related to BlackBasta Ransomware.\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            with get_openai_callback() as cb:\n",
        "                initial_state = AgentState(\n",
        "                    messages=[{\"role\": \"human\", \"content\": query}],\n",
        "                    current_agent=\"researcher\",\n",
        "                    scratchpad=[]\n",
        "                )\n",
        "                final_state = multi_agent_system.invoke(initial_state)\n",
        "\n",
        "                # Process and display the final response\n",
        "                final_response = final_state['messages'][-1]['content']\n",
        "                print(f\"Response: {final_response}\")\n",
        "\n",
        "                # Update knowledge graph based on the response\n",
        "                update_knowledge_graph([{\"source\": \"User Query\", \"title\": query}])\n",
        "\n",
        "                logger.info(f\"Tokens used: {cb.total_tokens}\")\n",
        "                logger.info(f\"Cost of query: ${cb.total_cost:.4f}\")\n",
        "\n",
        "        # Generate final report\n",
        "        generate_final_report(scraped_data, processed_texts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "h4bDIMu1nfMC",
        "outputId": "f1ea43f4-bb87-4d1c-afd8-2228380c6b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entry Point**"
      ],
      "metadata": {
        "id": "ba190KyWOBEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load your scraped data here\n",
        "        with open('scraped_data.json', 'r') as f:\n",
        "            scraped_data = json.load(f)\n",
        "        main(scraped_data)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"scraped_data.json file not found. Please ensure the file exists in the current directory.\")\n",
        "    except json.JSONDecodeError:\n",
        "        logger.error(\"Error decoding JSON from scraped_data.json. Please ensure the file contains valid JSON.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "DzVfVSJRnfPa",
        "outputId": "e2e3c94a-4b9e-441f-870d-4cabf92296e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "ERROR:__main__:scraped_data.json file not found. Please ensure the file exists in the current directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data collection\n",
        "scraped_data = scrape_websites(WEBSITES)\n",
        "logger.info(f\"Scraped data: {scraped_data}\")\n",
        "\n",
        "# Test data processing\n",
        "processed_texts = process_scraped_data(scraped_data)\n",
        "logger.info(f\"Processed texts: {processed_texts}\")\n",
        "\n",
        "# Test vector store creation\n",
        "vector_store = create_vector_store(processed_texts)\n",
        "logger.info(f\"Vector store created\")\n",
        "\n",
        "# Test agent tools\n",
        "tools = define_tools(vector_store, scraped_data)\n",
        "logger.info(f\"Tools defined: {tools}\")\n",
        "\n",
        "# Test multi-agent system\n",
        "multi_agent_system = create_multi_agent_system(tools)\n",
        "logger.info(f\"Multi-agent system created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "4wotORYwVS4o",
        "outputId": "9acd2774-4972-4d88-e0de-c01cfb313621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'source'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-712f44dd11f4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test data processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprocessed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_scraped_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraped_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed texts: {processed_texts}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1220882e3177>\u001b[0m in \u001b[0;36mprocess_scraped_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprocessed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Website\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprocessed_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Website Content ({item['url']}): {item['text']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Twitter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LinkedIn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'source'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJhrcUJBVVOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}