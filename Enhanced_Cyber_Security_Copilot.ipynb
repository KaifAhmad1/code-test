{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Enhanced_Cyber_Security_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem Statement\n",
        "\n",
        "##### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "##### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "##### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "###### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "##### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "##### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "AmgOoMzTPQu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15rtGXUXXidW",
        "outputId": "985205f7-251f-4e1c-b5ea-d0f6a20e07f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q apify-client langchain langchain-community langchain-groq networkx pyvis spacy transformers pandas\n",
        "%pip install -q sentence-transformers requests beautifulsoup4 ratelimit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any\n",
        "import logging\n",
        "import re\n",
        "from apify_client import ApifyClient\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "import json\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(\"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\")\n",
        "\n",
        "# Configure requests session with retries and timeouts\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "zpESwQ8lYW1z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "# Define system and human messages\n",
        "system_message = \"\"\"You are an expert cybersecurity analyst with extensive knowledge in threat analysis,\n",
        "vulnerability assessment, and security recommendations. Provide detailed, precise, and actionable insights.\n",
        "Always consider the latest threat intelligence and best practices in your analysis.\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"human\", \"{text}\")])"
      ],
      "metadata": {
        "id": "RRi1CmpSYlfI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()\n",
        "\n",
        "    def add_entity(self, entity: str, entity_type: str):\n",
        "        self.graph.add_node(entity, type=entity_type)\n",
        "        logger.info(f\"Added entity: {entity} of type: {entity_type}\")\n",
        "\n",
        "    def add_relation(self, entity1: str, entity2: str, relation: str):\n",
        "        self.graph.add_edge(entity1, entity2, relation=relation)\n",
        "        logger.info(f\"Added relation: {relation} between {entity1} and {entity2}\")\n",
        "\n",
        "    def get_related_entities(self, entity: str) -> List[Dict[str, str]]:\n",
        "        related = [{\"entity\": neighbor, \"relation\": self.graph.get_edge_data(entity, neighbor)[\"relation\"]}\n",
        "                   for neighbor in self.graph.neighbors(entity)]\n",
        "        logger.info(f\"Related entities for {entity}: {related}\")\n",
        "        return related\n",
        "\n",
        "    def visualize(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        net = Network(notebook=True, width=\"100%\", height=\"500px\")\n",
        "        for node, node_data in self.graph.nodes(data=True):\n",
        "            net.add_node(node, label=node, title=f\"Type: {node_data['type']}\")\n",
        "        for edge in self.graph.edges(data=True):\n",
        "            net.add_edge(edge[0], edge[1], title=edge[2]['relation'])\n",
        "        net.show(output_file)\n",
        "        logger.info(f\"Knowledge graph visualized at {output_file}\")\n",
        "\n",
        "# Initialize knowledge graph\n",
        "kg = KnowledgeGraph()\n",
        "kg.visualize()"
      ],
      "metadata": {
        "id": "pJS670uSYmY2",
        "outputId": "82cc447e-3a12-44cb-a74d-ca3a5a2214b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "knowledge_graph.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "    \"https://krebsonsecurity.com/\",\n",
        "    \"https://www.bleepingcomputer.com/\",\n",
        "    \"https://www.zdnet.com/topic/security/\",\n",
        "    \"https://www.securityweek.com/\",\n",
        "    \"https://www.sans.org/newsletters/newsbites/\",\n",
        "    \"https://www.cyberscoop.com/\",\n",
        "    \"https://www.csoonline.com/\",\n",
        "    \"https://www.infosecurity-magazine.com/\",\n",
        "    \"https://www.wired.com/category/security/\",\n",
        "    \"https://www.schneier.com/\",\n",
        "    \"https://www.theregister.com/security/\",\n",
        "    \"https://thehackernews.com/\",\n",
        "    \"https://www.cyberdefensemagazine.com/\",\n",
        "    \"https://www.fireeye.com/blog.html\",\n",
        "    \"https://unit42.paloaltonetworks.com/\",\n",
        "    \"https://www.microsoft.com/security/blog/\",\n",
        "    \"https://www.us-cert.gov/ncas/current-activity\",\n",
        "    \"https://nakedsecurity.sophos.com/\",\n",
        "    \"https://www.recordedfuture.com/blog/\",\n",
        "    \"https://www.cybersecurity-insiders.com/\",\n",
        "    \"https://www.malwarebytes.com/blog/\",\n",
        "]"
      ],
      "metadata": {
        "id": "Go2qxs8tYmib"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@sleep_and_retry\n",
        "@limits(calls=5, period=1)  # 5 calls per second\n",
        "def rate_limited_get(url: str, **kwargs) -> requests.Response:\n",
        "    return session.get(url, timeout=10, **kwargs)\n",
        "\n",
        "def search_google(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Searching Google for: {query}\")\n",
        "    run_input = {\n",
        "        \"queries\": query,  # Ensure the input is a string\n",
        "        \"maxPagesPerQuery\": max_results,\n",
        "        \"mobileResults\": False,\n",
        "        \"languageCode\": \"en\",\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/google-search-scraper\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        logger.info(f\"Found {len(dataset_items)} Google search results.\")\n",
        "        return dataset_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error searching Google: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def scrape_website(url: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        response = rate_limited_get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return {\"url\": url, \"text\": text, \"timestamp\": datetime.now().isoformat()}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
        "        return {\"url\": url, \"text\": \"\", \"timestamp\": datetime.now().isoformat(), \"error\": str(e)}\n",
        "\n",
        "def scrape_websites(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_url = {executor.submit(scrape_website, url): url for url in urls}\n",
        "        results = []\n",
        "        for future in as_completed(future_to_url):\n",
        "            results.append(future.result())\n",
        "    logger.info(f\"Successfully scraped {len(results)} pages.\")\n",
        "    return results\n",
        "\n",
        "def fetch_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"searchTerms\": [query],\n",
        "        \"maxTweets\": max_tweets,\n",
        "        \"languageCode\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"quacker/twitter-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching news for query: {query}\")\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"language\": \"en\",\n",
        "        \"pageSize\": max_results,\n",
        "        \"apiKey\": os.getenv(\"NEWS_API_KEY\"),\n",
        "        \"sortBy\": \"publishedAt\"\n",
        "    }\n",
        "    try:\n",
        "        response = rate_limited_get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "        logger.info(f\"Fetched {len(articles)} news articles.\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching news: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def scrape_reddit(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Scraping Reddit for query: {query}\")\n",
        "    run_input = {\n",
        "        \"subreddits\": [\"cybersecurity\", \"netsec\", \"security\", \"hacking\", \"infosec\"],\n",
        "        \"searchType\": \"posts\",\n",
        "        \"searchQuery\": query,\n",
        "        \"maxItems\": max_results\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/reddit-scraper\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        logger.info(f\"Scraped {len(dataset_items)} Reddit posts.\")\n",
        "        return dataset_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping Reddit: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_cve_data(days_back: int = 30) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching CVE data for the last {days_back} days\")\n",
        "    base_url = \"https://services.nvd.nist.gov/rest/json/cves/1.0\"\n",
        "    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%dT00:00:00:000 UTC-00:00\")\n",
        "    params = {\n",
        "        \"pubStartDate\": start_date,\n",
        "        \"resultsPerPage\": 2000\n",
        "    }\n",
        "    try:\n",
        "        response = rate_limited_get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        cve_data = response.json().get(\"result\", {}).get(\"CVE_Items\", [])\n",
        "        logger.info(f\"Fetched {len(cve_data)} CVE entries.\")\n",
        "        return cve_data\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching CVE data: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_google_news(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching Google News for query: {query}\")\n",
        "    run_input = {\n",
        "        \"queries\": [query],\n",
        "        \"maxPagesPerQuery\": max_results,\n",
        "        \"languageCode\": \"en\",\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/google-news-scraper\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        logger.info(f\"Fetched {len(dataset_items)} Google News articles.\")\n",
        "        return dataset_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching Google News: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_bing_search(query: str, max_results: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching Bing search results for query: {query}\")\n",
        "    run_input = {\n",
        "        \"queries\": [query],\n",
        "        \"maxPagesPerQuery\": max_results,\n",
        "        \"languageCode\": \"en\",\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/bing-search-scraper\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        logger.info(f\"Fetched {len(dataset_items)} Bing search results.\")\n",
        "        return dataset_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching Bing search results: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_linkedin_posts(query: str, max_posts: int = 50) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching LinkedIn posts for query: {query}\")\n",
        "    run_input = {\n",
        "        \"searchTerms\": [query],\n",
        "        \"maxPosts\": max_posts,\n",
        "        \"languageCode\": \"en\"\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/linkedin-scraper\").call(run_input=run_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} LinkedIn posts.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching LinkedIn posts: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_rss_feeds(urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    logger.info(f\"Fetching RSS feeds from {len(urls)} URLs\")\n",
        "    run_input = {\n",
        "        \"startUrls\": urls,\n",
        "        \"maxItems\": 50\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/rss-feed-scraper\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        logger.info(f\"Fetched {len(dataset_items)} RSS feed items.\")\n",
        "        return dataset_items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feeds: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "mKT9L-PdOIm6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.lower()\n",
        "\n",
        "def tag_content(content: List[Dict[str, Any]], tags: List[str]) -> List[Dict[str, Any]]:\n",
        "    tagged_content = []\n",
        "    for item in content:\n",
        "        cleaned_text = clean_text(item.get(\"text\", \"\"))\n",
        "        item_tags = [tag for tag in tags if tag.lower() in cleaned_text]\n",
        "        tagged_content.append({**item, \"tags\": item_tags})\n",
        "    return tagged_content\n",
        "\n",
        "def save_to_json(data: Dict[str, Any], filename: str):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "def collect_data(query: str, tags: List[str]):\n",
        "    google_results = search_google(query)\n",
        "    urls = websites + [result['url'] for result in google_results]\n",
        "    website_content = scrape_websites(urls)\n",
        "    tweets = fetch_tweets(query)\n",
        "    news = fetch_news(query)\n",
        "    reddit_posts = scrape_reddit(query)\n",
        "    cve_data = fetch_cve_data()\n",
        "    google_news = fetch_google_news(query)\n",
        "    bing_search = fetch_bing_search(query)\n",
        "    linkedin_posts = fetch_linkedin_posts(query)\n",
        "    rss_feeds = fetch_rss_feeds(websites)\n",
        "\n",
        "    tagged_google_results = tag_content(google_results, tags)\n",
        "    tagged_website_content = tag_content(website_content, tags)\n",
        "    tagged_tweets = tag_content(tweets, tags)\n",
        "    tagged_news = tag_content(news, tags)\n",
        "    tagged_reddit_posts = tag_content(reddit_posts, tags)\n",
        "    tagged_cve_data = tag_content(cve_data, tags)\n",
        "    tagged_google_news = tag_content(google_news, tags)\n",
        "    tagged_bing_search = tag_content(bing_search, tags)\n",
        "    tagged_linkedin_posts = tag_content(linkedin_posts, tags)\n",
        "    tagged_rss_feeds = tag_content(rss_feeds, tags)\n",
        "\n",
        "    collected_data = {\n",
        "        \"google_results\": tagged_google_results,\n",
        "        \"website_content\": tagged_website_content,\n",
        "        \"tweets\": tagged_tweets,\n",
        "        \"news\": tagged_news,\n",
        "        \"reddit_posts\": tagged_reddit_posts,\n",
        "        \"cve_data\": tagged_cve_data,\n",
        "        \"google_news\": tagged_google_news,\n",
        "        \"bing_search\": tagged_bing_search,\n",
        "        \"linkedin_posts\": tagged_linkedin_posts,\n",
        "        \"rss_feeds\": tagged_rss_feeds,\n",
        "        \"collection_timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    save_to_json(collected_data, f\"cybersecurity_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "    return collected_data"
      ],
      "metadata": {
        "id": "od_JcbrxOIpY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"latest cybersecurity threats\"\n",
        "    tags = [\n",
        "        \"malware\", \"ransomware\", \"threat\", \"cybersecurity\", \"phishing\",\n",
        "        \"data breach\", \"DDoS attack\", \"APT\", \"zero-day\", \"exploit\",\n",
        "        \"vulnerability\", \"spyware\", \"adware\", \"rootkit\", \"backdoor\",\n",
        "        \"botnet\", \"keylogger\", \"Trojans\", \"worms\", \"virus\",\n",
        "        \"incident response\", \"threat intelligence\", \"SIEM\", \"EDR\",\n",
        "        \"XDR\", \"cloud security\", \"IoT security\", \"AI security\",\n",
        "        \"blockchain security\", \"cryptography\", \"network security\",\n",
        "        \"application security\", \"DevSecOps\", \"container security\",\n",
        "        \"Kubernetes security\", \"SOAR\", \"threat hunting\", \"OSINT\",\n",
        "        \"penetration testing\", \"red teaming\", \"blue teaming\",\n",
        "        \"purple teaming\", \"cyber insurance\", \"compliance\", \"GDPR\",\n",
        "        \"HIPAA\", \"PCI DSS\", \"NIST\", \"ISO 27001\", \"zero trust\",\n",
        "        \"passwordless\", \"biometrics\", \"MFA\", \"IAM\", \"PAM\",\n",
        "        \"cyber resilience\", \"cyber hygiene\", \"security awareness\",\n",
        "        \"social engineering\", \"insider threat\", \"supply chain attack\",\n",
        "        \"quantum computing\", \"post-quantum cryptography\", \"5G security\",\n",
        "        \"OT security\", \"ICS security\", \"SCADA security\", \"mobile security\",\n",
        "        \"endpoint security\", \"email security\", \"web security\",\n",
        "        \"API security\", \"CASB\", \"CWPP\", \"CSPM\", \"CNAPP\",\n",
        "        \"cyber warfare\", \"cyber espionage\", \"hacktivism\", \"cyber terrorism\",\n",
        "        \"cyber crime\", \"dark web\", \"threat actor\", \"nation-state attack\"\n",
        "    ]\n",
        "    data = collect_data(query, tags)\n",
        "    print(\"Data collection complete. Results saved to JSON file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Mc5XRuOIrz",
        "outputId": "77059401-d180-4ea9-b319-4b13706d1a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error scraping https://www.securityweek.com/: 403 Client Error: Forbidden for url: https://www.securityweek.com/\n",
            "ERROR:__main__:Error scraping https://www.theregister.com/security/: 403 Client Error: Forbidden for url: https://www.theregister.com/security/\n",
            "ERROR:__main__:Error scraping https://www.us-cert.gov/ncas/current-activity: 404 Client Error: Not Found for url: https://www.cisa.gov/ncas/current-activity\n",
            "ERROR:__main__:Error scraping https://www.bleepingcomputer.com/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/\n",
            "ERROR:__main__:Error scraping https://www.cybersecurity-insiders.com/: 403 Client Error: Forbidden for url: https://www.cybersecurity-insiders.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9klGY7GnOIun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJ00vRJ-OIw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}