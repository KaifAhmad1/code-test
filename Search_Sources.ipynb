{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaql0BkOV9j00N6wkIIL4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Search_Sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQUT-suIDN0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ff4694-abda-49b5-91b5-265923a973ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.2/702.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU telethon tweepy feedparser google-api-python-client requests tavily-python exa_py python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "import os\n",
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from exa_py import Exa\n",
        "from tavily import TavilyClient\n",
        "from telethon import TelegramClient\n",
        "import tweepy\n",
        "import feedparser\n",
        "from dateutil import parser\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\", \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\")\n",
        "EXA_API_KEY = os.getenv(\"EXA_API_KEY\", \"953b5801-11be-4b37-a313-f8df8f37027c\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\")\n",
        "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\", \"63053004a7e2445c3\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"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\")\n",
        "\n",
        "# Telegram API Keys\n",
        "TELEGRAM_API_ID = 20441646\n",
        "TELEGRAM_API_HASH = \"d78a891287e9ba6a2a8c4bb0e4ca506a\"\n",
        "\n",
        "# Twitter API Keys\n",
        "TWITTER_API_KEY = \"WTIYHjD9r10EZKoUghhK6vqnh\"\n",
        "TWITTER_API_SECRET = \"0t4k1BiQnqa2RcZKFLUXMtka5t0BOc1F89eNWj8ee3AkNFyGRA\"\n",
        "TWITTER_ACCESS_TOKEN = \"1872197717612474368-CuATnNZBeXy7r3ymdSMnPW0MMkfVkf\"\n",
        "TWITTER_ACCESS_TOKEN_SECRET = \"wAfYioeyIpK0kpStzQFK5TxHeLCPVVwk6vtaAzjprYEFV\"\n",
        "TWITTER_BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAHMBxwEAAAAAXHty6DjTOsfJ0TXByHccMUqBK%2B0%3DYVCDqgobV2wQUr8WTAR3zVrmqsPU7PdPIQzyRMelPPYgawKEfh\""
      ],
      "metadata": {
        "id": "RfyEZQPAEBUr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SECURITY_RSS_FEEDS = [\n",
        "    \"https://www.bleepingcomputer.com/feed/\",\n",
        "    \"https://feeds.feedburner.com/TheHackersNews\",\n",
        "    \"https://krebsonsecurity.com/feed/\",\n",
        "    \"https://www.darkreading.com/rss.xml\",\n",
        "    \"https://www.securityweek.com/feed/\",\n",
        "    \"https://www.csoonline.com/feed/\",\n",
        "    \"https://www.threatpost.com/feed/\",\n",
        "    \"https://www.helpnetsecurity.com/feed/\",\n",
        "    \"https://www.infosecurity-magazine.com/rss/news/\",\n",
        "    \"https://www.cybersecurity-insiders.com/feed/\",\n",
        "    \"https://www.zdnet.com/topic/security/rss.xml\",\n",
        "    \"https://www.schneier.com/feed/atom/\",\n",
        "    \"https://www.theregister.com/security/headlines.atom\",\n",
        "    \"https://www.govinfosecurity.com/rss/feeds/rss\",\n",
        "    \"https://www.crowdstrike.com/blog/feed/\"\n",
        "]\n",
        "\n",
        "TELEGRAM_CHANNELS = [\n",
        "    'cybersecuritynews',\n",
        "    'ransomware_news',\n",
        "    'malware_news',\n",
        "    'infosec_latest',\n",
        "    'cyber_threat_intel',\n",
        "    'hacking_news',\n",
        "    'cyber_security_updates',\n",
        "    'data_breach_news',\n",
        "    'cybercrime_updates',\n",
        "    'security_research_news',\n",
        "    'cyber_attack_alerts',\n",
        "    'privacy_news',\n",
        "    'cyber_defense_updates',\n",
        "    'threat_intelligence',\n",
        "    'cyber_news_daily'\n",
        "]"
      ],
      "metadata": {
        "id": "F-09ZjgtAQcX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize services\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "google_service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "telegram_client = TelegramClient('session_name', TELEGRAM_API_ID, TELEGRAM_API_HASH)\n",
        "twitter_auth = tweepy.OAuth1UserHandler(TWITTER_API_KEY, TWITTER_API_SECRET, TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "twitter_api = tweepy.API(twitter_auth)"
      ],
      "metadata": {
        "id": "otVl9AaWEjDH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str] = None\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []"
      ],
      "metadata": {
        "id": "s1Pt1QYFE9h0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"autocorrect\": True\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in data.get(\"organic\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Serper Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5\n",
        "        ).execute()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                date=item.get(\"pagemap\", {}).get(\"metatags\", [{}])[0].get(\"article:published_time\")\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    url = \"https://google.serper.dev/images\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"imageUrl\", \"No link\"),\n",
        "                media=[result.get(\"imageUrl\", \"No link\")],\n",
        "                media_content=[{\"image_url\": result.get(\"imageUrl\", \"No link\")}]\n",
        "            ) for result in data.get(\"images\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Serper Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5,\n",
        "            searchType=\"image\"\n",
        "        ).execute()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                media=[item.get(\"link\", \"No link\")],\n",
        "                media_content=[{\"image_url\": item.get(\"link\", \"No link\")}]\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        response = exa.search_and_contents(\n",
        "            query,\n",
        "            use_autoprompt=True,\n",
        "            num_results=5,\n",
        "            text=True,\n",
        "            highlights=True\n",
        "        )\n",
        "        results = response.results\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.title,\n",
        "                snippet=result.highlights[0] if result.highlights else \"No snippet\",\n",
        "                url=result.url,\n",
        "                date=result.publishedDate if hasattr(result, 'publishedDate') else None,\n",
        "                media_content=[{\"image_url\": result.url}] if result.url else []\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        response = tavily_client.search(\n",
        "            query,\n",
        "            search_depth=\"advanced\",\n",
        "            include_answer=True,\n",
        "            include_raw_content=True,\n",
        "            include_images=True,\n",
        "            max_results=5\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"published_date\"),\n",
        "                media=[result.get(\"url\", \"No link\")] if result.get(\"url\") else [],\n",
        "                media_content=[{\"image_url\": result.get(\"url\", \"No link\")}] if result.get(\"url\") else []\n",
        "            ) for result in response.get(\"results\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "async def telegram_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        await telegram_client.start()\n",
        "        all_results = []\n",
        "        for channel in TELEGRAM_CHANNELS:\n",
        "            results = await telegram_client.get_messages(channel, search=query, limit=5)\n",
        "            all_results.extend([\n",
        "                SearchResult(\n",
        "                    source=f\"Telegram ({channel})\",\n",
        "                    title=result.message,\n",
        "                    snippet=result.message,\n",
        "                    url=f\"https://t.me/{channel}/{result.id}\",\n",
        "                    date=result.date.isoformat()\n",
        "                ) for result in results\n",
        "            ])\n",
        "        return all_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Telegram Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def twitter_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        tweets = twitter_api.search_tweets(q=query, count=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Twitter\",\n",
        "                title=tweet.user.name,\n",
        "                snippet=tweet.text,\n",
        "                url=f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
        "                date=tweet.created_at.isoformat()\n",
        "            ) for tweet in tweets\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Twitter Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def rss_feed_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        all_results = []\n",
        "        for feed_url in SECURITY_RSS_FEEDS:\n",
        "            try:\n",
        "                feed = feedparser.parse(feed_url)\n",
        "                all_results.extend([\n",
        "                    SearchResult(\n",
        "                        source=f\"RSS Feed ({feed_url})\",\n",
        "                        title=entry.title,\n",
        "                        snippet=entry.get(\"summary\", \"No summary available\"),\n",
        "                        url=entry.link,\n",
        "                        date=entry.published\n",
        "                    ) for entry in feed.entries if query.lower() in entry.title.lower() or query.lower() in entry.get(\"summary\", \"\").lower()\n",
        "                ])\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR parsing RSS feed {feed_url}: {str(e)}\")\n",
        "                continue\n",
        "        return all_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in RSS Feed Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def filter_by_date(results: List[SearchResult], days: int = 7) -> List[SearchResult]:\n",
        "    cutoff_date = datetime.now() - timedelta(days=days)\n",
        "    return [result for result in results if result.date and parser.parse(result.date) >= cutoff_date]\n",
        "\n",
        "def filter_by_domain(results: List[SearchResult], include_domains: List[str] = None, exclude_domains: List[str] = None) -> List[SearchResult]:\n",
        "    if include_domains:\n",
        "        results = [result for result in results if any(domain in result.url for domain in include_domains)]\n",
        "    if exclude_domains:\n",
        "        results = [result for result in results if not any(domain in result.url for domain in exclude_domains)]\n",
        "    return results"
      ],
      "metadata": {
        "id": "z6w_36f9FRIV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_search_results(*args: List[SearchResult]) -> List[SearchResult]:\n",
        "    all_results = []\n",
        "    media_content = []\n",
        "\n",
        "    for results in args:\n",
        "        all_results.extend(results)\n",
        "        media_content.extend([media for result in results for media in result.media_content])\n",
        "\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort results by date (most recent first)\n",
        "    unique_results.sort(key=lambda x: parser.parse(x.date) if x.date else datetime.min, reverse=True)\n",
        "\n",
        "    # Sort media content by date (most recent first)\n",
        "    media_content.sort(key=lambda x: parser.parse(x.get(\"date\", \"\")) if x.get(\"date\") else datetime.min, reverse=True)\n",
        "\n",
        "    return unique_results, media_content"
      ],
      "metadata": {
        "id": "upLiGPDEGHcy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def execute_searches(query: str, search_type: str = \"normal\", days: int = 7, include_domains: List[str] = None, exclude_domains: List[str] = None) -> Dict[str, Any]:\n",
        "    search_functions = [\n",
        "        google_serper_search,\n",
        "        google_programmable_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_serper_image_search if search_type == \"image\" else None,\n",
        "        google_programmable_image_search if search_type == \"image\" else None,\n",
        "        telegram_search,\n",
        "        lambda q: twitter_search(q),\n",
        "        lambda q: rss_feed_search(q)\n",
        "    ]\n",
        "    search_functions = [func for func in search_functions if func is not None]  # Remove None values\n",
        "\n",
        "    search_tasks = [asyncio.to_thread(func, query) if callable(func) and not asyncio.iscoroutinefunction(func) else func(query) for func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            print(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    combined_results, media_content = aggregate_search_results(*successful_results)\n",
        "    combined_results = filter_by_date(combined_results, days=days)\n",
        "    combined_results = filter_by_domain(combined_results, include_domains=include_domains, exclude_domains=exclude_domains)\n",
        "\n",
        "    urls = [result.url for result in combined_results]\n",
        "\n",
        "    return {\n",
        "        \"results\": combined_results,\n",
        "        \"urls\": urls,\n",
        "        \"media_content\": media_content\n",
        "    }"
      ],
      "metadata": {
        "id": "nFwcBz2JuYD4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents by Lockbit Ransomware Group?\"\n",
        "    results = asyncio.run(execute_searches(query, search_type=\"normal\", days=7, include_domains=[\"bleepingcomputer.com\"], exclude_domains=[\"twitter.com\"]))\n",
        "\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for result in results[\"results\"]:\n",
        "        print(f\"\\nTitle: {result.title}\")\n",
        "        print(f\"Source: {result.source}\")\n",
        "        print(f\"URL: {result.url}\")\n",
        "        print(f\"Snippet: {result.snippet}\")\n",
        "        if result.media_content:\n",
        "            print(f\"Media Content: {result.media_content}\")\n",
        "\n",
        "    print(\"\\nURLs List:\")\n",
        "    print(results[\"urls\"])\n",
        "\n",
        "    print(\"\\nMedia Content List:\")\n",
        "    print(results[\"media_content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "Ih6PPHvQGPfs",
        "outputId": "973beb63-6ada-4d10-e1c5-c83eb515167b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR in Twitter Search: 403 Forbidden\n",
            "453 - You currently have access to a subset of X API V2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.x.com/en/portal/product\n",
            "ERROR parsing RSS feed https://www.govinfosecurity.com/rss/feeds/rss: Remote end closed connection without response\n",
            "Please enter your phone (or bot token): +91 8755714681\n",
            "Please enter the code you received: 22789\n",
            "Signed in successfully as Sentdex; remember to not break the ToS or you will risk an account ban!\n",
            "ERROR in Telegram Search: No user has \"infosec_latest\" as username\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Unknown string format: 7 days ago",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-82f64047eca3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Latest Cyber Incidents by Lockbit Ransomware Group?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute_searches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_domains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bleepingcomputer.com\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_domains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"twitter.com\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSearch Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f848f80ec6b6>\u001b[0m in \u001b[0;36mexecute_searches\u001b[0;34m(query, search_type, days, include_domains, exclude_domains)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msuccessful_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcombined_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msuccessful_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mcombined_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_by_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcombined_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_by_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_domains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_domains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_domains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_domains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0519e3ba8acb>\u001b[0m in \u001b[0;36maggregate_search_results\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Sort results by date (most recent first)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0munique_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Sort media content by date (most recent first)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0519e3ba8acb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Sort results by date (most recent first)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0munique_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Sort media content by date (most recent first)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown string format: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Unknown string format: 7 days ago"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS3r64URGeDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}