{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBKS75qQKKrdlRDY0+D9qP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Search_Sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jQUT-suIDN0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45367cfb-1fef-46d0-fd17-d418c37be7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.2/702.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU telethon tweepy feedparser google-api-python-client requests tavily-python exa_py python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "import os\n",
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from exa_py import Exa\n",
        "from tavily import TavilyClient\n",
        "from telethon import TelegramClient\n",
        "import tweepy\n",
        "import feedparser\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\", \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\")\n",
        "EXA_API_KEY = os.getenv(\"EXA_API_KEY\", \"953b5801-11be-4b37-a313-f8df8f37027c\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\")\n",
        "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\", \"63053004a7e2445c3\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"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\")\n",
        "\n",
        "# Telegram API Keys\n",
        "TELEGRAM_API_ID = 20441646\n",
        "TELEGRAM_API_HASH = \"d78a891287e9ba6a2a8c4bb0e4ca506a\"\n",
        "\n",
        "# Twitter API Keys\n",
        "TWITTER_API_KEY = \"WTIYHjD9r10EZKoUghhK6vqnh\"\n",
        "TWITTER_API_SECRET = \"0t4k1BiQnqa2RcZKFLUXMtka5t0BOc1F89eNWj8ee3AkNFyGRA\"\n",
        "TWITTER_ACCESS_TOKEN = \"1872197717612474368-CuATnNZBeXy7r3ymdSMnPW0MMkfVkf\"\n",
        "TWITTER_ACCESS_TOKEN_SECRET = \"wAfYioeyIpK0kpStzQFK5TxHeLCPVVwk6vtaAzjprYEFV\"\n",
        "TWITTER_BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAHMBxwEAAAAAXHty6DjTOsfJ0TXByHccMUqBK%2B0%3DYVCDqgobV2wQUr8WTAR3zVrmqsPU7PdPIQzyRMelPPYgawKEfh\""
      ],
      "metadata": {
        "id": "RfyEZQPAEBUr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SECURITY_RSS_FEEDS = [\n",
        "    \"https://www.bleepingcomputer.com/feed/\",\n",
        "    \"https://feeds.feedburner.com/TheHackersNews\",\n",
        "    \"https://krebsonsecurity.com/feed/\",\n",
        "    \"https://www.darkreading.com/rss.xml\",\n",
        "    \"https://www.securityweek.com/feed/\",\n",
        "    \"https://www.csoonline.com/feed/\",\n",
        "    \"https://www.threatpost.com/feed/\",\n",
        "    \"https://www.helpnetsecurity.com/feed/\",\n",
        "    \"https://www.infosecurity-magazine.com/rss/news/\",\n",
        "    \"https://www.cybersecurity-insiders.com/feed/\",\n",
        "    \"https://www.zdnet.com/topic/security/rss.xml\",\n",
        "    \"https://www.schneier.com/feed/atom/\",\n",
        "    \"https://www.theregister.com/security/headlines.atom\",\n",
        "    \"https://www.govinfosecurity.com/rss/feeds/rss\",\n",
        "    \"https://www.crowdstrike.com/blog/feed/\"\n",
        "]\n",
        "\n",
        "TELEGRAM_CHANNELS = [\n",
        "    'cveNotify',\n",
        "    'ctinow',\n",
        "    'CyberSecurityTechnologies',\n",
        "    'cybersecurity_outlook',\n",
        "    'cibsecurity',\n",
        "    'thehackernews',\n",
        "    'Cyber_Security_Channel',\n",
        "    'cloudandcybersecurity',\n",
        "    'androidMalware',\n",
        "    'DarkfeedNews',\n",
        "    'PentestingNews',\n",
        "    'malwr'\n",
        "]"
      ],
      "metadata": {
        "id": "F-09ZjgtAQcX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize services\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "google_service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "telegram_client = TelegramClient('session_name', TELEGRAM_API_ID, TELEGRAM_API_HASH)\n",
        "twitter_auth = tweepy.OAuth1UserHandler(TWITTER_API_KEY, TWITTER_API_SECRET, TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "twitter_api = tweepy.API(twitter_auth)"
      ],
      "metadata": {
        "id": "otVl9AaWEjDH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str] = None\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []"
      ],
      "metadata": {
        "id": "s1Pt1QYFE9h0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Google Serper Search\")\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"autocorrect\": True\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        print(f\"Google Serper Search Example Result: {data.get('organic', [])[0] if data.get('organic') else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in data.get(\"organic\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Google Serper Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Google Programmable Search\")\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5\n",
        "        ).execute()\n",
        "        print(f\"Google Programmable Search Example Result: {search_results.get('items', [])[0] if search_results.get('items') else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                date=item.get(\"pagemap\", {}).get(\"metatags\", [{}])[0].get(\"article:published_time\")\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Google Programmable Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Google Serper Image Search\")\n",
        "    url = \"https://google.serper.dev/images\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        print(f\"Google Serper Image Search Example Result: {data.get('images', [])[0] if data.get('images') else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"imageUrl\", \"No link\"),\n",
        "                media=[result.get(\"imageUrl\", \"No link\")],\n",
        "                media_content=[{\"image_url\": result.get(\"imageUrl\", \"No link\")}]\n",
        "            ) for result in data.get(\"images\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Google Serper Image Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Google Programmable Image Search\")\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5,\n",
        "            searchType=\"image\"\n",
        "        ).execute()\n",
        "        print(f\"Google Programmable Image Search Example Result: {search_results.get('items', [])[0] if search_results.get('items') else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                media=[item.get(\"link\", \"No link\")],\n",
        "                media_content=[{\"image_url\": item.get(\"link\", \"No link\")}]\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Google Programmable Image Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Exa Search\")\n",
        "    try:\n",
        "        response = exa.search_and_contents(\n",
        "            query,\n",
        "            use_autoprompt=True,\n",
        "            num_results=5,\n",
        "            text=True,\n",
        "            highlights=True\n",
        "        )\n",
        "        results = response.results\n",
        "        print(f\"Exa Search Example Result: {results[0] if results else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.title,\n",
        "                snippet=result.highlights[0] if result.highlights else \"No snippet\",\n",
        "                url=result.url,\n",
        "                date=result.publishedDate if hasattr(result, 'publishedDate') else None,\n",
        "                media_content=[{\"image_url\": result.url}] if result.url else []\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Exa Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Tavily Search\")\n",
        "    try:\n",
        "        response = tavily_client.search(\n",
        "            query,\n",
        "            search_depth=\"advanced\",\n",
        "            include_answer=True,\n",
        "            include_raw_content=True,\n",
        "            include_images=True,\n",
        "            max_results=5\n",
        "        )\n",
        "        print(f\"Tavily Search Example Result: {response.get('results', [])[0] if response.get('results') else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"published_date\"),\n",
        "                media=[result.get(\"url\", \"No link\")] if result.get(\"url\") else [],\n",
        "                media_content=[{\"image_url\": result.get(\"url\", \"No link\")}] if result.get(\"url\") else []\n",
        "            ) for result in response.get(\"results\", [])\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Tavily Search: {e}\")\n",
        "        return []\n",
        "\n",
        "async def telegram_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Telegram Search\")\n",
        "    try:\n",
        "        await telegram_client.start()\n",
        "        print(\"Telegram client started successfully\")\n",
        "        all_results = []\n",
        "        for channel in TELEGRAM_CHANNELS:\n",
        "            print(f\"Searching in channel: {channel}\")\n",
        "            results = await telegram_client.get_messages(channel, search=query, limit=5)\n",
        "            if results:\n",
        "                all_results.extend([\n",
        "                    SearchResult(\n",
        "                        source=f\"Telegram ({channel})\",\n",
        "                        title=result.message,\n",
        "                        snippet=result.message,\n",
        "                        url=f\"https://t.me/{channel}/{result.id}\",\n",
        "                        date=result.date.isoformat()\n",
        "                    ) for result in results\n",
        "                ])\n",
        "                print(f\"Found {len(results)} results in channel: {channel}\")\n",
        "            else:\n",
        "                print(f\"No results found in channel: {channel}\")\n",
        "        print(f\"Telegram Search Example Result: {all_results[0] if all_results else 'No results'}\")\n",
        "        return all_results\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Telegram Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def twitter_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing Twitter Search\")\n",
        "    try:\n",
        "        tweets = twitter_api.search_tweets(q=query, count=5)\n",
        "        print(f\"Twitter Search Example Result: {tweets[0] if tweets else 'No results'}\")\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Twitter\",\n",
        "                title=tweet.user.name,\n",
        "                snippet=tweet.text,\n",
        "                url=f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
        "                date=tweet.created_at.isoformat()\n",
        "            ) for tweet in tweets\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Twitter Search: {e}\")\n",
        "        return []\n",
        "\n",
        "def rss_feed_search(query: str) -> List[SearchResult]:\n",
        "    print(\"Executing RSS Feed Search\")\n",
        "    all_results = []\n",
        "    for feed_url in SECURITY_RSS_FEEDS:\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            if feed.entries:\n",
        "                all_results.extend([\n",
        "                    SearchResult(\n",
        "                        source=f\"RSS Feed ({feed_url})\",\n",
        "                        title=entry.title,\n",
        "                        snippet=entry.get(\"summary\", \"No summary available\"),\n",
        "                        url=entry.link,\n",
        "                        date=entry.published\n",
        "                    ) for entry in feed.entries if query.lower() in entry.title.lower() or query.lower() in entry.get(\"summary\", \"\").lower()\n",
        "                ])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing RSS feed {feed_url}: {e}\")\n",
        "            continue\n",
        "    print(f\"RSS Feed Search Example Result: {all_results[0] if all_results else 'No results'}\")\n",
        "    return all_results\n",
        "\n",
        "def parse_date(date_str: str) -> datetime:\n",
        "    if date_str:\n",
        "        try:\n",
        "            return parser.parse(date_str).replace(tzinfo=None)  # Make all dates naive\n",
        "        except parser.ParserError:\n",
        "            if \"days ago\" in date_str:\n",
        "                days = int(date_str.split()[0])\n",
        "                return (datetime.now() - timedelta(days=days)).replace(tzinfo=None)\n",
        "            elif \"months ago\" in date_str:\n",
        "                months = int(date_str.split()[0])\n",
        "                return (datetime.now() - relativedelta(months=months)).replace(tzinfo=None)\n",
        "            elif \"years ago\" in date_str:\n",
        "                years = int(date_str.split()[0])\n",
        "                return (datetime.now() - relativedelta(years=years)).replace(tzinfo=None)\n",
        "    return datetime.min\n",
        "\n",
        "def filter_by_date(results: List[SearchResult], days: int = 7) -> List[SearchResult]:\n",
        "    cutoff_date = datetime.now() - timedelta(days=days)\n",
        "    return [result for result in results if result.date and parse_date(result.date) >= cutoff_date]\n",
        "\n",
        "def filter_by_domain(results: List[SearchResult], include_domains: List[str] = None, exclude_domains: List[str] = None) -> List[SearchResult]:\n",
        "    if include_domains:\n",
        "        results = [result for result in results if any(domain in result.url for domain in include_domains)]\n",
        "    if exclude_domains:\n",
        "        results = [result for result in results if not any(domain in result.url for domain in exclude_domains)]\n",
        "    return results"
      ],
      "metadata": {
        "id": "z6w_36f9FRIV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_search_results(*args: List[SearchResult]) -> Dict[str, List[str]]:\n",
        "    all_results = []\n",
        "    urls = []\n",
        "    images = []\n",
        "\n",
        "    for results in args:\n",
        "        all_results.extend(results)\n",
        "        for result in results:\n",
        "            urls.append(result.url)\n",
        "            if result.media_content:\n",
        "                images.extend([media[\"image_url\"] for media in result.media_content])\n",
        "\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort results by date (most recent first)\n",
        "    unique_results.sort(key=lambda x: parse_date(x.date) if x.date else datetime.min, reverse=True)\n",
        "\n",
        "    return {\n",
        "        \"results\": unique_results,\n",
        "        \"urls\": list(set(urls)),  # Deduplicate URLs\n",
        "        \"images\": list(set(images))  # Deduplicate image URLs\n",
        "    }"
      ],
      "metadata": {
        "id": "upLiGPDEGHcy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def execute_searches(query: str, search_type: str = \"normal\", days: int = 7, include_domains: List[str] = None, exclude_domains: List[str] = None) -> Dict[str, Any]:\n",
        "    search_functions = [\n",
        "        google_serper_search,\n",
        "        google_programmable_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_serper_image_search if search_type == \"image\" else None,\n",
        "        google_programmable_image_search if search_type == \"image\" else None,\n",
        "        telegram_search,\n",
        "        lambda q: twitter_search(q),\n",
        "        lambda q: rss_feed_search(q)\n",
        "    ]\n",
        "    search_functions = [func for func in search_functions if func is not None]  # Remove None values\n",
        "\n",
        "    search_tasks = [asyncio.to_thread(func, query) if callable(func) and not asyncio.iscoroutinefunction(func) else func(query) for func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if not isinstance(results, Exception):\n",
        "            successful_results.append(results)\n",
        "\n",
        "    combined_results = aggregate_search_results(*successful_results)\n",
        "    combined_results[\"results\"] = filter_by_date(combined_results[\"results\"], days=days)\n",
        "    combined_results[\"results\"] = filter_by_domain(combined_results[\"results\"], include_domains=include_domains, exclude_domains=exclude_domains)\n",
        "\n",
        "    return combined_results"
      ],
      "metadata": {
        "id": "AcAJdBQhOIO0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents by Lockbit Ransomware Group?\"\n",
        "    results = asyncio.run(execute_searches(query, search_type=\"normal\", days=7, include_domains=[\"bleepingcomputer.com\"], exclude_domains=[\"twitter.com\"]))\n",
        "\n",
        "    print(\"\\nTextual URLs List:\")\n",
        "    print(results[\"urls\"])\n",
        "\n",
        "    print(\"\\nImage URLs List:\")\n",
        "    print(results[\"images\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih6PPHvQGPfs",
        "outputId": "b96869e4-8325-4ecc-f45b-94a596d2c294"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Google Serper Search\n",
            "Executing Google Programmable Search\n",
            "Executing Exa SearchExecuting Tavily SearchExecuting Telegram Search\n",
            "\n",
            "\n",
            "Executing Twitter Search\n",
            "Executing RSS Feed Search\n",
            "Error in Twitter Search: 403 Forbidden\n",
            "453 - You currently have access to a subset of X API V2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.x.com/en/portal/product\n",
            "Google Programmable Search Example Result: {'kind': 'customsearch#result', 'title': 'United States Sanctions Affiliates of Russia-Based LockBit ...', 'htmlTitle': 'United States Sanctions Affiliates of Russia-Based <b>LockBit</b> ...', 'link': 'https://home.treasury.gov/news/press-releases/jy2114', 'displayLink': 'home.treasury.gov', 'snippet': 'Feb 20, 2024 ... ... last year, the Cybersecurity ... Cybercrime Group with the United Kingdom ... Lockbit ransomware group affiliate and has actively engaged in LockBit\\xa0...', 'htmlSnippet': 'Feb 20, 2024 <b>...</b> ... <b>last</b> year, the <b>Cybersecurity</b> ... <b>Cybercrime Group</b> with the United Kingdom ... <b>Lockbit ransomware group</b> affiliate and has actively engaged in LockBit&nbsp;...', 'formattedUrl': 'https://home.treasury.gov/news/press-releases/jy2114', 'htmlFormattedUrl': 'https://home.treasury.gov/news/press-releases/jy2114', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ-91IfwYYNnVbB_SsWhMb954aObHlbTxojgkmXpbpy7fdA3rsWflH3E6el&s', 'width': '318', 'height': '159'}], 'metatags': [{'og:image': 'https://home.treasury.gov/system/files/291/treasury-preview-1900x950.jpg', 'og:type': 'article', 'og:image:width': '1900px', 'og:image:alt': 'U.S. Department of the Treasury', 'twitter:card': 'summary_large_image', 'twitter:title': 'United States Sanctions Affiliates of Russia-Based LockBit Ransomware Group', 'og:site_name': 'U.S. Department of the Treasury', 'handheldfriendly': 'true', 'og:title': 'United States Sanctions Affiliates of Russia-Based LockBit Ransomware Group', 'og:image:height': '950px', 'twitter:image:height': '1080', 'og:image:type': 'image/jpeg', 'og:updated_time': '2024-02-20', 'og:description': 'The United States imposes sanctions on affiliates of group responsible for ransomware attacks on the U.S. financial sectorWASHINGTON — Today, the United States is designating two individuals who are affiliates of the Russia-based ransomware group LockBit. This action is the first in an ongoing collaborative effort with the U.S. Department of Justice, Federal Bureau of Investigation, and our international partners targeting LockBit.“The United States will not tolerate attempts to extort and steal from our citizens and institutions,” said Deputy Secretary of the Treasury Wally Adeyemo. “We will continue our whole-of-government approach to defend against malicious cyber activities, and will use all available tools to hold the actors that enable these threats accountable.”\\xa0Russia continues to offer safe harbor for cybercriminals where groups such as LockBit are free to launch ransomware attacks against the United States, its allies, and partners. These ransomware attacks have targeted critical infrastr', 'twitter:image': 'https://home.treasury.gov/system/files/291/treasury-preview-1080x1080.jpg', 'twitter:image:alt': 'U.S. Department of the Treasury', 'twitter:site': '@USTreasury', 'twitter:image:width': '1080', 'viewport': 'width=device-width, initial-scale=1.0', 'mobileoptimized': 'width', 'og:url': 'https://home.treasury.gov/news/press-releases/jy2114'}], 'cse_image': [{'src': 'https://home.treasury.gov/system/files/291/treasury-preview-1900x950.jpg'}]}}\n",
            "Google Serper Search Example Result: {'title': 'United States Charges Dual Russian and Israeli National as ...', 'link': 'https://www.justice.gov/opa/pr/united-states-charges-dual-russian-and-israeli-national-developer-lockbit-ransomware-group', 'snippet': \"“Three of the individuals who we allege are responsible for LockBit's cyberattacks against thousands of victims are now in custody, and we will ...\", 'date': 'Dec 20, 2024', 'position': 1}\n",
            "Exa Search Example Result: Title: \n",
            "URL: https://twitter.com/vxunderground/status/1872807625412493614\n",
            "ID: https://twitter.com/vxunderground/status/1872807625412493614\n",
            "Score: 0.15399488806724548\n",
            "Published Date: 2024-12-28T00:51:47.000Z\n",
            "Author: vxunderground\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: .@Cloudflare Lockbit4-dot-com isn't malware. It's our website (we purchased it when Lockbit ransomware group mentioned the domain name) (we're memesters) https://t.co/BsiCiWOG8L| created_at: Sat Dec 28 00:51:47 +0000 2024 | favorite_count: 733 | quote_count: 4 | reply_count: 8 | retweet_count: 25 | is_quote_status: False | retweeted: False | lang: en\n",
            "Highlights: [\". @Cloudflare Lockbit4-dot-com isn't malware. It's our website (we purchased it when Lockbit ransomware group mentioned the domain name) (we're memesters) https://t.co/BsiCiWOG8L| created_at: Sat Dec 28 00:51:47 +0000 2024 | favorite_count: 733 | quote_count: 4 | reply_count: 8 | retweet_count: 25 | is_quote_status: False | retweeted: False | lang: en\"]\n",
            "Highlight Scores: [0.22972479462623596]\n",
            "Summary: None\n",
            "\n",
            "Error parsing RSS feed https://www.govinfosecurity.com/rss/feeds/rss: Remote end closed connection without response\n",
            "RSS Feed Search Example Result: No results\n",
            "Tavily Search Example Result: {'title': 'LockBit back online, already targeting hospitals with ransomware', 'url': 'https://cybernews.com/news/lockbit-back-online-already-targeting-hospitals-with-ransomware/', 'content': \"The LockBit ransom gang is back up since global police allege it decimated the group's infrastructure and claimed the Ernest Health hospital network as its latest victim. ... LockBit, the infamous ransomware-as-a-service cybercrime gang, is back online just days after a global police bust claimed to have decimated the group's infrastructure\", 'score': 0.7206637, 'raw_content': None}\n",
            "Please enter your phone (or bot token): 20441646\n",
            "Error in Telegram Search: The phone number is invalid (caused by SendCodeRequest)\n",
            "\n",
            "Textual URLs List:\n",
            "['https://cybersecurityventures.com/ransomware-report/', 'https://home.treasury.gov/news/press-releases/jy2114', 'https://thehackernews.com/2024/12/brazilian-hacker-charged-for-extorting.html', 'https://www.justice.gov/opa/pr/us-and-uk-disrupt-lockbit-ransomware-variant', 'https://x.com/vxunderground/status/1872807625412493614', 'https://www.justice.gov/usao-nj/pr/us-charges-dual-russian-and-israeli-national-developer-lockbit-ransomware-group', 'https://www.pcmag.com/news/lockbit-ransomware-gang-strikes-back-after-fbi-takedown-with-5-new-attacks', 'https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-165a', 'https://therecord.media/tag/lockbit', 'https://www.halcyon.ai/attacks/lockbit-ransomware-hits-five-star-products-in-major-cyber-attack', 'https://www.justice.gov/opa/pr/united-states-charges-dual-russian-and-israeli-national-developer-lockbit-ransomware-group', 'https://www.bleepingcomputer.com/tag/lockbit/', 'https://dailyhodl.com/2024/12/26/hacker-hijacks-15-accounts-on-x-launches-memecoin-scams-and-steals-500000-in-one-month-blockchain-investigator/', 'https://cybernews.com/news/lockbit-back-online-already-targeting-hospitals-with-ransomware/', 'https://www.justice.gov/opa/pr/two-foreign-nationals-plead-guilty-participating-lockbit-ransomware-group', 'https://www.cyberdaily.au/security/11181-four-lockbit-members-arrested-major-affiliated-ousted-in-latest-operation-cronos-activity', 'https://www.itpro.com/security/cyber-crime/lockbit-developer-snared-in-latest-blow-for-infamous-hacker-group', 'https://twitter.com/vxunderground/status/1872807625412493614', 'https://www.forbes.com/sites/daveywinder/2024/12/21/notorious-ransomware-gang-warns-new-attacks-incoming-on-feb-3-2025/', 'https://www.halcyon.ai/attacks/lockbit-ransomware-hits-efile-com-in-major-cyber-attack', 'https://www.theregister.com/2024/12/28/lockbit_alphv_disruptions_ransomhub_rise/', 'https://www.cbc.ca/news/world/lockbit-hackers-fbi-ransomware-cronos-1.7119651', 'https://securityscorecard.com/research/lockbit-claims-attack-against-prominent-semiconductor-firm/', 'https://www.lumiun.com/blog/en/ransomware-largest-groups-responsible-for-2024-attacks/', 'https://www.aha.org/news/headline/2024-05-09-doj-charges-russian-national-developing-operating-lockbit-ransomware']\n",
            "\n",
            "Image URLs List:\n",
            "['https://x.com/vxunderground/status/1872807625412493614', 'https://dailyhodl.com/2024/12/26/hacker-hijacks-15-accounts-on-x-launches-memecoin-scams-and-steals-500000-in-one-month-blockchain-investigator/', 'https://twitter.com/vxunderground/status/1872807625412493614', 'https://www.pcmag.com/news/lockbit-ransomware-gang-strikes-back-after-fbi-takedown-with-5-new-attacks', 'https://www.theregister.com/2024/12/28/lockbit_alphv_disruptions_ransomhub_rise/', 'https://www.cbc.ca/news/world/lockbit-hackers-fbi-ransomware-cronos-1.7119651', 'https://cybernews.com/news/lockbit-back-online-already-targeting-hospitals-with-ransomware/', 'https://www.cyberdaily.au/security/11181-four-lockbit-members-arrested-major-affiliated-ousted-in-latest-operation-cronos-activity', 'https://thehackernews.com/2024/12/brazilian-hacker-charged-for-extorting.html', 'https://www.itpro.com/security/cyber-crime/lockbit-developer-snared-in-latest-blow-for-infamous-hacker-group']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS3r64URGeDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}