{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ1syEStCIN9/kZ1sf8n/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Search_Sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jQUT-suIDN0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21165eda-9807-4b6b-df5c-cf9699b46e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.2/702.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU telethon tweepy feedparser google-api-python-client requests tavily-python exa_py python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "import os\n",
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from exa_py import Exa\n",
        "from tavily import TavilyClient\n",
        "from telethon import TelegramClient\n",
        "import tweepy\n",
        "import feedparser\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\", \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\")\n",
        "EXA_API_KEY = os.getenv(\"EXA_API_KEY\", \"953b5801-11be-4b37-a313-f8df8f37027c\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\")\n",
        "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\", \"63053004a7e2445c3\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"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\")\n",
        "\n",
        "# Telegram API Keys\n",
        "TELEGRAM_API_ID = 20441646\n",
        "TELEGRAM_API_HASH = \"d78a891287e9ba6a2a8c4bb0e4ca506a\"\n",
        "\n",
        "# Twitter API Keys\n",
        "TWITTER_API_KEY = \"WTIYHjD9r10EZKoUghhK6vqnh\"\n",
        "TWITTER_API_SECRET = \"0t4k1BiQnqa2RcZKFLUXMtka5t0BOc1F89eNWj8ee3AkNFyGRA\"\n",
        "TWITTER_ACCESS_TOKEN = \"1872197717612474368-CuATnNZBeXy7r3ymdSMnPW0MMkfVkf\"\n",
        "TWITTER_ACCESS_TOKEN_SECRET = \"wAfYioeyIpK0kpStzQFK5TxHeLCPVVwk6vtaAzjprYEFV\"\n",
        "TWITTER_BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAHMBxwEAAAAAXHty6DjTOsfJ0TXByHccMUqBK%2B0%3DYVCDqgobV2wQUr8WTAR3zVrmqsPU7PdPIQzyRMelPPYgawKEfh\""
      ],
      "metadata": {
        "id": "RfyEZQPAEBUr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SECURITY_RSS_FEEDS = [\n",
        "    \"https://www.bleepingcomputer.com/feed/\",\n",
        "    \"https://feeds.feedburner.com/TheHackersNews\",\n",
        "    \"https://krebsonsecurity.com/feed/\",\n",
        "    \"https://www.darkreading.com/rss.xml\",\n",
        "    \"https://www.securityweek.com/feed/\",\n",
        "    \"https://www.csoonline.com/feed/\",\n",
        "    \"https://www.threatpost.com/feed/\",\n",
        "    \"https://www.helpnetsecurity.com/feed/\",\n",
        "    \"https://www.infosecurity-magazine.com/rss/news/\",\n",
        "    \"https://www.cybersecurity-insiders.com/feed/\",\n",
        "    \"https://www.zdnet.com/topic/security/rss.xml\",\n",
        "    \"https://www.schneier.com/feed/atom/\",\n",
        "    \"https://www.theregister.com/security/headlines.atom\",\n",
        "    \"https://www.govinfosecurity.com/rss/feeds/rss\",\n",
        "    \"https://www.crowdstrike.com/blog/feed/\"\n",
        "]\n",
        "\n",
        "TELEGRAM_CHANNELS = [\n",
        "    'cveNotify',\n",
        "    'ctinow',\n",
        "    'CyberSecurityTechnologies',\n",
        "    'cybersecurity_outlook',\n",
        "    'cibsecurity',\n",
        "    'thehackernews',\n",
        "    'Cyber_Security_Channel',\n",
        "    'cloudandcybersecurity',\n",
        "    'androidMalware',\n",
        "    'DarkfeedNews',\n",
        "    'PentestingNews',\n",
        "    'malwr'\n",
        "]"
      ],
      "metadata": {
        "id": "F-09ZjgtAQcX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize services\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "google_service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "telegram_client = TelegramClient('session_name', TELEGRAM_API_ID, TELEGRAM_API_HASH)\n",
        "twitter_auth = tweepy.OAuth1UserHandler(TWITTER_API_KEY, TWITTER_API_SECRET, TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "twitter_api = tweepy.API(twitter_auth)"
      ],
      "metadata": {
        "id": "otVl9AaWEjDH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str] = None\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []"
      ],
      "metadata": {
        "id": "s1Pt1QYFE9h0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def google_serper_search(query: str) -> List[SearchResult]:\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\",\n",
        "        \"autocorrect\": True\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\")\n",
        "            ) for result in data.get(\"organic\", [])\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def google_programmable_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5\n",
        "        ).execute()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                date=item.get(\"pagemap\", {}).get(\"metatags\", [{}])[0].get(\"article:published_time\")\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    url = \"https://google.serper.dev/images\"\n",
        "    payload = {\n",
        "        \"q\": query,\n",
        "        \"gl\": \"us\",\n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    headers = {\n",
        "        \"X-API-KEY\": SERPER_API_KEY,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Serper Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"imageUrl\", \"No link\"),\n",
        "                media=[result.get(\"imageUrl\", \"No link\")],\n",
        "                media_content=[{\"image_url\": result.get(\"imageUrl\", \"No link\")}]\n",
        "            ) for result in data.get(\"images\", [])\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def google_programmable_image_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        search_results = google_service.cse().list(\n",
        "            q=query,\n",
        "            cx=GOOGLE_CSE_ID,\n",
        "            num=5,\n",
        "            searchType=\"image\"\n",
        "        ).execute()\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=item.get(\"title\", \"No title\"),\n",
        "                snippet=item.get(\"snippet\", \"No snippet\"),\n",
        "                url=item.get(\"link\", \"No link\"),\n",
        "                media=[item.get(\"link\", \"No link\")],\n",
        "                media_content=[{\"image_url\": item.get(\"link\", \"No link\")}]\n",
        "            ) for item in search_results.get(\"items\", [])\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def exa_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        response = exa.search_and_contents(\n",
        "            query,\n",
        "            use_autoprompt=True,\n",
        "            num_results=5,\n",
        "            text=True,\n",
        "            highlights=True\n",
        "        )\n",
        "        results = response.results\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.title,\n",
        "                snippet=result.highlights[0] if result.highlights else \"No snippet\",\n",
        "                url=result.url,\n",
        "                date=result.publishedDate if hasattr(result, 'publishedDate') else None,\n",
        "                media_content=[{\"image_url\": result.url}] if result.url else []\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def tavily_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        response = tavily_client.search(\n",
        "            query,\n",
        "            search_depth=\"advanced\",\n",
        "            include_answer=True,\n",
        "            include_raw_content=True,\n",
        "            include_images=True,\n",
        "            max_results=5\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"published_date\"),\n",
        "                media=[result.get(\"url\", \"No link\")] if result.get(\"url\") else [],\n",
        "                media_content=[{\"image_url\": result.get(\"url\", \"No link\")}] if result.get(\"url\") else []\n",
        "            ) for result in response.get(\"results\", [])\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "async def telegram_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        await telegram_client.start()\n",
        "        all_results = []\n",
        "        for channel in TELEGRAM_CHANNELS:\n",
        "            results = await telegram_client.get_messages(channel, search=query, limit=5)\n",
        "            all_results.extend([\n",
        "                SearchResult(\n",
        "                    source=f\"Telegram ({channel})\",\n",
        "                    title=result.message,\n",
        "                    snippet=result.message,\n",
        "                    url=f\"https://t.me/{channel}/{result.id}\",\n",
        "                    date=result.date.isoformat()\n",
        "                ) for result in results\n",
        "            ])\n",
        "        return all_results\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def twitter_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        tweets = twitter_api.search_tweets(q=query, count=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Twitter\",\n",
        "                title=tweet.user.name,\n",
        "                snippet=tweet.text,\n",
        "                url=f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
        "                date=tweet.created_at.isoformat()\n",
        "            ) for tweet in tweets\n",
        "        ]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def rss_feed_search(query: str) -> List[SearchResult]:\n",
        "    try:\n",
        "        all_results = []\n",
        "        for feed_url in SECURITY_RSS_FEEDS:\n",
        "            try:\n",
        "                feed = feedparser.parse(feed_url)\n",
        "                all_results.extend([\n",
        "                    SearchResult(\n",
        "                        source=f\"RSS Feed ({feed_url})\",\n",
        "                        title=entry.title,\n",
        "                        snippet=entry.get(\"summary\", \"No summary available\"),\n",
        "                        url=entry.link,\n",
        "                        date=entry.published\n",
        "                    ) for entry in feed.entries if query.lower() in entry.title.lower() or query.lower() in entry.get(\"summary\", \"\").lower()\n",
        "                ])\n",
        "            except Exception:\n",
        "                continue\n",
        "        return all_results\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def parse_date(date_str: str) -> datetime:\n",
        "    if date_str:\n",
        "        try:\n",
        "            return parser.parse(date_str).replace(tzinfo=None)  # Make all dates naive\n",
        "        except parser.ParserError:\n",
        "            if \"days ago\" in date_str:\n",
        "                days = int(date_str.split()[0])\n",
        "                return (datetime.now() - timedelta(days=days)).replace(tzinfo=None)\n",
        "            elif \"months ago\" in date_str:\n",
        "                months = int(date_str.split()[0])\n",
        "                return (datetime.now() - relativedelta(months=months)).replace(tzinfo=None)\n",
        "            elif \"years ago\" in date_str:\n",
        "                years = int(date_str.split()[0])\n",
        "                return (datetime.now() - relativedelta(years=years)).replace(tzinfo=None)\n",
        "    return datetime.min\n",
        "\n",
        "def filter_by_date(results: List[SearchResult], days: int = 7) -> List[SearchResult]:\n",
        "    cutoff_date = datetime.now() - timedelta(days=days)\n",
        "    return [result for result in results if result.date and parse_date(result.date) >= cutoff_date]\n",
        "\n",
        "def filter_by_domain(results: List[SearchResult], include_domains: List[str] = None, exclude_domains: List[str] = None) -> List[SearchResult]:\n",
        "    if include_domains:\n",
        "        results = [result for result in results if any(domain in result.url for domain in include_domains)]\n",
        "    if exclude_domains:\n",
        "        results = [result for result in results if not any(domain in result.url for domain in exclude_domains)]\n",
        "    return results"
      ],
      "metadata": {
        "id": "z6w_36f9FRIV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_search_results(*args: List[SearchResult]) -> List[SearchResult]:\n",
        "    all_results = []\n",
        "    media_content = []\n",
        "\n",
        "    for results in args:\n",
        "        all_results.extend(results)\n",
        "        media_content.extend([media for result in results for media in result.media_content])\n",
        "\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort results by date (most recent first)\n",
        "    unique_results.sort(key=lambda x: parse_date(x.date) if x.date else datetime.min, reverse=True)\n",
        "\n",
        "    return unique_results, media_content"
      ],
      "metadata": {
        "id": "upLiGPDEGHcy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def execute_searches(query: str, search_type: str = \"normal\", days: int = 7, include_domains: List[str] = None, exclude_domains: List[str] = None) -> Dict[str, Any]:\n",
        "    search_functions = [\n",
        "        google_serper_search,\n",
        "        google_programmable_search,\n",
        "        exa_search,\n",
        "        tavily_search,\n",
        "        google_serper_image_search if search_type == \"image\" else None,\n",
        "        google_programmable_image_search if search_type == \"image\" else None,\n",
        "        telegram_search,\n",
        "        lambda q: twitter_search(q),\n",
        "        lambda q: rss_feed_search(q)\n",
        "    ]\n",
        "    search_functions = [func for func in search_functions if func is not None]  # Remove None values\n",
        "\n",
        "    search_tasks = [asyncio.to_thread(func, query) if callable(func) and not asyncio.iscoroutinefunction(func) else func(query) for func in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if not isinstance(results, Exception):\n",
        "            successful_results.append(results)\n",
        "\n",
        "    combined_results, media_content = aggregate_search_results(*successful_results)\n",
        "    combined_results = filter_by_date(combined_results, days=days)\n",
        "    combined_results = filter_by_domain(combined_results, include_domains=include_domains, exclude_domains=exclude_domains)\n",
        "\n",
        "    urls = [result.url for result in combined_results]\n",
        "\n",
        "    return {\n",
        "        \"results\": combined_results,\n",
        "        \"urls\": urls,\n",
        "        \"media_content\": media_content\n",
        "    }"
      ],
      "metadata": {
        "id": "nFwcBz2JuYD4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    query = \"Latest Cyber Incidents by Lockbit Ransomware Group?\"\n",
        "    results = asyncio.run(execute_searches(query, search_type=\"normal\", days=7, include_domains=[\"bleepingcomputer.com\"], exclude_domains=[\"twitter.com\"]))\n",
        "\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for result in results[\"results\"]:\n",
        "        print(f\"\\nTitle: {result.title}\")\n",
        "        print(f\"Source: {result.source}\")\n",
        "        print(f\"URL: {result.url}\")\n",
        "        print(f\"Snippet: {result.snippet}\")\n",
        "        if result.media_content:\n",
        "            print(f\"Media Content: {result.media_content}\")\n",
        "\n",
        "    print(\"\\nURLs List:\")\n",
        "    print(results[\"urls\"])\n",
        "\n",
        "    print(\"\\nMedia Content List:\")\n",
        "    print(results[\"media_content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih6PPHvQGPfs",
        "outputId": "67ddf497-961c-4368-dce5-481bd459ee69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your phone (or bot token): +91 8755714681\n",
            "Please enter the code you received: 64191\n",
            "Signed in successfully as Sentdex; remember to not break the ToS or you will risk an account ban!\n",
            "\n",
            "Search Results:\n",
            "\n",
            "URLs List:\n",
            "[]\n",
            "\n",
            "Media Content List:\n",
            "[{'image_url': 'https://dailyhodl.com/2024/12/26/hacker-hijacks-15-accounts-on-x-launches-memecoin-scams-and-steals-500000-in-one-month-blockchain-investigator/'}, {'image_url': 'https://thehackernews.com/2024/12/brazilian-hacker-charged-for-extorting.html'}, {'image_url': 'https://www.wired.com/story/worst-hacks-2024/'}, {'image_url': 'https://securityaffairs.com/172333/cyber-crime/pittsburgh-regional-transit-ransomware-attack.html'}, {'image_url': 'https://apnews.com/article/japan-jal-cyberattack-flights-travel-04fbd4848f3015a77057339a5c90ca32'}, {'image_url': 'https://thecyberexpress.com/lockbit-seizure-nca-operation-cronos/'}, {'image_url': 'https://www.wired.com/story/lockbit-ransomware-takedown-website-nca-fbi/'}, {'image_url': 'https://www.cyberdaily.au/security/11181-four-lockbit-members-arrested-major-affiliated-ousted-in-latest-operation-cronos-activity'}, {'image_url': 'https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-165a'}, {'image_url': 'https://www.pcmag.com/news/lockbit-ransomware-gang-strikes-back-after-fbi-takedown-with-5-new-attacks'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS3r64URGeDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}