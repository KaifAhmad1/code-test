{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdh0WGYBy6t6nEbiuz8Uhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Search_Sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jQUT-suIDN0z",
        "outputId": "afc0e0a1-9af5-4d20-ef39-444683e02935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet sentence-transformers langchain langchain-groq langchain-pinecone langchain_cohere\n",
        "!pip install --quiet -U \"langchain-community>=0.2.16\" langchain-exa langchain-google-community goose3 crawl4ai[all]\n",
        "!pip install --upgrade --quiet faiss-cpu langchain_cohere\n",
        "!pip install -qU langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "import os\n",
        "import getpass\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from exa_py import Exa\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# API Keys (hidden for security purposes)\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"8e15b925-3b96-497d-b20a-08d308782b83\")\n",
        "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
        "ASKNEWS_CLIENT_ID = os.getenv(\"ASKNEWS_CLIENT_ID\", \"a0de4609-b760-4c83-9609-5c04d7743b84\")\n",
        "ASKNEWS_CLIENT_SECRET = os.getenv(\"ASKNEWS_CLIENT_SECRET\", \"D5Mlhkztk4TcW24diUgcW0FA2w\")\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\", \"d8e815ef6caa94dbef7b977a0ea7d505b43a5a06\")\n",
        "EXA_API_KEY = os.getenv(\"EXA_API_KEY\", \"953b5801-11be-4b37-a313-f8df8f37027c\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyBIQo9X6acoBazBfte9jF9Pl0QEZ9oe8pk\")\n",
        "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\", \"63053004a7e2445c3\")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"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\")\n",
        "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\", \"fc-9c7bf92d1db44ae1a34f9dc56a6031e6\")\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\", \"7e9js19mjC1pb3dNHKg012u6J9LRl8614KFL4ZmL\")\n",
        "\n",
        "# Set environment variables for Search Tools\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"ASKNEWS_CLIENT_ID\"] = ASKNEWS_CLIENT_ID\n",
        "os.environ[\"ASKNEWS_CLIENT_SECRET\"] = ASKNEWS_CLIENT_SECRET\n",
        "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
        "os.environ[\"EXA_API_KEY\"] = EXA_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID\n",
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
        "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfyEZQPAEBUr",
        "outputId": "14bec63d-76f5-4b6c-804d-3025b9ca2018"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.2-3b-preview\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize the embeddings with advanced BGE model\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Initialize Pinecone and vector store\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "pinecone_index = pc.Index(\"new-cyber-search\")\n",
        "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)\n",
        "\n",
        "# Initialize search tools\n",
        "google_serper = GoogleSerperAPIWrapper()\n",
        "tavily_search = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        "    include_images=True,\n",
        ")\n",
        "google_search = GoogleSearchAPIWrapper()\n",
        "exa = Exa(api_key=EXA_API_KEY)\n",
        "\n",
        "# Define the retriever\n",
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "otVl9AaWEjDH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "class SearchResult(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    url: str\n",
        "    date: Optional[str]\n",
        "    media: Optional[List[str]] = []\n",
        "    media_content: Optional[List[Dict[str, str]]] = []\n",
        "    links: Optional[List[str]] = []\n",
        "    source_weight: Optional[float] = None\n",
        "    source_name: Optional[str] = None\n",
        "    final_score: Optional[float] = None\n",
        "    metadata: Optional[Dict[str, Any]] = {}\n",
        "\n",
        "def parse_date(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "s1Pt1QYFE9h0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(query: str, vector_store: PineconeVectorStore) -> List[SearchResult]:\n",
        "    print(f\"Performing vector search with query: {query}\")\n",
        "    results = vector_store.similarity_search(query, k=5)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Vector Search\",\n",
        "            title=f\"Result {i+1}\",\n",
        "            snippet=doc.page_content,\n",
        "            url=doc.metadata.get(\"source\", \"No URL\"),\n",
        "            date=doc.metadata.get(\"date\"),\n",
        "            metadata=doc.metadata\n",
        "        ) for i, doc in enumerate(results)\n",
        "    ]\n",
        "\n",
        "def google_serper_search(query: str, google_serper: GoogleSerperAPIWrapper) -> List[SearchResult]:\n",
        "    print(f\"Performing Google Serper search with query: {query}\")\n",
        "    results = google_serper.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"link\", \"No link\"),\n",
        "            date=result.get(\"date\"),\n",
        "            metadata={\n",
        "                \"author\": result.get(\"author\"),\n",
        "                \"location\": result.get(\"location\")\n",
        "            }\n",
        "        ) for result in results.get(\"organic\", [])\n",
        "    ]\n",
        "\n",
        "def exa_search(query: str, exa: Exa) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"Starting Exa Search with query: {query}\")\n",
        "        response = exa.search_and_contents(\n",
        "            query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
        "        )\n",
        "        print(f\"Raw results from Exa Search: {response}\")\n",
        "\n",
        "        results = response.results  # Extract the list of results from the SearchResponse object\n",
        "\n",
        "        search_results = [\n",
        "            SearchResult(\n",
        "                source=\"Exa Search\",\n",
        "                title=result.title,  # Access the attributes directly\n",
        "                snippet=result.snippet,\n",
        "                url=result.link,\n",
        "                date=result.published_date,\n",
        "                metadata={\n",
        "                    \"author\": result.author,\n",
        "                    \"location\": result.location\n",
        "                },\n",
        "                media_content=[{\"image_url\": result.image}] if result.image else []\n",
        "            ) for result in results\n",
        "        ]\n",
        "\n",
        "        print(f\"Processed Exa Search results: {search_results}\")\n",
        "        return search_results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Exa Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def tavily_search(query: str, tavily_search: TavilySearchResults) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"Performing Tavily search with query: {query}\")\n",
        "        results = tavily_search.search({\"query\": query})  # Use the correct method to perform the search\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Tavily Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"content\", \"No snippet\"),\n",
        "                url=result.get(\"url\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Tavily Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def google_programmable_search(query: str, google_search: GoogleSearchAPIWrapper) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"Performing Google Programmable search with query: {query}\")\n",
        "        results = google_search.results(query, num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=result.get(\"date\"),\n",
        "                metadata={\n",
        "                    \"author\": result.get(\"author\"),\n",
        "                    \"location\": result.get(\"location\")\n",
        "                }\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def google_serper_image_search(query: str) -> List[SearchResult]:\n",
        "    print(f\"Performing Google Serper Image search with query: {query}\")\n",
        "    search_images = GoogleSerperAPIWrapper(type=\"images\")\n",
        "    results_images = search_images.results(query)\n",
        "    return [\n",
        "        SearchResult(\n",
        "            source=\"Google Serper Image Search\",\n",
        "            title=result.get(\"title\", \"No title\"),\n",
        "            snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "            url=result.get(\"imageUrl\", \"No link\"),\n",
        "            date=None,\n",
        "            media=[result.get(\"imageUrl\", \"No link\")],\n",
        "            media_content=[{\"image_url\": result.get(\"imageUrl\", \"No link\")}]\n",
        "        ) for result in results_images.get(\"images\", [])\n",
        "    ]\n",
        "\n",
        "def google_programmable_image_search(query: str, google_search: GoogleSearchAPIWrapper) -> List[SearchResult]:\n",
        "    try:\n",
        "        print(f\"Performing Google Programmable Image search with query: {query}\")\n",
        "        results = google_search.results(query + \" image\", num_results=5)\n",
        "        return [\n",
        "            SearchResult(\n",
        "                source=\"Google Programmable Image Search\",\n",
        "                title=result.get(\"title\", \"No title\"),\n",
        "                snippet=result.get(\"snippet\", \"No snippet\"),\n",
        "                url=result.get(\"link\", \"No link\"),\n",
        "                date=None,\n",
        "                media=[result.get(\"link\", \"No link\")],\n",
        "                media_content=[{\"image_url\": result.get(\"link\", \"No link\")}]\n",
        "            ) for result in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in Google Programmable Image Search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def aggregate_search_results(query: str, *args: List[SearchResult]) -> List[SearchResult]:\n",
        "    all_results = []\n",
        "    media_content = []\n",
        "    sources = ['vector', 'serper', 'exa', 'tavily', 'google', 'google_serper_image', 'google_programmable_image']\n",
        "    weights = [0.6, 1.0, 0.9, 0.85, 0.8, 0.75, 0.7]  # Adjusted weights to prioritize Google Serper, Google Programmable Search, Exa.ai, and Tavily\n",
        "\n",
        "    for results, source, weight in zip(args, sources, weights):\n",
        "        all_results.extend([(result, source, weight, result.source_weight or 0, parse_date(result.date)) for result in results])\n",
        "        media_content.extend([media for result in results for media in result.media_content])\n",
        "\n",
        "    seen_urls = set()\n",
        "    unique_results = []\n",
        "\n",
        "    for result, source, weight, source_weight, date in all_results:\n",
        "        if result.url not in seen_urls:\n",
        "            seen_urls.add(result.url)\n",
        "            result.source_weight = source_weight\n",
        "            result.source_name = source\n",
        "            date_score = calculate_recency_score(date)\n",
        "            final_score = weight + source_weight + date_score\n",
        "            result.final_score = final_score\n",
        "            unique_results.append(result)\n",
        "\n",
        "    unique_results.sort(reverse=True, key=lambda x: x.final_score)\n",
        "    return unique_results, media_content\n",
        "\n",
        "def calculate_recency_score(date: Optional[datetime]) -> float:\n",
        "    if date is None:\n",
        "        return 0.0\n",
        "    current_date = datetime.now(pytz.utc)\n",
        "    days_old = (current_date - date).days\n",
        "    if days_old < 0:  # Future date\n",
        "        return 0.0\n",
        "    return 0.9 ** days_old  # Exponential decay with base 0.9"
      ],
      "metadata": {
        "id": "z6w_36f9FRIV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def execute_searches(query: str, tools: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    print(f\"Executing searches for query: {query}\")\n",
        "\n",
        "    # Execute all searches in parallel\n",
        "    search_functions = [\n",
        "        (google_serper_search, tools[\"google_serper\"]),\n",
        "        (google_programmable_search, tools[\"google_search\"]),\n",
        "        (exa_search, tools[\"exa\"]),\n",
        "        (tavily_search, tools[\"tavily_search\"]),\n",
        "        (vector_search, tools[\"vector_store\"]),\n",
        "        (google_serper_image_search, None),\n",
        "        (google_programmable_image_search, tools[\"google_search\"])\n",
        "    ]\n",
        "    search_tasks = [asyncio.to_thread(func, query, tool) if tool else asyncio.to_thread(func, query) for func, tool in search_functions]\n",
        "    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "\n",
        "    # Handle exceptions and filter out failed searches\n",
        "    successful_results = []\n",
        "    for results in search_results:\n",
        "        if isinstance(results, Exception):\n",
        "            print(f\"ERROR in search: {str(results)}\")\n",
        "        else:\n",
        "            successful_results.append(results)\n",
        "\n",
        "    # Aggregate and deduplicate results with metadata scoring\n",
        "    combined_results, media_content = aggregate_search_results(query, *successful_results)\n",
        "\n",
        "    # Extract URLs from the combined results\n",
        "    urls = [result.url for result in combined_results]\n",
        "\n",
        "    return {\n",
        "        \"results\": combined_results,\n",
        "        \"urls\": urls,\n",
        "        \"media_content\": media_content\n",
        "    }\n",
        "\n",
        "def initialize_api_keys():\n",
        "    # This function is already handled by setting environment variables directly\n",
        "    pass\n",
        "\n",
        "def initialize_models_and_tools():\n",
        "    return {\n",
        "        \"google_serper\": google_serper,\n",
        "        \"google_search\": google_search,\n",
        "        \"exa\": exa,\n",
        "        \"tavily_search\": tavily_search,\n",
        "        \"vector_store\": vector_store\n",
        "    }\n",
        "\n",
        "# Define a model for MITRE ATT&CK data\n",
        "class MitreAttackTechnique(BaseModel):\n",
        "    id: str\n",
        "    name: str\n",
        "    description: str\n",
        "    url: str\n",
        "    tactics: List[str]\n",
        "\n",
        "# Function to fetch MITRE ATT&CK data\n",
        "def fetch_mitre_attack_data() -> Dict[str, MitreAttackTechnique]:\n",
        "    base_url = \"https://attack.mitre.org\"\n",
        "    techniques_url = f\"{base_url}/api/v1/techniques\"\n",
        "    tactics_url = f\"{base_url}/api/v1/tactics\"\n",
        "\n",
        "    # Fetch techniques\n",
        "    techniques_response = requests.get(techniques_url)\n",
        "    techniques_data = techniques_response.json()\n",
        "\n",
        "    # Fetch tactics\n",
        "    tactics_response = requests.get(tactics_url)\n",
        "    tactics_data = tactics_response.json()\n",
        "\n",
        "    # Map tactics to techniques\n",
        "    tactics_map = {tactic['id']: tactic['name'] for tactic in tactics_data}\n",
        "\n",
        "    techniques = {}\n",
        "    for technique in techniques_data:\n",
        "        tactics = [tactics_map[tactic_id] for tactic_id in technique['tactics']]\n",
        "        techniques[technique['id']] = MitreAttackTechnique(\n",
        "            id=technique['id'],\n",
        "            name=technique['name'],\n",
        "            description=technique['description'],\n",
        "            url=f\"{base_url}/techniques/{technique['id']}\",\n",
        "            tactics=tactics\n",
        "        )\n",
        "\n",
        "    return techniques\n",
        "\n",
        "# Function to map search results to MITRE ATT&CK techniques\n",
        "def map_to_mitre_attack(search_results: List[SearchResult], mitre_data: Dict[str, MitreAttackTechnique]) -> List[SearchResult]:\n",
        "    for result in search_results:\n",
        "        # Example: Check if the snippet contains any MITRE ATT&CK technique IDs\n",
        "        for technique_id, technique in mitre_data.items():\n",
        "            if technique_id in result.snippet:\n",
        "                result.metadata['mitre_attack'] = technique\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "upLiGPDEGHcy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to execute the pipeline\n",
        "def main():\n",
        "    initialize_api_keys()\n",
        "    tools = initialize_models_and_tools()\n",
        "\n",
        "    query = \"Latest Cyber Incidents by Lockbit Ransomware Group?\"\n",
        "    results = asyncio.run(execute_searches(query, tools))\n",
        "\n",
        "    # Fetch MITRE ATT&CK data\n",
        "    mitre_data = fetch_mitre_attack_data()\n",
        "\n",
        "    # Map search results to MITRE ATT&CK techniques\n",
        "    enriched_results = map_to_mitre_attack(results[\"results\"], mitre_data)\n",
        "\n",
        "    print(\"Search Results:\")\n",
        "    for result in enriched_results:\n",
        "        print(f\"Title: {result.title}, URL: {result.url}\")\n",
        "        if 'mitre_attack' in result.metadata:\n",
        "            mitre_technique = result.metadata['mitre_attack']\n",
        "            print(f\"MITRE ATT&CK Technique: {mitre_technique.name}, URL: {mitre_technique.url}\")\n",
        "        if result.media_content:\n",
        "            print(f\"Media Content: {result.media_content}\")\n",
        "\n",
        "    print(\"URLs List:\")\n",
        "    print(results[\"urls\"])\n",
        "\n",
        "    print(\"Media Content List:\")\n",
        "    print(results[\"media_content\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ih6PPHvQGPfs",
        "outputId": "e4747116-0801-49d6-ff76-0530c2e347e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing searches for query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "Performing Google Serper search with query: Latest Cyber Incidents by Lockbit Ransomware Group?Performing Google Programmable search with query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "\n",
            "Starting Exa Search with query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "Performing Tavily search with query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "ERROR in Tavily Search: 'function' object has no attribute 'search'\n",
            "Performing vector search with query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "Performing Google Serper Image search with query: Latest Cyber Incidents by Lockbit Ransomware Group?Performing Google Programmable Image search with query: Latest Cyber Incidents by Lockbit Ransomware Group?\n",
            "\n",
            "Raw results from Exa Search: Title: 240,000 Credit Union Customers Exposed As Hackers Access Trove of Sensitive Data: Report\n",
            "URL: https://dailyhodl.com/2024/12/22/240000-credit-union-customers-exposed-as-hackers-access-trove-of-sensitive-data-report/\n",
            "ID: https://dailyhodl.com/2024/12/22/240000-credit-union-customers-exposed-as-hackers-access-trove-of-sensitive-data-report/\n",
            "Score: 0.1454087793827057\n",
            "Published Date: 2024-12-22T00:00:00.000Z\n",
            "Author: Alex Richardson\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: A South Carolina credit union says 240,000 customers have been exposed in an apparent cyberattack. SRP Federal Credit Union, one of the largest in the state with over $1.6 billion in assets as of 2022, filed breach notifications with regulators in Maine and Texas on December 13th, The Record reports . A law enforcement investigation discovered hackers had accessed the bank’s internal systems multiple times between September 5th and November 4th, 2024. SRP says the hackers “potentially acquired certain files from our network during that time.” While SRP hasn’t publicly stated what specific information was stolen from them, The Record reports that the credit union told Texas regulators that Social Security numbers, driver’s license numbers, dates of birth and financial information like account numbers as well as credit or debit card numbers are compromised. According to cyber security management and threat analysis firm Hackmanac, a well-known ransomware group called Nitrogen took credit for the attack, which claimed to also have customers’ full names and credit ratings. “Nitrogen ransomware group claims to have breached SRP Federal Credit Union. Allegedly, 650 GB of confidential customer data, including full names, SSNs, DOBs, addresses, account numbers, and credit ratings, were exfiltrated.” Source: Hackmanac/X Despite Nitrogen’s claims, SRP has not said whether it was a ransomware attack, and reportedly told regulators that the hack didn’t impact its online banking system or core processing systems. Earlier this year, Nitrogen took credit for six other victims , including five US firms and one Canadian company, according to Hackmanac. Several law firms are now investigating claims from SRP customers in regard to the breach. Disclaimer: Opinions expressed at The Daily Hodl are not investment advice. Investors should do their due diligence before making any high-risk investments in Bitcoin, cryptocurrency or digital assets. Please be advised that your transfers and trades are at your own risk, and any losses you may incur are your responsibility. The Daily Hodl does not recommend the buying or selling of any cryptocurrencies or digital assets, nor is The Daily Hodl an investment advisor. Please note that The Daily Hodl participates in affiliate marketing.\n",
            "Highlights: ['Several law firms are now investigating claims from SRP customers in regard to the breach. Disclaimer: Opinions expressed at The Daily Hodl are not investment advice. Investors should do their due diligence before making any high-risk investments in Bitcoin, cryptocurrency or digital assets. Please be advised that your transfers and trades are at your own risk, and any losses you may incur are your responsibility. The Daily Hodl does not recommend the buying or selling of any cryptocurrencies or digital assets, nor is The Daily Hodl an investment advisor.']\n",
            "Highlight Scores: [0.4533884823322296]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: US charged Dual Russian and Israeli National as LockBit Ransomware developer\n",
            "URL: https://securityaffairs.com/172201/uncategorized/us-authorities-charged-lockbit-ransomware-developer.html\n",
            "ID: https://securityaffairs.com/172201/uncategorized/us-authorities-charged-lockbit-ransomware-developer.html\n",
            "Score: 0.1356431543827057\n",
            "Published Date: 2024-12-22T08:52:05.000Z\n",
            "Author: Pierluigi Paganini\n",
            "Image: https://securityaffairs.com/wp-content/uploads/2024/10/image-1.png\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Pierluigi Paganini December 22, 2024 US authorities charged a dual Russian and Israeli national for being a developer of the LockBit ransomware group. Rostislav Panev, 51, a dual Russian-Israeli national, was charged as a LockBit ransomware developer. Arrested in Israel, he awaits extradition to the U.S. Panev was arrested in Israel in August and is awaiting extradition to the U.S. on criminal charges. The man is accused of being a LockBit ransomware developer from 2019 through at least February 2024. The leading ransomware group attacked over 2,500 victims worldwide, including 1,800 in the United States, extracted $500M in ransoms, and caused billions in damages. The list of victims included individuals, small businesses, and multinational corporations. The ransomware gang and its affiliates targeted hospitals, schools, nonprofit organizations, critical infrastructure, and government and law-enforcement agencies. Panev and other developers were tasked to create and maintain the malware and infrastructure, while affiliates executed attacks and extorted ransoms, splitting the proceeds. “As alleged in the superseding complaint, at the time of Panev’s arrest in Israel in August, law enforcement discovered on Panev’s computer administrator credentials for an online repository that was hosted on the dark web and stored source code for multiple versions of the LockBit builder, which allowed LockBit’s affiliates to generate custom builds of the LockBit ransomware malware for particular victims. On that repository, law enforcement also discovered source code for LockBit’s StealBit tool, which helped LockBit affiliates exfiltrate data stolen through LockBit attacks.” reads the press release published by DoJ. “Law enforcement also discovered access credentials for the LockBit control panel, an online dashboard maintained by LockBit developers for LockBit’s affiliates and hosted by those developers on the dark web. The complaint alleges Panev communicated with LockBit’s leader, Dmitry Khoroshev , about developing ransomware tools. Panev received over $230,000 in laundered cryptocurrency from Khoroshev between 2022 and 2024. After his August arrest, Panev admitted to having had a role in coding, developing, and consulting for the LockBit group. He developed the code to disable antivirus software, deploy malware, and print ransom notes to all printers connected to a victim network. A $10 million reward was offered for information on Khoroshev through the U.S. State Department’s TOC Rewards Program via the FBI tip website. Khoroshev and other three members of the gang (Matveev, Sungatov, and Kondratyev) were sanctioned by the U.S. Treasury’s OFAC for their involvement in cyberattacks. Seven LockBit members have been charged in New Jersey. Panev and Khoroshev face charges; Vasiliev and Astamirov pleaded guilty and await sentencing. Sungatov , Kondratyev , and Matveev , also indicted, remain at large. Matveev has a $10M U.S. reward for information leading to his arrest. “As alleged by the complaint, Rostislav Panev for years built and maintained the digital weapons that enabled his LockBit coconspirators to wreak havoc and cause billions of dollars in damage around the world,” said U.S. Attorney Philip R. Sellinger for the District of New Jersey. “But just like the six other LockBit members previously identified and charged by this office and our FBI and Criminal Division partners, Panev could not remain anonymous and avoid justice indefinitely. He must now answer for his crimes. Today’s announcement represents another blow struck by the United States and our international partners against the LockBit organization, and our efforts will continue relentlessly until the group is fully dismantled and its members brought to justice.” Follow me on Twitter: @securityaffairs and Facebook and Mastodon Pierluigi Paganini ( SecurityAffairs – hacking, ransomware)\n",
            "Highlights: ['The man is accused of being a LockBit ransomware developer from 2019 through at least February 2024. The leading ransomware group attacked over 2,500 victims worldwide, including 1,800 in the United States, extracted $500M in ransoms, and caused billions in damages. The list of victims included individuals, small businesses, and multinational corporations. The ransomware gang and its affiliates targeted hospitals, schools, nonprofit organizations, critical infrastructure, and government and law-enforcement agencies. Panev and other developers were tasked to create and maintain the malware and infrastructure, while affiliates executed attacks and extorted ransoms, splitting the proceeds.']\n",
            "Highlight Scores: [0.46989062428474426]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: \n",
            "URL: https://twitter.com/jonst0kes/status/1870634500935544926\n",
            "ID: https://twitter.com/jonst0kes/status/1870634500935544926\n",
            "Score: 0.1193203255534172\n",
            "Published Date: 2024-12-22T00:56:34.000Z\n",
            "Author: jonst0kes\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Ok, this has been sorted. I just got off the phone with @haseeb and as luck would have it this is /not/ a hack but he is also the founder of a mobile security startup so he was quickly able to figure out exactly what happened....| created_at: Sun Dec 22 00:56:34 +0000 2024 | favorite_count: 24 | quote_count: 0 | reply_count: 1 | retweet_count: 1 | is_quote_status: False | retweeted: False | lang: en\n",
            "Highlights: ['Ok, this has been sorted. I just got off the phone with @haseeb and as luck would have it this is /not/ a hack but he is also the founder of a mobile security startup so he was quickly able to figure out exactly what happened....| created_at: Sun Dec 22 00:56:34 +0000 2024 | favorite_count: 24 | quote_count: 0 | reply_count: 1 | retweet_count: 1 | is_quote_status: False | retweeted: False | lang: en']\n",
            "Highlight Scores: [0.19561660289764404]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: \n",
            "URL: https://twitter.com/BowTiedIguana/status/1870863864109355209\n",
            "ID: https://twitter.com/BowTiedIguana/status/1870863864109355209\n",
            "Score: 0.10991285741329193\n",
            "Published Date: 2024-12-22T16:07:58.000Z\n",
            "Author: BowTiedIguana\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: Somebody else in my DMs got hacked, lost all their crypto. This time the likely attack vector was putting their seed phrase in a cloud based password manager (Lastpass I believe) Will have more security guides for you guys in the new year, it's getting dangerous out there...| created_at: Sun Dec 22 16:07:58 +0000 2024 | favorite_count: 61 | quote_count: 0 | reply_count: 4 | retweet_count: 3 | is_quote_status: False | retweeted: False | lang: en\n",
            "Highlights: [\"Somebody else in my DMs got hacked, lost all their crypto. This time the likely attack vector was putting their seed phrase in a cloud based password manager (Lastpass I believe) Will have more security guides for you guys in the new year, it's getting dangerous out there...| created_at: Sun Dec 22 16:07:58 +0000 2024 | favorite_count: 61 | quote_count: 0 | reply_count: 4 | retweet_count: 3 | is_quote_status: False | retweeted: False | lang: en\"]\n",
            "Highlight Scores: [0.21810729801654816]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Title: SECURITY AFFAIRS MALWARE NEWSLETTER – ROUND 25\n",
            "URL: https://securityaffairs.com/172214/breaking-news/security-affairs-malware-newsletter-round-25.html\n",
            "ID: https://securityaffairs.com/172214/breaking-news/security-affairs-malware-newsletter-round-25.html\n",
            "Score: 0.10898243635892868\n",
            "Published Date: 2024-12-22T00:00:00.000Z\n",
            "Author: Pierluigi Paganini\n",
            "Image: None\n",
            "Extras None\n",
            "Subpages: None\n",
            "Text: \n",
            "Highlights: ['']\n",
            "Highlight Scores: [0.13114508986473083]\n",
            "Summary: None\n",
            "\n",
            "\n",
            "Autoprompt String: Heres the latest cyber incident involving the Lockbit ransomware group:\n",
            "Resolved Search Type: 2024-12-22T06:47:08.474Z\n",
            "ERROR in Exa Search: 'Result' object has no attribute 'snippet'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 2 column 1 (char 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a899b015a52b>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-a899b015a52b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Fetch MITRE ATT&CK data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmitre_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_mitre_attack_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Map search results to MITRE ATT&CK techniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cf46d000687d>\u001b[0m in \u001b[0;36mfetch_mitre_attack_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Fetch techniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtechniques_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtechniques_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtechniques_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtechniques_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Fetch tactics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS3r64URGeDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}