{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdwLRJ6aOv0Pw3ejS+32UZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TEMrzAzRW_w4"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "%pip install -q langchain langchain-community langgraph torch transformers opencv-python librosa numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import Graph, END\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForVideoClassification,\n",
        "    AutoModelForAudioClassification,\n",
        "    CLIPProcessor,\n",
        "    CLIPModel\n",
        ")\n",
        "import cv2\n",
        "import librosa\n",
        "import json"
      ],
      "metadata": {
        "id": "JNL83H4CUsgb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup\n",
        "def setup_environment():\n",
        "    \"\"\"Setup necessary environment variables and configurations\"\"\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
        "\n",
        "    # Configure device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return device"
      ],
      "metadata": {
        "id": "JVJJvhKNU27U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing functions\n",
        "def preprocess_video(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extract frames and audio from video\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    # Extract audio\n",
        "    y, sr = librosa.load(video_path)\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": (y, sr),\n",
        "        \"metadata\": extract_metadata(video_path)\n",
        "    }\n",
        "\n",
        "def extract_metadata(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extract metadata from media file\"\"\"\n",
        "    metadata = {}\n",
        "    try:\n",
        "        # Extract EXIF data\n",
        "        metadata[\"file_info\"] = os.stat(file_path)\n",
        "        # Add more metadata extraction as needed\n",
        "    except Exception as e:\n",
        "        metadata[\"error\"] = str(e)\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "D4JBongFVeob"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual Analysis Agents\n",
        "def spatial_inconsistency_agent(frames: List[np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"Detect spatial inconsistencies in frames\"\"\"\n",
        "    model = AutoModelForVideoClassification.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "\n",
        "    results = []\n",
        "    for frame in frames:\n",
        "        inputs = processor(images=frame, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        results.append(outputs.logits.softmax(dim=1))\n",
        "\n",
        "    return {\"spatial_confidence\": float(np.mean([r[0].item() for r in results]))}\n",
        "\n",
        "def temporal_coherence_agent(frames: List[np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"Analyze temporal coherence between frames\"\"\"\n",
        "    # Implementation using TimeSformer or similar\n",
        "    return {\"temporal_confidence\": 0.85}  # Placeholder"
      ],
      "metadata": {
        "id": "2GqkOYLmWF9y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Analysis Agents\n",
        "def audio_analysis_agent(audio_data: tuple) -> Dict[str, float]:\n",
        "    \"\"\"Analyze audio for signs of manipulation\"\"\"\n",
        "    y, sr = audio_data\n",
        "\n",
        "    # Spectral analysis\n",
        "    spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "\n",
        "    # Implement more sophisticated audio analysis here\n",
        "    return {\"audio_confidence\": 0.75}  # Placeholder"
      ],
      "metadata": {
        "id": "-eiewMRcWVpV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Analysis Agents\n",
        "def setup_llm_agent():\n",
        "    \"\"\"Setup LLM agent with LangChain\"\"\"\n",
        "    llm = ChatOpenAI(model=\"gpt-4-vision-preview\")\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Analyze the following media for signs of manipulation.\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    return llm, prompt\n",
        "\n",
        "@tool\n",
        "def semantic_analysis_agent(input_data: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Perform semantic analysis using LLMs\"\"\"\n",
        "    llm, prompt = setup_llm_agent()\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "    result = chain.invoke({\"input\": str(input_data)})\n",
        "\n",
        "    return {\"semantic_confidence\": result.get(\"confidence\", 0.0)}"
      ],
      "metadata": {
        "id": "chodssfUWaQw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integration and Decision Making\n",
        "def feature_fusion(results: List[Dict[str, float]]) -> Dict[str, float]:\n",
        "    \"\"\"Fuse results from different agents\"\"\"\n",
        "    confidences = []\n",
        "    weights = []\n",
        "\n",
        "    for result in results:\n",
        "        for key, value in result.items():\n",
        "            if \"confidence\" in key:\n",
        "                confidences.append(value)\n",
        "                weights.append(1.0)  # Can be adjusted based on agent reliability\n",
        "\n",
        "    weighted_avg = np.average(confidences, weights=weights)\n",
        "    return {\"final_confidence\": float(weighted_avg)}\n",
        "\n",
        "def make_decision(fusion_result: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Make final decision based on fused results\"\"\"\n",
        "    confidence = fusion_result[\"final_confidence\"]\n",
        "    threshold = 0.7  # Adjustable threshold\n",
        "\n",
        "    return {\n",
        "        \"is_fake\": confidence < threshold,\n",
        "        \"confidence\": confidence,\n",
        "        \"explanation\": f\"Detection confidence: {confidence:.2f}\"\n",
        "    }"
      ],
      "metadata": {
        "id": "PU9Qd2DrWgHk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "def create_detection_graph():\n",
        "    \"\"\"Create LangGraph workflow\"\"\"\n",
        "    workflow = Graph()\n",
        "\n",
        "    @workflow.node(\"preprocess\")\n",
        "    def preprocess(state):\n",
        "        video_data = preprocess_video(state[\"input_path\"])\n",
        "        return {\"video_data\": video_data}\n",
        "\n",
        "    @workflow.node(\"visual_analysis\")\n",
        "    def visual_analysis(state):\n",
        "        frames = state[\"video_data\"][\"frames\"]\n",
        "        spatial_results = spatial_inconsistency_agent(frames)\n",
        "        temporal_results = temporal_coherence_agent(frames)\n",
        "        return {\n",
        "            \"visual_results\": {\n",
        "                \"spatial\": spatial_results,\n",
        "                \"temporal\": temporal_results\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @workflow.node(\"audio_analysis\")\n",
        "    def audio_analysis(state):\n",
        "        audio_data = state[\"video_data\"][\"audio\"]\n",
        "        results = audio_analysis_agent(audio_data)\n",
        "        return {\"audio_results\": results}\n",
        "\n",
        "    @workflow.node(\"semantic_analysis\")\n",
        "    def semantic_analysis(state):\n",
        "        results = semantic_analysis_agent(state[\"video_data\"])\n",
        "        return {\"semantic_results\": results}\n",
        "\n",
        "    @workflow.node(\"decision\")\n",
        "    def decision(state):\n",
        "        all_results = [\n",
        "            state[\"visual_results\"][\"spatial\"],\n",
        "            state[\"visual_results\"][\"temporal\"],\n",
        "            state[\"audio_results\"],\n",
        "            state[\"semantic_results\"]\n",
        "        ]\n",
        "\n",
        "        fusion_result = feature_fusion(all_results)\n",
        "        final_decision = make_decision(fusion_result)\n",
        "        return {\"decision\": final_decision, \"end\": True}\n",
        "\n",
        "    # Define workflow\n",
        "    workflow.set_entry_point(\"preprocess\")\n",
        "    workflow.add_edge(\"preprocess\", \"visual_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"audio_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"semantic_analysis\")\n",
        "    workflow.add_edge(\"visual_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"audio_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"semantic_analysis\", \"decision\")\n",
        "\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "7NT6HEePWlQR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function\n",
        "def detect_deepfake(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Main function to run deepfake detection\"\"\"\n",
        "    # Setup\n",
        "    device = setup_environment()\n",
        "\n",
        "    # Create and compile workflow\n",
        "    workflow = create_detection_graph()\n",
        "\n",
        "    # Run detection\n",
        "    config = {\"input_path\": video_path}\n",
        "    result = workflow.run(config)\n",
        "\n",
        "    return result[\"decision\"]"
      ],
      "metadata": {
        "id": "OK68SZCQWtzw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = \"path/to/your/video.mp4\"\n",
        "    result = detect_deepfake(video_path)\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "sgeWgPlHWyCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}