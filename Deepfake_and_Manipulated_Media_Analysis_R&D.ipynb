{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOts97XxN3T0xsZ7xVtVLqN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4",
        "outputId": "e0327c53-f4e6-480a-d91c-ce30d7db1ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "%pip install -q langchain langchain-community langgraph torch transformers opencv-python librosa numpy face-recognition dlib mediapipe scipy pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import os\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import Graph, END\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForVideoClassification,\n",
        "    AutoModelForAudioClassification,\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    AutoModelForImageClassification,\n",
        "    ViTForImageClassification,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    AutoFeatureExtractor\n",
        ")\n",
        "import cv2\n",
        "import librosa\n",
        "import json\n",
        "import face_recognition\n",
        "import dlib\n",
        "import scipy\n",
        "from scipy.signal import welch\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "import logging\n",
        "import warnings\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JNL83H4CUsgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "mqUWnhgtbTaH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup necessary environment variables and configurations\"\"\"\n",
        "    # Configure device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Load face detection model\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Initialize MediaPipe\n",
        "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        min_detection_confidence=0.5\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"device\": device,\n",
        "        \"face_detector\": face_detector,\n",
        "        \"face_mesh\": mp_face_mesh\n",
        "    }"
      ],
      "metadata": {
        "id": "JVJJvhKNU27U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced video preprocessing with advanced feature extraction\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    face_landmarks_seq = []\n",
        "    optical_flow_seq = []\n",
        "\n",
        "    # Initialize optical flow\n",
        "    ret, prev_frame = cap.read()\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Store original frame\n",
        "        frames.append(frame)\n",
        "\n",
        "        # Calculate optical flow\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "        optical_flow_seq.append(flow)\n",
        "        prev_gray = gray\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Extract audio with advanced features\n",
        "    y, sr = librosa.load(video_path)\n",
        "\n",
        "    # Extract audio features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": {\n",
        "            \"raw\": (y, sr),\n",
        "            \"mfcc\": mfcc,\n",
        "            \"chroma\": chroma,\n",
        "            \"spectral_contrast\": spectral_contrast\n",
        "        },\n",
        "        \"optical_flow\": optical_flow_seq,\n",
        "        \"metadata\": extract_metadata(video_path)\n",
        "    }\n",
        "\n",
        "def extract_metadata(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced metadata extraction\"\"\"\n",
        "    metadata = {}\n",
        "    try:\n",
        "        # Basic file info\n",
        "        metadata[\"file_info\"] = {\n",
        "            \"size\": os.path.getsize(file_path),\n",
        "            \"created\": os.path.getctime(file_path),\n",
        "            \"modified\": os.path.getmtime(file_path)\n",
        "        }\n",
        "\n",
        "        # Video-specific metadata\n",
        "        cap = cv2.VideoCapture(file_path)\n",
        "        metadata[\"video_info\"] = {\n",
        "            \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "            \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "            \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "            \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        }\n",
        "        cap.release()\n",
        "\n",
        "    except Exception as e:\n",
        "        metadata[\"error\"] = str(e)\n",
        "        logger.error(f\"Error extracting metadata: {e}\")\n",
        "\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "D4JBongFVeob"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual Analysis Agents\n",
        "\n",
        "def spatial_inconsistency_agent(frames: List[np.ndarray], device: torch.device) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced spatial inconsistency detection\"\"\"\n",
        "    # Load multiple models for ensemble detection\n",
        "    models = {\n",
        "        \"xclip\": AutoModelForVideoClassification.from_pretrained(\"microsoft/xclip-base-patch32\"),\n",
        "        \"vit\": ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\"),\n",
        "        \"clip\": CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    }\n",
        "\n",
        "    processors = {\n",
        "        \"xclip\": AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\"),\n",
        "        \"vit\": AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\"),\n",
        "        \"clip\": CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for frame in tqdm(frames, desc=\"Analyzing spatial inconsistencies\"):\n",
        "        frame_results = []\n",
        "\n",
        "        # XClip analysis\n",
        "        xclip_inputs = processors[\"xclip\"](images=frame, return_tensors=\"pt\").to(device)\n",
        "        xclip_outputs = models[\"xclip\"](**xclip_inputs)\n",
        "        frame_results.append(xclip_outputs.logits.softmax(dim=1))\n",
        "\n",
        "        # ViT analysis\n",
        "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        vit_inputs = processors[\"vit\"](images=frame_pil, return_tensors=\"pt\").to(device)\n",
        "        vit_outputs = models[\"vit\"](**vit_inputs)\n",
        "        frame_results.append(vit_outputs.logits.softmax(dim=1))\n",
        "\n",
        "        # Ensemble results\n",
        "        frame_confidence = torch.mean(torch.stack([r.mean() for r in frame_results]))\n",
        "        results.append(frame_confidence.item())\n",
        "\n",
        "    return {\n",
        "        \"spatial_confidence\": float(np.mean(results)),\n",
        "        \"frame_confidences\": results\n",
        "    }"
      ],
      "metadata": {
        "id": "2GqkOYLmWF9y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def facial_analysis_agent(frames: List[np.ndarray], face_detector: dlib.fhog_object_detector) -> Dict[str, float]:\n",
        "    \"\"\"Analyze facial features for inconsistencies\"\"\"\n",
        "    face_confidences = []\n",
        "    landmark_movements = []\n",
        "\n",
        "    for i in range(len(frames) - 1):\n",
        "        # Detect faces\n",
        "        curr_faces = face_recognition.face_locations(frames[i])\n",
        "        next_faces = face_recognition.face_locations(frames[i + 1])\n",
        "\n",
        "        if curr_faces and next_faces:\n",
        "            # Get facial landmarks\n",
        "            curr_landmarks = face_recognition.face_landmarks(frames[i], curr_faces)\n",
        "            next_landmarks = face_recognition.face_landmarks(frames[i + 1], next_faces)\n",
        "\n",
        "            # Calculate landmark movement consistency\n",
        "            if curr_landmarks and next_landmarks:\n",
        "                movement = calculate_landmark_movement(curr_landmarks[0], next_landmarks[0])\n",
        "                landmark_movements.append(movement)\n",
        "\n",
        "                # Calculate face similarity\n",
        "                curr_encoding = face_recognition.face_encodings(frames[i], [curr_faces[0]])[0]\n",
        "                next_encoding = face_recognition.face_encodings(frames[i + 1], [next_faces[0]])[0]\n",
        "                similarity = face_recognition.face_distance([curr_encoding], next_encoding)[0]\n",
        "                face_confidences.append(1 - similarity)\n",
        "\n",
        "    return {\n",
        "        \"facial_confidence\": float(np.mean(face_confidences)) if face_confidences else 0.0,\n",
        "        \"landmark_consistency\": float(np.mean(landmark_movements)) if landmark_movements else 0.0\n",
        "    }"
      ],
      "metadata": {
        "id": "-eiewMRcWVpV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_landmark_movement(curr_landmarks: Dict, next_landmarks: Dict) -> float:\n",
        "    \"\"\"Calculate consistency of facial landmark movements\"\"\"\n",
        "    total_movement = 0\n",
        "    num_points = 0\n",
        "\n",
        "    for feature in curr_landmarks.keys():\n",
        "        curr_points = np.array(curr_landmarks[feature])\n",
        "        next_points = np.array(next_landmarks[feature])\n",
        "\n",
        "        # Calculate movement vectors\n",
        "        movements = next_points - curr_points\n",
        "\n",
        "        # Check for consistent movement\n",
        "        movement_directions = np.sign(movements)\n",
        "        consistency = np.mean(np.abs(np.diff(movement_directions, axis=0)))\n",
        "\n",
        "        total_movement += consistency\n",
        "        num_points += 1\n",
        "\n",
        "    return total_movement / num_points if num_points > 0 else 0.0"
      ],
      "metadata": {
        "id": "chodssfUWaQw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_coherence_agent(frames: List[np.ndarray], optical_flow: List[np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced temporal coherence analysis\"\"\"\n",
        "    flow_consistency = []\n",
        "    frame_consistency = []\n",
        "\n",
        "    for i in range(len(frames) - 1):\n",
        "        # Analyze optical flow consistency\n",
        "        flow = optical_flow[i]\n",
        "        flow_magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
        "        flow_consistency.append(np.mean(flow_magnitude))\n",
        "\n",
        "        # Analyze frame-to-frame consistency\n",
        "        curr_frame = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        next_frame = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Calculate structural similarity\n",
        "        ssim = calculate_ssim(curr_frame, next_frame)\n",
        "        frame_consistency.append(ssim)\n",
        "\n",
        "    return {\n",
        "        \"temporal_confidence\": float(np.mean(frame_consistency)),\n",
        "        \"flow_consistency\": float(np.mean(flow_consistency))\n",
        "    }\n",
        "\n",
        "def calculate_ssim(img1: np.ndarray, img2: np.ndarray, window_size: int = 7) -> float:\n",
        "    \"\"\"Calculate Structural Similarity Index (SSIM)\"\"\"\n",
        "    C1 = (0.01 * 255)**2\n",
        "    C2 = (0.03 * 255)**2\n",
        "\n",
        "    window = cv2.getGaussianKernel(window_size, 1.5)\n",
        "    window = np.outer(window, window)\n",
        "\n",
        "    mu1 = cv2.filter2D(img1, -1, window)[::2, ::2]\n",
        "    mu2 = cv2.filter2D(img2, -1, window)[::2, ::2]\n",
        "    mu1_sq = mu1**2\n",
        "    mu2_sq = mu2**2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[::2, ::2] - mu1_sq\n",
        "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[::2, ::2] - mu2_sq\n",
        "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[::2, ::2] - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
        "               ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    return float(np.mean(ssim_map))"
      ],
      "metadata": {
        "id": "PU9Qd2DrWgHk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Analysis Agents\n",
        "\n",
        "def audio_analysis_agent(audio_data: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced audio analysis\"\"\"\n",
        "    # Unpack audio data\n",
        "    y, sr = audio_data[\"raw\"]\n",
        "    mfcc = audio_data[\"mfcc\"]\n",
        "    chroma = audio_data[\"chroma\"]\n",
        "    spectral_contrast = audio_data[\"spectral_contrast\"]\n",
        "\n",
        "    # Analyze MFCC consistency\n",
        "    mfcc_consistency = np.mean(np.std(mfcc, axis=1))\n",
        "\n",
        "    # Analyze spectral properties\n",
        "    frequencies, power = welch(y, sr, nperseg=2048)\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "\n",
        "    # Detect audio artifacts\n",
        "    zero_crossings = librosa.zero_crossings(y)\n",
        "    zero_crossing_rate = np.mean(zero_crossings)\n",
        "\n",
        "    # Calculate overall confidence\n",
        "    features = [\n",
        "        mfcc_consistency,\n",
        "        np.mean(spectral_contrast),\n",
        "        np.mean(spectral_rolloff),\n",
        "        np.mean(spectral_bandwidth),\n",
        "        zero_crossing_rate\n",
        "    ]\n",
        "\n",
        "    confidence = calculate_audio_confidence(features)\n",
        "\n",
        "    return {\n",
        "        \"audio_confidence\": float(confidence),\n",
        "        \"mfcc_consistency\": float(mfcc_consistency),\n",
        "        \"spectral_contrast\": float(np.mean(spectral_contrast)),\n",
        "        \"zero_crossing_rate\": float(zero_crossing_rate)\n",
        "    }\n",
        "\n",
        "def calculate_audio_confidence(features: List[float]) -> float:\n",
        "    \"\"\"Calculate overall audio confidence score\"\"\"\n",
        "    # Normalize features\n",
        "    normalized_features = []\n",
        "    for feature in features:\n",
        "        if np.isfinite(feature):\n",
        "            normalized = (feature - np.min(features)) / (np.max(features) - np.min(features))\n",
        "            normalized_features.append(normalized)\n",
        "\n",
        "    # Weight and combine features\n",
        "    weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Adjustable weights\n",
        "    confidence = np.average(normalized_features, weights=weights[:len(normalized_features)])\n",
        "\n",
        "    return float(confidence)"
      ],
      "metadata": {
        "id": "7NT6HEePWlQR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Analysis Agents\n",
        "\n",
        "def setup_llm_agent():\n",
        "    \"\"\"Setup enhanced LLM agent\"\"\"\n",
        "    llm = ChatOpenAI(model=\"gpt-4-vision-preview\", temperature=0.2)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Analyze the following media content for signs of manipulation.\n",
        "        Consider:\n",
        "        1. Visual consistency and artifacts\n",
        "        2. Audio-visual synchronization\n",
        "        3. Natural movement and expressions\n",
        "        4. Lighting and shadow consistency\n",
        "        5. Edge artifacts and blending issues\n",
        "\n",
        "        Provide a detailed analysis with confidence scores.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    return llm, prompt\n",
        "\n",
        "@tool\n",
        "def semantic_analysis_agent(input_data: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced semantic analysis\"\"\"\n",
        "    llm, prompt = setup_llm_agent()\n",
        "\n",
        "    # Prepare input data\n",
        "    analysis_text = {\n",
        "        \"video_metadata\": input_data.get(\"metadata\", {}),\n",
        "        \"detected_faces\": len(face_recognition.face_locations(input_data[\"frames\"][0])) if \"frames\" in input_data else 0,\n",
        "        \"video_length\": len(input_data[\"frames\"]) if \"frames\" in input_data else 0,\n",
        "        \"audio_features\": bool(input_data.get(\"audio\", {}))\n",
        "    }\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "    try:\n",
        "        result = chain.invoke({\"input\": json.dumps(analysis_text)})\n",
        "        confidence = result.get(\"confidence\", 0.0)\n",
        "\n",
        "        # Additional analysis of response content\n",
        "        response_detail = result.get(\"analysis\", \"\")\n",
        "        detail_score = analyze_llm_response_detail(response_detail)\n",
        "\n",
        "        return {\n",
        "            \"semantic_confidence\": float(confidence),\n",
        "            \"detail_score\": float(detail_score),\n",
        "            \"analysis\": response_detail\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in semantic analysis: {e}\")\n",
        "        return {\"semantic_confidence\": 0.0, \"error\": str(e)}\n",
        "\n",
        "def analyze_llm_response_detail(response: str) -> float:\n",
        "    \"\"\"Analyze the detail level of LLM response\"\"\"\n",
        "    # Consider factors like response length, specific terminology use, etc.\n",
        "    detail_metrics = {\n",
        "        \"length\": len(response) / 1000,  # Normalize by 1000 chars\n",
        "        \"technical_terms\": len([word for word in response.lower().split()\n",
        "                              if word in [\"artifact\", \"consistency\", \"synchronization\",\n",
        "                                        \"manipulation\", \"synthetic\", \"generated\"]]) / 10\n",
        "    }\n",
        "    return np.mean(list(detail_metrics.values()))"
      ],
      "metadata": {
        "id": "OK68SZCQWtzw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integration and Decision Making\n",
        "\n",
        "def feature_fusion(results: List[Dict[str, float]], metadata: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced feature fusion with adaptive weighting\"\"\"\n",
        "    # Initialize weights based on metadata\n",
        "    base_weights = {\n",
        "        \"spatial_confidence\": 0.25,\n",
        "        \"temporal_confidence\": 0.20,\n",
        "        \"facial_confidence\": 0.20,\n",
        "        \"audio_confidence\": 0.15,\n",
        "        \"semantic_confidence\": 0.20\n",
        "    }\n",
        "\n",
        "    # Adjust weights based on video properties\n",
        "    if metadata.get(\"video_info\", {}).get(\"fps\", 0) < 20:\n",
        "        base_weights[\"temporal_confidence\"] *= 0.8\n",
        "        base_weights[\"spatial_confidence\"] *= 1.2\n",
        "\n",
        "    # Collect all confidence scores\n",
        "    confidence_scores = {}\n",
        "    for result in results:\n",
        "        for key, value in result.items():\n",
        "            if \"confidence\" in key and isinstance(value, (int, float)):\n",
        "                confidence_scores[key] = value\n",
        "\n",
        "    # Calculate weighted average\n",
        "    total_weight = 0\n",
        "    weighted_sum = 0\n",
        "\n",
        "    for key, value in confidence_scores.items():\n",
        "        weight = base_weights.get(key, 0.1)  # Default weight for unknown metrics\n",
        "        weighted_sum += value * weight\n",
        "        total_weight += weight\n",
        "\n",
        "    weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    # Calculate uncertainty\n",
        "    variances = [(score - weighted_avg) ** 2 for score in confidence_scores.values()]\n",
        "    uncertainty = np.sqrt(np.mean(variances)) if variances else 0.0\n",
        "\n",
        "    return {\n",
        "        \"final_confidence\": float(weighted_avg),\n",
        "        \"uncertainty\": float(uncertainty),\n",
        "        \"individual_scores\": confidence_scores\n",
        "    }"
      ],
      "metadata": {
        "id": "sgeWgPlHWyCG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_decision(fusion_result: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced decision making with detailed analysis\"\"\"\n",
        "    confidence = fusion_result[\"final_confidence\"]\n",
        "    uncertainty = fusion_result[\"uncertainty\"]\n",
        "    individual_scores = fusion_result[\"individual_scores\"]\n",
        "\n",
        "    # Dynamic thresholding based on uncertainty\n",
        "    base_threshold = 0.7\n",
        "    adjusted_threshold = base_threshold + (uncertainty * 0.1)\n",
        "\n",
        "    # Analyze score distribution\n",
        "    score_distribution = analyze_score_distribution(individual_scores)\n",
        "\n",
        "    # Decision making\n",
        "    is_fake = confidence < adjusted_threshold\n",
        "    certainty_level = calculate_certainty_level(confidence, uncertainty)\n",
        "\n",
        "    # Generate detailed explanation\n",
        "    explanation = generate_detailed_explanation(\n",
        "        is_fake, confidence, uncertainty,\n",
        "        individual_scores, score_distribution\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"is_fake\": is_fake,\n",
        "        \"confidence\": confidence,\n",
        "        \"uncertainty\": uncertainty,\n",
        "        \"certainty_level\": certainty_level,\n",
        "        \"threshold_used\": adjusted_threshold,\n",
        "        \"score_distribution\": score_distribution,\n",
        "        \"explanation\": explanation\n",
        "    }\n",
        "\n",
        "def analyze_score_distribution(scores: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze the distribution of individual scores\"\"\"\n",
        "    values = np.array(list(scores.values()))\n",
        "    return {\n",
        "        \"mean\": float(np.mean(values)),\n",
        "        \"std\": float(np.std(values)),\n",
        "        \"min\": float(np.min(values)),\n",
        "        \"max\": float(np.max(values)),\n",
        "        \"range\": float(np.ptp(values)),\n",
        "        \"consistency\": float(1 - (np.std(values) / np.mean(values))) if np.mean(values) != 0 else 0.0\n",
        "    }\n",
        "\n",
        "def calculate_certainty_level(confidence: float, uncertainty: float) -> str:\n",
        "    \"\"\"Calculate the certainty level of the decision\"\"\"\n",
        "    if uncertainty > 0.3:\n",
        "        return \"Low\"\n",
        "    elif uncertainty > 0.15:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "def generate_detailed_explanation(\n",
        "    is_fake: bool,\n",
        "    confidence: float,\n",
        "    uncertainty: float,\n",
        "    individual_scores: Dict[str, float],\n",
        "    score_distribution: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"Generate a detailed explanation of the decision\"\"\"\n",
        "    explanation_parts = []\n",
        "\n",
        "    # Overall decision\n",
        "    decision_text = \"likely manipulated\" if is_fake else \"likely authentic\"\n",
        "    explanation_parts.append(f\"The media is {decision_text} with {confidence:.1%} confidence.\")\n",
        "\n",
        "    # Uncertainty analysis\n",
        "    explanation_parts.append(\n",
        "        f\"The uncertainty level is {uncertainty:.1%}, indicating a \"\n",
        "        f\"{'high' if uncertainty > 0.3 else 'moderate' if uncertainty > 0.15 else 'low'} \"\n",
        "        \"level of prediction variability.\"\n",
        "    )\n",
        "\n",
        "    # Individual score analysis\n",
        "    strongest_evidence = max(individual_scores.items(), key=lambda x: x[1])\n",
        "    weakest_evidence = min(individual_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    explanation_parts.append(\n",
        "        f\"The strongest evidence comes from {strongest_evidence[0]} \"\n",
        "        f\"({strongest_evidence[1]:.1%}), while the weakest indicator is \"\n",
        "        f\"{weakest_evidence[0]} ({weakest_evidence[1]:.1%}).\"\n",
        "    )\n",
        "\n",
        "    # Score consistency\n",
        "    explanation_parts.append(\n",
        "        f\"The consistency between different detection methods is \"\n",
        "        f\"{score_distribution['consistency']:.1%}.\"\n",
        "    )\n",
        "\n",
        "    return \" \".join(explanation_parts)"
      ],
      "metadata": {
        "id": "laob1DtvdjFr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "\n",
        "def create_detection_graph():\n",
        "    \"\"\"Create enhanced LangGraph workflow\"\"\"\n",
        "    workflow = Graph()\n",
        "\n",
        "    @workflow.node(\"preprocess\")\n",
        "    def preprocess(state):\n",
        "        video_data = preprocess_video(state[\"input_path\"])\n",
        "        return {\"video_data\": video_data}\n",
        "\n",
        "    @workflow.node(\"visual_analysis\")\n",
        "    def visual_analysis(state):\n",
        "        frames = state[\"video_data\"][\"frames\"]\n",
        "        optical_flow = state[\"video_data\"][\"optical_flow\"]\n",
        "\n",
        "        # Run visual analysis agents in parallel\n",
        "        spatial_results = spatial_inconsistency_agent(frames, state[\"env\"][\"device\"])\n",
        "        temporal_results = temporal_coherence_agent(frames, optical_flow)\n",
        "        facial_results = facial_analysis_agent(frames, state[\"env\"][\"face_detector\"])\n",
        "\n",
        "        return {\n",
        "            \"visual_results\": {\n",
        "                \"spatial\": spatial_results,\n",
        "                \"temporal\": temporal_results,\n",
        "                \"facial\": facial_results\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @workflow.node(\"audio_analysis\")\n",
        "    def audio_analysis(state):\n",
        "        audio_data = state[\"video_data\"][\"audio\"]\n",
        "        results = audio_analysis_agent(audio_data)\n",
        "        return {\"audio_results\": results}\n",
        "\n",
        "    @workflow.node(\"semantic_analysis\")\n",
        "    def semantic_analysis(state):\n",
        "        results = semantic_analysis_agent(state[\"video_data\"])\n",
        "        return {\"semantic_results\": results}\n",
        "\n",
        "    @workflow.node(\"decision\")\n",
        "    def decision(state):\n",
        "        all_results = [\n",
        "            state[\"visual_results\"][\"spatial\"],\n",
        "            state[\"visual_results\"][\"temporal\"],\n",
        "            state[\"visual_results\"][\"facial\"],\n",
        "            state[\"audio_results\"],\n",
        "            state[\"semantic_results\"]\n",
        "        ]\n",
        "\n",
        "        fusion_result = feature_fusion(all_results, state[\"video_data\"][\"metadata\"])\n",
        "        final_decision = make_decision(fusion_result)\n",
        "\n",
        "        return {\"decision\": final_decision, \"end\": True}\n",
        "\n",
        "    # Define workflow\n",
        "    workflow.set_entry_point(\"preprocess\")\n",
        "    workflow.add_edge(\"preprocess\", \"visual_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"audio_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"semantic_analysis\")\n",
        "    workflow.add_edge(\"visual_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"audio_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"semantic_analysis\", \"decision\")\n",
        "\n",
        "    return workflow\n",
        "\n",
        "def detect_deepfake(video_path: str, verbose: bool = False) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced main function to run deepfake detection\"\"\"\n",
        "    try:\n",
        "        # Setup environment\n",
        "        env = setup_environment()\n",
        "\n",
        "        # Create and compile workflow\n",
        "        workflow = create_detection_graph()\n",
        "\n",
        "        # Configure logging\n",
        "        if verbose:\n",
        "            logging.getLogger().setLevel(logging.INFO)\n",
        "        else:\n",
        "            logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "        # Run detection\n",
        "        config = {\n",
        "            \"input_path\": video_path,\n",
        "            \"env\": env\n",
        "        }\n",
        "\n",
        "        logger.info(\"Starting deepfake detection pipeline...\")\n",
        "        result = workflow.run(config)\n",
        "        logger.info(\"Detection pipeline completed successfully.\")\n",
        "\n",
        "        return result[\"decision\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in deepfake detection: {e}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"is_fake\": None,\n",
        "            \"confidence\": 0.0,\n",
        "            \"explanation\": \"An error occurred during detection.\"\n",
        "        }"
      ],
      "metadata": {
        "id": "XtVtW8mzd0ho"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Deepfake Detection System\")\n",
        "    parser.add_argument(\"video_path\", help=\"Path to the video file to analyze\")\n",
        "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose logging\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    result = detect_deepfake(args.video_path, verbose=args.verbose)\n",
        "    print(\"\\nDeepfake Detection Results:\")\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "ld-3_m_Md7cN",
        "outputId": "a1ca5338-20e3-489e-a5f3-2745155b4eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--verbose] video_path\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUAlljFyeBL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}