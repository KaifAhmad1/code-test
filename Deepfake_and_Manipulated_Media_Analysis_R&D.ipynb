{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7552a6e5d9f74eac801f2b874f0aa9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d59c74e47464ab9a96576940d5dcdaa",
              "IPY_MODEL_fa9a5e66b5594e5d8f92bcf25ab161b4",
              "IPY_MODEL_bd340deb99f049baba9cb421db07a8d0"
            ],
            "layout": "IPY_MODEL_ef1b5fd9f0ec4d1388c321d389629984"
          }
        },
        "3d59c74e47464ab9a96576940d5dcdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32cf60c338fa426288b02fd781999ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_583f5108bc354b6d8f55129b2a0be290",
            "value": "config.json: 100%"
          }
        },
        "fa9a5e66b5594e5d8f92bcf25ab161b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c894cb197641406e9850057921a628de",
            "max": 22865,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bde1c24b7f1a443f822afd7266188e24",
            "value": 22865
          }
        },
        "bd340deb99f049baba9cb421db07a8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b37b67a3964213bb22e31b51999ae4",
            "placeholder": "​",
            "style": "IPY_MODEL_d48d5f3e0395430bb9ddbf4a235370d6",
            "value": " 22.9k/22.9k [00:00&lt;00:00, 1.47MB/s]"
          }
        },
        "ef1b5fd9f0ec4d1388c321d389629984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cf60c338fa426288b02fd781999ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583f5108bc354b6d8f55129b2a0be290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c894cb197641406e9850057921a628de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde1c24b7f1a443f822afd7266188e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84b37b67a3964213bb22e31b51999ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d48d5f3e0395430bb9ddbf4a235370d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed0f9b42b1f9488db5772451662d803d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1580e06b61d94448b814cc046386e4a8",
              "IPY_MODEL_3b03aac8954f43c5b3a3a1020dfc23f0",
              "IPY_MODEL_de565afb52e14c2b94332753b3e4983e"
            ],
            "layout": "IPY_MODEL_2632f4c03f5c4a18a131dbc97ae24fcb"
          }
        },
        "1580e06b61d94448b814cc046386e4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e92cbc0dcab4602b7c1a750a8c0169f",
            "placeholder": "​",
            "style": "IPY_MODEL_543c6c8d003e42afa8b776fed7ce4150",
            "value": "model.safetensors: 100%"
          }
        },
        "3b03aac8954f43c5b3a3a1020dfc23f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb23ca02913432d8f29af600644d5e0",
            "max": 346161616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5ed3cea8a004c148e080fcc5f473d1e",
            "value": 346161616
          }
        },
        "de565afb52e14c2b94332753b3e4983e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_031566c5107b4934a977ca58f159cdd0",
            "placeholder": "​",
            "style": "IPY_MODEL_c284454d0828442d88315d13ac3d4d36",
            "value": " 346M/346M [00:02&lt;00:00, 203MB/s]"
          }
        },
        "2632f4c03f5c4a18a131dbc97ae24fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e92cbc0dcab4602b7c1a750a8c0169f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543c6c8d003e42afa8b776fed7ce4150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bb23ca02913432d8f29af600644d5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5ed3cea8a004c148e080fcc5f473d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "031566c5107b4934a977ca58f159cdd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c284454d0828442d88315d13ac3d4d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa18674f4f3b48798806a7cb7be91d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9457c5169e344966a31737a7f1de7a9a",
              "IPY_MODEL_2799d0a6635741908c78fa1110858592",
              "IPY_MODEL_eea0053c54534803b5157bbaca6973f3"
            ],
            "layout": "IPY_MODEL_fd962dd306a148d2b5c440b5121ab712"
          }
        },
        "9457c5169e344966a31737a7f1de7a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa95ea235cd640719c55462b2e6269f1",
            "placeholder": "​",
            "style": "IPY_MODEL_db5e51529bb54444b6b63f9146d86744",
            "value": "config.json: 100%"
          }
        },
        "2799d0a6635741908c78fa1110858592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d4bca25f5884ffeb558f278c146118c",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8ee631ab4ef4631aa84f8b610e43889",
            "value": 22723
          }
        },
        "eea0053c54534803b5157bbaca6973f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f62e5f92fe6472db508e0194ac37fe7",
            "placeholder": "​",
            "style": "IPY_MODEL_375798728e09482dbbe6357daf413ac8",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 1.08MB/s]"
          }
        },
        "fd962dd306a148d2b5c440b5121ab712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa95ea235cd640719c55462b2e6269f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5e51529bb54444b6b63f9146d86744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d4bca25f5884ffeb558f278c146118c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ee631ab4ef4631aa84f8b610e43889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f62e5f92fe6472db508e0194ac37fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "375798728e09482dbbe6357daf413ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f02ce9ed96449d99b5477a4a94cd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d073a617e9714e5881994433068666ed",
              "IPY_MODEL_ab9441e3ba7241b99fb8da174f89dec2",
              "IPY_MODEL_9ff3bfd5bb6646358e1133590dbd9369"
            ],
            "layout": "IPY_MODEL_ba22f95f060d4e1bb0a83e68fcc51d54"
          }
        },
        "d073a617e9714e5881994433068666ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35cbd78282384ece8b92c27aeda07027",
            "placeholder": "​",
            "style": "IPY_MODEL_20c14ab7af684341ab2170588c12faac",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ab9441e3ba7241b99fb8da174f89dec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d94324466d74681930f6e4082c08d61",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9579990de2b45ae8865e3dcd78efb8f",
            "value": 486348721
          }
        },
        "9ff3bfd5bb6646358e1133590dbd9369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b776a336bdff4254864f43a5326ebc3d",
            "placeholder": "​",
            "style": "IPY_MODEL_1c5232fabfab418a9a50c35d218a9a9d",
            "value": " 486M/486M [00:02&lt;00:00, 197MB/s]"
          }
        },
        "ba22f95f060d4e1bb0a83e68fcc51d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35cbd78282384ece8b92c27aeda07027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c14ab7af684341ab2170588c12faac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d94324466d74681930f6e4082c08d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9579990de2b45ae8865e3dcd78efb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b776a336bdff4254864f43a5326ebc3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5232fabfab418a9a50c35d218a9a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Packages**"
      ],
      "metadata": {
        "id": "mBe0gJFvUEOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4",
        "outputId": "1dc99675-fc38-4ecc-eccf-ee2f18650c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q groq langchain langchain-community langgraph torch transformers opencv-python librosa numpy face-recognition dlib mediapipe scipy pillow tqdm pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up the Groq Client**"
      ],
      "metadata": {
        "id": "-ytDSCQDUAje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "import getpass\n",
        "\n",
        "# Prompt for the Groq API key and use it directly\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq(\n",
        "    api_key=GROQ_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIxEeTxeUNJz",
        "outputId": "7e537db2-837d-4b98-c838-2c22304499f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Models**\n",
        "- Define the data models for storing analysis results and reports:\n"
      ],
      "metadata": {
        "id": "X8RzT9-AWZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float = Field(..., description=\"Confidence score (0-1)\")\n",
        "    label: str = Field(..., description=\"Classification label\")\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float = Field(..., description=\"Model confidence\")\n",
        "    method: str = Field(..., description=\"Detection method used\")\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Dict[str, Any]]\n",
        "    metadata: Dict[str, Any]"
      ],
      "metadata": {
        "id": "MyJ0N4mSWZio"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Preprocessing**\n",
        "- Define helper functions for preprocessing audio, image, and video data:"
      ],
      "metadata": {
        "id": "0ktikzemXOd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def reduce_noise(audio_data: np.ndarray) -> np.ndarray:\n",
        "    return librosa.effects.preemphasis(audio_data)\n",
        "\n",
        "def enhance_resolution(image: np.ndarray) -> np.ndarray:\n",
        "    sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
        "    return sr.upsample(image)\n",
        "\n",
        "def deblur_image(image: np.ndarray) -> np.ndarray:\n",
        "    return cv2.filter2D(image, -1, np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]))\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:\n",
        "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
        "    mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "    return {\"mfcc\": mfcc, \"chroma\": chroma, \"mel\": mel}\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return {\"keypoints\": keypoints, \"descriptors\": descriptors}\n",
        "\n",
        "def calculate_dense_optical_flow(prev_frame: np.ndarray, curr_frame: np.ndarray) -> np.ndarray:\n",
        "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def extract_temporal_features(optical_flow: np.ndarray) -> Dict[str, Any]:\n",
        "    mag, ang = cv2.cartToPolar(optical_flow[..., 0], optical_flow[..., 1])\n",
        "    return {\"magnitude\": mag, \"angle\": ang}\n",
        "\n",
        "def estimate_noise(frame: np.ndarray) -> float:\n",
        "    return np.mean(cv2.Laplacian(frame, cv2.CV_64F).var())\n",
        "\n",
        "def calculate_contrast(frame: np.ndarray) -> float:\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    return gray.std()\n",
        "\n",
        "def detect_compression_artifacts(frame: np.ndarray) -> float:\n",
        "    dct = cv2.dct(np.float32(frame) / 255.0)\n",
        "    return np.mean(np.abs(dct))"
      ],
      "metadata": {
        "id": "9A9KPnzU36TX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Preprocessing Functions**\n",
        "- Define functions for preprocessing audio, image, and video data"
      ],
      "metadata": {
        "id": "13iwCPIrX-Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import base64\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "async def enhanced_preprocessing(file_path: str) -> Dict[str, Any]:\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension in [\".mp3\", \".wav\", \".flac\"]:\n",
        "        return await enhanced_audio_preprocessing(file_path)\n",
        "    elif file_extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
        "        return await enhanced_image_preprocessing(file_path)\n",
        "    elif file_extension in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "        return await enhanced_video_preprocessing(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "async def enhanced_audio_preprocessing(audio_path: str) -> Dict[str, Any]:\n",
        "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
        "    audio_data = reduce_noise(audio_data)\n",
        "    audio_features = extract_audio_features(audio_data, sample_rate)\n",
        "    return {\"audio\": audio_data, \"sample_rate\": sample_rate, \"audio_features\": audio_features}\n",
        "\n",
        "async def enhanced_image_preprocessing(image_path: str) -> Dict[str, Any]:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = enhance_resolution(image)\n",
        "    image = deblur_image(image)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_features = extract_image_features(image_rgb)\n",
        "    return {\"image\": image_rgb, \"image_features\": image_features}\n",
        "\n",
        "async def enhanced_video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "        \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "        \"duration\": float(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / float(cap.get(cv2.CAP_PROP_FPS)),\n",
        "        \"codec\": int(cap.get(cv2.CAP_PROP_FOURCC)).to_bytes(4, byteorder='little').decode(),\n",
        "        \"file_size\": os.path.getsize(video_path)\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "    cap.release()\n",
        "\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is not None:\n",
        "            audio_array = audio.to_soundarray()\n",
        "            audio_data = extract_audio_features(audio_array, audio.fps)\n",
        "        video.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Audio extraction error: {e}\")\n",
        "        audio_data = None\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }"
      ],
      "metadata": {
        "id": "ltWcOSlP36Ww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analysis**\n",
        "- functions for analyzing transcription, vision response, and other features"
      ],
      "metadata": {
        "id": "GExLYgMaYVBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_transcription(transcription: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for repetitive phrases\n",
        "    if \"repeated phrase\" in transcription.lower():\n",
        "        anomalies.append(\"Repetitive phrases detected\")\n",
        "    # Example: Check for inconsistent timestamps\n",
        "    if \"inconsistent timestamp\" in transcription.lower():\n",
        "        anomalies.append(\"Inconsistent timestamps detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_vision_response(response: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for blurry regions\n",
        "    if \"blurry region\" in response.lower():\n",
        "        anomalies.append(\"Blurry regions detected\")\n",
        "    # Example: Check for unnatural shadows\n",
        "    if \"unnatural shadow\" in response.lower():\n",
        "        anomalies.append(\"Unnatural shadows detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_av_sync(frames: List[np.ndarray], audio: np.ndarray) -> float:\n",
        "    # Example: Check for lip-sync consistency\n",
        "    sync_score = 0.9  # Placeholder score\n",
        "    return sync_score\n",
        "\n",
        "def analyze_temporal_features(temporal_features: List[Dict[str, Any]]) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for abrupt changes in motion\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_optical_flow(optical_flow: List[np.ndarray]) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for inconsistent flow patterns\n",
        "    for flow in optical_flow:\n",
        "        if flow.max() > 1.0:\n",
        "            anomalies.append(\"Inconsistent flow patterns detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_biometric_consistency(frames: List[np.ndarray], models: Dict[str, Any]) -> float:\n",
        "    # Example: Check for consistent facial landmarks\n",
        "    consistency_score = 0.9  # Placeholder score\n",
        "    return consistency_score"
      ],
      "metadata": {
        "id": "YkN1p_l2u-KL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- Define functions for analyzing audio, image, and video data using Groq models"
      ],
      "metadata": {
        "id": "05aBMDMgY5CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_audio_analysis(audio_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    if audio_data is None:\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.0,\n",
        "            label=\"NO_AUDIO\",\n",
        "            confidence=0.0,\n",
        "            method=\"audio_analysis\",\n",
        "            anomalies=[\"No audio data available\"]\n",
        "        )\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Use Groq's speech-to-text model\n",
        "    response = client.audio.transcriptions.create(\n",
        "        file=audio_data[\"waveform\"],\n",
        "        model=\"whisper-large-v3-turbo\",\n",
        "    )\n",
        "    transcription = response.text\n",
        "\n",
        "    # Analyze the transcription for anomalies\n",
        "    anomalies.extend(analyze_transcription(transcription))\n",
        "    scores.append(0.9)  # Placeholder score, adjust based on your analysis\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"audio_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )\n",
        "\n",
        "async def advanced_image_analysis(image: np.ndarray, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Use Groq's vision model\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze this image for signs of digital manipulation or inconsistencies.\",\n",
        "                \"image_url\": \"data:image/png;base64,\" + base64.b64encode(cv2.imencode('.png', image)[1]).decode()\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.2-90b-vision-preview\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        scores.append(float(score_match.group(1)))\n",
        "    anomalies.extend(analyze_vision_response(content))\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"image_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )\n",
        "\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "    videomae_model = models[\"videomae\"].to(device)\n",
        "    videomae_processor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
        "    inputs = videomae_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        videomae_output = videomae_model(**inputs)\n",
        "        videomae_score = torch.softmax(videomae_output.logits, dim=-1)\n",
        "        scores.append(videomae_score.max().item())\n",
        "    timesformer_model = models[\"timesformer\"].to(device)\n",
        "    timesformer_processor = VideoMAEFeatureExtractor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "    inputs = timesformer_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        timesformer_output = timesformer_model(**inputs)\n",
        "        timesformer_score = torch.softmax(timesformer_output.logits, dim=-1)\n",
        "        scores.append(timesformer_score.max().item())\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this video for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(video=video_data[\"frames\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "    temporal_anomalies = analyze_temporal_features(video_data[\"temporal_features\"])\n",
        "    anomalies.extend(temporal_anomalies)\n",
        "    optical_flow_anomalies = analyze_optical_flow(video_data[\"optical_flow\"])\n",
        "    anomalies.extend(optical_flow_anomalies)\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"video_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "GDaz8dMlu-No"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tool Creation Functions**\n",
        "- Define functions for creating deepfake detection tools and agents"
      ],
      "metadata": {
        "id": "HoftOwSUZPye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "\n",
        "def create_deepfake_detection_tools(models: Dict[str, Any], device: torch.device) -> List[Tool]:\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"analyze_video\",\n",
        "            func=lambda x: advanced_video_analysis(x, models, device),\n",
        "            description=\"Analyzes video content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_audio\",\n",
        "            func=lambda x: advanced_audio_analysis(x, models, device),\n",
        "            description=\"Analyzes audio content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_image\",\n",
        "            func=lambda x: advanced_image_analysis(x, models, device),\n",
        "            description=\"Analyzes image content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"semantic_analysis\",\n",
        "            func=lambda x: semantic_consistency_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes semantic consistency across modalities\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"face_forgery_detection\",\n",
        "            func=lambda x: face_forgery_detection(x, models, device),\n",
        "            description=\"Detects face forgeries in video content\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"lip_sync_detection\",\n",
        "            func=lambda x: lip_sync_detection(x, models, device),\n",
        "            description=\"Analyzes lip-sync consistency between audio and video\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"background_consistency\",\n",
        "            func=lambda x: background_consistency_analysis(x, models, device),\n",
        "            description=\"Analyzes background consistency across frames\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"real_time_streaming_analysis\",\n",
        "            func=lambda x: real_time_streaming_analysis(x, models, device),\n",
        "            description=\"Analyzes live video streams for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"text_analysis\",\n",
        "            func=lambda x: text_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes text content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"metadata_analysis\",\n",
        "            func=lambda x: metadata_analysis(x),\n",
        "            description=\"Analyzes metadata for signs of manipulation\"\n",
        "        )\n",
        "    ]\n",
        "    return tools\n",
        "\n",
        "def create_detection_agent(tools: List[Tool], llm: ChatOpenAI) -> AgentExecutor:\n",
        "    class DeepfakeDetectionOutputParser:\n",
        "        def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "            try:\n",
        "                if \"Final Answer:\" in llm_output:\n",
        "                    return AgentFinish(\n",
        "                        return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                        log=llm_output,\n",
        "                    )\n",
        "                action_match = re.search(r\"Action: (.*?)\\nAction Input: (.*)\", llm_output, re.DOTALL)\n",
        "                if not action_match:\n",
        "                    raise ValueError(\"Could not parse LLM output: \" + llm_output)\n",
        "                action = action_match.group(1).strip()\n",
        "                action_input = action_match.group(2).strip()\n",
        "                return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not parse LLM output: {llm_output}\") from e\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"\n",
        "            You are an expert deepfake detection system. Your goal is to analyze content across multiple modalities\n",
        "            to determine authenticity. Consider all available evidence and patterns including:\n",
        "\n",
        "            1. Visual elements: inconsistencies, artifacts, unnatural patterns\n",
        "            2. Audio characteristics: synthetic artifacts, unnatural transitions\n",
        "            3. Semantic coherence: logical consistency across modalities\n",
        "            4. Temporal patterns: synchronization, continuity\n",
        "            5. Biometric features: facial landmarks, expressions, movements\n",
        "\n",
        "            Available tools:\n",
        "            {tools}\n",
        "\n",
        "            Process:\n",
        "            1. Analyze the input using appropriate tools\n",
        "            2. Evaluate evidence across modalities\n",
        "            3. Make a final determination on authenticity\n",
        "\n",
        "            Format your response as:\n",
        "            Action: [tool name]\n",
        "            Action Input: [tool input]\n",
        "            Observation: [result]\n",
        "            ... (repeat for additional tools as needed)\n",
        "            Final Answer: [detailed analysis and verdict]\n",
        "            \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "    ])\n",
        "\n",
        "    return AgentExecutor.from_agent_and_tools(\n",
        "        agent=LLMSingleActionAgent(\n",
        "            llm_chain=LLMChain(llm=llm, prompt=prompt),\n",
        "            output_parser=DeepfakeDetectionOutputParser(),\n",
        "            stop=[\"Observation:\", \"Final Answer:\"],\n",
        "            allowed_tools=[tool.name for tool in tools]\n",
        "        ),\n",
        "        tools=tools,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "PEWzNSs3ZGK8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Detection Graph**\n",
        "- Define the detection graph for processing the input data"
      ],
      "metadata": {
        "id": "W9ZBdOffZosP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_detection_graph() -> StateGraph:\n",
        "    def preprocess(state):\n",
        "        input_data = state[\"input\"]\n",
        "        processed_data = asyncio.run(enhanced_preprocessing(input_data))\n",
        "        return {**state, \"processed_data\": processed_data}\n",
        "\n",
        "    def analyze_modalities(state):\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        device = state[\"device\"]\n",
        "        results = {\n",
        "            \"video\": asyncio.run(advanced_video_analysis(processed_data, models, device)) if \"frames\" in processed_data else None,\n",
        "            \"audio\": asyncio.run(advanced_audio_analysis(processed_data.get(\"audio\"), models, device)) if \"audio\" in processed_data else None,\n",
        "            \"image\": asyncio.run(advanced_image_analysis(processed_data[\"image\"], models, device)) if \"image\" in processed_data else None,\n",
        "            \"text\": asyncio.run(text_analysis(processed_data.get(\"text\"), models[\"llms\"])) if \"text\" in processed_data else None\n",
        "        }\n",
        "        return {**state, \"modality_results\": results}\n",
        "\n",
        "    def cross_modal_analysis(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        cross_modal_score = analyze_cross_modal_consistency(results, processed_data, models)\n",
        "        return {**state, \"cross_modal_score\": cross_modal_score}\n",
        "\n",
        "    def generate_report(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        cross_modal_score = state[\"cross_modal_score\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        report = generate_comprehensive_report(results, cross_modal_score, processed_data)\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    workflow = StateGraph(nodes=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"analyze_modalities\", analyze_modalities),\n",
        "        (\"cross_modal_analysis\", cross_modal_analysis),\n",
        "        (\"generate_report\", generate_report)\n",
        "    ])\n",
        "    workflow.add_edge(\"preprocess\", \"analyze_modalities\")\n",
        "    workflow.add_edge(\"analyze_modalities\", \"cross_modal_analysis\")\n",
        "    workflow.add_edge(\"cross_modal_analysis\", \"generate_report\")\n",
        "    workflow.add_edge(\"generate_report\", END)\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "2_x9xL7gZZs-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analyzing Cross-Modal Consistency and Generating Reports**"
      ],
      "metadata": {
        "id": "EF3CZ3zAaAPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_cross_modal_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    models: Dict[str, Any]\n",
        ") -> float:\n",
        "    scores = []\n",
        "    if results[\"audio\"] and results[\"video\"]:\n",
        "        sync_score = analyze_av_sync(processed_data[\"frames\"], processed_data[\"audio\"])\n",
        "        scores.append(sync_score)\n",
        "    semantic_score = analyze_semantic_consistency(results, processed_data, models[\"llms\"])\n",
        "    scores.append(semantic_score)\n",
        "    temporal_score = analyze_temporal_coherence(processed_data[\"temporal_features\"])\n",
        "    scores.append(temporal_score)\n",
        "    bio_score = analyze_biometric_consistency(processed_data[\"frames\"], models)\n",
        "    scores.append(bio_score)\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "def analyze_semantic_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    llms: Dict[str, ChatOpenAI]\n",
        ") -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"gpt4\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5\n",
        "\n",
        "def generate_comprehensive_report(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    cross_modal_score: float,\n",
        "    processed_data: Dict[str, Any]\n",
        ") -> MultimodalAnalysisReport:\n",
        "    scores = [\n",
        "        results[\"video\"].score if results[\"video\"] else 0.5,\n",
        "        results[\"audio\"].score if results[\"audio\"] else 0.5,\n",
        "        results[\"image\"].score if results[\"image\"] else 0.5,\n",
        "        results[\"text\"].score if results[\"text\"] else 0.5,\n",
        "        cross_modal_score\n",
        "    ]\n",
        "    weights = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "    final_score = sum(s * w for s, w in zip(scores, weights))\n",
        "    evidence = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            evidence.extend([\n",
        "                {\n",
        "                    \"type\": modality,\n",
        "                    \"description\": anomaly,\n",
        "                    \"confidence\": result.confidence,\n",
        "                    \"method\": result.method\n",
        "                }\n",
        "                for anomaly in result.anomalies\n",
        "            ])\n",
        "    return MultimodalAnalysisReport(\n",
        "        case_id=f\"DFD-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        file_info=processed_data[\"metadata\"] if \"metadata\" in processed_data else {},\n",
        "        video_analysis=results[\"video\"],\n",
        "        audio_analysis=results[\"audio\"],\n",
        "        image_analysis=results[\"image\"],\n",
        "        text_analysis=results[\"text\"],\n",
        "        multimodal_score=float(final_score),\n",
        "        verdict=\"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\",\n",
        "        evidence=evidence,\n",
        "        metadata={\n",
        "            \"processing_time\": datetime.now().isoformat(),\n",
        "            \"models_used\": list(results.keys()),\n",
        "            \"cross_modal_score\": cross_modal_score,\n",
        "            \"confidence_distribution\": {\n",
        "                modality: result.confidence\n",
        "                for modality, result in results.items() if result\n",
        "            }\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "H0mTuH2hZ5de"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Real-Time Streaming Analysis**\n",
        "- Define a function for real-time streaming analysis"
      ],
      "metadata": {
        "id": "Ov48BT01aOO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def real_time_streaming_analysis(video_stream: Any, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": 30,  # Assuming 30 FPS for real-time streaming\n",
        "        \"frame_count\": 0,\n",
        "        \"width\": 0,\n",
        "        \"height\": 0,\n",
        "        \"duration\": 0,\n",
        "        \"codec\": \"N/A\",\n",
        "        \"file_size\": 0\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while True:\n",
        "        frame = await video_stream.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "    video_data = {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }\n",
        "\n",
        "    return await advanced_video_analysis(video_data, models, device)"
      ],
      "metadata": {
        "id": "e6-q4yeeaKEU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Text Analysis**\n",
        "- Define a function for text analysis using Groq models:"
      ],
      "metadata": {
        "id": "hHpkFFGDaqtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def text_analysis(text: str, llms: Dict[str, ChatOpenAI]) -> DeepfakeAnalysisResult:\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze the following text for signs of manipulation or inconsistencies:\\n\\n{text}\"\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        score = float(score_match.group(1))\n",
        "    else:\n",
        "        score = 0.5\n",
        "    anomalies = [line.strip() for line in content.split(\"Reasoning:\")[-1].strip().split(\"\\n\") if line.strip()]\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=score,\n",
        "        label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"text_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "UpL42dfZaRF7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Metadata Analysis**\n",
        "- Define a function for metadata analysis"
      ],
      "metadata": {
        "id": "8WGV0gpibBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_analysis(metadata: Dict[str, Any]) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Example metadata analysis\n",
        "    if metadata.get(\"fps\") < 10:\n",
        "        anomalies.append(\"Unusually low frame rate\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"duration\") < 1:\n",
        "        anomalies.append(\"Unusually short duration\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"file_size\") < 100000:\n",
        "        anomalies.append(\"Unusually small file size\")\n",
        "        scores.append(0.2)\n",
        "\n",
        "    final_score = np.mean(scores) if scores else 0.5\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)) if scores else 0.0,\n",
        "        method=\"metadata_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "lr_VZJS_aRIf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Environment Setup**\n",
        "- Define the function to set up the enhanced detection environment"
      ],
      "metadata": {
        "id": "OM8ZJ7S8bwks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VideoMAEForVideoClassification, TimesformerForVideoClassification, LlavaForConditionalGeneration, CLIPModel, InstructBlipForConditionalGeneration, VisionEncoderDecoderModel, EfficientNetForImageClassification, Wav2Vec2ForSequenceClassification, WhisperForAudioClassification, HubertForCTC, AutoModelForSequenceClassification, BlenderbotForConditionalGeneration\n",
        "from transformers import CLIPProcessor, InstructBlipProcessor, ViTImageProcessor, Wav2Vec2Processor, WhisperProcessor, AutoTokenizer, BlenderbotTokenizer, AutoImageProcessor, LlavaProcessor\n",
        "import mediapipe as mp\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def setup_enhanced_detection_environment():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    models = {\n",
        "        \"videomae\": VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\"),\n",
        "        \"timesformer\": TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\"),\n",
        "        \"llava\": LlavaForConditionalGeneration.from_pretrained(\"llava/llava-v1\"),\n",
        "        \"clip\": CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "        \"instructblip\": InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "        \"vit_gpt2\": VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"),\n",
        "        \"efficientnet\": EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\"),\n",
        "        \"wav2vec2\": Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-large-960h\"),\n",
        "        \"whisper\": WhisperForAudioClassification.from_pretrained(\"openai/whisper-large-v3\"),\n",
        "        \"hubert\": HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\"),\n",
        "        \"roberta_fake\": AutoModelForSequenceClassification.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "        \"blenderbot\": BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\"),\n",
        "        \"face_detector\": mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.7),\n",
        "        \"face_mesh\": mp.solutions.face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            min_detection_confidence=0.7\n",
        "        ),\n",
        "        \"processors\": {\n",
        "            \"clip\": CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "            \"instructblip\": InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "            \"vit\": ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\"),\n",
        "            \"wav2vec2\": Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\"),\n",
        "            \"whisper\": WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\"),\n",
        "            \"roberta\": AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "            \"blenderbot\": BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\"),\n",
        "            \"efficientnet\": AutoImageProcessor.from_pretrained(\"google/efficientnet-b7\"),\n",
        "            \"hubert\": Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\"),\n",
        "            \"llava\": LlavaProcessor.from_pretrained(\"llava/llava-v1\")\n",
        "        },\n",
        "        \"llms\": {\n",
        "            \"gpt4\": ChatOpenAI(model=\"gpt-4\", temperature=0.2),\n",
        "            \"claude\": ChatOpenAI(model=\"claude-3-opus\", temperature=0.2),\n",
        "            \"groq_llm\": ChatOpenAI(model=\"groq-llm\", temperature=0.2)  # Add Groq LLM here\n",
        "        }\n",
        "    }\n",
        "    return {\"device\": device, \"models\": models}"
      ],
      "metadata": {
        "id": "1zl-XfTIcdou"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Main Function**\n",
        "- Define the main function to run the deepfake detection system"
      ],
      "metadata": {
        "id": "iZRCoZgobKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_deepfake_detection(file_path: str, mode: str = \"all\") -> MultimodalAnalysisReport:\n",
        "    try:\n",
        "        env = setup_enhanced_detection_environment()\n",
        "        tools = create_deepfake_detection_tools(env[\"models\"], env[\"device\"])\n",
        "        agent = create_detection_agent(tools, env[\"models\"][\"llms\"][\"gpt4\"])\n",
        "        workflow = create_detection_graph()\n",
        "        initial_state = {\n",
        "            \"input\": file_path,\n",
        "            \"mode\": mode,\n",
        "            \"models\": env[\"models\"],\n",
        "            \"device\": env[\"device\"]\n",
        "        }\n",
        "        final_state = workflow.run(initial_state)\n",
        "        return final_state[\"final_report\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"actual/path/to/your/content.mp4\"\n",
        "    report = asyncio.run(run_deepfake_detection(file_path))\n",
        "    print(\"\\nDeepfake Detection Report:\")\n",
        "    print(json.dumps(report.dict(), indent=2, default=str))"
      ],
      "metadata": {
        "id": "Xseky1jpaRK3",
        "outputId": "5a9f98be-2856-4a0b-9a24-233e709b4a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938,
          "referenced_widgets": [
            "7552a6e5d9f74eac801f2b874f0aa9c1",
            "3d59c74e47464ab9a96576940d5dcdaa",
            "fa9a5e66b5594e5d8f92bcf25ab161b4",
            "bd340deb99f049baba9cb421db07a8d0",
            "ef1b5fd9f0ec4d1388c321d389629984",
            "32cf60c338fa426288b02fd781999ef9",
            "583f5108bc354b6d8f55129b2a0be290",
            "c894cb197641406e9850057921a628de",
            "bde1c24b7f1a443f822afd7266188e24",
            "84b37b67a3964213bb22e31b51999ae4",
            "d48d5f3e0395430bb9ddbf4a235370d6",
            "ed0f9b42b1f9488db5772451662d803d",
            "1580e06b61d94448b814cc046386e4a8",
            "3b03aac8954f43c5b3a3a1020dfc23f0",
            "de565afb52e14c2b94332753b3e4983e",
            "2632f4c03f5c4a18a131dbc97ae24fcb",
            "3e92cbc0dcab4602b7c1a750a8c0169f",
            "543c6c8d003e42afa8b776fed7ce4150",
            "0bb23ca02913432d8f29af600644d5e0",
            "a5ed3cea8a004c148e080fcc5f473d1e",
            "031566c5107b4934a977ca58f159cdd0",
            "c284454d0828442d88315d13ac3d4d36",
            "aa18674f4f3b48798806a7cb7be91d13",
            "9457c5169e344966a31737a7f1de7a9a",
            "2799d0a6635741908c78fa1110858592",
            "eea0053c54534803b5157bbaca6973f3",
            "fd962dd306a148d2b5c440b5121ab712",
            "fa95ea235cd640719c55462b2e6269f1",
            "db5e51529bb54444b6b63f9146d86744",
            "4d4bca25f5884ffeb558f278c146118c",
            "a8ee631ab4ef4631aa84f8b610e43889",
            "8f62e5f92fe6472db508e0194ac37fe7",
            "375798728e09482dbbe6357daf413ac8",
            "a0f02ce9ed96449d99b5477a4a94cd27",
            "d073a617e9714e5881994433068666ed",
            "ab9441e3ba7241b99fb8da174f89dec2",
            "9ff3bfd5bb6646358e1133590dbd9369",
            "ba22f95f060d4e1bb0a83e68fcc51d54",
            "35cbd78282384ece8b92c27aeda07027",
            "20c14ab7af684341ab2170588c12faac",
            "3d94324466d74681930f6e4082c08d61",
            "e9579990de2b45ae8865e3dcd78efb8f",
            "b776a336bdff4254864f43a5326ebc3d",
            "1c5232fabfab418a9a50c35d218a9a9d"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/22.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7552a6e5d9f74eac801f2b874f0aa9c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed0f9b42b1f9488db5772451662d803d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/22.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa18674f4f3b48798806a7cb7be91d13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0f02ce9ed96449d99b5477a4a94cd27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in deepfake detection: llava/llava-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "llava/llava-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/llava/llava-v1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-678791d9-557cb92a4887c3d51f0fdbd0;8982e899-c16a-474a-b821-54563bba7645)\n\nRepository Not Found for url: https://huggingface.co/llava/llava-v1/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c07fafecf925>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"actual/path/to/your/content.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_deepfake_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDeepfake Detection Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c07fafecf925>\u001b[0m in \u001b[0;36mrun_deepfake_detection\u001b[0;34m(file_path, mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_deepfake_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMultimodalAnalysisReport\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_enhanced_detection_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_deepfake_detection_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_detection_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gpt4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-0dd609d56035>\u001b[0m in \u001b[0;36msetup_enhanced_detection_environment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"videomae\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVideoMAEForVideoClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MCG-NJU/videomae-base-finetuned-kinetics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"timesformer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimesformerForVideoClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/timesformer-base-finetuned-k400\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m\"llava\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlavaForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llava/llava-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openai/clip-vit-large-patch14\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m\"instructblip\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstructBlipForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salesforce/instructblip-vicuna-7b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3519\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: llava/llava-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yEbjYmL9aROX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}