{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49ae369e9e66439c9786eb607bd2141f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bf5bf2896ee4a58a995d54933994362",
              "IPY_MODEL_76590b92609748a6a2b02da1a14e43d6"
            ],
            "layout": "IPY_MODEL_8e6417994e24407f896e0612987633ba"
          }
        },
        "7bf5bf2896ee4a58a995d54933994362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47f7366385674e708e94c0daa028ebe6",
              "IPY_MODEL_c0c32a1b2fdf4032a2d0d15fb07217fa",
              "IPY_MODEL_bb711aa2a6bf455c917f09018ec67995",
              "IPY_MODEL_d13690cf18ed4fa1b28c674ae66ee2f4"
            ],
            "layout": "IPY_MODEL_eb5754e5685f4727806790b1bf2a1547"
          }
        },
        "76590b92609748a6a2b02da1a14e43d6": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_bb6b00baa64a424cba2c3e61ca4b3052",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "[2025-02-06 11:12:16.182466] Starting deepfake detection for file:\n",
                  "uploads/egmayyfjft.mp4\n",
                  "in mode: all\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "8e6417994e24407f896e0612987633ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47f7366385674e708e94c0daa028ebe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload File",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_c21e21b4231048c680aa7af9d9027fd8",
            "metadata": [
              {
                "name": "egmayyfjft.mp4",
                "type": "video/mp4",
                "size": 3346104,
                "lastModified": 1738818209434
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_244b0d17d51d4626942e5ea7c09f5efc"
          }
        },
        "c0c32a1b2fdf4032a2d0d15fb07217fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "All",
              "Audio",
              "Video",
              "Image"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Mode:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_f93b14c4be524ad58dd8512f16530de3",
            "style": "IPY_MODEL_414d6af612e0425f9289369ddbbd1b08"
          }
        },
        "bb711aa2a6bf455c917f09018ec67995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Start Detection",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f5ca9efa870a4dfe83821322dadc5006",
            "style": "IPY_MODEL_c24bc4a3f6384785ae9535e30f336638",
            "tooltip": ""
          }
        },
        "d13690cf18ed4fa1b28c674ae66ee2f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Progress:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6713595276e42528231ba79f69aa16c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15b7e2215bdf44c6b5e0d961c216cd8f",
            "value": 10
          }
        },
        "eb5754e5685f4727806790b1bf2a1547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21e21b4231048c680aa7af9d9027fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "244b0d17d51d4626942e5ea7c09f5efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": "lightblue",
            "font_weight": ""
          }
        },
        "f93b14c4be524ad58dd8512f16530de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "414d6af612e0425f9289369ddbbd1b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5ca9efa870a4dfe83821322dadc5006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "c24bc4a3f6384785ae9535e30f336638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f6713595276e42528231ba79f69aa16c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "300px"
          }
        },
        "15b7e2215bdf44c6b5e0d961c216cd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb6b00baa64a424cba2c3e61ca4b3052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "200px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install -q vllm transformers torch opencv-python librosa numpy face-recognition groq\n",
        "!pip install -qU dlib mediapipe scipy pillow tqdm pydantic moviepy langchain_community langgraph dtw-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import base64\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from pydantic import BaseModel, Field\n",
        "from transformers import (\n",
        "    CLIPProcessor, Wav2Vec2Processor, VideoMAEFeatureExtractor,\n",
        "    EfficientNetForImageClassification, LlavaForConditionalGeneration,\n",
        ")\n",
        "from vllm import LLM, SamplingParams\n",
        "from groq import Groq  # Groq Python client\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "from moviepy import VideoFileClip\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from functools import lru_cache\n",
        "from skimage import exposure\n",
        "from scipy.signal import savgol_filter\n",
        "from dtw import dtw\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Allow nested asyncio loops (useful in notebooks)\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "YvGEVqyrw7k3",
        "outputId": "b86482d4-7afe-428a-810b-f83a83b97f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-07 10:21:02 __init__.py:190] Automatically detected platform cuda.\n",
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up the Groq Client and More Models for Text, Audio and Vedio Analysis**"
      ],
      "metadata": {
        "id": "-ytDSCQDUAje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Retrieve Groq API key from environment or prompt the user.\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your Groq API key: \")\n",
        "\n",
        "# Define model paths for vLLM-based models.\n",
        "MODEL_PATHS = {\n",
        "    \"llava_next_video\": \"llava-hf/llava-next-video\",  # Video modality\n",
        "    \"internvl\": \"OpenGVLab/InternVL2_5-4B\",            # Video modality\n",
        "    \"xclip\": \"xclip-model\",                           # Video modality\n",
        "    \"wav2vec2\": \"facebook/wav2vec2-large-960h\",        # Audio modality\n",
        "    \"clip\": \"openai/clip-vit-large-patch14\"           # Image modality\n",
        "}\n",
        "\n",
        "# Optimization parameters.\n",
        "GPU_MEMORY_UTILIZATION = 0.85  # Lower than default (0.95) to leave headroom.\n",
        "CPU_OFFLOAD_GB = 6             # Increase CPU offload (from 4GB to 6GB).\n",
        "MAX_SEQ_LEN = 1024             # Reduced maximum sequence length.\n",
        "\n",
        "# --- vLLM Models Initialization ---\n",
        "loaded_models = {}\n",
        "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading vLLM models...\")\n",
        "\n",
        "for model_key in [\"llava_next_video\", \"internvl\", \"xclip\", \"wav2vec2\", \"clip\"]:\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading {model_key} ...\")\n",
        "    model = LLM(\n",
        "        model=MODEL_PATHS[model_key],\n",
        "        task=\"generate\",\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "        cpu_offload_gb=CPU_OFFLOAD_GB,\n",
        "        dtype=torch.float16,                   # Use half precision for Tesla T4 compatibility.\n",
        "        max_seq_len_to_capture=MAX_SEQ_LEN       # Use the accepted parameter name.\n",
        "    )\n",
        "    loaded_models[model_key] = model\n",
        "    torch.cuda.empty_cache()  # Clear transient GPU memory after each model load.\n",
        "\n",
        "print(\"vLLM models loaded:\", list(loaded_models.keys()))\n",
        "\n",
        "# --- Groq Client Initialization ---\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "groq_models = {\n",
        "    \"text\": \"llama-3.3-70b-versatile\",         # For text completions.\n",
        "    \"vision\": \"llama-3.2-90b-vision-preview\",   # For vision tasks.\n",
        "    \"audio\": \"whisper-large-v3-turbo\"           # For audio tasks.\n",
        "}\n",
        "print(\"Groq models initialized:\", groq_models)"
      ],
      "metadata": {
        "id": "BIxEeTxeUNJz",
        "outputId": "3336f2c1-a077-40ee-f511-9a0f48677116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Groq API key: ··········\n",
            "[10:29:36] Loading vLLM models...\n",
            "[10:29:36] Loading llava_next_video ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No supported config format found in llava-hf/llava-next-video",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-48742f2c76af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"llava_next_video\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"internvl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xclip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wav2vec2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{datetime.now().strftime('%H:%M:%S')}] Loading {model_key} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     model = LLM(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"generate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     )\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# to avoid import order issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_engine_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         self.llm_engine = self.engine_class.from_engine_args(\n\u001b[0m\u001b[1;32m    243\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;34m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# Create the engine configs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mengine_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mdevice_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeviceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         if (model_config.is_multimodal_model and not envs.VLLM_USE_V1\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36mcreate_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m         return ModelConfig(\n\u001b[0m\u001b[1;32m    999\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, allowed_local_media_path, revision, code_revision, rope_scaling, rope_theta, tokenizer_revision, max_model_len, spec_target_max_model_len, quantization, enforce_eager, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name, limit_mm_per_prompt, use_async_output_proc, config_format, hf_overrides, mm_processor_kwargs, disable_mm_preprocessor_cache, override_neuron_config, override_pooler_config, logits_processor_pattern, generation_config, enable_sleep_mode, override_generation_config, model_impl)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep mode is only supported on CUDA devices.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         hf_config = get_config(self.model, trust_remote_code, revision,\n\u001b[0m\u001b[1;32m    303\u001b[0m                                code_revision, config_format)\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(model, trust_remote_code, revision, code_revision, config_format, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m                         token=HF_TOKEN)\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No supported config format found in {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mConfigFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No supported config format found in llava-hf/llava-next-video"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- vLLM Model Loading Functions ---\n",
        "loaded_models = {}\n",
        "\n",
        "def load_vllm_model(model_name: str, task: str = \"generate\") -> LLM:\n",
        "    return LLM(\n",
        "        model=Config.MODEL_PATHS[model_name],\n",
        "        task=task,\n",
        "        gpu_memory_utilization=Config.GPU_MEMORY_UTILIZATION,\n",
        "        cpu_offload_gb=Config.CPU_OFFLOAD_GB,\n",
        "        dtype=torch.float16,           # Use half precision for compatibility with Tesla T4\n",
        "        max_seq_len=Config.MAX_SEQ_LEN   # Reduced maximum sequence length\n",
        "    )\n",
        "\n",
        "def get_model(model_name: str, task: str = \"generate\") -> LLM:\n",
        "    if model_name not in loaded_models:\n",
        "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading model: {model_name} ...\")\n",
        "        loaded_models[model_name] = load_vllm_model(model_name, task)\n",
        "        torch.cuda.empty_cache()  # Clear transient GPU memory after each load\n",
        "    return loaded_models[model_name]\n",
        "\n",
        "def load_models_sequential(model_names, task: str = \"generate\"):\n",
        "    for name in model_names:\n",
        "        get_model(name, task)"
      ],
      "metadata": {
        "id": "nOqYZDPK-0Xy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Groq Client Initialization ---\n",
        "def initialize_groq_client():\n",
        "    client = Groq(api_key=Config.GROQ_API_KEY)\n",
        "    groq_models = {\n",
        "        \"text\": \"llama-3.3-70b-versatile\",         # Text model for completions\n",
        "        \"vision\": \"llama-3.2-90b-vision-preview\",   # Vision model\n",
        "        \"audio\": \"whisper-large-v3-turbo\"           # Audio model\n",
        "    }\n",
        "    print(\"Groq models initialized successfully!\")\n",
        "    return client, groq_models"
      ],
      "metadata": {
        "id": "VdyWgw66Qm8R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "def main():\n",
        "    # Load vLLM-based models sequentially\n",
        "    vllm_model_names = [\"llava_next_video\", \"internvl\", \"xclip\", \"wav2vec2\", \"clip\"]\n",
        "    load_models_sequential(vllm_model_names)\n",
        "\n",
        "    # Initialize the Groq client and retrieve Groq model names\n",
        "    groq_client, groq_models = initialize_groq_client()\n",
        "\n",
        "    # Build an environment dictionary that aggregates both Groq and vLLM models\n",
        "    env = {\n",
        "        \"models\": {\n",
        "            \"text\": {\n",
        "                # For text completions, we simply store the Groq model name.\n",
        "                \"llama\": groq_models[\"text\"]\n",
        "            },\n",
        "            \"video\": {\n",
        "                \"llava_next_video\": get_model(\"llava_next_video\"),\n",
        "                \"internvl\": get_model(\"internvl\"),\n",
        "                \"xclip\": get_model(\"xclip\")\n",
        "            },\n",
        "            \"image\": {\n",
        "                # For image, we combine Groq (vision) and vLLM (clip) models.\n",
        "                \"vision\": groq_models[\"vision\"],\n",
        "                \"clip\": get_model(\"clip\")\n",
        "            },\n",
        "            \"audio\": {\n",
        "                # For audio, store Groq model for Whisper and load vLLM for wav2vec2.\n",
        "                \"whisper\": groq_models[\"audio\"],\n",
        "                \"wav2vec2\": get_model(\"wav2vec2\")\n",
        "            }\n",
        "        },\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    }\n",
        "\n",
        "    print(\"All models loaded successfully!\")\n",
        "    print(\"Environment configuration:\")\n",
        "    for modality, models in env[\"models\"].items():\n",
        "        print(f\"  {modality}: {list(models.keys())}\")\n",
        "\n",
        "    # Example: Set up sampling parameters (adjust as needed)\n",
        "    sampling_params = SamplingParams(temperature=0.7, top_p=0.9)\n",
        "    # (Optional) You may test a generation using one of the vLLM models.\n",
        "    # For example, using the \"internvl\" model for a dummy prompt:\n",
        "    # result = env[\"models\"][\"video\"][\"internvl\"].generate([\"Test prompt for vLLM generation\"], sampling_params)\n",
        "    # print(\"Generation result:\", result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "1tw71P6-QsyP",
        "outputId": "6936b6ff-7211-4fcf-a9ea-51f6758fb068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10:23:28] Loading model: llava_next_video ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EngineArgs.__init__() got an unexpected keyword argument 'max_seq_len'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2fad9e464ad5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-2fad9e464ad5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load vLLM-based models sequentially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvllm_model_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"llava_next_video\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"internvl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xclip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wav2vec2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mload_models_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_model_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Initialize the Groq client and retrieve Groq model names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-27e568d55b38>\u001b[0m in \u001b[0;36mload_models_sequential\u001b[0;34m(model_names, task)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_models_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"generate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-27e568d55b38>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model_name, task)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{datetime.now().strftime('%H:%M:%S')}] Loading model: {model_name} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloaded_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vllm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear transient GPU memory after each load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-27e568d55b38>\u001b[0m in \u001b[0;36mload_vllm_model\u001b[0;34m(model_name, task)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vllm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"generate\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     return LLM(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_PATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     )\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcompilation_config_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         engine_args = EngineArgs(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EngineArgs.__init__() got an unexpected keyword argument 'max_seq_len'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Models**\n",
        "- Define the data models for storing analysis results and reports:\n"
      ],
      "metadata": {
        "id": "X8RzT9-AWZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float = Field(..., description=\"Confidence score (0-1)\")\n",
        "    label: str = Field(..., description=\"Classification label\")\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float = Field(..., description=\"Model confidence\")\n",
        "    method: str = Field(..., description=\"Detection method used\")\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Dict[str, Any]]\n",
        "    metadata: Dict[str, Any]"
      ],
      "metadata": {
        "id": "MyJ0N4mSWZio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frame Stabilization**\n"
      ],
      "metadata": {
        "id": "vZdtcqEPytTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stabilize_frames(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    stabilized_frames = []\n",
        "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
        "    transforms = []\n",
        "\n",
        "    for i in range(1, len(frames)):\n",
        "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        transform = cv2.estimateRigidTransform(prev_gray, curr_gray, False)\n",
        "        transforms.append(transform)\n",
        "        prev_gray = curr_gray\n",
        "\n",
        "    for i, frame in enumerate(frames):\n",
        "        if i == 0:\n",
        "            stabilized_frames.append(frame)\n",
        "        else:\n",
        "            stabilized_frame = cv2.warpAffine(frame, transforms[i-1], (frame.shape[1], frame.shape[0]))\n",
        "            stabilized_frames.append(stabilized_frame)\n",
        "\n",
        "    return stabilized_frames\n",
        "\n",
        "def adaptive_noise_reduction(audio_data: np.ndarray) -> np.ndarray:\n",
        "    noise_profile = np.mean(audio_data[:1000])\n",
        "    reduced_noise_audio = audio_data - noise_profile\n",
        "    return reduced_noise_audio"
      ],
      "metadata": {
        "id": "kNzO5fgyy0sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Preprocessing**\n",
        "- Define helper functions for preprocessing audio, image, and video data:"
      ],
      "metadata": {
        "id": "0ktikzemXOd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_image_resolution(image_path: str) -> np.ndarray:\n",
        "    image = cv2.imread(image_path)\n",
        "    image_upscaled = cv2.resize(image, (image.shape[1] * 2, image.shape[0] * 2), interpolation=cv2.INTER_CUBIC)\n",
        "    return image_upscaled\n",
        "\n",
        "def adaptive_histogram_equalization(image: np.ndarray) -> np.ndarray:\n",
        "    return exposure.equalize_adapthist(image, clip_limit=0.03)\n",
        "\n",
        "def spectral_noise_reduction(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:\n",
        "    reduced_noise_audio = savgol_filter(audio_data, window_length=51, polyorder=3)\n",
        "    return reduced_noise_audio\n",
        "\n",
        "def temporal_alignment_dtw(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    aligned_frames = []\n",
        "    for i in range(1, len(frames)):\n",
        "        alignment = dtw(frames[i-1], frames[i])\n",
        "        aligned_frames.append(alignment.index2)\n",
        "    return aligned_frames\n",
        "\n",
        "def deblur_image(image: np.ndarray) -> np.ndarray:\n",
        "    gaussian_blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
        "    unsharp_image = cv2.addWeighted(image, 1.5, gaussian_blur, -0.5, 0)\n",
        "    return unsharp_image\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:\n",
        "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
        "    mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "    return {\n",
        "        \"mfcc\": mfcc,\n",
        "        \"chroma\": chroma,\n",
        "        \"mel\": mel\n",
        "    }\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return {\n",
        "        \"keypoints\": keypoints,\n",
        "        \"descriptors\": descriptors\n",
        "    }\n",
        "\n",
        "def calculate_dense_optical_flow(prev_frame: np.ndarray, curr_frame: np.ndarray) -> np.ndarray:\n",
        "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def extract_temporal_features(optical_flow: np.ndarray) -> Dict[str, Any]:\n",
        "    mag, ang = cv2.cartToPolar(optical_flow[..., 0], optical_flow[..., 1])\n",
        "    return {\n",
        "        \"magnitude\": mag,\n",
        "        \"angle\": ang\n",
        "    }\n",
        "\n",
        "def estimate_noise(frame: np.ndarray) -> float:\n",
        "    return np.mean(cv2.Laplacian(frame, cv2.CV_64F).var())\n",
        "\n",
        "def calculate_contrast(frame: np.ndarray) -> float:\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    return gray.std()\n",
        "\n",
        "def detect_compression_artifacts(frame: np.ndarray) -> float:\n",
        "    dct = cv2.dct(np.float32(frame) / 255.0)\n",
        "    return np.mean(np.abs(dct))"
      ],
      "metadata": {
        "id": "9A9KPnzU36TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Preprocessing Functions**\n",
        "- Define functions for preprocessing audio, image, and video data"
      ],
      "metadata": {
        "id": "13iwCPIrX-Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def enhanced_preprocessing(file_path: str) -> Dict[str, Any]:\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension in [\".mp3\", \".wav\", \".flac\"]:\n",
        "        return await enhanced_audio_preprocessing(file_path)\n",
        "    elif file_extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
        "        return await enhanced_image_preprocessing(file_path)\n",
        "    elif file_extension in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "        return await enhanced_video_preprocessing(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "async def enhanced_audio_preprocessing(audio_path: str) -> Dict[str, Any]:\n",
        "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
        "    audio_data = spectral_noise_reduction(audio_data, sample_rate)\n",
        "    audio_features = extract_audio_features(audio_data, sample_rate)\n",
        "    return {\"audio\": audio_data, \"sample_rate\": sample_rate, \"audio_features\": audio_features}\n",
        "\n",
        "async def enhanced_image_preprocessing(image_path: str) -> Dict[str, Any]:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = enhance_image_resolution(image)\n",
        "    image = adaptive_histogram_equalization(image)\n",
        "    image = deblur_image(image)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_features = extract_image_features(image_rgb)\n",
        "    return {\"image\": image_rgb, \"image_features\": image_features}\n",
        "\n",
        "async def enhanced_video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "        \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "        \"duration\": float(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / float(cap.get(cv2.CAP_PROP_FPS)),\n",
        "        \"codec\": int(cap.get(cv2.CAP_PROP_FOURCC)).to_bytes(4, byteorder='little').decode(),\n",
        "        \"file_size\": os.path.getsize(video_path)\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_image_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "    cap.release()\n",
        "\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is not None:\n",
        "            audio_array = audio.to_soundarray()\n",
        "            audio_data = extract_audio_features(audio_array, audio.fps)\n",
        "        video.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Audio extraction error: {e}\")\n",
        "        audio_data = None\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }"
      ],
      "metadata": {
        "id": "ltWcOSlP36Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- functions for analyzing transcription, vision response, and other features"
      ],
      "metadata": {
        "id": "GExLYgMaYVBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_transcription(transcription: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    if \"repeated phrase\" in transcription.lower():\n",
        "        anomalies.append(\"Repetitive phrases detected\")\n",
        "    if \"inconsistent timestamp\" in transcription.lower():\n",
        "        anomalies.append(\"Inconsistent timestamps detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_vision_response(response: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    if \"blurry region\" in response.lower():\n",
        "        anomalies.append(\"Blurry regions detected\")\n",
        "    if \"unnatural shadow\" in response.lower():\n",
        "        anomalies.append(\"Unnatural shadows detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_av_sync(frames: List[np.ndarray], audio: np.ndarray) -> float:\n",
        "    sync_score = 0.9\n",
        "    return sync_score\n",
        "\n",
        "def analyze_temporal_features(temporal_features: List[Dict[str, Any]]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_optical_flow(optical_flow: List[np.ndarray]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for flow in optical_flow:\n",
        "        if flow.max() > 1.0:\n",
        "            anomalies.append(\"Inconsistent flow patterns detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_biometric_consistency(frames: List[np.ndarray], models: Dict[str, Any]) -> float:\n",
        "    consistency_score = 0.9\n",
        "    return consistency_score\n",
        "\n",
        "def analyze_llava_response(response: str) -> float:\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    return float(score_match.group(1)) if score_match else 0.5"
      ],
      "metadata": {
        "id": "YkN1p_l2u-KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- Define functions for analyzing audio, image, and video data using Groq models"
      ],
      "metadata": {
        "id": "05aBMDMgY5CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Audio Analysis**"
      ],
      "metadata": {
        "id": "_KKDSAFn1uLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_audio_analysis(audio_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    if audio_data is None:\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.0,\n",
        "            label=\"NO_AUDIO\",\n",
        "            confidence=0.0,\n",
        "            method=\"audio_analysis\",\n",
        "            anomalies=[\"No audio data available\"]\n",
        "        )\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model 1: Whisper (Transcription)\n",
        "    response = groq_client.audio.transcriptions.create(\n",
        "        file=audio_data[\"waveform\"],\n",
        "        model=\"whisper-large-v3-turbo\",\n",
        "    )\n",
        "    transcription = response.text\n",
        "    anomalies.extend(analyze_transcription(transcription))\n",
        "    scores.append(0.9)\n",
        "\n",
        "    # Model 2: Wav2Vec2 (Audio Feature Extraction)\n",
        "    wav2vec2_model = models[\"wav2vec2\"].to(device)\n",
        "    wav2vec2_processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATHS[\"wav2vec2_processor\"])\n",
        "    inputs = wav2vec2_processor(audio_data[\"waveform\"], return_tensors=\"pt\", sampling_rate=16000).to(device)\n",
        "    with torch.no_grad():\n",
        "        wav2vec2_output = wav2vec2_model(**inputs)\n",
        "        wav2vec2_score = torch.softmax(wav2vec2_output.logits, dim=-1)\n",
        "        scores.append(wav2vec2_score.max().item())\n",
        "\n",
        "    # Model 3: Llava (Text Analysis)\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this audio for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(audio=audio_data[\"waveform\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "\n",
        "    # Model 4: DeepseekVL (Text Analysis)\n",
        "    deepseekvl_model = models[\"deepseek_vl2\"].to(device)\n",
        "    deepseekvl_processor = models[\"processors\"][\"deepseek_vl2\"]\n",
        "    inputs = deepseekvl_processor(audio=audio_data[\"waveform\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = deepseekvl_model.generate(**inputs)\n",
        "        analysis = deepseekvl_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        deepseekvl_score = analyze_llava_response(analysis)\n",
        "        scores.append(deepseekvl_score)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"audio_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "GDaz8dMlu-No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Image Analysis**"
      ],
      "metadata": {
        "id": "bWICXYdq1y2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_image_analysis(image: np.ndarray, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model: Llava (Text Analysis)\n",
        "    response = models['llava_model'].chat_completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze this image for signs of digital manipulation or inconsistencies.\",\n",
        "                \"image_url\": \"data:image/png;base64,\" + base64.b64encode(cv2.imencode('.png', image)[1]).decode()\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.2-90b-vision-preview\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        scores.append(float(score_match.group(1)))\n",
        "    anomalies.extend(analyze_vision_response(content))\n",
        "\n",
        "    # Model: CLIP (Image Feature Extraction)\n",
        "    clip_model = models[\"clip\"].to(device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        clip_output = clip_model(**inputs)\n",
        "        clip_score = torch.softmax(clip_output.logits_per_image, dim=-1)\n",
        "        scores.append(clip_score.max().item())\n",
        "\n",
        "    # Model: EfficientNet (Image Classification)\n",
        "    efficientnet_model = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b0\")\n",
        "    efficientnet_model = efficientnet_model.to(device)\n",
        "    efficientnet_processor = EfficientNetFeatureExtractor.from_pretrained(\"google/efficientnet-b0\")\n",
        "    inputs = efficientnet_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        efficientnet_output = efficientnet_model(**inputs)\n",
        "        efficientnet_score = torch.softmax(efficientnet_output.logits, dim=-1)\n",
        "        scores.append(efficientnet_score.max().item())\n",
        "\n",
        "    # Model: DeepseekVL (Text Analysis)\n",
        "    deepseekvl_model = models[\"deepseek_vl2\"].to(device)\n",
        "    deepseekvl_processor = models[\"processors\"][\"deepseek_vl2\"]\n",
        "    prompt = \"Analyze this image for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = deepseekvl_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = deepseekvl_model.generate(**inputs)\n",
        "        analysis = deepseekvl_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        deepseekvl_score = analyze_llava_response(analysis)\n",
        "        scores.append(deepseekvl_score)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"image_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "fVZT1Ci_13nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "def analyze_eye_movements(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "    blink_threshold = 0.2\n",
        "    blink_count = 0\n",
        "    for frame in frames:\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray)\n",
        "\n",
        "        for face in faces:\n",
        "            landmarks = predictor(gray, face)\n",
        "            left_eye_ratio = get_eye_aspect_ratio(landmarks, [36, 37, 38, 39, 40, 41])\n",
        "            right_eye_ratio = get_eye_aspect_ratio(landmarks, [42, 43, 44, 45, 46, 47])\n",
        "            if left_eye_ratio < blink_threshold or right_eye_ratio < blink_threshold:\n",
        "                blink_count += 1\n",
        "\n",
        "    if blink_count > len(frames) * 0.2:  # Example threshold for abnormal blink rate\n",
        "        anomalies.append(\"Abnormal blink rate detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def get_eye_aspect_ratio(landmarks, eye_points):\n",
        "    A = np.linalg.norm(np.array([landmarks.part(eye_points[1]).x, landmarks.part(eye_points[1]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[5]).x, landmarks.part(eye_points[5]).y]))\n",
        "    B = np.linalg.norm(np.array([landmarks.part(eye_points[2]).x, landmarks.part(eye_points[2]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[4]).x, landmarks.part(eye_points[4]).y]))\n",
        "    C = np.linalg.norm(np.array([landmarks.part(eye_points[0]).x, landmarks.part(eye_points[0]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[3]).x, landmarks.part(eye_points[3]).y]))\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def analyze_facial_blood_flow(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "    for frame in frames:\n",
        "        results = mp_face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks:\n",
        "            for face_landmarks in results.multi_face_landmarks:\n",
        "                if detect_blood_flow_anomaly(frame, face_landmarks):\n",
        "                    anomalies.append(\"Inconsistent blood flow patterns detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def detect_blood_flow_anomaly(frame, face_landmarks):\n",
        "    # Example logic to detect blood flow anomalies using color consistency\n",
        "    mean_color = np.mean(frame, axis=(0, 1))\n",
        "    if np.std(mean_color) > 10:  # Example threshold for color inconsistency\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def analyze_physics_based_patterns(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for frame in frames:\n",
        "        if detect_physics_anomaly(frame):\n",
        "            anomalies.append(\"Physics-based inconsistencies detected\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_physics_anomaly(frame):\n",
        "    # Example logic for detecting hair movement and shadow consistency\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    if np.mean(edges) > 50:  # Example threshold for detecting sharp edges\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def detect_neural_rendering_artifacts(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    gan_artifact_model = models[\"gan_artifact_model\"]\n",
        "\n",
        "    for frame in frames:\n",
        "        if detect_gan_artifacts(frame, gan_artifact_model):\n",
        "            anomalies.append(\"Neural rendering artifacts detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def detect_gan_artifacts(frame, model):\n",
        "    # Example logic for detecting GAN artifacts using a neural network\n",
        "    resized_frame = cv2.resize(frame, (224, 224))\n",
        "    frame_tensor = torch.tensor(resized_frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    with torch.no_grad():\n",
        "        output = model(frame_tensor.to(device))\n",
        "    if output.argmax() == 1:  # Assuming the model outputs 1 for GAN artifacts\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "HB7oFZEZy05y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Video Analysis**"
      ],
      "metadata": {
        "id": "MlQVINuo1_2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_video_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model 1: VideoMAE\n",
        "    videomae_model = models[\"videomae\"].to(device)\n",
        "    videomae_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"videomae\"])\n",
        "    inputs = videomae_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        videomae_output = videomae_model(**inputs)\n",
        "        videomae_score = torch.softmax(videomae_output.logits, dim=-1)\n",
        "        scores.append(videomae_score.max().item())\n",
        "\n",
        "    # Model 2: Timesformer\n",
        "    timesformer_model = models[\"timesformer\"].to(device)\n",
        "    timesformer_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"timesformer\"])\n",
        "    inputs = timesformer_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        timesformer_output = timesformer_model(**inputs)\n",
        "        timesformer_score = torch.softmax(timesformer_output.logits, dim=-1)\n",
        "        scores.append(timesformer_score.max().item())\n",
        "\n",
        "    # Model 3: Llava\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this video for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(video=video_data[\"frames\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "\n",
        "    # Model 4: InternVL\n",
        "    internvl_model = models[\"internvl\"].to(device)\n",
        "    internvl_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"internvl\"])\n",
        "    inputs = internvl_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        internvl_output = internvl_model(**inputs)\n",
        "        internvl_score = torch.softmax(internvl_output.logits, dim=-1)\n",
        "        scores.append(internvl_score.max().item())\n",
        "\n",
        "    # Eye Movement Analysis\n",
        "    eye_movement_anomalies = analyze_eye_movements(video_data[\"frames\"], models)\n",
        "    anomalies.extend(eye_movement_anomalies)\n",
        "\n",
        "    # Facial Blood Flow Patterns\n",
        "    blood_flow_anomalies = analyze_facial_blood_flow(video_data[\"frames\"], models)\n",
        "    anomalies.extend(blood_flow_anomalies)\n",
        "\n",
        "    # Physics-Based Verification\n",
        "    physics_based_anomalies = analyze_physics_based_patterns(video_data[\"frames\"], models)\n",
        "    anomalies.extend(physics_based_anomalies)\n",
        "\n",
        "    # Neural Rendering Artifacts\n",
        "    neural_rendering_anomalies = detect_neural_rendering_artifacts(video_data[\"frames\"], models)\n",
        "    anomalies.extend(neural_rendering_anomalies)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"video_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "jRAPWt8u2IsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, XCLIPModel\n",
        "from vllm import LLM, SamplingParams\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Lip Sync Detection\n",
        "def lip_sync_detection(frames, audio_text, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Detect lip synchronization using X-CLIP and vllm.\n",
        "\n",
        "    Args:\n",
        "        frames (torch.Tensor): Video frames\n",
        "        audio_text (str): Text representation of the audio\n",
        "        threshold (float): Sync threshold\n",
        "\n",
        "    Returns:\n",
        "        DeepfakeAnalysisResult: Lip sync analysis results\n",
        "    \"\"\"\n",
        "    # Load models and processor\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "    model = XCLIPModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "    llm = LLM(model=\"microsoft/xclip-base-patch32\", tensor_parallel_size=torch.cuda.device_count())\n",
        "\n",
        "    # Process video frames\n",
        "    video_inputs = processor(videos=list(frames), return_tensors=\"pt\")\n",
        "    video_features = model.get_video_features(**video_inputs).mean(dim=1)\n",
        "\n",
        "    # Generate text features\n",
        "    text_features = llm.generate(\n",
        "        inputs=[audio_text],\n",
        "        sampling_params=SamplingParams(temperature=0, max_tokens=64)\n",
        "    )[0].mean(dim=1)\n",
        "\n",
        "    # Compute cosine similarity scores\n",
        "    sync_scores = torch.nn.functional.cosine_similarity(video_features, text_features)\n",
        "    final_score = torch.mean(sync_scores).item()\n",
        "\n",
        "    # Detect anomalies and determine result\n",
        "    anomalies = [\"Lip-sync inconsistency detected\"] if final_score < threshold else []\n",
        "    result_label = \"REAL\" if final_score > threshold else \"FAKE\"\n",
        "    confidence = float(torch.std(sync_scores).item())\n",
        "\n",
        "    # Return analysis result using Pydantic schema\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=result_label,\n",
        "        confidence=confidence,\n",
        "        method=\"lip_sync_detection\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "49tH6GarH7Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_consistency_analysis(results: Dict[str, DeepfakeAnalysisResult], llms: Dict[str, Any]) -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"llava\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5"
      ],
      "metadata": {
        "id": "8K7h2pSWEbO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def face_forgery_detection(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model: Face X-ray\n",
        "    face_xray_model = models[\"face_xray\"].to(device)\n",
        "    face_xray_processor = CLIPProcessor.from_pretrained(Config.MODEL_PATHS[\"clip_processor\"])\n",
        "    inputs = face_xray_processor(images=video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        face_xray_output = face_xray_model(**inputs)\n",
        "        face_xray_score = torch.softmax(face_xray_output.logits_per_image, dim=-1)\n",
        "        scores.append(face_xray_score.max().item())\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"face_forgery_detection\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "q61-4f7OEeJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def background_consistency_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model: Vision Transformer\n",
        "    vit_model = models[\"vision\"].to(device)\n",
        "    vit_processor = CLIPProcessor.from_pretrained(Config.MODEL_PATHS[\"clip_processor\"])\n",
        "    inputs = vit_processor(images=video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        vit_output = vit_model(**inputs)\n",
        "        vit_score = torch.softmax(vit_output.logits_per_image, dim=-1)\n",
        "        scores.append(vit_score.max().item())\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"background_consistency_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "r_JDhnaAEkUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_temporal_coherence(temporal_features: List[Dict[str, Any]]) -> float:\n",
        "    anomalies = []\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return 1.0 - (len(anomalies) / len(temporal_features))"
      ],
      "metadata": {
        "id": "fnLHMoz0Epy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tool Creation Functions**\n",
        "- Define functions for creating deepfake detection tools and agents"
      ],
      "metadata": {
        "id": "HoftOwSUZPye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_deepfake_detection_tools(models: Dict[str, Any], device: torch.device) -> List[Tool]:\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"analyze_video\",\n",
        "            func=lambda x: advanced_video_analysis(x, models, device),\n",
        "            description=\"Analyzes video content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_audio\",\n",
        "            func=lambda x: advanced_audio_analysis(x, models, device),\n",
        "            description=\"Analyzes audio content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_image\",\n",
        "            func=lambda x: advanced_image_analysis(x, models, device),\n",
        "            description=\"Analyzes image content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"semantic_analysis\",\n",
        "            func=lambda x: semantic_consistency_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes semantic consistency across modalities\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"face_forgery_detection\",\n",
        "            func=lambda x: face_forgery_detection(x, models, device),\n",
        "            description=\"Detects face forgeries in video content\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"lip_sync_detection\",\n",
        "            func=lambda x: lip_sync_detection(x, models, device),\n",
        "            description=\"Analyzes lip-sync consistency between audio and video\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"background_consistency\",\n",
        "            func=lambda x: background_consistency_analysis(x, models, device),\n",
        "            description=\"Analyzes background consistency across frames\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"real_time_streaming_analysis\",\n",
        "            func=lambda x: real_time_streaming_analysis(x, models, device),\n",
        "            description=\"Analyzes live video streams for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"text_analysis\",\n",
        "            func=lambda x: text_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes text content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"metadata_analysis\",\n",
        "            func=lambda x: metadata_analysis(x),\n",
        "            description=\"Analyzes metadata for signs of manipulation\"\n",
        "        )\n",
        "    ]\n",
        "    return tools"
      ],
      "metadata": {
        "id": "PEWzNSs3ZGK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Detection Agent**\n",
        "- Define the detection graph for processing the input data"
      ],
      "metadata": {
        "id": "IgsLH-Xp2eW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langchain.tools import Tool\n",
        "\n",
        "def create_detection_agent(tools: List[Tool], llm: ChatOpenAI) -> AgentExecutor:\n",
        "    class DeepfakeDetectionOutputParser:\n",
        "        def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "            try:\n",
        "                if \"Final Answer:\" in llm_output:\n",
        "                    return AgentFinish(\n",
        "                        return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                        log=llm_output,\n",
        "                    )\n",
        "                action_match = re.search(r\"Action: (.*?)\\nAction Input: (.*)\", llm_output, re.DOTALL)\n",
        "                if not action_match:\n",
        "                    raise ValueError(\"Could not parse LLM output: \" + llm_output)\n",
        "                action = action_match.group(1).strip()\n",
        "                action_input = action_match.group(2).strip()\n",
        "                return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not parse LLM output: {llm_output}\") from e\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"\n",
        "            You are an expert forensic deepfake detection analyst.\n",
        "            You have received detailed outputs from various specialized modality tools:\n",
        "            - Video (spatiotemporal analysis)\n",
        "            - Audio (transcription and acoustic anomaly analysis)\n",
        "            - Image (ensemble analysis using multiple vision models)\n",
        "            - Text (extraction and reasoning)\n",
        "            - Face forgery and background consistency analysis\n",
        "            Your task is to review these outputs and provide a comprehensive, step-by-step explanation of the findings.\n",
        "            For each modality:\n",
        "              1. Summarize the key evidence.\n",
        "              2. Explain any detected anomalies or inconsistencies.\n",
        "            Finally, provide an overall verdict on the authenticity of the media and justify your decision.\n",
        "            Format your response as:\n",
        "            Action: [Tool Name]\n",
        "            Action Input: [Summary of results]\n",
        "            Observation: [Detailed step-by-step reasoning]\n",
        "            Final Answer: [Overall verdict with supporting details]\n",
        "            Available tools: {tools}\n",
        "        \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "    ])\n",
        "\n",
        "    return AgentExecutor.from_agent_and_tools(\n",
        "        agent=LLMSingleActionAgent(\n",
        "            llm_chain=LLMChain(llm=llm, prompt=prompt),\n",
        "            output_parser=DeepfakeDetectionOutputParser(),\n",
        "            stop=[\"Observation:\", \"Final Answer:\"],\n",
        "            allowed_tools=[tool.name for tool in tools]\n",
        "        ),\n",
        "        tools=tools,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "def create_detection_graph() -> StateGraph:\n",
        "    def preprocess(state):\n",
        "        input_data = state[\"input\"]\n",
        "        processed_data = asyncio.run(enhanced_preprocessing(input_data))\n",
        "        return {**state, \"processed_data\": processed_data}\n",
        "\n",
        "    def analyze_modalities(state):\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        device = state[\"device\"]\n",
        "        results = {\n",
        "            \"video\": asyncio.run(advanced_video_analysis(processed_data, models, device)) if \"frames\" in processed_data else None,\n",
        "            \"audio\": asyncio.run(advanced_audio_analysis(processed_data.get(\"audio\"), models, device)) if \"audio\" in processed_data else None,\n",
        "            \"image\": asyncio.run(advanced_image_analysis(processed_data[\"image\"], models, device)) if \"image\" in processed_data else None,\n",
        "            \"text\": asyncio.run(text_analysis(processed_data.get(\"text\"), models[\"llms\"])) if \"text\" in processed_data else None,\n",
        "            \"face_forgery\": face_forgery_detection(processed_data, models, device) if \"frames\" in processed_data else None,\n",
        "            \"background\": background_consistency_analysis(processed_data, models, device) if \"frames\" in processed_data else None\n",
        "        }\n",
        "        return {**state, \"modality_results\": results}\n",
        "\n",
        "    def cross_modal_analysis(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        cross_modal_score = analyze_cross_modal_consistency(results, processed_data, models)\n",
        "        return {**state, \"cross_modal_score\": cross_modal_score}\n",
        "\n",
        "    def generate_report(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        cross_modal_score = state[\"cross_modal_score\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        report = generate_comprehensive_report(results, cross_modal_score, processed_data)\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    workflow = StateGraph(nodes=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"analyze_modalities\", analyze_modalities),\n",
        "        (\"cross_modal_analysis\", cross_modal_analysis),\n",
        "        (\"generate_report\", generate_report)\n",
        "    ])\n",
        "    workflow.add_edge(\"preprocess\", \"analyze_modalities\")\n",
        "    workflow.add_edge(\"analyze_modalities\", \"cross_modal_analysis\")\n",
        "    workflow.add_edge(\"cross_modal_analysis\", \"generate_report\")\n",
        "    workflow.add_edge(\"generate_report\", END)\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "epoIrEie2nx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analyzing Cross-Modal Consistency and Generating Reports**"
      ],
      "metadata": {
        "id": "EF3CZ3zAaAPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_cross_modal_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    models: Dict[str, Any]\n",
        ") -> float:\n",
        "    scores = []\n",
        "    if results[\"audio\"] and results[\"video\"]:\n",
        "        sync_score = analyze_av_sync(processed_data[\"frames\"], processed_data[\"audio\"])\n",
        "        scores.append(sync_score)\n",
        "    semantic_score = analyze_semantic_consistency(results, processed_data, models[\"llms\"])\n",
        "    scores.append(semantic_score)\n",
        "    temporal_score = analyze_temporal_coherence(processed_data[\"temporal_features\"])\n",
        "    scores.append(temporal_score)\n",
        "    bio_score = analyze_biometric_consistency(processed_data[\"frames\"], models)\n",
        "    scores.append(bio_score)\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "def analyze_semantic_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    llms: Dict[str, ChatOpenAI]\n",
        ") -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"gpt4\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5\n",
        "\n",
        "def generate_comprehensive_report(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    cross_modal_score: float,\n",
        "    processed_data: Dict[str, Any]\n",
        ") -> MultimodalAnalysisReport:\n",
        "    scores = [\n",
        "        results[\"video\"].score if results[\"video\"] else 0.5,\n",
        "        results[\"audio\"].score if results[\"audio\"] else 0.5,\n",
        "        results[\"image\"].score if results[\"image\"] else 0.5,\n",
        "        results[\"text\"].score if results[\"text\"] else 0.5,\n",
        "        cross_modal_score\n",
        "    ]\n",
        "    weights = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "    final_score = sum(s * w for s, w in zip(scores, weights))\n",
        "    evidence = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            evidence.extend([\n",
        "                {\n",
        "                    \"type\": modality,\n",
        "                    \"description\": anomaly,\n",
        "                    \"confidence\": result.confidence,\n",
        "                    \"method\": result.method\n",
        "                }\n",
        "                for anomaly in result.anomalies\n",
        "            ])\n",
        "    return MultimodalAnalysisReport(\n",
        "        case_id=f\"DFD-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        file_info=processed_data[\"metadata\"] if \"metadata\" in processed_data else {},\n",
        "        video_analysis=results[\"video\"],\n",
        "        audio_analysis=results[\"audio\"],\n",
        "        image_analysis=results[\"image\"],\n",
        "        text_analysis=results[\"text\"],\n",
        "        multimodal_score=float(final_score),\n",
        "        verdict=\"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\",\n",
        "        evidence=evidence,\n",
        "        metadata={\n",
        "            \"processing_time\": datetime.now().isoformat(),\n",
        "            \"models_used\": list(results.keys()),\n",
        "            \"cross_modal_score\": cross_modal_score,\n",
        "            \"confidence_distribution\": {\n",
        "                modality: result.confidence\n",
        "                for modality, result in results.items() if result\n",
        "            }\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "H0mTuH2hZ5de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Real-Time Streaming Analysis**\n",
        "- Define a function for real-time streaming analysis"
      ],
      "metadata": {
        "id": "Ov48BT01aOO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def real_time_streaming_analysis(video_stream: Any, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": 30,\n",
        "        \"frame_count\": 0,\n",
        "        \"width\": 0,\n",
        "        \"height\": 0,\n",
        "        \"duration\": 0,\n",
        "        \"codec\": \"N/A\",\n",
        "        \"file_size\": 0\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_image_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while True:\n",
        "        frame = await video_stream.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "    video_data = {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }\n",
        "\n",
        "    return await advanced_video_analysis(video_data, models, device)"
      ],
      "metadata": {
        "id": "e6-q4yeeaKEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Text Analysis**\n",
        "- Define a function for text analysis using Groq models:"
      ],
      "metadata": {
        "id": "hHpkFFGDaqtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def text_analysis(text: str, llms: Dict[str, ChatOpenAI]) -> DeepfakeAnalysisResult:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze the following text for signs of manipulation or inconsistencies:\\n\\n{text}\"\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        score = float(score_match.group(1))\n",
        "    else:\n",
        "        score = 0.5\n",
        "    anomalies = [line.strip() for line in content.split(\"Reasoning:\")[-1].strip().split(\"\\n\") if line.strip()]\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=score,\n",
        "        label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"text_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "UpL42dfZaRF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Metadata Analysis**\n",
        "- Define a function for metadata analysis"
      ],
      "metadata": {
        "id": "8WGV0gpibBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_analysis(metadata: Dict[str, Any]) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    if metadata.get(\"fps\") < 10:\n",
        "        anomalies.append(\"Unusually low frame rate\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"duration\") < 1:\n",
        "        anomalies.append(\"Unusually short duration\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"file_size\") < 100000:\n",
        "        anomalies.append(\"Unusually small file size\")\n",
        "        scores.append(0.2)\n",
        "\n",
        "    final_score = np.mean(scores) if scores else 0.5\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)) if scores else 0.0,\n",
        "        method=\"metadata_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "lr_VZJS_aRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Main Function**\n",
        "- Define the main function to run the deepfake detection system"
      ],
      "metadata": {
        "id": "iZRCoZgobKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "lwK0ZBvj6NxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_uploaded_file(uploaded_file):\n",
        "    \"\"\"\n",
        "    Saves the uploaded file to a local directory and returns its file path.\n",
        "    \"\"\"\n",
        "    file_name = uploaded_file['metadata']['name']\n",
        "    upload_dir = \"uploads\"\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "    file_path = os.path.join(upload_dir, file_name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file['content'])\n",
        "    return file_path"
      ],
      "metadata": {
        "id": "gWC65JLB89k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_deepfake_detection(file_path: str, mode: str = \"all\") -> MultimodalAnalysisReport:\n",
        "    \"\"\"\n",
        "    Runs the deepfake detection pipeline asynchronously.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        state = {\n",
        "            \"input\": file_path,\n",
        "            \"mode\": mode,\n",
        "            \"models\": env[\"models\"],\n",
        "            \"device\": env[\"device\"]\n",
        "        }\n",
        "        workflow = create_detection_graph()\n",
        "        final_state = workflow.run(state)\n",
        "        return final_state[\"final_report\"]\n",
        "    except Exception as e:\n",
        "        print(f\"[{datetime.now()}] Error in deepfake detection: {str(e)}\")\n",
        "        raise RuntimeError(\"Deepfake detection failed. Check input and models.\") from e"
      ],
      "metadata": {
        "id": "yAEKfJjTyDVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Enhanced UI with ipywidgets\n",
        "# ---------------------------\n",
        "\n",
        "# File upload widget with custom style.\n",
        "file_upload = widgets.FileUpload(\n",
        "    accept='',\n",
        "    multiple=False,\n",
        "    description='Upload File',\n",
        "    style={'button_color': 'lightblue'},\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Dropdown to choose detection mode.\n",
        "mode_dropdown = widgets.Dropdown(\n",
        "    options=[('All', 'all'), ('Audio', 'audio'), ('Video', 'video'), ('Image', 'image')],\n",
        "    value='all',\n",
        "    description='Mode:',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Start button with a clear label.\n",
        "start_button = widgets.Button(\n",
        "    description=\"Start Detection\",\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Progress bar for visual feedback.\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=10,\n",
        "    description='Progress:',\n",
        "    bar_style='info',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Log output widget (scrollable for lengthy logs).\n",
        "log_output = widgets.Output(layout={'border': '1px solid black', 'height': '200px', 'overflow_y': 'auto'})\n",
        "\n",
        "# Global variable to store the uploaded file path.\n",
        "uploaded_file_path = None\n",
        "\n",
        "def on_file_upload_change(change):\n",
        "    global uploaded_file_path\n",
        "    if file_upload.value:\n",
        "        uploaded_file = list(file_upload.value.values())[0]\n",
        "        uploaded_file_path = save_uploaded_file(uploaded_file)\n",
        "        with log_output:\n",
        "            clear_output()\n",
        "            print(f\"[{datetime.now()}] File uploaded: {uploaded_file_path}\")\n",
        "        start_button.disabled = False\n",
        "\n",
        "async def process_detection(file_path, mode):\n",
        "    with log_output:\n",
        "        clear_output()\n",
        "        print(f\"[{datetime.now()}] Starting deepfake detection for file:\\n{file_path}\\nin mode: {mode}\\n\")\n",
        "    progress_bar.value = 0\n",
        "    # Simulate processing with periodic progress updates.\n",
        "    for i in range(10):\n",
        "        await asyncio.sleep(1)\n",
        "        progress_bar.value = i + 1\n",
        "    report = await run_deepfake_detection(file_path, mode)\n",
        "    with log_output:\n",
        "        print(\"\\nFinal Forensic Report:\")\n",
        "        print(json.dumps(report.dict(), indent=2, default=str))\n",
        "\n",
        "def on_start_button_click(b):\n",
        "    if uploaded_file_path:\n",
        "        asyncio.run(process_detection(uploaded_file_path, mode_dropdown.value))\n",
        "\n",
        "# Attach event handlers.\n",
        "file_upload.observe(on_file_upload_change, names='value')\n",
        "start_button.on_click(on_start_button_click)\n",
        "\n",
        "# Initially disable the start button until a file is uploaded.\n",
        "start_button.disabled = True\n",
        "\n",
        "# Layout: arrange widgets in a clean UI using containers.\n",
        "ui_left = widgets.VBox([file_upload, mode_dropdown, start_button, progress_bar])\n",
        "ui = widgets.HBox([ui_left, log_output])\n",
        "\n",
        "# Display the complete UI.\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "f32-YNDWJlpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592,
          "referenced_widgets": [
            "49ae369e9e66439c9786eb607bd2141f",
            "7bf5bf2896ee4a58a995d54933994362",
            "76590b92609748a6a2b02da1a14e43d6",
            "8e6417994e24407f896e0612987633ba",
            "47f7366385674e708e94c0daa028ebe6",
            "c0c32a1b2fdf4032a2d0d15fb07217fa",
            "bb711aa2a6bf455c917f09018ec67995",
            "d13690cf18ed4fa1b28c674ae66ee2f4",
            "eb5754e5685f4727806790b1bf2a1547",
            "c21e21b4231048c680aa7af9d9027fd8",
            "244b0d17d51d4626942e5ea7c09f5efc",
            "f93b14c4be524ad58dd8512f16530de3",
            "414d6af612e0425f9289369ddbbd1b08",
            "f5ca9efa870a4dfe83821322dadc5006",
            "c24bc4a3f6384785ae9535e30f336638",
            "f6713595276e42528231ba79f69aa16c",
            "15b7e2215bdf44c6b5e0d961c216cd8f",
            "bb6b00baa64a424cba2c3e61ca4b3052"
          ]
        },
        "outputId": "c2258851-e011-49da-da36-b823c298f724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(VBox(children=(FileUpload(value={}, description='Upload File', layout=Layout(width='300px'), st…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49ae369e9e66439c9786eb607bd2141f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-06 11:12:26.200228] Error in deepfake detection: name 'env' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Deepfake detection failed. Check input and models.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c074d52ee42d>\u001b[0m in \u001b[0;36mrun_deepfake_detection\u001b[0;34m(file_path, mode)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;34m\"device\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-dcb05f9a900f>\u001b[0m in \u001b[0;36mon_start_button_click\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mon_start_button_click\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muploaded_file_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_dropdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Attach event handlers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-dcb05f9a900f>\u001b[0m in \u001b[0;36mprocess_detection\u001b[0;34m(file_path, mode)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrun_deepfake_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlog_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinal Forensic Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c074d52ee42d>\u001b[0m in \u001b[0;36mrun_deepfake_detection\u001b[0;34m(file_path, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{datetime.now()}] Error in deepfake detection: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deepfake detection failed. Check input and models.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Deepfake detection failed. Check input and models."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEbvz2Uo8Muw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}