{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgr2hSO8XIk5O6/+aQEH0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d892d6b0974b4475ba91d6cff55f1156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e77d831c43274bcd991780c3291e6f54",
              "IPY_MODEL_b7c58f4027d540459e1ae10ae272b60c",
              "IPY_MODEL_a5c6bf7acb3d4428a7e5475c5550e7aa"
            ],
            "layout": "IPY_MODEL_f540ed3e745846e598e39fb79a98b8ea"
          }
        },
        "e77d831c43274bcd991780c3291e6f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eed41c55862e4052bc95a628a2839774",
            "placeholder": "​",
            "style": "IPY_MODEL_e8be241c1ed541dea002d53bd3fa5100",
            "value": "config.json: 100%"
          }
        },
        "b7c58f4027d540459e1ae10ae272b60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d5e972f58a4756ae38c3e928e8b9bb",
            "max": 22865,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9567fb8ff3304a8987d3d82d73bda287",
            "value": 22865
          }
        },
        "a5c6bf7acb3d4428a7e5475c5550e7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f783763107e496e93c3903c0b539bf1",
            "placeholder": "​",
            "style": "IPY_MODEL_0717abb327ea4b1b959d72bc96f62957",
            "value": " 22.9k/22.9k [00:00&lt;00:00, 1.51MB/s]"
          }
        },
        "f540ed3e745846e598e39fb79a98b8ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed41c55862e4052bc95a628a2839774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8be241c1ed541dea002d53bd3fa5100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5d5e972f58a4756ae38c3e928e8b9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9567fb8ff3304a8987d3d82d73bda287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f783763107e496e93c3903c0b539bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0717abb327ea4b1b959d72bc96f62957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "095060cf43f643beae5f7c1bfc118368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d90c839ad9c84a90b6b6bae54ff60660",
              "IPY_MODEL_9b8d89d8601a453581dcd8860449676c",
              "IPY_MODEL_0d33528061584b0ca21e0c8ae5b1af31"
            ],
            "layout": "IPY_MODEL_a942544dabff412283fa43161a85452a"
          }
        },
        "d90c839ad9c84a90b6b6bae54ff60660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9d5039cd54d463eb67ddf43c32c601f",
            "placeholder": "​",
            "style": "IPY_MODEL_d4722216e3a54e53b8b72fef46117243",
            "value": "model.safetensors: 100%"
          }
        },
        "9b8d89d8601a453581dcd8860449676c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb973c24879947d7ae7dbcdf90b2eba6",
            "max": 346161616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42dbf905acb9425c94aa1419b6375192",
            "value": 346161616
          }
        },
        "0d33528061584b0ca21e0c8ae5b1af31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c06b35e5efd4de0864c39bc42ed47af",
            "placeholder": "​",
            "style": "IPY_MODEL_0e8bd97078ee47d0b98c24e0af7fc3f1",
            "value": " 346M/346M [00:14&lt;00:00, 24.0MB/s]"
          }
        },
        "a942544dabff412283fa43161a85452a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9d5039cd54d463eb67ddf43c32c601f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4722216e3a54e53b8b72fef46117243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb973c24879947d7ae7dbcdf90b2eba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42dbf905acb9425c94aa1419b6375192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c06b35e5efd4de0864c39bc42ed47af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8bd97078ee47d0b98c24e0af7fc3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a9c95d3210d4f07bbbebb559b30a499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8b0886d4aa740cea464c94aeaf7c285",
              "IPY_MODEL_a13605e1d3ff4dd38dfcbff119399640",
              "IPY_MODEL_7bc462801f294d5bbfd20b575ec6b4dc"
            ],
            "layout": "IPY_MODEL_8f5f4da0becd4954b0bd805068000fcf"
          }
        },
        "f8b0886d4aa740cea464c94aeaf7c285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b118f91f2ea4fb9b0df00353427c8f2",
            "placeholder": "​",
            "style": "IPY_MODEL_f442462d3937403a8ddca89c7ac2d20b",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "a13605e1d3ff4dd38dfcbff119399640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d04d06ae7a4a4ab18322d529431b1c",
            "max": 271,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fc7a4884ac84993989e42298dc0fc35",
            "value": 271
          }
        },
        "7bc462801f294d5bbfd20b575ec6b4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91defa4c05aa4834a0bd94cfb60d6134",
            "placeholder": "​",
            "style": "IPY_MODEL_77d59c57a0f9428dbb1dbbb65509341b",
            "value": " 271/271 [00:00&lt;00:00, 13.0kB/s]"
          }
        },
        "8f5f4da0becd4954b0bd805068000fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b118f91f2ea4fb9b0df00353427c8f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f442462d3937403a8ddca89c7ac2d20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63d04d06ae7a4a4ab18322d529431b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc7a4884ac84993989e42298dc0fc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91defa4c05aa4834a0bd94cfb60d6134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d59c57a0f9428dbb1dbbb65509341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4142f890ed403d9cac2d24b670f181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2774531abe7485d8b9e1bb41cecb628",
              "IPY_MODEL_4243e68e4ec44754a53f53afba75b7c7",
              "IPY_MODEL_120a586b774648d49cdd1013afd33328"
            ],
            "layout": "IPY_MODEL_bca719aaa06f4f50be0cedb7632a1ab0"
          }
        },
        "c2774531abe7485d8b9e1bb41cecb628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7cfc1e6200484397d987667315c920",
            "placeholder": "​",
            "style": "IPY_MODEL_283b8ff71bd248a69b3906b49cb213b3",
            "value": "config.json: 100%"
          }
        },
        "4243e68e4ec44754a53f53afba75b7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc8237cfbaf647f3a72ce27b424a32ee",
            "max": 4519,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d2e1edb480c4898b05b896724f5b79e",
            "value": 4519
          }
        },
        "120a586b774648d49cdd1013afd33328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03e955b7f0514cd181bf3395bf2bc846",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd205892d844e7583863fcca0726fd2",
            "value": " 4.52k/4.52k [00:00&lt;00:00, 293kB/s]"
          }
        },
        "bca719aaa06f4f50be0cedb7632a1ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7cfc1e6200484397d987667315c920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283b8ff71bd248a69b3906b49cb213b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc8237cfbaf647f3a72ce27b424a32ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2e1edb480c4898b05b896724f5b79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03e955b7f0514cd181bf3395bf2bc846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd205892d844e7583863fcca0726fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4",
        "outputId": "13a6679d-3162-42f0-91d9-79cc27af95ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-community langgraph torch transformers opencv-python librosa numpy face-recognition dlib mediapipe scipy pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import os\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.tools import tool\n",
        "from langgraph.graph import Graph, END\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForVideoClassification,\n",
        "    AutoModelForAudioClassification,\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    Blip2Processor,\n",
        "    Blip2ForConditionalGeneration,\n",
        "    VideoMAEFeatureExtractor,\n",
        "    VideoMAEForVideoClassification,\n",
        "    WhisperProcessor,\n",
        "    WhisperForAudioClassification,\n",
        "    LayoutLMv3Processor,\n",
        "    LayoutLMv3ForSequenceClassification\n",
        ")\n",
        "import cv2\n",
        "import librosa\n",
        "import json\n",
        "import face_recognition\n",
        "import dlib\n",
        "import scipy\n",
        "from scipy.signal import welch\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "import warnings\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JNL83H4CUsgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_models(device: torch.device) -> dict:\n",
        "    \"\"\"Initialize all AI models used in the pipeline\"\"\"\n",
        "    models = {}\n",
        "\n",
        "    try:\n",
        "        # Video understanding models\n",
        "        models[\"videomae\"] = VideoMAEForVideoClassification.from_pretrained(\n",
        "            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
        "        ).to(device)\n",
        "        models[\"videomae_processor\"] = VideoMAEFeatureExtractor.from_pretrained(\n",
        "            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
        "        )\n",
        "\n",
        "        # Visual analysis models\n",
        "        models[\"clip\"] = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "        models[\"clip_processor\"] = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "        # Multimodal understanding\n",
        "        models[\"blip2\"] = Blip2ForConditionalGeneration.from_pretrained(\n",
        "            \"Salesforce/blip2-opt-2.7b\"\n",
        "        ).to(device)\n",
        "        models[\"blip2_processor\"] = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "\n",
        "        # Audio analysis\n",
        "        models[\"whisper\"] = WhisperForAudioClassification.from_pretrained(\n",
        "            \"openai/whisper-large-v3\"\n",
        "        ).to(device)\n",
        "        models[\"whisper_processor\"] = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
        "\n",
        "        # Layout analysis for visual artifacts\n",
        "        models[\"layoutlm\"] = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
        "            \"microsoft/layoutlmv3-base\"\n",
        "        ).to(device)\n",
        "        models[\"layoutlm_processor\"] = LayoutLMv3Processor.from_pretrained(\n",
        "            \"microsoft/layoutlmv3-base\"\n",
        "        )\n",
        "\n",
        "        # Face analysis\n",
        "        models[\"face_detector\"] = dlib.get_frontal_face_detector()\n",
        "        models[\"face_predictor\"] = dlib.shape_predictor(\n",
        "            \"shape_predictor_68_face_landmarks.dat\"\n",
        "        )\n",
        "\n",
        "        # MediaPipe for advanced face mesh analysis\n",
        "        models[\"face_mesh\"] = mp.solutions.face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            min_detection_confidence=0.5,\n",
        "            min_tracking_confidence=0.5\n",
        "        )\n",
        "\n",
        "        # Additional models for specific tasks\n",
        "        models[\"action_recognition\"] = hub.load(\n",
        "            \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "JVJJvhKNU27U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def enhanced_video_processing(video_path: str, models: dict) -> dict:\n",
        "    \"\"\"Enhanced video preprocessing with advanced feature extraction\"\"\"\n",
        "    processed_data = {\n",
        "        \"frames\": [],\n",
        "        \"audio\": {},\n",
        "        \"metadata\": {},\n",
        "        \"features\": {},\n",
        "        \"face_data\": [],\n",
        "        \"motion_data\": [],\n",
        "        \"temporal_features\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Extract frames and basic metadata\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Process frames with advanced features\n",
        "        prev_frame = None\n",
        "        flow_accumulator = []\n",
        "\n",
        "        with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Store original frame\n",
        "                processed_data[\"frames\"].append(frame)\n",
        "\n",
        "                # Extract face data\n",
        "                face_data = extract_face_features(frame, models)\n",
        "                processed_data[\"face_data\"].append(face_data)\n",
        "\n",
        "                # Calculate optical flow if we have a previous frame\n",
        "                if prev_frame is not None:\n",
        "                    flow = calculate_optical_flow(prev_frame, frame)\n",
        "                    flow_accumulator.append(flow)\n",
        "\n",
        "                prev_frame = frame.copy()\n",
        "                pbar.update(1)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Process audio\n",
        "        processed_data[\"audio\"] = extract_audio_features(video_path)\n",
        "\n",
        "        # Extract temporal features\n",
        "        processed_data[\"temporal_features\"] = analyze_temporal_coherence(\n",
        "            processed_data[\"frames\"],\n",
        "            flow_accumulator\n",
        "        )\n",
        "\n",
        "        # Generate comprehensive metadata\n",
        "        processed_data[\"metadata\"] = extract_enhanced_metadata(\n",
        "            video_path,\n",
        "            processed_data\n",
        "        )\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in video processing: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def enhanced_image_processing(image_path: str, models: dict) -> dict:\n",
        "    \"\"\"Enhanced image preprocessing with advanced feature extraction\"\"\"\n",
        "    processed_data = {\n",
        "        \"image\": None,\n",
        "        \"metadata\": {},\n",
        "        \"features\": {},\n",
        "        \"face_data\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        processed_data[\"image\"] = image\n",
        "\n",
        "        # Extract face data\n",
        "        face_data = extract_face_features(image, models)\n",
        "        processed_data[\"face_data\"].append(face_data)\n",
        "\n",
        "        # Generate comprehensive metadata\n",
        "        processed_data[\"metadata\"] = extract_enhanced_metadata(\n",
        "            image_path,\n",
        "            processed_data\n",
        "        )\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image processing: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def extract_face_features(frame: np.ndarray, models: dict) -> dict:\n",
        "    \"\"\"Extract comprehensive face features using multiple models\"\"\"\n",
        "    face_data = {\n",
        "        \"landmarks\": [],\n",
        "        \"mesh\": [],\n",
        "        \"emotions\": [],\n",
        "        \"quality_metrics\": {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Basic face detection with dlib\n",
        "        faces = models[\"face_detector\"](frame)\n",
        "\n",
        "        for face in faces:\n",
        "            # Get facial landmarks\n",
        "            shape = models[\"face_predictor\"](frame, face)\n",
        "            landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n",
        "            face_data[\"landmarks\"].append(landmarks)\n",
        "\n",
        "            # MediaPipe face mesh for detailed analysis\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            mesh_results = models[\"face_mesh\"].process(frame_rgb)\n",
        "\n",
        "            if mesh_results.multi_face_landmarks:\n",
        "                mesh_points = np.array([\n",
        "                    [point.x, point.y, point.z]\n",
        "                    for point in mesh_results.multi_face_landmarks[0].landmark\n",
        "                ])\n",
        "                face_data[\"mesh\"].append(mesh_points)\n",
        "\n",
        "            # Extract face quality metrics\n",
        "            quality_metrics = calculate_face_quality_metrics(frame, face, landmarks)\n",
        "            face_data[\"quality_metrics\"].update(quality_metrics)\n",
        "\n",
        "            # Emotion detection using pretrained model\n",
        "            emotions = detect_emotions(frame, face)\n",
        "            face_data[\"emotions\"].append(emotions)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in face feature extraction: {str(e)}\")\n",
        "\n",
        "    return face_data\n",
        "\n",
        "def calculate_face_quality_metrics(\n",
        "    frame: np.ndarray,\n",
        "    face: dlib.rectangle,\n",
        "    landmarks: np.ndarray\n",
        ") -> dict:\n",
        "    \"\"\"Calculate various metrics for face quality assessment\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    try:\n",
        "        # Face size relative to frame\n",
        "        face_size = (face.right() - face.left()) * (face.bottom() - face.top())\n",
        "        frame_size = frame.shape[0] * frame.shape[1]\n",
        "        metrics[\"relative_size\"] = face_size / frame_size\n",
        "\n",
        "        # Face symmetry\n",
        "        symmetry_score = calculate_face_symmetry(landmarks)\n",
        "        metrics[\"symmetry\"] = symmetry_score\n",
        "\n",
        "        # Blur detection\n",
        "        laplacian_var = cv2.Laplacian(\n",
        "            cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[\n",
        "                face.top():face.bottom(),\n",
        "                face.left():face.right()\n",
        "            ]\n",
        "        ).var()\n",
        "        metrics[\"sharpness\"] = laplacian_var\n",
        "\n",
        "        # Lighting consistency\n",
        "        lighting_score = analyze_lighting_consistency(\n",
        "            frame[face.top():face.bottom(), face.left():face.right()]\n",
        "        )\n",
        "        metrics[\"lighting\"] = lighting_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating face quality metrics: {str(e)}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def detect_emotions(frame: np.ndarray, face: dlib.rectangle) -> dict:\n",
        "    \"\"\"Detect emotions in facial expressions\"\"\"\n",
        "    emotions = {}\n",
        "\n",
        "    try:\n",
        "        # Extract face region\n",
        "        face_img = frame[face.top():face.bottom(), face.left():face.right()]\n",
        "        face_img = cv2.resize(face_img, (224, 224))\n",
        "\n",
        "        # Use pre-trained emotion detection model\n",
        "        # This is a placeholder - you would need to implement actual emotion detection\n",
        "        emotions = {\n",
        "            \"happy\": 0.0,\n",
        "            \"sad\": 0.0,\n",
        "            \"angry\": 0.0,\n",
        "            \"neutral\": 0.0,\n",
        "            \"surprise\": 0.0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in emotion detection: {str(e)}\")\n",
        "\n",
        "    return emotions\n",
        "\n",
        "def extract_audio_features(video_path: str) -> dict:\n",
        "    \"\"\"Extract comprehensive audio features\"\"\"\n",
        "    audio_features = {}\n",
        "\n",
        "    try:\n",
        "        # Load audio\n",
        "        y, sr = librosa.load(video_path)\n",
        "\n",
        "        # Basic features\n",
        "        audio_features[\"mfcc\"] = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "        audio_features[\"spectral_contrast\"] = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "        audio_features[\"chroma\"] = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "        # Advanced features\n",
        "        audio_features[\"spectral_rolloff\"] = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "        audio_features[\"spectral_bandwidth\"] = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "        audio_features[\"spectral_flatness\"] = librosa.feature.spectral_flatness(y=y)\n",
        "\n",
        "        # Temporal features\n",
        "        audio_features[\"onset_strength\"] = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "        audio_features[\"tempo\"] = librosa.beat.tempo(onset_envelope=audio_features[\"onset_strength\"], sr=sr)\n",
        "\n",
        "        # Additional analysis\n",
        "        audio_features[\"zero_crossing_rate\"] = librosa.feature.zero_crossing_rate(y)\n",
        "        audio_features[\"rms\"] = librosa.feature.rms(y=y)\n",
        "\n",
        "        # Calculate overall audio quality metrics\n",
        "        audio_features[\"quality_metrics\"] = calculate_audio_quality_metrics(y, sr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio features: {str(e)}\")\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "def calculate_audio_quality_metrics(y: np.ndarray, sr: int) -> dict:\n",
        "    \"\"\"Calculate various audio quality metrics\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    try:\n",
        "        # Signal-to-noise ratio estimation\n",
        "        noise_floor = np.mean(np.abs(y[y < np.mean(y)]))\n",
        "        signal_power = np.mean(np.abs(y[y >= np.mean(y)]))\n",
        "        metrics[\"snr\"] = 20 * np.log10(signal_power / noise_floor) if noise_floor > 0 else 0\n",
        "\n",
        "        # Dynamic range\n",
        "        metrics[\"dynamic_range\"] = np.max(np.abs(y)) - np.min(np.abs(y))\n",
        "\n",
        "        # Spectral centroid (brightness)\n",
        "        metrics[\"spectral_centroid\"] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "\n",
        "        # Clarity metrics\n",
        "        metrics[\"clarity\"] = calculate_audio_clarity(y, sr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating audio quality metrics: {str(e)}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def analyze_temporal_coherence(\n",
        "    frames: List[np.ndarray],\n",
        "    optical_flow: List[np.ndarray]\n",
        ") -> dict:\n",
        "    \"\"\"Analyze temporal coherence with advanced metrics\"\"\"\n",
        "    temporal_features = {}\n",
        "\n",
        "    try:\n",
        "        # Motion analysis\n",
        "        motion_metrics = analyze_motion_patterns(optical_flow)\n",
        "        temporal_features[\"motion\"] = motion_metrics\n",
        "\n",
        "        # Frame consistency\n",
        "        consistency_metrics = analyze_frame_consistency(frames)\n",
        "        temporal_features[\"consistency\"] = consistency_metrics\n",
        "\n",
        "        # Scene transition analysis\n",
        "        transition_metrics = detect_scene_transitions(frames)\n",
        "        temporal_features[\"transitions\"] = transition_metrics\n",
        "\n",
        "        # Calculate overall temporal quality score\n",
        "        temporal_features[\"quality_score\"] = calculate_temporal_quality(\n",
        "            motion_metrics,\n",
        "            consistency_metrics,\n",
        "            transition_metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing temporal coherence: {str(e)}\")\n",
        "\n",
        "    return temporal_features"
      ],
      "metadata": {
        "id": "D4JBongFVeob"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def analyze_visual_content(\n",
        "    frames: List[np.ndarray],\n",
        "    models: dict,\n",
        "    device: torch.device\n",
        ") -> dict:\n",
        "    \"\"\"Analyze visual content using advanced models\"\"\"\n",
        "    visual_results = {}\n",
        "\n",
        "    try:\n",
        "        # CLIP analysis\n",
        "        clip_inputs = models[\"clip_processor\"](images=frames, return_tensors=\"pt\").to(device)\n",
        "        clip_outputs = models[\"clip\"](**clip_inputs)\n",
        "        visual_results[\"clip\"] = clip_outputs.logits.softmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # VideoMAE analysis\n",
        "        if len(frames) > 1:\n",
        "            videomae_inputs = models[\"videomae_processor\"](frames, return_tensors=\"pt\").to(device)\n",
        "            videomae_outputs = models[\"videomae\"](**videomae_inputs)\n",
        "            visual_results[\"videomae\"] = videomae_outputs.logits.softmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Blip2 analysis\n",
        "        blip2_inputs = models[\"blip2_processor\"](images=frames, return_tensors=\"pt\").to(device)\n",
        "        blip2_outputs = models[\"blip2\"].generate(**blip2_inputs)\n",
        "        visual_results[\"blip2\"] = blip2_outputs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visual content analysis: {str(e)}\")\n",
        "\n",
        "    return visual_results\n",
        "\n",
        "def analyze_audio_content(\n",
        "    audio_data: dict,\n",
        "    models: dict,\n",
        "    device: torch.device\n",
        ") -> dict:\n",
        "    \"\"\"Analyze audio content using advanced models\"\"\"\n",
        "    audio_results = {}\n",
        "\n",
        "    try:\n",
        "        # Whisper analysis\n",
        "        whisper_inputs = models[\"whisper_processor\"](audio_data[\"raw\"], return_tensors=\"pt\").to(device)\n",
        "        whisper_outputs = models[\"whisper\"](**whisper_inputs)\n",
        "        audio_results[\"whisper\"] = whisper_outputs.logits.softmax(dim=1).cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in audio content analysis: {str(e)}\")\n",
        "\n",
        "    return audio_results\n",
        "\n",
        "def analyze_crossmodal_coherence(\n",
        "    visual_results: dict,\n",
        "    audio_results: dict,\n",
        "    models: dict,\n",
        "    device: torch.device\n",
        ") -> dict:\n",
        "    \"\"\"Analyze cross-modal coherence using advanced models\"\"\"\n",
        "    coherence_results = {}\n",
        "\n",
        "    try:\n",
        "        # LayoutLM analysis\n",
        "        layoutlm_inputs = models[\"layoutlm_processor\"](visual_results, audio_results, return_tensors=\"pt\").to(device)\n",
        "        layoutlm_outputs = models[\"layoutlm\"](**layoutlm_inputs)\n",
        "        coherence_results[\"layoutlm\"] = layoutlm_outputs.logits.softmax(dim=1).cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cross-modal coherence analysis: {str(e)}\")\n",
        "\n",
        "    return coherence_results"
      ],
      "metadata": {
        "id": "2GqkOYLmWF9y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def feature_fusion(results: List[dict], metadata: dict) -> dict:\n",
        "    \"\"\"Enhanced feature fusion with adaptive weighting\"\"\"\n",
        "    # Initialize weights based on metadata\n",
        "    base_weights = {\n",
        "        \"spatial_confidence\": 0.25,\n",
        "        \"temporal_confidence\": 0.20,\n",
        "        \"facial_confidence\": 0.20,\n",
        "        \"audio_confidence\": 0.15,\n",
        "        \"semantic_confidence\": 0.20\n",
        "    }\n",
        "\n",
        "    # Adjust weights based on video properties\n",
        "    if metadata.get(\"video_info\", {}).get(\"fps\", 0) < 20:\n",
        "        base_weights[\"temporal_confidence\"] *= 0.8\n",
        "        base_weights[\"spatial_confidence\"] *= 1.2\n",
        "\n",
        "    # Collect all confidence scores\n",
        "    confidence_scores = {}\n",
        "    for result in results:\n",
        "        for key, value in result.items():\n",
        "            if \"confidence\" in key and isinstance(value, (int, float)):\n",
        "                confidence_scores[key] = value\n",
        "\n",
        "    # Calculate weighted average\n",
        "    total_weight = 0\n",
        "    weighted_sum = 0\n",
        "\n",
        "    for key, value in confidence_scores.items():\n",
        "        weight = base_weights.get(key, 0.1)  # Default weight for unknown metrics\n",
        "        weighted_sum += value * weight\n",
        "        total_weight += weight\n",
        "\n",
        "    weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    # Calculate uncertainty\n",
        "    variances = [(score - weighted_avg) ** 2 for score in confidence_scores.values()]\n",
        "    uncertainty = np.sqrt(np.mean(variances)) if variances else 0.0\n",
        "\n",
        "    return {\n",
        "        \"final_confidence\": float(weighted_avg),\n",
        "        \"uncertainty\": float(uncertainty),\n",
        "        \"individual_scores\": confidence_scores\n",
        "    }\n",
        "\n",
        "def make_decision(fusion_result: dict) -> dict:\n",
        "    \"\"\"Enhanced decision making with detailed analysis\"\"\"\n",
        "    confidence = fusion_result[\"final_confidence\"]\n",
        "    uncertainty = fusion_result[\"uncertainty\"]\n",
        "    individual_scores = fusion_result[\"individual_scores\"]\n",
        "\n",
        "    # Dynamic thresholding based on uncertainty\n",
        "    base_threshold = 0.7\n",
        "    adjusted_threshold = base_threshold + (uncertainty * 0.1)\n",
        "\n",
        "    # Analyze score distribution\n",
        "    score_distribution = analyze_score_distribution(individual_scores)\n",
        "\n",
        "    # Decision making\n",
        "    is_fake = confidence < adjusted_threshold\n",
        "    certainty_level = calculate_certainty_level(confidence, uncertainty)\n",
        "\n",
        "    # Generate detailed explanation\n",
        "    explanation = generate_detailed_explanation(\n",
        "        is_fake, confidence, uncertainty,\n",
        "        individual_scores, score_distribution\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"is_fake\": is_fake,\n",
        "        \"confidence\": confidence,\n",
        "        \"uncertainty\": uncertainty,\n",
        "        \"certainty_level\": certainty_level,\n",
        "        \"threshold_used\": adjusted_threshold,\n",
        "        \"score_distribution\": score_distribution,\n",
        "        \"explanation\": explanation\n",
        "    }\n",
        "\n",
        "def analyze_score_distribution(scores: dict) -> dict:\n",
        "    \"\"\"Analyze the distribution of individual scores\"\"\"\n",
        "    values = np.array(list(scores.values()))\n",
        "    return {\n",
        "        \"mean\": float(np.mean(values)),\n",
        "        \"std\": float(np.std(values)),\n",
        "        \"min\": float(np.min(values)),\n",
        "        \"max\": float(np.max(values)),\n",
        "        \"range\": float(np.ptp(values)),\n",
        "        \"consistency\": float(1 - (np.std(values) / np.mean(values))) if np.mean(values) != 0 else 0.0\n",
        "    }\n",
        "\n",
        "def calculate_certainty_level(confidence: float, uncertainty: float) -> str:\n",
        "    \"\"\"Calculate the certainty level of the decision\"\"\"\n",
        "    if uncertainty > 0.3:\n",
        "        return \"Low\"\n",
        "    elif uncertainty > 0.15:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "def generate_detailed_explanation(\n",
        "    is_fake: bool,\n",
        "    confidence: float,\n",
        "    uncertainty: float,\n",
        "    individual_scores: dict,\n",
        "    score_distribution: dict\n",
        ") -> str:\n",
        "    \"\"\"Generate a detailed explanation of the decision\"\"\"\n",
        "    explanation_parts = []\n",
        "\n",
        "    # Overall decision\n",
        "    decision_text = \"likely manipulated\" if is_fake else \"likely authentic\"\n",
        "    explanation_parts.append(f\"The media is {decision_text} with {confidence:.1%} confidence.\")\n",
        "\n",
        "    # Uncertainty analysis\n",
        "    explanation_parts.append(\n",
        "        f\"The uncertainty level is {uncertainty:.1%}, indicating a \"\n",
        "        f\"{'high' if uncertainty > 0.3 else 'moderate' if uncertainty > 0.15 else 'low'} \"\n",
        "        \"level of prediction variability.\"\n",
        "    )\n",
        "\n",
        "    # Individual score analysis\n",
        "    strongest_evidence = max(individual_scores.items(), key=lambda x: x[1])\n",
        "    weakest_evidence = min(individual_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    explanation_parts.append(\n",
        "        f\"The strongest evidence comes from {strongest_evidence[0]} \"\n",
        "        f\"({strongest_evidence[1]:.1%}), while the weakest indicator is \"\n",
        "        f\"{weakest_evidence[0]} ({weakest_evidence[1]:.1%}).\"\n",
        "    )\n",
        "\n",
        "    # Score consistency\n",
        "    explanation_parts.append(\n",
        "        f\"The consistency between different detection methods is \"\n",
        "        f\"{score_distribution['consistency']:.1%}.\"\n",
        "    )\n",
        "\n",
        "    return \" \".join(explanation_parts)"
      ],
      "metadata": {
        "id": "-eiewMRcWVpV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import Graph, END\n",
        "\n",
        "def create_detection_graph():\n",
        "    \"\"\"Create enhanced LangGraph workflow\"\"\"\n",
        "    workflow = Graph()\n",
        "\n",
        "    def preprocess(state):\n",
        "        if state[\"input_type\"] == \"video\":\n",
        "            video_data = enhanced_video_processing(state[\"input_path\"], state[\"env\"][\"models\"])\n",
        "        else:\n",
        "            video_data = enhanced_image_processing(state[\"input_path\"], state[\"env\"][\"models\"])\n",
        "        return {\"video_data\": video_data}\n",
        "\n",
        "    def visual_analysis(state):\n",
        "        frames = state[\"video_data\"][\"frames\"] if \"frames\" in state[\"video_data\"] else [state[\"video_data\"][\"image\"]]\n",
        "        optical_flow = state[\"video_data\"].get(\"optical_flow\", [])\n",
        "\n",
        "        # Run visual analysis agents in parallel\n",
        "        spatial_results = spatial_inconsistency_agent(frames, state[\"env\"][\"device\"])\n",
        "        temporal_results = temporal_coherence_agent(frames, optical_flow)\n",
        "        facial_results = facial_analysis_agent(frames, state[\"env\"][\"models\"][\"face_detector\"])\n",
        "\n",
        "        return {\n",
        "            \"visual_results\": {\n",
        "                \"spatial\": spatial_results,\n",
        "                \"temporal\": temporal_results,\n",
        "                \"facial\": facial_results\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def audio_analysis(state):\n",
        "        if \"audio\" in state[\"video_data\"]:\n",
        "            audio_data = state[\"video_data\"][\"audio\"]\n",
        "            results = audio_analysis_agent(audio_data)\n",
        "            return {\"audio_results\": results}\n",
        "        return {\"audio_results\": {}}\n",
        "\n",
        "    def semantic_analysis(state):\n",
        "        results = semantic_analysis_agent(state[\"video_data\"])\n",
        "        return {\"semantic_results\": results}\n",
        "\n",
        "    def decision(state):\n",
        "        all_results = [\n",
        "            state[\"visual_results\"][\"spatial\"],\n",
        "            state[\"visual_results\"][\"temporal\"],\n",
        "            state[\"visual_results\"][\"facial\"],\n",
        "            state[\"audio_results\"],\n",
        "            state[\"semantic_results\"]\n",
        "        ]\n",
        "\n",
        "        fusion_result = feature_fusion(all_results, state[\"video_data\"][\"metadata\"])\n",
        "        final_decision = make_decision(fusion_result)\n",
        "\n",
        "        return {\"decision\": final_decision, \"end\": True}\n",
        "\n",
        "    # Define workflow nodes\n",
        "    workflow.add_node(\"preprocess\", preprocess)\n",
        "    workflow.add_node(\"visual_analysis\", visual_analysis)\n",
        "    workflow.add_node(\"audio_analysis\", audio_analysis)\n",
        "    workflow.add_node(\"semantic_analysis\", semantic_analysis)\n",
        "    workflow.add_node(\"decision\", decision)\n",
        "\n",
        "    # Define workflow edges\n",
        "    workflow.add_edge(\"preprocess\", \"visual_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"audio_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"semantic_analysis\")\n",
        "    workflow.add_edge(\"visual_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"audio_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"semantic_analysis\", \"decision\")\n",
        "\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "chodssfUWaQw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "\n",
        "def detect_deepfake(input_path: str, input_type: str = \"video\", verbose: bool = False) -> dict:\n",
        "    \"\"\"Enhanced main function to run deepfake detection\"\"\"\n",
        "    try:\n",
        "        # Setup environment\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        models = setup_models(device)\n",
        "        env = {\n",
        "            \"device\": device,\n",
        "            \"models\": models\n",
        "        }\n",
        "\n",
        "        # Create and compile workflow\n",
        "        workflow = create_detection_graph()\n",
        "\n",
        "        # Configure logging\n",
        "        if verbose:\n",
        "            print(\"Verbose mode enabled\")\n",
        "        else:\n",
        "            print(\"Verbose mode disabled\")\n",
        "\n",
        "        # Run detection\n",
        "        config = {\n",
        "            \"input_path\": input_path,\n",
        "            \"input_type\": input_type,\n",
        "            \"env\": env\n",
        "        }\n",
        "\n",
        "        print(\"Starting deepfake detection pipeline...\")\n",
        "        result = workflow.run(config)\n",
        "        print(\"Detection pipeline completed successfully.\")\n",
        "\n",
        "        return format_forensic_report(result[\"decision\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {e}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"is_fake\": None,\n",
        "            \"confidence\": 0.0,\n",
        "            \"explanation\": \"An error occurred during detection.\"\n",
        "        }\n",
        "\n",
        "def format_forensic_report(decision: dict) -> dict:\n",
        "    \"\"\"Format the decision as a forensic report\"\"\"\n",
        "    report = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"is_fake\": decision[\"is_fake\"],\n",
        "        \"confidence\": decision[\"confidence\"],\n",
        "        \"uncertainty\": decision[\"uncertainty\"],\n",
        "        \"certainty_level\": decision[\"certainty_level\"],\n",
        "        \"threshold_used\": decision[\"threshold_used\"],\n",
        "        \"score_distribution\": decision[\"score_distribution\"],\n",
        "        \"explanation\": decision[\"explanation\"]\n",
        "    }\n",
        "    return report\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directly specify the input path, type, and verbosity\n",
        "    input_path = \"path/to/your/video.mp4\"  # or \"path/to/your/image.jpg\"\n",
        "    input_type = \"video\"  # or \"image\"\n",
        "    verbose = True\n",
        "\n",
        "    result = detect_deepfake(input_path, input_type, verbose=verbose)\n",
        "    print(\"\\nDeepfake Detection Forensic Report:\")\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "7NT6HEePWlQR",
        "outputId": "d76e74a5-ae5e-42e3-b5c4-ade637f793cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "d892d6b0974b4475ba91d6cff55f1156",
            "e77d831c43274bcd991780c3291e6f54",
            "b7c58f4027d540459e1ae10ae272b60c",
            "a5c6bf7acb3d4428a7e5475c5550e7aa",
            "f540ed3e745846e598e39fb79a98b8ea",
            "eed41c55862e4052bc95a628a2839774",
            "e8be241c1ed541dea002d53bd3fa5100",
            "d5d5e972f58a4756ae38c3e928e8b9bb",
            "9567fb8ff3304a8987d3d82d73bda287",
            "9f783763107e496e93c3903c0b539bf1",
            "0717abb327ea4b1b959d72bc96f62957",
            "095060cf43f643beae5f7c1bfc118368",
            "d90c839ad9c84a90b6b6bae54ff60660",
            "9b8d89d8601a453581dcd8860449676c",
            "0d33528061584b0ca21e0c8ae5b1af31",
            "a942544dabff412283fa43161a85452a",
            "b9d5039cd54d463eb67ddf43c32c601f",
            "d4722216e3a54e53b8b72fef46117243",
            "fb973c24879947d7ae7dbcdf90b2eba6",
            "42dbf905acb9425c94aa1419b6375192",
            "0c06b35e5efd4de0864c39bc42ed47af",
            "0e8bd97078ee47d0b98c24e0af7fc3f1",
            "2a9c95d3210d4f07bbbebb559b30a499",
            "f8b0886d4aa740cea464c94aeaf7c285",
            "a13605e1d3ff4dd38dfcbff119399640",
            "7bc462801f294d5bbfd20b575ec6b4dc",
            "8f5f4da0becd4954b0bd805068000fcf",
            "0b118f91f2ea4fb9b0df00353427c8f2",
            "f442462d3937403a8ddca89c7ac2d20b",
            "63d04d06ae7a4a4ab18322d529431b1c",
            "7fc7a4884ac84993989e42298dc0fc35",
            "91defa4c05aa4834a0bd94cfb60d6134",
            "77d59c57a0f9428dbb1dbbb65509341b",
            "1d4142f890ed403d9cac2d24b670f181",
            "c2774531abe7485d8b9e1bb41cecb628",
            "4243e68e4ec44754a53f53afba75b7c7",
            "120a586b774648d49cdd1013afd33328",
            "bca719aaa06f4f50be0cedb7632a1ab0",
            "0c7cfc1e6200484397d987667315c920",
            "283b8ff71bd248a69b3906b49cb213b3",
            "bc8237cfbaf647f3a72ce27b424a32ee",
            "6d2e1edb480c4898b05b896724f5b79e",
            "03e955b7f0514cd181bf3395bf2bc846",
            "2fd205892d844e7583863fcca0726fd2",
            "26baf84729ea4859ab13c7b29e8d5352",
            "620fad6023f5455bbd983f30cef54291",
            "bfcc6c69d99b4f79a49634d1dd871245",
            "c16734b6a5f54ee1853499b9c6b26fac",
            "8e0066daa27b4f29b63cb9732fdf2e3d",
            "1a006dee72d5490a9aa04bbdee6a470f",
            "decbe43cf771447385aaad5269c2b0d3",
            "cf9057685a3e41ceb675ac7de13b2c69",
            "64f09f9f00164253823a3757c759f793",
            "7a462ffeda254302b63ffed285ed5d27",
            "5a91af7d05564cb89eda06035702ec36"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/22.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d892d6b0974b4475ba91d6cff55f1156"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "095060cf43f643beae5f7c1bfc118368"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a9c95d3210d4f07bbbebb559b30a499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d4142f890ed403d9cac2d24b670f181"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Analysis Agents\n",
        "\n",
        "def setup_llm_agent():\n",
        "    \"\"\"Setup enhanced LLM agent\"\"\"\n",
        "    llm = ChatOpenAI(model=\"gpt-4-vision-preview\", temperature=0.2)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Analyze the following media content for signs of manipulation.\n",
        "        Consider:\n",
        "        1. Visual consistency and artifacts\n",
        "        2. Audio-visual synchronization\n",
        "        3. Natural movement and expressions\n",
        "        4. Lighting and shadow consistency\n",
        "        5. Edge artifacts and blending issues\n",
        "\n",
        "        Provide a detailed analysis with confidence scores.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    return llm, prompt\n",
        "\n",
        "@tool\n",
        "def semantic_analysis_agent(input_data: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced semantic analysis\"\"\"\n",
        "    llm, prompt = setup_llm_agent()\n",
        "\n",
        "    # Prepare input data\n",
        "    analysis_text = {\n",
        "        \"video_metadata\": input_data.get(\"metadata\", {}),\n",
        "        \"detected_faces\": len(face_recognition.face_locations(input_data[\"frames\"][0])) if \"frames\" in input_data else 0,\n",
        "        \"video_length\": len(input_data[\"frames\"]) if \"frames\" in input_data else 0,\n",
        "        \"audio_features\": bool(input_data.get(\"audio\", {}))\n",
        "    }\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "    try:\n",
        "        result = chain.invoke({\"input\": json.dumps(analysis_text)})\n",
        "        confidence = result.get(\"confidence\", 0.0)\n",
        "\n",
        "        # Additional analysis of response content\n",
        "        response_detail = result.get(\"analysis\", \"\")\n",
        "        detail_score = analyze_llm_response_detail(response_detail)\n",
        "\n",
        "        return {\n",
        "            \"semantic_confidence\": float(confidence),\n",
        "            \"detail_score\": float(detail_score),\n",
        "            \"analysis\": response_detail\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in semantic analysis: {e}\")\n",
        "        return {\"semantic_confidence\": 0.0, \"error\": str(e)}\n",
        "\n",
        "def analyze_llm_response_detail(response: str) -> float:\n",
        "    \"\"\"Analyze the detail level of LLM response\"\"\"\n",
        "    # Consider factors like response length, specific terminology use, etc.\n",
        "    detail_metrics = {\n",
        "        \"length\": len(response) / 1000,  # Normalize by 1000 chars\n",
        "        \"technical_terms\": len([word for word in response.lower().split()\n",
        "                              if word in [\"artifact\", \"consistency\", \"synchronization\",\n",
        "                                        \"manipulation\", \"synthetic\", \"generated\"]]) / 10\n",
        "    }\n",
        "    return np.mean(list(detail_metrics.values()))\n",
        "\n",
        "# Integration and Decision Making\n",
        "\n",
        "def feature_fusion(results: List[Dict[str, float]], metadata: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Enhanced feature fusion with adaptive weighting\"\"\"\n",
        "    # Initialize weights based on metadata\n",
        "    base_weights = {\n",
        "        \"spatial_confidence\": 0.25,\n",
        "        \"temporal_confidence\": 0.20,\n",
        "        \"facial_confidence\": 0.20,\n",
        "        \"audio_confidence\": 0.15,\n",
        "        \"semantic_confidence\": 0.20\n",
        "    }\n",
        "\n",
        "    # Adjust weights based on video properties\n",
        "    if metadata.get(\"video_info\", {}).get(\"fps\", 0) < 20:\n",
        "        base_weights[\"temporal_confidence\"] *= 0.8\n",
        "        base_weights[\"spatial_confidence\"] *= 1.2\n",
        "\n",
        "    # Collect all confidence scores\n",
        "    confidence_scores = {}\n",
        "    for result in results:\n",
        "        for key, value in result.items():\n",
        "            if \"confidence\" in key and isinstance(value, (int, float)):\n",
        "                confidence_scores[key] = value\n",
        "\n",
        "    # Calculate weighted average\n",
        "    total_weight = 0\n",
        "    weighted_sum = 0\n",
        "\n",
        "    for key, value in confidence_scores.items():\n",
        "        weight = base_weights.get(key, 0.1)  # Default weight for unknown metrics\n",
        "        weighted_sum += value * weight\n",
        "        total_weight += weight\n",
        "\n",
        "    weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    # Calculate uncertainty\n",
        "    variances = [(score - weighted_avg) ** 2 for score in confidence_scores.values()]\n",
        "    uncertainty = np.sqrt(np.mean(variances)) if variances else 0.0\n",
        "\n",
        "    return {\n",
        "        \"final_confidence\": float(weighted_avg),\n",
        "        \"uncertainty\": float(uncertainty),\n",
        "        \"individual_scores\": confidence_scores\n",
        "    }"
      ],
      "metadata": {
        "id": "OK68SZCQWtzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_decision(fusion_result: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced decision making with detailed analysis\"\"\"\n",
        "    confidence = fusion_result[\"final_confidence\"]\n",
        "    uncertainty = fusion_result[\"uncertainty\"]\n",
        "    individual_scores = fusion_result[\"individual_scores\"]\n",
        "\n",
        "    # Dynamic thresholding based on uncertainty\n",
        "    base_threshold = 0.7\n",
        "    adjusted_threshold = base_threshold + (uncertainty * 0.1)\n",
        "\n",
        "    # Analyze score distribution\n",
        "    score_distribution = analyze_score_distribution(individual_scores)\n",
        "\n",
        "    # Decision making\n",
        "    is_fake = confidence < adjusted_threshold\n",
        "    certainty_level = calculate_certainty_level(confidence, uncertainty)\n",
        "\n",
        "    # Generate detailed explanation\n",
        "    explanation = generate_detailed_explanation(\n",
        "        is_fake, confidence, uncertainty,\n",
        "        individual_scores, score_distribution\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"is_fake\": is_fake,\n",
        "        \"confidence\": confidence,\n",
        "        \"uncertainty\": uncertainty,\n",
        "        \"certainty_level\": certainty_level,\n",
        "        \"threshold_used\": adjusted_threshold,\n",
        "        \"score_distribution\": score_distribution,\n",
        "        \"explanation\": explanation\n",
        "    }\n",
        "\n",
        "def analyze_score_distribution(scores: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze the distribution of individual scores\"\"\"\n",
        "    values = np.array(list(scores.values()))\n",
        "    return {\n",
        "        \"mean\": float(np.mean(values)),\n",
        "        \"std\": float(np.std(values)),\n",
        "        \"min\": float(np.min(values)),\n",
        "        \"max\": float(np.max(values)),\n",
        "        \"range\": float(np.ptp(values)),\n",
        "        \"consistency\": float(1 - (np.std(values) / np.mean(values))) if np.mean(values) != 0 else 0.0\n",
        "    }\n",
        "\n",
        "def calculate_certainty_level(confidence: float, uncertainty: float) -> str:\n",
        "    \"\"\"Calculate the certainty level of the decision\"\"\"\n",
        "    if uncertainty > 0.3:\n",
        "        return \"Low\"\n",
        "    elif uncertainty > 0.15:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "def generate_detailed_explanation(\n",
        "    is_fake: bool,\n",
        "    confidence: float,\n",
        "    uncertainty: float,\n",
        "    individual_scores: Dict[str, float],\n",
        "    score_distribution: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"Generate a detailed explanation of the decision\"\"\"\n",
        "    explanation_parts = []\n",
        "\n",
        "    # Overall decision\n",
        "    decision_text = \"likely manipulated\" if is_fake else \"likely authentic\"\n",
        "    explanation_parts.append(f\"The media is {decision_text} with {confidence:.1%} confidence.\")\n",
        "\n",
        "    # Uncertainty analysis\n",
        "    explanation_parts.append(\n",
        "        f\"The uncertainty level is {uncertainty:.1%}, indicating a \"\n",
        "        f\"{'high' if uncertainty > 0.3 else 'moderate' if uncertainty > 0.15 else 'low'} \"\n",
        "        \"level of prediction variability.\"\n",
        "    )\n",
        "\n",
        "    # Individual score analysis\n",
        "    strongest_evidence = max(individual_scores.items(), key=lambda x: x[1])\n",
        "    weakest_evidence = min(individual_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    explanation_parts.append(\n",
        "        f\"The strongest evidence comes from {strongest_evidence[0]} \"\n",
        "        f\"({strongest_evidence[1]:.1%}), while the weakest indicator is \"\n",
        "        f\"{weakest_evidence[0]} ({weakest_evidence[1]:.1%}).\"\n",
        "    )\n",
        "\n",
        "    # Score consistency\n",
        "    explanation_parts.append(\n",
        "        f\"The consistency between different detection methods is \"\n",
        "        f\"{score_distribution['consistency']:.1%}.\"\n",
        "    )\n",
        "\n",
        "    return \" \".join(explanation_parts)"
      ],
      "metadata": {
        "id": "sgeWgPlHWyCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "\n",
        "def create_detection_graph():\n",
        "    \"\"\"Create enhanced LangGraph workflow\"\"\"\n",
        "    workflow = Graph()\n",
        "\n",
        "    @workflow.node(\"preprocess\")\n",
        "    def preprocess(state):\n",
        "        video_data = preprocess_video(state[\"input_path\"])\n",
        "        return {\"video_data\": video_data}\n",
        "\n",
        "    @workflow.node(\"visual_analysis\")\n",
        "    def visual_analysis(state):\n",
        "        frames = state[\"video_data\"][\"frames\"]\n",
        "        optical_flow = state[\"video_data\"][\"optical_flow\"]\n",
        "\n",
        "        # Run visual analysis agents in parallel\n",
        "        spatial_results = spatial_inconsistency_agent(frames, state[\"env\"][\"device\"])\n",
        "        temporal_results = temporal_coherence_agent(frames, optical_flow)\n",
        "        facial_results = facial_analysis_agent(frames, state[\"env\"][\"face_detector\"])\n",
        "\n",
        "        return {\n",
        "            \"visual_results\": {\n",
        "                \"spatial\": spatial_results,\n",
        "                \"temporal\": temporal_results,\n",
        "                \"facial\": facial_results\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @workflow.node(\"audio_analysis\")\n",
        "    def audio_analysis(state):\n",
        "        audio_data = state[\"video_data\"][\"audio\"]\n",
        "        results = audio_analysis_agent(audio_data)\n",
        "        return {\"audio_results\": results}\n",
        "\n",
        "    @workflow.node(\"semantic_analysis\")\n",
        "    def semantic_analysis(state):\n",
        "        results = semantic_analysis_agent(state[\"video_data\"])\n",
        "        return {\"semantic_results\": results}\n",
        "\n",
        "    @workflow.node(\"decision\")\n",
        "    def decision(state):\n",
        "        all_results = [\n",
        "            state[\"visual_results\"][\"spatial\"],\n",
        "            state[\"visual_results\"][\"temporal\"],\n",
        "            state[\"visual_results\"][\"facial\"],\n",
        "            state[\"audio_results\"],\n",
        "            state[\"semantic_results\"]\n",
        "        ]\n",
        "\n",
        "        fusion_result = feature_fusion(all_results, state[\"video_data\"][\"metadata\"])\n",
        "        final_decision = make_decision(fusion_result)\n",
        "\n",
        "        return {\"decision\": final_decision, \"end\": True}\n",
        "\n",
        "    # Define workflow\n",
        "    workflow.set_entry_point(\"preprocess\")\n",
        "    workflow.add_edge(\"preprocess\", \"visual_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"audio_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"semantic_analysis\")\n",
        "    workflow.add_edge(\"visual_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"audio_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"semantic_analysis\", \"decision\")\n",
        "\n",
        "    return workflow\n",
        "\n",
        "def detect_deepfake(video_path: str, verbose: bool = False) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced main function to run deepfake detection\"\"\"\n",
        "    try:\n",
        "        # Setup environment\n",
        "        env = setup_environment()\n",
        "\n",
        "        # Create and compile workflow\n",
        "        workflow = create_detection_graph()\n",
        "\n",
        "        # Configure logging\n",
        "        if verbose:\n",
        "            print(\"Verbose mode enabled\")\n",
        "        else:\n",
        "            print(\"Verbose mode disabled\")\n",
        "\n",
        "        # Run detection\n",
        "        config = {\n",
        "            \"input_path\": video_path,\n",
        "            \"env\": env\n",
        "        }\n",
        "\n",
        "        print(\"Starting deepfake detection pipeline...\")\n",
        "        result = workflow.run(config)\n",
        "        print(\"Detection pipeline completed successfully.\")\n",
        "\n",
        "        return result[\"decision\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {e}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"is_fake\": None,\n",
        "            \"confidence\": 0.0,\n",
        "            \"explanation\": \"An error occurred during detection.\"\n",
        "        }"
      ],
      "metadata": {
        "id": "laob1DtvdjFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "\n",
        "def create_detection_graph():\n",
        "    \"\"\"Create enhanced LangGraph workflow\"\"\"\n",
        "    workflow = Graph()\n",
        "\n",
        "    @workflow.node(\"preprocess\")\n",
        "    def preprocess(state):\n",
        "        video_data = preprocess_video(state[\"input_path\"])\n",
        "        return {\"video_data\": video_data}\n",
        "\n",
        "    @workflow.node(\"visual_analysis\")\n",
        "    def visual_analysis(state):\n",
        "        frames = state[\"video_data\"][\"frames\"]\n",
        "        optical_flow = state[\"video_data\"][\"optical_flow\"]\n",
        "\n",
        "        # Run visual analysis agents in parallel\n",
        "        spatial_results = spatial_inconsistency_agent(frames, state[\"env\"][\"device\"])\n",
        "        temporal_results = temporal_coherence_agent(frames, optical_flow)\n",
        "        facial_results = facial_analysis_agent(frames, state[\"env\"][\"face_detector\"])\n",
        "\n",
        "        return {\n",
        "            \"visual_results\": {\n",
        "                \"spatial\": spatial_results,\n",
        "                \"temporal\": temporal_results,\n",
        "                \"facial\": facial_results\n",
        "            }\n",
        "        }\n",
        "\n",
        "    @workflow.node(\"audio_analysis\")\n",
        "    def audio_analysis(state):\n",
        "        audio_data = state[\"video_data\"][\"audio\"]\n",
        "        results = audio_analysis_agent(audio_data)\n",
        "        return {\"audio_results\": results}\n",
        "\n",
        "    @workflow.node(\"semantic_analysis\")\n",
        "    def semantic_analysis(state):\n",
        "        results = semantic_analysis_agent(state[\"video_data\"])\n",
        "        return {\"semantic_results\": results}\n",
        "\n",
        "    @workflow.node(\"decision\")\n",
        "    def decision(state):\n",
        "        all_results = [\n",
        "            state[\"visual_results\"][\"spatial\"],\n",
        "            state[\"visual_results\"][\"temporal\"],\n",
        "            state[\"visual_results\"][\"facial\"],\n",
        "            state[\"audio_results\"],\n",
        "            state[\"semantic_results\"]\n",
        "        ]\n",
        "\n",
        "        fusion_result = feature_fusion(all_results, state[\"video_data\"][\"metadata\"])\n",
        "        final_decision = make_decision(fusion_result)\n",
        "\n",
        "        return {\"decision\": final_decision, \"end\": True}\n",
        "\n",
        "    # Define workflow\n",
        "    workflow.set_entry_point(\"preprocess\")\n",
        "    workflow.add_edge(\"preprocess\", \"visual_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"audio_analysis\")\n",
        "    workflow.add_edge(\"preprocess\", \"semantic_analysis\")\n",
        "    workflow.add_edge(\"visual_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"audio_analysis\", \"decision\")\n",
        "    workflow.add_edge(\"semantic_analysis\", \"decision\")\n",
        "\n",
        "    return workflow\n",
        "\n",
        "def detect_deepfake(video_path: str, verbose: bool = False) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced main function to run deepfake detection\"\"\"\n",
        "    try:\n",
        "        # Setup environment\n",
        "        env = setup_environment()\n",
        "\n",
        "        # Create and compile workflow\n",
        "        workflow = create_detection_graph()\n",
        "\n",
        "        # Configure logging\n",
        "        if verbose:\n",
        "            print(\"Verbose mode enabled\")\n",
        "        else:\n",
        "            print(\"Verbose mode disabled\")\n",
        "\n",
        "        # Run detection\n",
        "        config = {\n",
        "            \"input_path\": video_path,\n",
        "            \"env\": env\n",
        "        }\n",
        "\n",
        "        print(\"Starting deepfake detection pipeline...\")\n",
        "        result = workflow.run(config)\n",
        "        print(\"Detection pipeline completed successfully.\")\n",
        "\n",
        "        return result[\"decision\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {e}\")\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"is_fake\": None,\n",
        "            \"confidence\": 0.0,\n",
        "            \"explanation\": \"An error occurred during detection.\"\n",
        "        }"
      ],
      "metadata": {
        "id": "XtVtW8mzd0ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Directly specify the video path and verbosity\n",
        "    video_path = \"path/to/your/video.mp4\"\n",
        "    verbose = True\n",
        "\n",
        "    result = detect_deepfake(video_path, verbose=verbose)\n",
        "    print(\"\\nDeepfake Detection Results:\")\n",
        "    print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "ld-3_m_Md7cN",
        "outputId": "175724a7-e4a4-4b8e-ec91-efbf1695971c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Error in deepfake detection: 'Graph' object has no attribute 'node'\n",
            "\n",
            "Deepfake Detection Results:\n",
            "{\n",
            "  \"error\": \"'Graph' object has no attribute 'node'\",\n",
            "  \"is_fake\": null,\n",
            "  \"confidence\": 0.0,\n",
            "  \"explanation\": \"An error occurred during detection.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUAlljFyeBL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}