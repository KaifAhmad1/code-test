{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c83c38c9dce3454d872378bf043c10c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "image/*,video/*,audio/*",
            "button_style": "",
            "data": [],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_3c17045589454bb0bd53438a369b59f3",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_5451e6d03d294dde8595fb6929de1371"
          }
        },
        "3c17045589454bb0bd53438a369b59f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5451e6d03d294dde8595fb6929de1371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "463a38a3acef4c1682c9c30e4e99bf99": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f682933602744b80a7aa45f96fefe777",
            "msg_id": "",
            "outputs": []
          }
        },
        "f682933602744b80a7aa45f96fefe777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TEMrzAzRW_w4"
      },
      "outputs": [],
      "source": [
        "!pip install -q vllm transformers torch opencv-python librosa numpy face-recognition groq\n",
        "!pip install -qU dlib mediapipe scipy pillow tqdm pydantic moviepy langchain_community langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import getpass\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import websockets\n",
        "import base64\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from pydantic import BaseModel, Field\n",
        "from transformers import (\n",
        "    CLIPProcessor, Wav2Vec2Processor, VideoMAEFeatureExtractor,\n",
        "    EfficientNetForImageClassification, LlavaForConditionalGeneration,\n",
        ")\n",
        "from vllm import LLM, SamplingParams\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "from moviepy import VideoFileClip\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from functools import lru_cache\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "YvGEVqyrw7k3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up the Groq Client and More Models for Text, Audio and Vedio Analysis**"
      ],
      "metadata": {
        "id": "-ytDSCQDUAje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") or getpass.getpass(\"Enter your Groq API key: \")\n",
        "    MODEL_PATHS = {\n",
        "        \"llava\": \"llava-hf/llava-1.5-7b-hf\",\n",
        "        \"deepseek_vl2\": \"deepseek-ai/deepseek-vl2-tiny\",\n",
        "        \"fuyu\": \"adept/fuyu-8b\",\n",
        "        \"chatglm\": \"THUDM/glm-4v-9b\",\n",
        "        \"h2ovl\": \"h2oai/h2ovl-mississippi-800m\",\n",
        "        \"internvl\": \"OpenGVLab/InternVL2_5-4B\",\n",
        "        \"llava_onevision\": \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
        "        \"qwen2_vl\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "        \"clip\": \"openai/clip-vit-large-patch14\",\n",
        "        \"wav2vec2\": \"facebook/wav2vec2-large-960h\",\n",
        "        \"clip_processor\": \"openai/clip-vit-large-patch14\",\n",
        "        \"wav2vec2_processor\": \"facebook/wav2vec2-large-960h\",\n",
        "    }\n",
        "\n",
        "# Load vLLM models\n",
        "def load_vllm_model(model_name: str, task: str) -> LLM:\n",
        "    return LLM(model=Config.MODEL_PATHS[model_name], task=task)\n",
        "\n",
        "# Cache for loaded models\n",
        "loaded_models = {}\n",
        "\n",
        "def get_model(model_name: str, task: str) -> LLM:\n",
        "    if model_name not in loaded_models:\n",
        "        loaded_models[model_name] = load_vllm_model(model_name, task)\n",
        "    return loaded_models[model_name]\n",
        "\n",
        "def load_models_in_parallel(model_names: List[str], tasks: List[str]):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        executor.map(get_model, model_names, tasks)\n",
        "\n",
        "def free_memory(model_names: List[str]):\n",
        "    for model_name in model_names:\n",
        "        if model_name in loaded_models:\n",
        "            del loaded_models[model_name]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Example usage\n",
        "model_names = [\"llava\", \"deepseek_vl2\", \"fuyu\", \"chatglm\", \"h2ovl\", \"internvl\", \"llava_onevision\", \"qwen2_vl\"]\n",
        "tasks = [\"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\"]\n",
        "load_models_in_parallel(model_names, tasks)\n",
        "free_memory(model_names)\n",
        "print(\"Models loaded and memory freed successfully!\")\n",
        "\n",
        "# Initialize Groq client\n",
        "def initialize_groq_client():\n",
        "    from groq import Groq\n",
        "    client = Groq(api_key=Config.GROQ_API_KEY)\n",
        "    models = {\n",
        "        \"text\": \"llama-3.3-70b-versatile\",\n",
        "        \"vision\": \"llama-3.2-90b-vision-preview\",\n",
        "        \"audio\": \"whisper-large-v3-turbo\"\n",
        "    }\n",
        "    print(\"Groq models initialized successfully!\")\n",
        "    return client, models\n",
        "\n",
        "groq_client, groq_models = initialize_groq_client()"
      ],
      "metadata": {
        "id": "BIxEeTxeUNJz",
        "outputId": "3db0829c-08fc-4e10-ca2e-8f295b551ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Groq API key: ··········\n",
            "INFO 01-27 06:00:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:50 config.py:2272] Downcasting torch.float32 to torch.float16.\n",
            "INFO 01-27 06:00:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='adept/fuyu-8b', speculative_config=None, tokenizer='adept/fuyu-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=adept/fuyu-8b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='h2oai/h2ovl-mississippi-800m', speculative_config=None, tokenizer='h2oai/h2ovl-mississippi-800m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=h2oai/h2ovl-mississippi-800m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:51 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='THUDM/glm-4v-9b', speculative_config=None, tokenizer='THUDM/glm-4v-9b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4v-9b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:52 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='OpenGVLab/InternVL2_5-4B', speculative_config=None, tokenizer='OpenGVLab/InternVL2_5-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=OpenGVLab/InternVL2_5-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:52 model_runner.py:1094] Starting to load model llava-hf/llava-1.5-7b-hf...\n",
            "INFO 01-27 06:00:52 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='llava-hf/llava-onevision-qwen2-7b-ov-hf', speculative_config=None, tokenizer='llava-hf/llava-onevision-qwen2-7b-ov-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llava-hf/llava-onevision-qwen2-7b-ov-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 01-27 06:00:52 model_runner.py:1094] Starting to load model adept/fuyu-8b...\n",
            "INFO 01-27 06:00:55 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2-VL-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-VL-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "WARNING 01-27 06:00:56 config.py:3309] Current VLLM config is not set.\n",
            "Models loaded and memory freed successfully!\n",
            "Groq models initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Models**\n",
        "- Define the data models for storing analysis results and reports:\n"
      ],
      "metadata": {
        "id": "X8RzT9-AWZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Data Models\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float = Field(..., description=\"Confidence score (0-1)\")\n",
        "    label: str = Field(..., description=\"Classification label\")\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float = Field(..., description=\"Model confidence\")\n",
        "    method: str = Field(..., description=\"Detection method used\")\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Dict[str, Any]]\n",
        "    metadata: Dict[str, Any]"
      ],
      "metadata": {
        "id": "MyJ0N4mSWZio"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frame Stabilization**\n"
      ],
      "metadata": {
        "id": "vZdtcqEPytTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame Stabilization\n",
        "def stabilize_frames(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    stabilized_frames = []\n",
        "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
        "    transforms = []\n",
        "\n",
        "    for i in range(1, len(frames)):\n",
        "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        transform = cv2.estimateRigidTransform(prev_gray, curr_gray, False)\n",
        "        transforms.append(transform)\n",
        "        prev_gray = curr_gray\n",
        "\n",
        "    for i, frame in enumerate(frames):\n",
        "        if i == 0:\n",
        "            stabilized_frames.append(frame)\n",
        "        else:\n",
        "            stabilized_frame = cv2.warpAffine(frame, transforms[i-1], (frame.shape[1], frame.shape[0]))\n",
        "            stabilized_frames.append(stabilized_frame)\n",
        "\n",
        "    return stabilized_frames"
      ],
      "metadata": {
        "id": "kNzO5fgyy0sA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive Noise Reduction**"
      ],
      "metadata": {
        "id": "ghmD5Bf2zFVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def adaptive_noise_reduction(audio_data: np.ndarray) -> np.ndarray:\n",
        "    noise_profile = np.mean(audio_data[:1000])\n",
        "    reduced_noise_audio = audio_data - noise_profile\n",
        "    return reduced_noise_audio"
      ],
      "metadata": {
        "id": "b5SoGe-tzKx6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Preprocessing**\n",
        "- Define helper functions for preprocessing audio, image, and video data:"
      ],
      "metadata": {
        "id": "0ktikzemXOd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from typing import Dict, Any, List\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "def enhance_image_resolution(image_path: str) -> Image:\n",
        "    \"\"\"\n",
        "    Basic image resolution enhancement using OpenCV techniques.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        Image: Enhanced resolution image.\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Apply Gaussian pyramid upscaling\n",
        "    image_upscaled = cv2.pyrUp(image)\n",
        "\n",
        "    # Convert back to PIL Image\n",
        "    return Image.fromarray(cv2.cvtColor(image_upscaled, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "def deblur_image(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply deblurring filter to image.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Input image.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Deblurred image.\n",
        "    \"\"\"\n",
        "    # Unsharp masking for deblurring\n",
        "    gaussian_blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
        "    unsharp_image = cv2.addWeighted(image, 1.5, gaussian_blur, -0.5, 0)\n",
        "    return unsharp_image\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract audio features using librosa.\n",
        "\n",
        "    Args:\n",
        "        audio_data (np.ndarray): Input audio data.\n",
        "        sample_rate (int): Sampling rate of the audio.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary of extracted audio features.\n",
        "    \"\"\"\n",
        "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
        "    mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "    return {\n",
        "        \"mfcc\": mfcc,\n",
        "        \"chroma\": chroma,\n",
        "        \"mel\": mel\n",
        "    }\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract image features using OpenCV SIFT.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Input image.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary of extracted image features.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return {\n",
        "        \"keypoints\": keypoints,\n",
        "        \"descriptors\": descriptors\n",
        "    }\n",
        "\n",
        "def calculate_dense_optical_flow(prev_frame: np.ndarray, curr_frame: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate dense optical flow between two frames.\n",
        "\n",
        "    Args:\n",
        "        prev_frame (np.ndarray): Previous frame.\n",
        "        curr_frame (np.ndarray): Current frame.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Optical flow matrix.\n",
        "    \"\"\"\n",
        "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def extract_temporal_features(optical_flow: np.ndarray) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract temporal features from optical flow.\n",
        "\n",
        "    Args:\n",
        "        optical_flow (np.ndarray): Optical flow matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Magnitude and angle of optical flow.\n",
        "    \"\"\"\n",
        "    mag, ang = cv2.cartToPolar(optical_flow[..., 0], optical_flow[..., 1])\n",
        "    return {\n",
        "        \"magnitude\": mag,\n",
        "        \"angle\": ang\n",
        "    }\n",
        "\n",
        "def estimate_noise(frame: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Estimate noise level in an image.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input image frame.\n",
        "\n",
        "    Returns:\n",
        "        float: Noise estimation value.\n",
        "    \"\"\"\n",
        "    return np.mean(cv2.Laplacian(frame, cv2.CV_64F).var())\n",
        "\n",
        "def calculate_contrast(frame: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate image contrast.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input image frame.\n",
        "\n",
        "    Returns:\n",
        "        float: Contrast value.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    return gray.std()\n",
        "\n",
        "def detect_compression_artifacts(frame: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Detect compression artifacts.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input image frame.\n",
        "\n",
        "    Returns:\n",
        "        float: Compression artifact estimation.\n",
        "    \"\"\"\n",
        "    dct = cv2.dct(np.float32(frame) / 255.0)\n",
        "    return np.mean(np.abs(dct))"
      ],
      "metadata": {
        "id": "9A9KPnzU36TX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Preprocessing Functions**\n",
        "- Define functions for preprocessing audio, image, and video data"
      ],
      "metadata": {
        "id": "13iwCPIrX-Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Preprocessing Functions\n",
        "async def enhanced_preprocessing(file_path: str) -> Dict[str, Any]:\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension in [\".mp3\", \".wav\", \".flac\"]:\n",
        "        return await enhanced_audio_preprocessing(file_path)\n",
        "    elif file_extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
        "        return await enhanced_image_preprocessing(file_path)\n",
        "    elif file_extension in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "        return await enhanced_video_preprocessing(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "async def enhanced_audio_preprocessing(audio_path: str) -> Dict[str, Any]:\n",
        "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
        "    audio_data = adaptive_noise_reduction(audio_data)\n",
        "    audio_features = extract_audio_features(audio_data, sample_rate)\n",
        "    return {\"audio\": audio_data, \"sample_rate\": sample_rate, \"audio_features\": audio_features}\n",
        "\n",
        "async def enhanced_image_preprocessing(image_path: str) -> Dict[str, Any]:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = enhance_resolution(image)\n",
        "    image = deblur_image(image)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_features = extract_image_features(image_rgb)\n",
        "    return {\"image\": image_rgb, \"image_features\": image_features}\n",
        "\n",
        "async def enhanced_video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "        \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "        \"duration\": float(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / float(cap.get(cv2.CAP_PROP_FPS)),\n",
        "        \"codec\": int(cap.get(cv2.CAP_PROP_FOURCC)).to_bytes(4, byteorder='little').decode(),\n",
        "        \"file_size\": os.path.getsize(video_path)\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "    cap.release()\n",
        "\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is not None:\n",
        "            audio_array = audio.to_soundarray()\n",
        "            audio_data = extract_audio_features(audio_array, audio.fps)\n",
        "        video.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Audio extraction error: {e}\")\n",
        "        audio_data = None\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }"
      ],
      "metadata": {
        "id": "ltWcOSlP36Ww"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- functions for analyzing transcription, vision response, and other features"
      ],
      "metadata": {
        "id": "GExLYgMaYVBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Analysis Functions\n",
        "def analyze_transcription(transcription: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    if \"repeated phrase\" in transcription.lower():\n",
        "        anomalies.append(\"Repetitive phrases detected\")\n",
        "    if \"inconsistent timestamp\" in transcription.lower():\n",
        "        anomalies.append(\"Inconsistent timestamps detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_vision_response(response: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    if \"blurry region\" in response.lower():\n",
        "        anomalies.append(\"Blurry regions detected\")\n",
        "    if \"unnatural shadow\" in response.lower():\n",
        "        anomalies.append(\"Unnatural shadows detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_av_sync(frames: List[np.ndarray], audio: np.ndarray) -> float:\n",
        "    sync_score = 0.9\n",
        "    return sync_score\n",
        "\n",
        "def analyze_temporal_features(temporal_features: List[Dict[str, Any]]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_optical_flow(optical_flow: List[np.ndarray]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for flow in optical_flow:\n",
        "        if flow.max() > 1.0:\n",
        "            anomalies.append(\"Inconsistent flow patterns detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_biometric_consistency(frames: List[np.ndarray], models: Dict[str, Any]) -> float:\n",
        "    consistency_score = 0.9\n",
        "    return consistency_score\n",
        "\n",
        "def analyze_llava_response(response: str) -> float:\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    return float(score_match.group(1)) if score_match else 0.5"
      ],
      "metadata": {
        "id": "YkN1p_l2u-KL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- Define functions for analyzing audio, image, and video data using Groq models"
      ],
      "metadata": {
        "id": "05aBMDMgY5CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Audio Analysis**"
      ],
      "metadata": {
        "id": "_KKDSAFn1uLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Audio Analysis\n",
        "async def advanced_audio_analysis(audio_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    if audio_data is None:\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.0,\n",
        "            label=\"NO_AUDIO\",\n",
        "            confidence=0.0,\n",
        "            method=\"audio_analysis\",\n",
        "            anomalies=[\"No audio data available\"]\n",
        "        )\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model 1: Whisper\n",
        "    response = groq_client.audio.transcriptions.create(\n",
        "        file=audio_data[\"waveform\"],\n",
        "        model=\"whisper-large-v3-turbo\",\n",
        "    )\n",
        "    transcription = response.text\n",
        "    anomalies.extend(analyze_transcription(transcription))\n",
        "    scores.append(0.9)\n",
        "\n",
        "    # Model 2: Wav2Vec2\n",
        "    wav2vec2_model = models[\"wav2vec2\"].to(device)\n",
        "    wav2vec2_processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_PATHS[\"wav2vec2_processor\"])\n",
        "    inputs = wav2vec2_processor(audio_data[\"waveform\"], return_tensors=\"pt\", sampling_rate=16000).to(device)\n",
        "    with torch.no_grad():\n",
        "        wav2vec2_output = wav2vec2_model(**inputs)\n",
        "        wav2vec2_score = torch.softmax(wav2vec2_output.logits, dim=-1)\n",
        "        scores.append(wav2vec2_score.max().item())\n",
        "\n",
        "    # Model 3: Llava\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this audio for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(audio=audio_data[\"waveform\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "\n",
        "    # Model 4: DeepseekVL\n",
        "    deepseekvl_model = models[\"deepseek_vl2\"].to(device)\n",
        "    deepseekvl_processor = models[\"processors\"][\"deepseek_vl2\"]\n",
        "    inputs = deepseekvl_processor(audio=audio_data[\"waveform\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = deepseekvl_model.generate(**inputs)\n",
        "        analysis = deepseekvl_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        deepseekvl_score = analyze_llava_response(analysis)\n",
        "        scores.append(deepseekvl_score)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"audio_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "GDaz8dMlu-No"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Image Analysis**"
      ],
      "metadata": {
        "id": "bWICXYdq1y2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for image analysis\n",
        "async def advanced_image_analysis(image: np.ndarray, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model 1: Llava\n",
        "    response = models['llava_model'].chat_completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze this image for signs of digital manipulation or inconsistencies.\",\n",
        "                \"image_url\": \"data:image/png;base64,\" + base64.b64encode(cv2.imencode('.png', image)[1]).decode()\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.2-90b-vision-preview\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        scores.append(float(score_match.group(1)))\n",
        "    anomalies.extend(analyze_vision_response(content))\n",
        "\n",
        "    # Model 2: CLIP\n",
        "    clip_model = models[\"clip\"].to(device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        clip_output = clip_model(**inputs)\n",
        "        clip_score = torch.softmax(clip_output.logits_per_image, dim=-1)\n",
        "        scores.append(clip_score.max().item())\n",
        "\n",
        "    # Model 3: EfficientNet\n",
        "    efficientnet_model = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b0\")\n",
        "    efficientnet_model = efficientnet_model.to(device)\n",
        "    efficientnet_processor = EfficientNetFeatureExtractor.from_pretrained(\"google/efficientnet-b0\")\n",
        "    inputs = efficientnet_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        efficientnet_output = efficientnet_model(**inputs)\n",
        "        efficientnet_score = torch.softmax(efficientnet_output.logits, dim=-1)\n",
        "        scores.append(efficientnet_score.max().item())\n",
        "\n",
        "    # Model 4: DeepseekVL\n",
        "    deepseekvl_model = models[\"deepseek_vl2\"].to(device)\n",
        "    deepseekvl_processor = models[\"processors\"][\"deepseek_vl2\"]\n",
        "    prompt = \"Analyze this image for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = deepseekvl_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = deepseekvl_model.generate(**inputs)\n",
        "        analysis = deepseekvl_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        deepseekvl_score = analyze_llava_response(analysis)\n",
        "        scores.append(deepseekvl_score)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"image_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "fVZT1Ci_13nk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "def analyze_eye_movements(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "    blink_threshold = 0.2\n",
        "    blink_count = 0\n",
        "    for frame in frames:\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray)\n",
        "\n",
        "        for face in faces:\n",
        "            landmarks = predictor(gray, face)\n",
        "            left_eye_ratio = get_eye_aspect_ratio(landmarks, [36, 37, 38, 39, 40, 41])\n",
        "            right_eye_ratio = get_eye_aspect_ratio(landmarks, [42, 43, 44, 45, 46, 47])\n",
        "            if left_eye_ratio < blink_threshold or right_eye_ratio < blink_threshold:\n",
        "                blink_count += 1\n",
        "\n",
        "    if blink_count > len(frames) * 0.2:  # Example threshold for abnormal blink rate\n",
        "        anomalies.append(\"Abnormal blink rate detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def get_eye_aspect_ratio(landmarks, eye_points):\n",
        "    A = np.linalg.norm(np.array([landmarks.part(eye_points[1]).x, landmarks.part(eye_points[1]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[5]).x, landmarks.part(eye_points[5]).y]))\n",
        "    B = np.linalg.norm(np.array([landmarks.part(eye_points[2]).x, landmarks.part(eye_points[2]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[4]).x, landmarks.part(eye_points[4]).y]))\n",
        "    C = np.linalg.norm(np.array([landmarks.part(eye_points[0]).x, landmarks.part(eye_points[0]).y]) -\n",
        "                       np.array([landmarks.part(eye_points[3]).x, landmarks.part(eye_points[3]).y]))\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def analyze_facial_blood_flow(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "    for frame in frames:\n",
        "        results = mp_face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks:\n",
        "            for face_landmarks in results.multi_face_landmarks:\n",
        "                if detect_blood_flow_anomaly(frame, face_landmarks):\n",
        "                    anomalies.append(\"Inconsistent blood flow patterns detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def detect_blood_flow_anomaly(frame, face_landmarks):\n",
        "    # Example logic to detect blood flow anomalies using color consistency\n",
        "    mean_color = np.mean(frame, axis=(0, 1))\n",
        "    if np.std(mean_color) > 10:  # Example threshold for color inconsistency\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def analyze_physics_based_patterns(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    for frame in frames:\n",
        "        if detect_physics_anomaly(frame):\n",
        "            anomalies.append(\"Physics-based inconsistencies detected\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_physics_anomaly(frame):\n",
        "    # Example logic for detecting hair movement and shadow consistency\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    if np.mean(edges) > 50:  # Example threshold for detecting sharp edges\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def detect_neural_rendering_artifacts(frames: List[np.ndarray], models: Dict[str, Any]) -> List[str]:\n",
        "    anomalies = []\n",
        "    gan_artifact_model = models[\"gan_artifact_model\"]\n",
        "\n",
        "    for frame in frames:\n",
        "        if detect_gan_artifacts(frame, gan_artifact_model):\n",
        "            anomalies.append(\"Neural rendering artifacts detected\")\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "def detect_gan_artifacts(frame, model):\n",
        "    # Example logic for detecting GAN artifacts using a neural network\n",
        "    resized_frame = cv2.resize(frame, (224, 224))\n",
        "    frame_tensor = torch.tensor(resized_frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    with torch.no_grad():\n",
        "        output = model(frame_tensor.to(device))\n",
        "    if output.argmax() == 1:  # Assuming the model outputs 1 for GAN artifacts\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "YhuLWJ6k3NUV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Video Analysis**"
      ],
      "metadata": {
        "id": "MlQVINuo1_2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Video Analysis (continued)\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model 1: VideoMAE\n",
        "    videomae_model = models[\"videomae\"].to(device)\n",
        "    videomae_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"videomae\"])\n",
        "    inputs = videomae_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        videomae_output = videomae_model(**inputs)\n",
        "        videomae_score = torch.softmax(videomae_output.logits, dim=-1)\n",
        "        scores.append(videomae_score.max().item())\n",
        "\n",
        "    # Model 2: Timesformer\n",
        "    timesformer_model = models[\"timesformer\"].to(device)\n",
        "    timesformer_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"timesformer\"])\n",
        "    inputs = timesformer_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        timesformer_output = timesformer_model(**inputs)\n",
        "        timesformer_score = torch.softmax(timesformer_output.logits, dim=-1)\n",
        "        scores.append(timesformer_score.max().item())\n",
        "\n",
        "    # Model 3: Llava\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this video for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(video=video_data[\"frames\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "\n",
        "    # Model 4: InternVL\n",
        "    internvl_model = models[\"internvl\"].to(device)\n",
        "    internvl_processor = VideoMAEFeatureExtractor.from_pretrained(Config.MODEL_PATHS[\"internvl\"])\n",
        "    inputs = internvl_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        internvl_output = internvl_model(**inputs)\n",
        "        internvl_score = torch.softmax(internvl_output.logits, dim=-1)\n",
        "        scores.append(internvl_score.max().item())\n",
        "\n",
        "    # Eye Movement Analysis\n",
        "    eye_movement_anomalies = analyze_eye_movements(video_data[\"frames\"], models)\n",
        "    anomalies.extend(eye_movement_anomalies)\n",
        "\n",
        "    # Facial Blood Flow Patterns\n",
        "    blood_flow_anomalies = analyze_facial_blood_flow(video_data[\"frames\"], models)\n",
        "    anomalies.extend(blood_flow_anomalies)\n",
        "\n",
        "    # Physics-Based Verification\n",
        "    physics_based_anomalies = analyze_physics_based_patterns(video_data[\"frames\"], models)\n",
        "    anomalies.extend(physics_based_anomalies)\n",
        "\n",
        "    # Neural Rendering Artifacts\n",
        "    neural_rendering_anomalies = detect_neural_rendering_artifacts(video_data[\"frames\"], models)\n",
        "    anomalies.extend(neural_rendering_anomalies)\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"video_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "jRAPWt8u2IsV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, XCLIPModel\n",
        "from vllm import LLM, SamplingParams\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def lip_sync_detection(frames, audio_text, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Detect lip synchronization using X-CLIP and vllm.\n",
        "\n",
        "    Args:\n",
        "        frames (torch.Tensor): Video frames\n",
        "        audio_text (str): Text representation of the audio\n",
        "        threshold (float): Sync threshold\n",
        "\n",
        "    Returns:\n",
        "        dict: Lip sync analysis results\n",
        "    \"\"\"\n",
        "    # Load processor and model\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "    model = XCLIPModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "\n",
        "    # Preprocess video frames\n",
        "    video_inputs = processor(videos=list(frames), return_tensors=\"pt\")\n",
        "    video_features = model.get_video_features(**video_inputs)\n",
        "\n",
        "    # Preprocess audio text using vllm\n",
        "    llm = LLM(model=\"microsoft/xclip-base-patch32\",\n",
        "              tensor_parallel_size=torch.cuda.device_count())\n",
        "\n",
        "    # Generate text features using vllm\n",
        "    text_features = llm.generate(\n",
        "        inputs=[audio_text],\n",
        "        sampling_params=SamplingParams(temperature=0, max_tokens=64)\n",
        "    )\n",
        "\n",
        "    # Flatten the features for similarity comparison\n",
        "    video_features_flat = video_features.mean(dim=1).detach().numpy()\n",
        "    text_features_flat = text_features.mean(dim=1).detach().numpy()\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    sync_score = cosine_similarity(video_features_flat, text_features_flat)[0][0]\n",
        "\n",
        "    return {\n",
        "        'sync_score': sync_score,\n",
        "        'is_synced': sync_score > threshold\n",
        "    }"
      ],
      "metadata": {
        "id": "49tH6GarH7Tl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_consistency_analysis(results: Dict[str, DeepfakeAnalysisResult], llms: Dict[str, Any]) -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"llava\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5"
      ],
      "metadata": {
        "id": "8K7h2pSWEbO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def face_forgery_detection(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model: Face X-ray\n",
        "    face_xray_model = models[\"face_xray\"].to(device)\n",
        "    face_xray_processor = CLIPProcessor.from_pretrained(Config.MODEL_PATHS[\"clip_processor\"])\n",
        "    inputs = face_xray_processor(images=video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        face_xray_output = face_xray_model(**inputs)\n",
        "        face_xray_score = torch.softmax(face_xray_output.logits_per_image, dim=-1)\n",
        "        scores.append(face_xray_score.max().item())\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"face_forgery_detection\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "q61-4f7OEeJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def background_consistency_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Model: Vision Transformer\n",
        "    vit_model = models[\"vision\"].to(device)\n",
        "    vit_processor = CLIPProcessor.from_pretrained(Config.MODEL_PATHS[\"clip_processor\"])\n",
        "    inputs = vit_processor(images=video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        vit_output = vit_model(**inputs)\n",
        "        vit_score = torch.softmax(vit_output.logits_per_image, dim=-1)\n",
        "        scores.append(vit_score.max().item())\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"background_consistency_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "r_JDhnaAEkUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_temporal_coherence(temporal_features: List[Dict[str, Any]]) -> float:\n",
        "    anomalies = []\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return 1.0 - (len(anomalies) / len(temporal_features))"
      ],
      "metadata": {
        "id": "fnLHMoz0Epy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tool Creation Functions**\n",
        "- Define functions for creating deepfake detection tools and agents"
      ],
      "metadata": {
        "id": "HoftOwSUZPye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_deepfake_detection_tools(models: Dict[str, Any], device: torch.device) -> List[Tool]:\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"analyze_video\",\n",
        "            func=lambda x: advanced_video_analysis(x, models, device),\n",
        "            description=\"Analyzes video content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_audio\",\n",
        "            func=lambda x: advanced_audio_analysis(x, models, device),\n",
        "            description=\"Analyzes audio content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_image\",\n",
        "            func=lambda x: advanced_image_analysis(x, models, device),\n",
        "            description=\"Analyzes image content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"semantic_analysis\",\n",
        "            func=lambda x: semantic_consistency_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes semantic consistency across modalities\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"face_forgery_detection\",\n",
        "            func=lambda x: face_forgery_detection(x, models, device),\n",
        "            description=\"Detects face forgeries in video content\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"lip_sync_detection\",\n",
        "            func=lambda x: lip_sync_detection(x, models, device),\n",
        "            description=\"Analyzes lip-sync consistency between audio and video\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"background_consistency\",\n",
        "            func=lambda x: background_consistency_analysis(x, models, device),\n",
        "            description=\"Analyzes background consistency across frames\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"real_time_streaming_analysis\",\n",
        "            func=lambda x: real_time_streaming_analysis(x, models, device),\n",
        "            description=\"Analyzes live video streams for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"text_analysis\",\n",
        "            func=lambda x: text_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes text content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"metadata_analysis\",\n",
        "            func=lambda x: metadata_analysis(x),\n",
        "            description=\"Analyzes metadata for signs of manipulation\"\n",
        "        )\n",
        "    ]\n",
        "    return tools"
      ],
      "metadata": {
        "id": "PEWzNSs3ZGK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Detection Agent**\n",
        "- Define the detection graph for processing the input data"
      ],
      "metadata": {
        "id": "IgsLH-Xp2eW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langchain.tools import Tool\n",
        "\n",
        "def create_detection_agent(tools: List[Tool], llm: ChatOpenAI) -> AgentExecutor:\n",
        "    class DeepfakeDetectionOutputParser:\n",
        "        def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "            try:\n",
        "                if \"Final Answer:\" in llm_output:\n",
        "                    return AgentFinish(\n",
        "                        return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                        log=llm_output,\n",
        "                    )\n",
        "                action_match = re.search(r\"Action: (.*?)\\nAction Input: (.*)\", llm_output, re.DOTALL)\n",
        "                if not action_match:\n",
        "                    raise ValueError(\"Could not parse LLM output: \" + llm_output)\n",
        "                action = action_match.group(1).strip()\n",
        "                action_input = action_match.group(2).strip()\n",
        "                return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not parse LLM output: {llm_output}\") from e\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"\n",
        "            You are an expert deepfake detection system. Your goal is to analyze content across multiple modalities\n",
        "            to determine authenticity. Consider all available evidence and patterns including:\n",
        "\n",
        "            1. Visual elements: inconsistencies, artifacts, unnatural patterns\n",
        "            2. Audio characteristics: synthetic artifacts, unnatural transitions\n",
        "            3. Semantic coherence: logical consistency across modalities\n",
        "            4. Temporal patterns: synchronization, continuity\n",
        "            5. Biometric features: facial landmarks, expressions, movements\n",
        "\n",
        "            Available tools:\n",
        "            {tools}\n",
        "\n",
        "            Process:\n",
        "            1. Analyze the input using appropriate tools\n",
        "            2. Evaluate evidence across modalities\n",
        "            3. Make a final determination on authenticity\n",
        "\n",
        "            Format your response as:\n",
        "            Action: [tool name]\n",
        "            Action Input: [tool input]\n",
        "            Observation: [result]\n",
        "            ... (repeat for additional tools as needed)\n",
        "            Final Answer: [detailed analysis and verdict]\n",
        "        \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "    ])\n",
        "\n",
        "    return AgentExecutor.from_agent_and_tools(\n",
        "        agent=LLMSingleActionAgent(\n",
        "            llm_chain=LLMChain(llm=llm, prompt=prompt),\n",
        "            output_parser=DeepfakeDetectionOutputParser(),\n",
        "            stop=[\"Observation:\", \"Final Answer:\"],\n",
        "            allowed_tools=[tool.name for tool in tools]\n",
        "        ),\n",
        "        tools=tools,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "epoIrEie2nx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Detection Graph**\n",
        "- Define the detection graph for processing the input data"
      ],
      "metadata": {
        "id": "W9ZBdOffZosP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate the functions into the pipeline\n",
        "def create_detection_graph() -> StateGraph:\n",
        "    def preprocess(state):\n",
        "        input_data = state[\"input\"]\n",
        "        processed_data = asyncio.run(enhanced_preprocessing(input_data))\n",
        "        return {**state, \"processed_data\": processed_data}\n",
        "\n",
        "    def analyze_modalities(state):\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        device = state[\"device\"]\n",
        "        results = {\n",
        "            \"video\": asyncio.run(advanced_video_analysis(processed_data, models, device)) if \"frames\" in processed_data else None,\n",
        "            \"audio\": asyncio.run(advanced_audio_analysis(processed_data.get(\"audio\"), models, device)) if \"audio\" in processed_data else None,\n",
        "            \"image\": asyncio.run(advanced_image_analysis(processed_data[\"image\"], models, device)) if \"image\" in processed_data else None,\n",
        "            \"text\": asyncio.run(text_analysis(processed_data.get(\"text\"), models[\"llms\"])) if \"text\" in processed_data else None,\n",
        "            \"face_forgery\": face_forgery_detection(processed_data, models, device) if \"frames\" in processed_data else None,\n",
        "            \"background\": background_consistency_analysis(processed_data, models, device) if \"frames\" in processed_data else None\n",
        "        }\n",
        "        return {**state, \"modality_results\": results}\n",
        "\n",
        "    def cross_modal_analysis(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        cross_modal_score = analyze_cross_modal_consistency(results, processed_data, models)\n",
        "        return {**state, \"cross_modal_score\": cross_modal_score}\n",
        "\n",
        "    def generate_report(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        cross_modal_score = state[\"cross_modal_score\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        report = generate_comprehensive_report(results, cross_modal_score, processed_data)\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    workflow = StateGraph(nodes=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"analyze_modalities\", analyze_modalities),\n",
        "        (\"cross_modal_analysis\", cross_modal_analysis),\n",
        "        (\"generate_report\", generate_report)\n",
        "    ])\n",
        "    workflow.add_edge(\"preprocess\", \"analyze_modalities\")\n",
        "    workflow.add_edge(\"analyze_modalities\", \"cross_modal_analysis\")\n",
        "    workflow.add_edge(\"cross_modal_analysis\", \"generate_report\")\n",
        "    workflow.add_edge(\"generate_report\", END)\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "2_x9xL7gZZs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analyzing Cross-Modal Consistency and Generating Reports**"
      ],
      "metadata": {
        "id": "EF3CZ3zAaAPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_cross_modal_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    models: Dict[str, Any]\n",
        ") -> float:\n",
        "    scores = []\n",
        "    if results[\"audio\"] and results[\"video\"]:\n",
        "        sync_score = analyze_av_sync(processed_data[\"frames\"], processed_data[\"audio\"])\n",
        "        scores.append(sync_score)\n",
        "    semantic_score = analyze_semantic_consistency(results, processed_data, models[\"llms\"])\n",
        "    scores.append(semantic_score)\n",
        "    temporal_score = analyze_temporal_coherence(processed_data[\"temporal_features\"])\n",
        "    scores.append(temporal_score)\n",
        "    bio_score = analyze_biometric_consistency(processed_data[\"frames\"], models)\n",
        "    scores.append(bio_score)\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "def analyze_semantic_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    llms: Dict[str, ChatOpenAI]\n",
        ") -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"gpt4\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5\n",
        "\n",
        "def generate_comprehensive_report(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    cross_modal_score: float,\n",
        "    processed_data: Dict[str, Any]\n",
        ") -> MultimodalAnalysisReport:\n",
        "    scores = [\n",
        "        results[\"video\"].score if results[\"video\"] else 0.5,\n",
        "        results[\"audio\"].score if results[\"audio\"] else 0.5,\n",
        "        results[\"image\"].score if results[\"image\"] else 0.5,\n",
        "        results[\"text\"].score if results[\"text\"] else 0.5,\n",
        "        cross_modal_score\n",
        "    ]\n",
        "    weights = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "    final_score = sum(s * w for s, w in zip(scores, weights))\n",
        "    evidence = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            evidence.extend([\n",
        "                {\n",
        "                    \"type\": modality,\n",
        "                    \"description\": anomaly,\n",
        "                    \"confidence\": result.confidence,\n",
        "                    \"method\": result.method\n",
        "                }\n",
        "                for anomaly in result.anomalies\n",
        "            ])\n",
        "    return MultimodalAnalysisReport(\n",
        "        case_id=f\"DFD-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        file_info=processed_data[\"metadata\"] if \"metadata\" in processed_data else {},\n",
        "        video_analysis=results[\"video\"],\n",
        "        audio_analysis=results[\"audio\"],\n",
        "        image_analysis=results[\"image\"],\n",
        "        text_analysis=results[\"text\"],\n",
        "        multimodal_score=float(final_score),\n",
        "        verdict=\"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\",\n",
        "        evidence=evidence,\n",
        "        metadata={\n",
        "            \"processing_time\": datetime.now().isoformat(),\n",
        "            \"models_used\": list(results.keys()),\n",
        "            \"cross_modal_score\": cross_modal_score,\n",
        "            \"confidence_distribution\": {\n",
        "                modality: result.confidence\n",
        "                for modality, result in results.items() if result\n",
        "            }\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "H0mTuH2hZ5de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Real-Time Streaming Analysis**\n",
        "- Define a function for real-time streaming analysis"
      ],
      "metadata": {
        "id": "Ov48BT01aOO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def real_time_streaming_analysis(video_stream: Any, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": 30,\n",
        "        \"frame_count\": 0,\n",
        "        \"width\": 0,\n",
        "        \"height\": 0,\n",
        "        \"duration\": 0,\n",
        "        \"codec\": \"N/A\",\n",
        "        \"file_size\": 0\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while True:\n",
        "        frame = await video_stream.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "    video_data = {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }\n",
        "\n",
        "    return await advanced_video_analysis(video_data, models, device)"
      ],
      "metadata": {
        "id": "e6-q4yeeaKEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Text Analysis**\n",
        "- Define a function for text analysis using Groq models:"
      ],
      "metadata": {
        "id": "hHpkFFGDaqtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Text Analysis\n",
        "async def text_analysis(text: str, llms: Dict[str, ChatOpenAI]) -> DeepfakeAnalysisResult:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze the following text for signs of manipulation or inconsistencies:\\n\\n{text}\"\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        score = float(score_match.group(1))\n",
        "    else:\n",
        "        score = 0.5\n",
        "    anomalies = [line.strip() for line in content.split(\"Reasoning:\")[-1].strip().split(\"\\n\") if line.strip()]\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=score,\n",
        "        label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"text_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "UpL42dfZaRF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Metadata Analysis**\n",
        "- Define a function for metadata analysis"
      ],
      "metadata": {
        "id": "8WGV0gpibBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_analysis(metadata: Dict[str, Any]) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    if metadata.get(\"fps\") < 10:\n",
        "        anomalies.append(\"Unusually low frame rate\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"duration\") < 1:\n",
        "        anomalies.append(\"Unusually short duration\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"file_size\") < 100000:\n",
        "        anomalies.append(\"Unusually small file size\")\n",
        "        scores.append(0.2)\n",
        "\n",
        "    final_score = np.mean(scores) if scores else 0.5\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)) if scores else 0.0,\n",
        "        method=\"metadata_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "lr_VZJS_aRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Main Function**\n",
        "- Define the main function to run the deepfake detection system"
      ],
      "metadata": {
        "id": "iZRCoZgobKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function to Run Deepfake Detection\n",
        "async def run_deepfake_detection(file_path: str, mode: str = \"all\") -> MultimodalAnalysisReport:\n",
        "    try:\n",
        "        tools = create_deepfake_detection_tools(env[\"models\"], env[\"device\"])\n",
        "        agent = create_detection_agent(tools, env[\"models\"][\"llms\"][\"gpt4\"])\n",
        "        workflow = create_detection_graph()\n",
        "        initial_state = {\n",
        "            \"input\": file_path,\n",
        "            \"mode\": mode,\n",
        "            \"models\": env[\"models\"],\n",
        "            \"device\": env[\"device\"]\n",
        "        }\n",
        "        final_state = workflow.run(initial_state)\n",
        "        return final_state[\"final_report\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# File Upload Widget\n",
        "upload_widget = widgets.FileUpload(\n",
        "    accept='image/*,video/*,audio/*',  # Accept images, videos, and audio files\n",
        "    multiple=False  # Do not allow multiple files\n",
        ")\n",
        "\n",
        "# Create an output widget to display the forensic report\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "# Function to handle file upload and deepfake detection\n",
        "def on_upload_change(change):\n",
        "    # Get the uploaded file\n",
        "    uploaded_file = list(upload_widget.value.values())[0]\n",
        "    file_content = uploaded_file['content']\n",
        "\n",
        "    # Save the uploaded file to a temporary location\n",
        "    file_path = '/content/' + uploaded_file['name']\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file_content)\n",
        "\n",
        "    # Run the deepfake detection\n",
        "    asyncio.run(handle_uploaded_file(file_path))\n",
        "\n",
        "async def handle_uploaded_file(file_path):\n",
        "    # Run the deepfake detection\n",
        "    report = await run_deepfake_detection(file_path)\n",
        "\n",
        "    # Display the forensic report\n",
        "    with output_widget:\n",
        "        output_widget.clear_output()\n",
        "        print(json.dumps(report.dict(), indent=2, default=str))\n",
        "\n",
        "upload_widget.observe(on_upload_change, names='value')\n",
        "\n",
        "# Display the widgets\n",
        "display(upload_widget)\n",
        "display(output_widget)"
      ],
      "metadata": {
        "id": "u20AY1RxnL24",
        "outputId": "28840740-68f8-48aa-f7eb-a21b82730b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c83c38c9dce3454d872378bf043c10c3",
            "3c17045589454bb0bd53438a369b59f3",
            "5451e6d03d294dde8595fb6929de1371",
            "463a38a3acef4c1682c9c30e4e99bf99",
            "f682933602744b80a7aa45f96fefe777"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='image/*,video/*,audio/*', description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c83c38c9dce3454d872378bf043c10c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "463a38a3acef4c1682c9c30e4e99bf99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAEKfJjTyDVx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}