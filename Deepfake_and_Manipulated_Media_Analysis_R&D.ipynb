{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjPyaMIFFG1OYXJlORPH/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4",
        "outputId": "882fbaac-4d41-41e7-f97f-48943a6fb7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langgraph torch transformers opencv-python librosa numpy face-recognition dlib mediapipe scipy pillow tqdm pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import librosa\n",
        "import face_recognition\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoProcessor, AutoModelForVideoClassification, AutoModelForAudioClassification,\n",
        "    CLIPProcessor, CLIPModel, Blip2Processor, Blip2ForConditionalGeneration,\n",
        "    VideoMAEFeatureExtractor, VideoMAEForVideoClassification, WhisperProcessor,\n",
        "    WhisperForAudioClassification, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification,\n",
        "    OwlViTProcessor, OwlViTForObjectDetection, InstructBlipProcessor,\n",
        "    InstructBlipForConditionalGeneration, ImageGPTForCausalImageModeling,\n",
        "    TimesformerForVideoClassification, AutoModelForAudioClassification,\n",
        "    ASTForAudioClassification, Wav2Vec2ForSequenceClassification\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langgraph.graph import Graph, END"
      ],
      "metadata": {
        "id": "JNL83H4CUsgb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic models for structured output\n",
        "class AnalysisMetrics(BaseModel):\n",
        "    score: float = Field(..., description=\"Confidence score between 0 and 1\")\n",
        "    anomalies: List[str] = Field(default_factory=list, description=\"Detected anomalies\")\n",
        "    confidence_level: str = Field(..., description=\"Low/Medium/High confidence assessment\")\n",
        "\n",
        "class ModalityAnalysis(BaseModel):\n",
        "    visual: AnalysisMetrics\n",
        "    temporal: AnalysisMetrics\n",
        "    facial: AnalysisMetrics\n",
        "    audio: AnalysisMetrics\n",
        "    semantic: AnalysisMetrics\n",
        "    behavioral: AnalysisMetrics\n",
        "\n",
        "class ForensicReport(BaseModel):\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    file_metadata: Dict[str, Any]\n",
        "    analysis_results: ModalityAnalysis\n",
        "    final_verdict: str\n",
        "    confidence_score: float\n",
        "    risk_assessment: str\n",
        "    evidence_summary: List[str]\n",
        "    recommendations: List[str]"
      ],
      "metadata": {
        "id": "LuooedJepXXG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_enhanced_environment():\n",
        "    \"\"\"Initialize advanced environment with additional models\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    models = {\n",
        "        # Video Analysis Models\n",
        "        \"videomae\": VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\"),\n",
        "        \"timesformer\": TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\"),\n",
        "\n",
        "        # Vision Models\n",
        "        \"clip\": CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "        \"owlvit\": OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\"),\n",
        "        \"instructblip\": InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "\n",
        "        # Audio Models\n",
        "        \"whisper\": WhisperForAudioClassification.from_pretrained(\"openai/whisper-large-v3\"),\n",
        "        \"wav2vec2\": Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\"),\n",
        "        \"ast\": ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\"),\n",
        "\n",
        "        # Face Analysis\n",
        "        \"face_detector\": mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.7),\n",
        "        \"face_mesh\": mp.solutions.face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            min_detection_confidence=0.7,\n",
        "            min_tracking_confidence=0.7\n",
        "        ),\n",
        "\n",
        "        # Processors\n",
        "        \"processors\": {\n",
        "            \"clip\": CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "            \"owlvit\": OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\"),\n",
        "            \"instructblip\": InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "            \"whisper\": WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Move models to device\n",
        "    for key, model in models.items():\n",
        "        if isinstance(model, torch.nn.Module):\n",
        "            models[key] = model.to(device)\n",
        "\n",
        "    return {\"device\": device, \"models\": models}"
      ],
      "metadata": {
        "id": "JVJJvhKNU27U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_preprocess_video(video_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced preprocessing with additional feature extraction\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "        \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "        \"duration\": float(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / float(cap.get(cv2.CAP_PROP_FPS))\n",
        "    }\n",
        "\n",
        "    # Enhanced frame extraction with quality metrics\n",
        "    frame_quality_metrics = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "\n",
        "        # Calculate frame quality metrics\n",
        "        blur = cv2.Laplacian(frame, cv2.CV_64F).var()\n",
        "        noise = np.std(frame)\n",
        "        frame_quality_metrics.append({\n",
        "            \"blur\": blur,\n",
        "            \"noise\": noise\n",
        "        })\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Enhanced audio extraction\n",
        "    try:\n",
        "        y, sr = librosa.load(video_path)\n",
        "\n",
        "        # Extract advanced audio features\n",
        "        audio_data = {\n",
        "            \"raw\": y,\n",
        "            \"sr\": sr,\n",
        "            \"mfcc\": librosa.feature.mfcc(y=y, sr=sr),\n",
        "            \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
        "            \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
        "            \"onset_env\": librosa.onset.onset_strength(y=y, sr=sr),\n",
        "            \"tempo\": librosa.beat.tempo(y=y, sr=sr)[0]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Audio extraction error: {e}\")\n",
        "        audio_data = None\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"frame_quality\": frame_quality_metrics\n",
        "    }\n",
        "\n",
        "def advanced_visual_analysis_agent(\n",
        "    frames: List[np.ndarray],\n",
        "    models: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> AnalysisMetrics:\n",
        "    \"\"\"Advanced visual analysis using multiple models\"\"\"\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # CLIP Analysis\n",
        "    for frame in frames[::10]:  # Analyze every 10th frame for efficiency\n",
        "        clip_inputs = models[\"processors\"][\"clip\"](images=frame, return_tensors=\"pt\").to(device)\n",
        "        clip_features = models[\"clip\"](**clip_inputs).image_features\n",
        "        clip_score = float(clip_features.mean().item())\n",
        "        scores.append(clip_score)\n",
        "\n",
        "        if clip_score < 0.5:\n",
        "            anomalies.append(f\"Low CLIP confidence at frame {len(scores) * 10}\")\n",
        "\n",
        "    # OWL-ViT Object Detection\n",
        "    for frame in frames[::30]:\n",
        "        inputs = models[\"processors\"][\"owlvit\"](images=frame, return_tensors=\"pt\").to(device)\n",
        "        outputs = models[\"owlvit\"](**inputs)\n",
        "\n",
        "        # Analyze object detection confidence\n",
        "        if outputs.logits.mean().item() < 0.3:\n",
        "            anomalies.append(\"Suspicious object detection patterns detected\")\n",
        "\n",
        "    # InstructBLIP Analysis\n",
        "    prompts = [\n",
        "        \"Describe any visual artifacts or inconsistencies in this image.\",\n",
        "        \"Are there any unnatural elements in this image?\",\n",
        "        \"Analyze the lighting and shadows in this image.\"\n",
        "    ]\n",
        "\n",
        "    for frame in frames[::60]:\n",
        "        for prompt in prompts:\n",
        "            inputs = models[\"processors\"][\"instructblip\"](images=frame, text=prompt, return_tensors=\"pt\").to(device)\n",
        "            outputs = models[\"instructblip\"].generate(**inputs)\n",
        "\n",
        "            # Analysis logic here\n",
        "            if \"artifact\" in outputs or \"unnatural\" in outputs:\n",
        "                anomalies.append(f\"InstructBLIP detected potential artifacts: {outputs}\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_score = np.mean(scores)\n",
        "    confidence_level = \"High\" if avg_score > 0.8 else \"Medium\" if avg_score > 0.6 else \"Low\"\n",
        "\n",
        "    return AnalysisMetrics(\n",
        "        score=float(avg_score),\n",
        "        anomalies=anomalies,\n",
        "        confidence_level=confidence_level\n",
        "    )\n",
        "\n",
        "def advanced_audio_analysis_agent(\n",
        "    audio_data: Dict[str, Any],\n",
        "    models: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> AnalysisMetrics:\n",
        "    \"\"\"Advanced audio analysis using multiple models\"\"\"\n",
        "    if not audio_data:\n",
        "        return AnalysisMetrics(\n",
        "            score=0.0,\n",
        "            anomalies=[\"No audio data available\"],\n",
        "            confidence_level=\"Low\"\n",
        "        )\n",
        "\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Whisper Analysis\n",
        "    whisper_inputs = models[\"processors\"][\"whisper\"](\n",
        "        audio_data[\"raw\"],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    whisper_outputs = models[\"whisper\"](**whisper_inputs)\n",
        "    whisper_score = whisper_outputs.logits.softmax(dim=1).mean().item()\n",
        "    scores.append(whisper_score)\n",
        "\n",
        "    # Wav2Vec2 Analysis\n",
        "    wav2vec_outputs = models[\"wav2vec2\"](\n",
        "        torch.tensor(audio_data[\"raw\"]).unsqueeze(0).to(device)\n",
        "    )\n",
        "    wav2vec_score = wav2vec_outputs.logits.softmax(dim=1).max().item()\n",
        "    scores.append(wav2vec_score)\n",
        "\n",
        "    # AST Analysis\n",
        "    ast_outputs = models[\"ast\"](\n",
        "        torch.tensor(audio_data[\"raw\"]).unsqueeze(0).to(device)\n",
        "    )\n",
        "    ast_score = ast_outputs.logits.softmax(dim=1).max().item()\n",
        "    scores.append(ast_score)\n",
        "\n",
        "    # Advanced audio feature analysis\n",
        "    mfcc_variance = np.var(audio_data[\"mfcc\"])\n",
        "    if mfcc_variance > 1.5:\n",
        "        anomalies.append(\"Unusual MFCC patterns detected\")\n",
        "\n",
        "    tempo_consistency = np.std(audio_data[\"onset_env\"]) / np.mean(audio_data[\"onset_env\"])\n",
        "    if tempo_consistency > 0.5:\n",
        "        anomalies.append(\"Inconsistent tempo patterns detected\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_score = np.mean(scores)\n",
        "    confidence_level = \"High\" if avg_score > 0.8 else \"Medium\" if avg_score > 0.6 else \"Low\"\n",
        "\n",
        "    return AnalysisMetrics(\n",
        "        score=float(avg_score),\n",
        "        anomalies=anomalies,\n",
        "        confidence_level=confidence_level\n",
        "    )"
      ],
      "metadata": {
        "id": "D4JBongFVeob"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_forensic_report(\n",
        "    analysis_results: Dict[str, AnalysisMetrics],\n",
        "    metadata: Dict[str, Any]\n",
        ") -> ForensicReport:\n",
        "    \"\"\"Generate comprehensive forensic report\"\"\"\n",
        "    modality_analysis = ModalityAnalysis(\n",
        "        visual=analysis_results[\"visual\"],\n",
        "        temporal=analysis_results[\"temporal\"],\n",
        "        facial=analysis_results[\"facial\"],\n",
        "        audio=analysis_results[\"audio\"],\n",
        "        semantic=analysis_results[\"semantic\"],\n",
        "        behavioral=analysis_results[\"behavioral\"]\n",
        "    )\n",
        "\n",
        "    # Calculate final confidence score\n",
        "    weights = {\n",
        "        \"visual\": 0.25,\n",
        "        \"temporal\": 0.2,\n",
        "        \"facial\": 0.2,\n",
        "        \"audio\": 0.15,\n",
        "        \"semantic\": 0.1,\n",
        "        \"behavioral\": 0.1\n",
        "    }\n",
        "\n",
        "    final_score = sum(\n",
        "        analysis_results[key].score * weights[key]\n",
        "        for key in weights\n",
        "    )\n",
        "\n",
        "    # Determine verdict\n",
        "    verdict = \"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\"\n",
        "\n",
        "    # Risk assessment\n",
        "    risk_levels = {\n",
        "        (0.8, 1.0): \"Low Risk - High confidence in authenticity\",\n",
        "        (0.6, 0.8): \"Medium Risk - Some suspicious patterns detected\",\n",
        "        (0.0, 0.6): \"High Risk - Strong indicators of manipulation\"\n",
        "    }\n",
        "\n",
        "    risk_assessment = next(\n",
        "        desc for (lower, upper), desc in risk_levels.items()\n",
        "        if lower <= final_score < upper\n",
        "    )\n",
        "\n",
        "    # Compile evidence summary\n",
        "    evidence_summary = []\n",
        "    for modality, metrics in analysis_results.items():\n",
        "        if metrics.anomalies:\n",
        "            evidence_summary.extend(\n",
        "                f\"{modality.capitalize()} Analysis: {anomaly}\"\n",
        "                for anomaly in metrics.anomalies\n",
        "            )\n",
        "\n",
        "    # Generate recommendations\n",
        "    recommendations = [\n",
        "        \"Conduct additional manual review by forensic experts\",\n",
        "        \"Cross-reference with original source material if available\",\n",
        "        \"Verify temporal consistency across all frames\",\n",
        "        \"Check audio-visual synchronization in detail\"\n",
        "    ]\n",
        "\n",
        "    if final_score < 0.6:\n",
        "        recommendations.extend([\n",
        "            \"Flag content for immediate review\",\n",
        "            \"Consider restricting distribution until verification\"\n",
        "        ])\n",
        "\n",
        "    return ForensicReport(\n",
        "        file_metadata=metadata,\n",
        "        analysis_results=modality_analysis,\n",
        "        final_verdict=verdict,\n",
        "        confidence_score=final_score,\n",
        "        risk_assessment=risk_assessment,\n",
        "        evidence_summary=evidence_summary,\n",
        "        recommendations=recommendations\n",
        "    )\n",
        "\n",
        "def run_enhanced_detection(video_path: str, verbose: bool = False) -> ForensicReport:\n",
        "    \"\"\"Main function to run enhanced deepfake detection pipeline\"\"\"\n",
        "    try:\n",
        "        # Setup environment\n",
        "        env = setup_enhanced_environment()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Initialized environment and models\")\n",
        "\n",
        "        # Preprocess video\n",
        "        preprocessed_data = enhanced_preprocess_video(video_path)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Completed video preprocessing\")\n",
        "\n",
        "        # Run analysis agents\n",
        "        analysis_results = {\n",
        "            \"visual\": advanced_visual_analysis_agent(\n",
        "                preprocessed_data[\"frames\"],\n",
        "                env[\"models\"],\n",
        "                env[\"device\"]\n",
        "            ),\n",
        "            \"temporal\": temporal_coherence_agent(\n",
        "                preprocessed_data[\"frames\"],\n",
        "                preprocessed_data.get(\"optical_flow\", [])\n",
        "            ),\n",
        "            \"facial\": facial_analysis_agent(\n",
        "                preprocessed_data[\"frames\"],\n",
        "                env[\"models\"][\"face_detector\"]\n",
        "            ),\n",
        "            \"audio\": advanced_audio_analysis_agent(\n",
        "                preprocessed_data[\"audio\"],\n",
        "                env[\"models\"],\n",
        "                env[\"device\"]\n",
        "            ),\n",
        "            \"semantic\": semantic_analysis_agent(preprocessed_data),\n",
        "            \"behavioral\": behavioral_analysis_agent(preprocessed_data)"
      ],
      "metadata": {
        "id": "a_eCU_wTqspb",
        "outputId": "515fb0dd-5403-4834-d2f2-58b50985c478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-8-f070329f0ab6>, line 114)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f070329f0ab6>\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    \"behavioral\": behavioral_analysis_agent(preprocessed_data)\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_coherence_agent(\n",
        "    frames: List[np.ndarray],\n",
        "    optical_flow: List[np.ndarray]\n",
        ") -> AnalysisMetrics:\n",
        "    \"\"\"Advanced temporal coherence analysis\"\"\"\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Calculate frame-to-frame consistency\n",
        "    for i in range(len(frames) - 1):\n",
        "        # Calculate structural similarity\n",
        "        ssim = cv2.compareSSIM(frames[i], frames[i+1], multichannel=True)\n",
        "        scores.append(ssim)\n",
        "\n",
        "        if ssim < 0.5:\n",
        "            anomalies.append(f\"Abrupt scene change detected at frame {i}\")\n",
        "\n",
        "    # Analyze motion patterns\n",
        "    if optical_flow:\n",
        "        flow_magnitudes = [np.mean(np.abs(flow)) for flow in optical_flow]\n",
        "        flow_consistency = np.std(flow_magnitudes) / np.mean(flow_magnitudes)\n",
        "\n",
        "        if flow_consistency > 0.5:\n",
        "            anomalies.append(\"Inconsistent motion patterns detected\")\n",
        "            scores.append(1 - flow_consistency)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    confidence_level = \"High\" if avg_score > 0.8 else \"Medium\" if avg_score > 0.6 else \"Low\"\n",
        "\n",
        "    return AnalysisMetrics(\n",
        "        score=float(avg_score),\n",
        "        anomalies=anomalies,\n",
        "        confidence_level=confidence_level\n",
        "    )"
      ],
      "metadata": {
        "id": "sDmTIovRrObf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def behavioral_analysis_agent(video_data: Dict[str, Any]) -> AnalysisMetrics:\n",
        "    \"\"\"Analyze behavioral patterns and inconsistencies\"\"\"\n",
        "    frames = video_data[\"frames\"]\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Initialize pose estimation\n",
        "    pose_estimator = mp.solutions.pose.Pose(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=0.7,\n",
        "        min_tracking_confidence=0.7\n",
        "    )\n",
        "\n",
        "    # Analyze pose landmarks over time\n",
        "    pose_landmarks = []\n",
        "    for frame in frames:\n",
        "        results = pose_estimator.process(frame)\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
        "            pose_landmarks.append(landmarks)\n",
        "\n",
        "    if pose_landmarks:\n",
        "        # Analyze movement smoothness\n",
        "        movement_smoothness = analyze_movement_smoothness(pose_landmarks)\n",
        "        scores.append(movement_smoothness)\n",
        "\n",
        "        if movement_smoothness < 0.6:\n",
        "            anomalies.append(\"Unnatural body movement patterns detected\")\n",
        "\n",
        "        # Analyze joint angles\n",
        "        joint_consistency = analyze_joint_angles(pose_landmarks)\n",
        "        scores.append(joint_consistency)\n",
        "\n",
        "        if joint_consistency < 0.5:\n",
        "            anomalies.append(\"Inconsistent joint movements detected\")\n",
        "\n",
        "    # Add gaze analysis if faces are detected\n",
        "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        min_detection_confidence=0.7\n",
        "    )\n",
        "\n",
        "    gaze_patterns = analyze_gaze_patterns(frames, face_mesh)\n",
        "    if gaze_patterns:\n",
        "        scores.append(gaze_patterns)\n",
        "        if gaze_patterns < 0.6:\n",
        "            anomalies.append(\"Unnatural gaze patterns detected\")\n",
        "\n",
        "    avg_score = np.mean(scores) if scores else 0.0\n",
        "    confidence_level = \"High\" if avg_score > 0.8 else \"Medium\" if avg_score > 0.6 else \"Low\"\n",
        "\n",
        "    return AnalysisMetrics(\n",
        "        score=float(avg_score),\n",
        "        anomalies=anomalies,\n",
        "        confidence_level=confidence_level\n",
        "    )"
      ],
      "metadata": {
        "id": "zx2aut8orOeC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_movement_smoothness(pose_landmarks: List[np.ndarray]) -> float:\n",
        "    \"\"\"Analyze smoothness of movement trajectories\"\"\"\n",
        "    if len(pose_landmarks) < 3:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate velocity and acceleration\n",
        "    velocities = np.diff(pose_landmarks, axis=0)\n",
        "    accelerations = np.diff(velocities, axis=0)\n",
        "\n",
        "    # Calculate jerk (rate of change of acceleration)\n",
        "    jerk = np.diff(accelerations, axis=0)\n",
        "\n",
        "    # Normalize jerk score (lower jerk = smoother movement)\n",
        "    jerk_score = 1.0 - min(np.mean(np.abs(jerk)), 1.0)\n",
        "\n",
        "    return float(jerk_score)\n",
        "\n",
        "def analyze_joint_angles(pose_landmarks: List[np.ndarray]) -> float:\n",
        "    \"\"\"Analyze consistency of joint angles over time\"\"\"\n",
        "    if len(pose_landmarks) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # Define key joint connections\n",
        "    joint_connections = [\n",
        "        # Upper body\n",
        "        ([11, 13, 15], \"right_arm\"),  # right shoulder -> elbow -> wrist\n",
        "        ([12, 14, 16], \"left_arm\"),   # left shoulder -> elbow -> wrist\n",
        "        # Lower body\n",
        "        ([23, 25, 27], \"right_leg\"),  # right hip -> knee -> ankle\n",
        "        ([24, 26, 28], \"left_leg\")    # left hip -> knee -> ankle\n",
        "    ]\n",
        "\n",
        "    angle_scores = []\n",
        "    for joints, _ in joint_connections:\n",
        "        angles = []\n",
        "        for landmarks in pose_landmarks:\n",
        "            v1 = landmarks[joints[1]] - landmarks[joints[0]]\n",
        "            v2 = landmarks[joints[2]] - landmarks[joints[1]]\n",
        "            angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "            angles.append(angle)\n",
        "\n",
        "        # Calculate angle consistency\n",
        "        angle_consistency = 1.0 - min(np.std(angles) / np.pi, 1.0)\n",
        "        angle_scores.append(angle_consistency)\n",
        "\n",
        "    return float(np.mean(angle_scores)) if angle_scores else 0.0\n",
        "\n",
        "def analyze_gaze_patterns(\n",
        "    frames: List[np.ndarray],\n",
        "    face_mesh: mp.solutions.face_mesh.FaceMesh\n",
        ") -> float:\n",
        "    \"\"\"Analyze naturalness of gaze patterns\"\"\"\n",
        "    # Eye landmarks indices for MediaPipe Face Mesh\n",
        "    LEFT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]\n",
        "    RIGHT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]\n",
        "\n",
        "    gaze_directions = []\n",
        "    for frame in frames:\n",
        "        results = face_mesh.process(frame)\n",
        "        if results.multi_face_landmarks:\n",
        "            face_landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "            # Extract eye landmarks\n",
        "            left_eye_points = np.array([[lm.x, lm.y, lm.z] for i, lm in enumerate(face_landmarks.landmark) if i in LEFT_EYE])\n",
        "            right_eye_points = np.array([[lm.x, lm.y, lm.z] for i, lm in enumerate(face_landmarks.landmark) if i in RIGHT_EYE])\n",
        "\n",
        "            # Calculate gaze direction\n",
        "            left_gaze = calculate_gaze_direction(left_eye_points)\n",
        "            right_gaze = calculate_gaze_direction(right_eye_points)\n",
        "\n",
        "            # Check for gaze consistency between eyes\n",
        "            gaze_consistency = 1.0 - min(np.linalg.norm(left_gaze - right_gaze), 1.0)\n",
        "            gaze_directions.append(gaze_consistency)\n",
        "\n",
        "    return float(np.mean(gaze_directions)) if gaze_directions else 0.0\n",
        "\n",
        "def calculate_gaze_direction(eye_points: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculate gaze direction from eye landmarks\"\"\"\n",
        "    if len(eye_points) < 2:\n",
        "        return np.zeros(3)\n",
        "\n",
        "    # Calculate eye center\n",
        "    eye_center = np.mean(eye_points, axis=0)\n",
        "\n",
        "    # Calculate principal direction (approximate gaze direction)\n",
        "    U, S, Vt = np.linalg.svd(eye_points - eye_center)\n",
        "    gaze_direction = Vt[0]\n",
        "\n",
        "    return gaze_direction"
      ],
      "metadata": {
        "id": "Rmsey005rOhi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_analysis_agent(video_data: Dict[str, Any]) -> AnalysisMetrics:\n",
        "    \"\"\"Enhanced semantic analysis using LLM\"\"\"\n",
        "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
        "\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Analyze the following video content for potential manipulation:\n",
        "        1. Evaluate visual coherence and continuity\n",
        "        2. Assess natural behavior patterns\n",
        "        3. Check for physical inconsistencies\n",
        "        4. Analyze temporal logic\n",
        "        5. Look for contextual anomalies\n",
        "\n",
        "        Provide detailed analysis with specific anomalies found.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # Prepare context for LLM\n",
        "    context = {\n",
        "        \"frame_count\": len(video_data[\"frames\"]),\n",
        "        \"duration\": video_data[\"metadata\"][\"duration\"],\n",
        "        \"resolution\": f\"{video_data['metadata']['width']}x{video_data['metadata']['height']}\",\n",
        "        \"has_audio\": video_data[\"audio\"] is not None\n",
        "    }\n",
        "\n",
        "    parser = PydanticOutputParser(pydantic_object=AnalysisMetrics)\n",
        "    chain = analysis_prompt | llm | parser\n",
        "\n",
        "    try:\n",
        "        result = chain.invoke({\"input\": str(context)})\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Semantic analysis error: {e}\")\n",
        "        return AnalysisMetrics(\n",
        "            score=0.0,\n",
        "            anomalies=[\"Failed to perform semantic analysis\"],\n",
        "            confidence_level=\"Low\"\n",
        "        )"
      ],
      "metadata": {
        "id": "u1y5eDPrrdlH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = \"path/to/your/video.mp4\"\n",
        "    forensic_report = run_enhanced_detection(video_path, verbose=True)\n",
        "\n",
        "    # Print report in formatted JSON\n",
        "    print(\"\\nDeepfake Detection Forensic Report:\")\n",
        "    print(json.dumps(forensic_report.dict(), indent=2, default=str))"
      ],
      "metadata": {
        "id": "SLte616OrhMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}