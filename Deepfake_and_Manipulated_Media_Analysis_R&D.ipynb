{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Deepfake_and_Manipulated_Media_Analysis_R%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "TREWxWZdT5iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Packages**"
      ],
      "metadata": {
        "id": "mBe0gJFvUEOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEMrzAzRW_w4",
        "outputId": "367970b0-9608-40ca-a0f1-1fa9e382e6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.2/138.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q groq langchain langchain-community langgraph torch transformers opencv-python librosa numpy face-recognition dlib mediapipe scipy pillow tqdm pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up the Groq Client**"
      ],
      "metadata": {
        "id": "-ytDSCQDUAje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "import getpass\n",
        "\n",
        "# Prompt for the Groq API key and use it directly\n",
        "GROQ_API_KEY = getpass.getpass(\"Enter your Groq API key: \")\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq(\n",
        "    api_key=GROQ_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIxEeTxeUNJz",
        "outputId": "28f80281-70a5-4161-c6bf-3e012967972c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Models**\n",
        "- Define the data models for storing analysis results and reports:\n"
      ],
      "metadata": {
        "id": "X8RzT9-AWZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float = Field(..., description=\"Confidence score (0-1)\")\n",
        "    label: str = Field(..., description=\"Classification label\")\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float = Field(..., description=\"Model confidence\")\n",
        "    method: str = Field(..., description=\"Detection method used\")\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Dict[str, Any]]\n",
        "    metadata: Dict[str, Any]"
      ],
      "metadata": {
        "id": "MyJ0N4mSWZio"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Preprocessing**\n",
        "- Define helper functions for preprocessing audio, image, and video data:"
      ],
      "metadata": {
        "id": "0ktikzemXOd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def reduce_noise(audio_data: np.ndarray) -> np.ndarray:\n",
        "    return librosa.effects.preemphasis(audio_data)\n",
        "\n",
        "def enhance_resolution(image: np.ndarray) -> np.ndarray:\n",
        "    sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
        "    return sr.upsample(image)\n",
        "\n",
        "def deblur_image(image: np.ndarray) -> np.ndarray:\n",
        "    return cv2.filter2D(image, -1, np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]))\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:\n",
        "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
        "    mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "    return {\"mfcc\": mfcc, \"chroma\": chroma, \"mel\": mel}\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return {\"keypoints\": keypoints, \"descriptors\": descriptors}\n",
        "\n",
        "def calculate_dense_optical_flow(prev_frame: np.ndarray, curr_frame: np.ndarray) -> np.ndarray:\n",
        "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def extract_temporal_features(optical_flow: np.ndarray) -> Dict[str, Any]:\n",
        "    mag, ang = cv2.cartToPolar(optical_flow[..., 0], optical_flow[..., 1])\n",
        "    return {\"magnitude\": mag, \"angle\": ang}\n",
        "\n",
        "def estimate_noise(frame: np.ndarray) -> float:\n",
        "    return np.mean(cv2.Laplacian(frame, cv2.CV_64F).var())\n",
        "\n",
        "def calculate_contrast(frame: np.ndarray) -> float:\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    return gray.std()\n",
        "\n",
        "def detect_compression_artifacts(frame: np.ndarray) -> float:\n",
        "    dct = cv2.dct(np.float32(frame) / 255.0)\n",
        "    return np.mean(np.abs(dct))"
      ],
      "metadata": {
        "id": "9A9KPnzU36TX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Preprocessing Functions**\n",
        "- Define functions for preprocessing audio, image, and video data"
      ],
      "metadata": {
        "id": "13iwCPIrX-Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import base64\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "async def enhanced_preprocessing(file_path: str) -> Dict[str, Any]:\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension in [\".mp3\", \".wav\", \".flac\"]:\n",
        "        return await enhanced_audio_preprocessing(file_path)\n",
        "    elif file_extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
        "        return await enhanced_image_preprocessing(file_path)\n",
        "    elif file_extension in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "        return await enhanced_video_preprocessing(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "async def enhanced_audio_preprocessing(audio_path: str) -> Dict[str, Any]:\n",
        "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
        "    audio_data = reduce_noise(audio_data)\n",
        "    audio_features = extract_audio_features(audio_data, sample_rate)\n",
        "    return {\"audio\": audio_data, \"sample_rate\": sample_rate, \"audio_features\": audio_features}\n",
        "\n",
        "async def enhanced_image_preprocessing(image_path: str) -> Dict[str, Any]:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = enhance_resolution(image)\n",
        "    image = deblur_image(image)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_features = extract_image_features(image_rgb)\n",
        "    return {\"image\": image_rgb, \"image_features\": image_features}\n",
        "\n",
        "async def enhanced_video_preprocessing(video_path: str) -> Dict[str, Any]:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
        "        \"frame_count\": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        \"height\": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "        \"duration\": float(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / float(cap.get(cv2.CAP_PROP_FPS)),\n",
        "        \"codec\": int(cap.get(cv2.CAP_PROP_FOURCC)).to_bytes(4, byteorder='little').decode(),\n",
        "        \"file_size\": os.path.getsize(video_path)\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "    cap.release()\n",
        "\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is not None:\n",
        "            audio_array = audio.to_soundarray()\n",
        "            audio_data = extract_audio_features(audio_array, audio.fps)\n",
        "        video.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Audio extraction error: {e}\")\n",
        "        audio_data = None\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }"
      ],
      "metadata": {
        "id": "ltWcOSlP36Ww"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analysis**\n",
        "- functions for analyzing transcription, vision response, and other features"
      ],
      "metadata": {
        "id": "GExLYgMaYVBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_transcription(transcription: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for repetitive phrases\n",
        "    if \"repeated phrase\" in transcription.lower():\n",
        "        anomalies.append(\"Repetitive phrases detected\")\n",
        "    # Example: Check for inconsistent timestamps\n",
        "    if \"inconsistent timestamp\" in transcription.lower():\n",
        "        anomalies.append(\"Inconsistent timestamps detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_vision_response(response: str) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for blurry regions\n",
        "    if \"blurry region\" in response.lower():\n",
        "        anomalies.append(\"Blurry regions detected\")\n",
        "    # Example: Check for unnatural shadows\n",
        "    if \"unnatural shadow\" in response.lower():\n",
        "        anomalies.append(\"Unnatural shadows detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_av_sync(frames: List[np.ndarray], audio: np.ndarray) -> float:\n",
        "    # Example: Check for lip-sync consistency\n",
        "    sync_score = 0.9  # Placeholder score\n",
        "    return sync_score\n",
        "\n",
        "def analyze_temporal_features(temporal_features: List[Dict[str, Any]]) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for abrupt changes in motion\n",
        "    for feature in temporal_features:\n",
        "        if feature[\"magnitude\"].max() > 1.0:\n",
        "            anomalies.append(\"Abrupt changes in motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_optical_flow(optical_flow: List[np.ndarray]) -> List[str]:\n",
        "    anomalies = []\n",
        "    # Example: Check for inconsistent flow patterns\n",
        "    for flow in optical_flow:\n",
        "        if flow.max() > 1.0:\n",
        "            anomalies.append(\"Inconsistent flow patterns detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_biometric_consistency(frames: List[np.ndarray], models: Dict[str, Any]) -> float:\n",
        "    # Example: Check for consistent facial landmarks\n",
        "    consistency_score = 0.9  # Placeholder score\n",
        "    return consistency_score"
      ],
      "metadata": {
        "id": "YkN1p_l2u-KL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Analysis Functions**\n",
        "- Define functions for analyzing audio, image, and video data using Groq models"
      ],
      "metadata": {
        "id": "05aBMDMgY5CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_audio_analysis(audio_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    if audio_data is None:\n",
        "        return DeepfakeAnalysisResult(\n",
        "            score=0.0,\n",
        "            label=\"NO_AUDIO\",\n",
        "            confidence=0.0,\n",
        "            method=\"audio_analysis\",\n",
        "            anomalies=[\"No audio data available\"]\n",
        "        )\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Use Groq's speech-to-text model\n",
        "    response = client.audio.transcriptions.create(\n",
        "        file=audio_data[\"waveform\"],\n",
        "        model=\"whisper-large-v3-turbo\",\n",
        "    )\n",
        "    transcription = response.text\n",
        "\n",
        "    # Analyze the transcription for anomalies\n",
        "    anomalies.extend(analyze_transcription(transcription))\n",
        "    scores.append(0.9)  # Placeholder score, adjust based on your analysis\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"audio_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )\n",
        "\n",
        "async def advanced_image_analysis(image: np.ndarray, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Use Groq's vision model\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze this image for signs of digital manipulation or inconsistencies.\",\n",
        "                \"image_url\": \"data:image/png;base64,\" + base64.b64encode(cv2.imencode('.png', image)[1]).decode()\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.2-90b-vision-preview\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        scores.append(float(score_match.group(1)))\n",
        "    anomalies.extend(analyze_vision_response(content))\n",
        "\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"image_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )\n",
        "\n",
        "async def advanced_video_analysis(video_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "    videomae_model = models[\"videomae\"].to(device)\n",
        "    videomae_processor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
        "    inputs = videomae_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        videomae_output = videomae_model(**inputs)\n",
        "        videomae_score = torch.softmax(videomae_output.logits, dim=-1)\n",
        "        scores.append(videomae_score.max().item())\n",
        "    timesformer_model = models[\"timesformer\"].to(device)\n",
        "    timesformer_processor = VideoMAEFeatureExtractor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "    inputs = timesformer_processor(video_data[\"frames\"], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        timesformer_output = timesformer_model(**inputs)\n",
        "        timesformer_score = torch.softmax(timesformer_output.logits, dim=-1)\n",
        "        scores.append(timesformer_score.max().item())\n",
        "    llava_model = models[\"llava\"].to(device)\n",
        "    llava_processor = models[\"processors\"][\"llava\"]\n",
        "    prompt = \"Analyze this video for signs of digital manipulation or inconsistencies.\"\n",
        "    inputs = llava_processor(video=video_data[\"frames\"], text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs)\n",
        "        analysis = llava_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        llava_score = analyze_llava_response(analysis)\n",
        "        scores.append(llava_score)\n",
        "    temporal_anomalies = analyze_temporal_features(video_data[\"temporal_features\"])\n",
        "    anomalies.extend(temporal_anomalies)\n",
        "    optical_flow_anomalies = analyze_optical_flow(video_data[\"optical_flow\"])\n",
        "    anomalies.extend(optical_flow_anomalies)\n",
        "    final_score = np.mean(scores)\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"video_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "GDaz8dMlu-No"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tool Creation Functions**\n",
        "- Define functions for creating deepfake detection tools and agents"
      ],
      "metadata": {
        "id": "HoftOwSUZPye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "\n",
        "def create_deepfake_detection_tools(models: Dict[str, Any], device: torch.device) -> List[Tool]:\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"analyze_video\",\n",
        "            func=lambda x: advanced_video_analysis(x, models, device),\n",
        "            description=\"Analyzes video content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_audio\",\n",
        "            func=lambda x: advanced_audio_analysis(x, models, device),\n",
        "            description=\"Analyzes audio content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"analyze_image\",\n",
        "            func=lambda x: advanced_image_analysis(x, models, device),\n",
        "            description=\"Analyzes image content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"semantic_analysis\",\n",
        "            func=lambda x: semantic_consistency_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes semantic consistency across modalities\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"face_forgery_detection\",\n",
        "            func=lambda x: face_forgery_detection(x, models, device),\n",
        "            description=\"Detects face forgeries in video content\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"lip_sync_detection\",\n",
        "            func=lambda x: lip_sync_detection(x, models, device),\n",
        "            description=\"Analyzes lip-sync consistency between audio and video\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"background_consistency\",\n",
        "            func=lambda x: background_consistency_analysis(x, models, device),\n",
        "            description=\"Analyzes background consistency across frames\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"real_time_streaming_analysis\",\n",
        "            func=lambda x: real_time_streaming_analysis(x, models, device),\n",
        "            description=\"Analyzes live video streams for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"text_analysis\",\n",
        "            func=lambda x: text_analysis(x, models[\"llms\"]),\n",
        "            description=\"Analyzes text content for signs of manipulation\"\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"metadata_analysis\",\n",
        "            func=lambda x: metadata_analysis(x),\n",
        "            description=\"Analyzes metadata for signs of manipulation\"\n",
        "        )\n",
        "    ]\n",
        "    return tools\n",
        "\n",
        "def create_detection_agent(tools: List[Tool], llm: ChatOpenAI) -> AgentExecutor:\n",
        "    class DeepfakeDetectionOutputParser:\n",
        "        def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "            try:\n",
        "                if \"Final Answer:\" in llm_output:\n",
        "                    return AgentFinish(\n",
        "                        return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                        log=llm_output,\n",
        "                    )\n",
        "                action_match = re.search(r\"Action: (.*?)\\nAction Input: (.*)\", llm_output, re.DOTALL)\n",
        "                if not action_match:\n",
        "                    raise ValueError(\"Could not parse LLM output: \" + llm_output)\n",
        "                action = action_match.group(1).strip()\n",
        "                action_input = action_match.group(2).strip()\n",
        "                return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not parse LLM output: {llm_output}\") from e\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"\n",
        "            You are an expert deepfake detection system. Your goal is to analyze content across multiple modalities\n",
        "            to determine authenticity. Consider all available evidence and patterns including:\n",
        "\n",
        "            1. Visual elements: inconsistencies, artifacts, unnatural patterns\n",
        "            2. Audio characteristics: synthetic artifacts, unnatural transitions\n",
        "            3. Semantic coherence: logical consistency across modalities\n",
        "            4. Temporal patterns: synchronization, continuity\n",
        "            5. Biometric features: facial landmarks, expressions, movements\n",
        "\n",
        "            Available tools:\n",
        "            {tools}\n",
        "\n",
        "            Process:\n",
        "            1. Analyze the input using appropriate tools\n",
        "            2. Evaluate evidence across modalities\n",
        "            3. Make a final determination on authenticity\n",
        "\n",
        "            Format your response as:\n",
        "            Action: [tool name]\n",
        "            Action Input: [tool input]\n",
        "            Observation: [result]\n",
        "            ... (repeat for additional tools as needed)\n",
        "            Final Answer: [detailed analysis and verdict]\n",
        "            \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "    ])\n",
        "\n",
        "    return AgentExecutor.from_agent_and_tools(\n",
        "        agent=LLMSingleActionAgent(\n",
        "            llm_chain=LLMChain(llm=llm, prompt=prompt),\n",
        "            output_parser=DeepfakeDetectionOutputParser(),\n",
        "            stop=[\"Observation:\", \"Final Answer:\"],\n",
        "            allowed_tools=[tool.name for tool in tools]\n",
        "        ),\n",
        "        tools=tools,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "PEWzNSs3ZGK8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Detection Graph**\n",
        "- Define the detection graph for processing the input data"
      ],
      "metadata": {
        "id": "W9ZBdOffZosP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_detection_graph() -> StateGraph:\n",
        "    def preprocess(state):\n",
        "        input_data = state[\"input\"]\n",
        "        processed_data = asyncio.run(enhanced_preprocessing(input_data))\n",
        "        return {**state, \"processed_data\": processed_data}\n",
        "\n",
        "    def analyze_modalities(state):\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        device = state[\"device\"]\n",
        "        results = {\n",
        "            \"video\": asyncio.run(advanced_video_analysis(processed_data, models, device)) if \"frames\" in processed_data else None,\n",
        "            \"audio\": asyncio.run(advanced_audio_analysis(processed_data.get(\"audio\"), models, device)) if \"audio\" in processed_data else None,\n",
        "            \"image\": asyncio.run(advanced_image_analysis(processed_data[\"image\"], models, device)) if \"image\" in processed_data else None,\n",
        "            \"text\": asyncio.run(text_analysis(processed_data.get(\"text\"), models[\"llms\"])) if \"text\" in processed_data else None\n",
        "        }\n",
        "        return {**state, \"modality_results\": results}\n",
        "\n",
        "    def cross_modal_analysis(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models = state[\"models\"]\n",
        "        cross_modal_score = analyze_cross_modal_consistency(results, processed_data, models)\n",
        "        return {**state, \"cross_modal_score\": cross_modal_score}\n",
        "\n",
        "    def generate_report(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        cross_modal_score = state[\"cross_modal_score\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        report = generate_comprehensive_report(results, cross_modal_score, processed_data)\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    workflow = StateGraph(nodes=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"analyze_modalities\", analyze_modalities),\n",
        "        (\"cross_modal_analysis\", cross_modal_analysis),\n",
        "        (\"generate_report\", generate_report)\n",
        "    ])\n",
        "    workflow.add_edge(\"preprocess\", \"analyze_modalities\")\n",
        "    workflow.add_edge(\"analyze_modalities\", \"cross_modal_analysis\")\n",
        "    workflow.add_edge(\"cross_modal_analysis\", \"generate_report\")\n",
        "    workflow.add_edge(\"generate_report\", END)\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "2_x9xL7gZZs-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Analyzing Cross-Modal Consistency and Generating Reports**"
      ],
      "metadata": {
        "id": "EF3CZ3zAaAPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_cross_modal_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    models: Dict[str, Any]\n",
        ") -> float:\n",
        "    scores = []\n",
        "    if results[\"audio\"] and results[\"video\"]:\n",
        "        sync_score = analyze_av_sync(processed_data[\"frames\"], processed_data[\"audio\"])\n",
        "        scores.append(sync_score)\n",
        "    semantic_score = analyze_semantic_consistency(results, processed_data, models[\"llms\"])\n",
        "    scores.append(semantic_score)\n",
        "    temporal_score = analyze_temporal_coherence(processed_data[\"temporal_features\"])\n",
        "    scores.append(temporal_score)\n",
        "    bio_score = analyze_biometric_consistency(processed_data[\"frames\"], models)\n",
        "    scores.append(bio_score)\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "def analyze_semantic_consistency(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    processed_data: Dict[str, Any],\n",
        "    llms: Dict[str, ChatOpenAI]\n",
        ") -> float:\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Analyze the consistency between different modalities in the content:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Consider:\n",
        "        1. Do the modalities tell a coherent story?\n",
        "        2. Are there logical contradictions?\n",
        "        3. Do temporal patterns align?\n",
        "        4. Is the emotional content consistent?\n",
        "\n",
        "        Rate the consistency from 0 to 1, where 1 is perfectly consistent.\n",
        "        Provide detailed reasoning for your rating.\n",
        "\n",
        "        Output format:\n",
        "        Score: [0-1]\n",
        "        Reasoning: [detailed explanation]\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llms[\"gpt4\"], prompt=prompt)\n",
        "    response = chain.run({\n",
        "        \"video_analysis\": results[\"video\"].dict() if results[\"video\"] else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results[\"audio\"] else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results[\"image\"] else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results[\"text\"] else \"N/A\"\n",
        "    })\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    if score_match:\n",
        "        return float(score_match.group(1))\n",
        "    return 0.5\n",
        "\n",
        "def generate_comprehensive_report(\n",
        "    results: Dict[str, DeepfakeAnalysisResult],\n",
        "    cross_modal_score: float,\n",
        "    processed_data: Dict[str, Any]\n",
        ") -> MultimodalAnalysisReport:\n",
        "    scores = [\n",
        "        results[\"video\"].score if results[\"video\"] else 0.5,\n",
        "        results[\"audio\"].score if results[\"audio\"] else 0.5,\n",
        "        results[\"image\"].score if results[\"image\"] else 0.5,\n",
        "        results[\"text\"].score if results[\"text\"] else 0.5,\n",
        "        cross_modal_score\n",
        "    ]\n",
        "    weights = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "    final_score = sum(s * w for s, w in zip(scores, weights))\n",
        "    evidence = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            evidence.extend([\n",
        "                {\n",
        "                    \"type\": modality,\n",
        "                    \"description\": anomaly,\n",
        "                    \"confidence\": result.confidence,\n",
        "                    \"method\": result.method\n",
        "                }\n",
        "                for anomaly in result.anomalies\n",
        "            ])\n",
        "    return MultimodalAnalysisReport(\n",
        "        case_id=f\"DFD-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        file_info=processed_data[\"metadata\"] if \"metadata\" in processed_data else {},\n",
        "        video_analysis=results[\"video\"],\n",
        "        audio_analysis=results[\"audio\"],\n",
        "        image_analysis=results[\"image\"],\n",
        "        text_analysis=results[\"text\"],\n",
        "        multimodal_score=float(final_score),\n",
        "        verdict=\"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\",\n",
        "        evidence=evidence,\n",
        "        metadata={\n",
        "            \"processing_time\": datetime.now().isoformat(),\n",
        "            \"models_used\": list(results.keys()),\n",
        "            \"cross_modal_score\": cross_modal_score,\n",
        "            \"confidence_distribution\": {\n",
        "                modality: result.confidence\n",
        "                for modality, result in results.items() if result\n",
        "            }\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "H0mTuH2hZ5de"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Real-Time Streaming Analysis**\n",
        "- Define a function for real-time streaming analysis"
      ],
      "metadata": {
        "id": "Ov48BT01aOO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def real_time_streaming_analysis(video_stream: Any, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": 30,  # Assuming 30 FPS for real-time streaming\n",
        "        \"frame_count\": 0,\n",
        "        \"width\": 0,\n",
        "        \"height\": 0,\n",
        "        \"duration\": 0,\n",
        "        \"codec\": \"N/A\",\n",
        "        \"file_size\": 0\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while True:\n",
        "        frame = await video_stream.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow, temp_features = await asyncio.gather(\n",
        "                process_optical_flow(prev_frame, frame),\n",
        "                process_temporal_features(flow)\n",
        "            )\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "    video_data = {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }\n",
        "\n",
        "    return await advanced_video_analysis(video_data, models, device)"
      ],
      "metadata": {
        "id": "e6-q4yeeaKEU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Text Analysis**\n",
        "- Define a function for text analysis using Groq models:"
      ],
      "metadata": {
        "id": "hHpkFFGDaqtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def text_analysis(text: str, llms: Dict[str, ChatOpenAI]) -> DeepfakeAnalysisResult:\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze the following text for signs of manipulation or inconsistencies:\\n\\n{text}\"\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", content)\n",
        "    if score_match:\n",
        "        score = float(score_match.group(1))\n",
        "    else:\n",
        "        score = 0.5\n",
        "    anomalies = [line.strip() for line in content.split(\"Reasoning:\")[-1].strip().split(\"\\n\") if line.strip()]\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=score,\n",
        "        label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"text_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "UpL42dfZaRF7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Metadata Analysis**\n",
        "- Define a function for metadata analysis"
      ],
      "metadata": {
        "id": "8WGV0gpibBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_analysis(metadata: Dict[str, Any]) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "\n",
        "    # Example metadata analysis\n",
        "    if metadata.get(\"fps\") < 10:\n",
        "        anomalies.append(\"Unusually low frame rate\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"duration\") < 1:\n",
        "        anomalies.append(\"Unusually short duration\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"file_size\") < 100000:\n",
        "        anomalies.append(\"Unusually small file size\")\n",
        "        scores.append(0.2)\n",
        "\n",
        "    final_score = np.mean(scores) if scores else 0.5\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)) if scores else 0.0,\n",
        "        method=\"metadata_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "lr_VZJS_aRIf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Environment Setup**\n",
        "- Define the function to set up the enhanced detection environment"
      ],
      "metadata": {
        "id": "OM8ZJ7S8bwks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_enhanced_detection_environment():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    models = {\n",
        "        \"videomae\": VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\"),\n",
        "        \"timesformer\": TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\"),\n",
        "        \"llava\": LlavaForConditionalGeneration.from_pretrained(\"llava/llava-v1\"),\n",
        "        \"clip\": CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "        \"instructblip\": InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "        \"vit_gpt2\": VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\"),\n",
        "        \"efficientnet\": EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\"),\n",
        "        \"wav2vec2\": Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-large-960h\"),\n",
        "        \"whisper\": WhisperForAudioClassification.from_pretrained(\"openai/whisper-large-v3\"),\n",
        "        \"hubert\": HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\"),\n",
        "        \"llama3\": LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-30b\"),\n",
        "        \"llama3_1\": LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-33b\"),\n",
        "        \"roberta_fake\": AutoModelForSequenceClassification.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "        \"blenderbot\": BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\"),\n",
        "        \"bert\": BertForSequenceClassification.from_pretrained(\"bert-base-uncased\"),\n",
        "        \"t5\": T5ForConditionalGeneration.from_pretrained(\"t5-base\"),\n",
        "        \"face_detector\": mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.7),\n",
        "        \"face_mesh\": mp.solutions.face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            min_detection_confidence=0.7\n",
        "        ),\n",
        "        \"processors\": {\n",
        "            \"clip\": CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
        "            \"instructblip\": InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"),\n",
        "            \"vit\": ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\"),\n",
        "            \"wav2vec2\": Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\"),\n",
        "            \"whisper\": WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\"),\n",
        "            \"roberta\": AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\"),\n",
        "            \"blenderbot\": BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\"),\n",
        "            \"bert\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "            \"t5\": T5Tokenizer.from_pretrained(\"t5-base\"),\n",
        "            \"efficientnet\": AutoImageProcessor.from_pretrained(\"google/efficientnet-b7\"),\n",
        "            \"hubert\": Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\"),\n",
        "            \"llava\": LlavaProcessor.from_pretrained(\"llava/llava-v1\"),\n",
        "            \"llama3\": LlamaTokenizer.from_pretrained(\"meta-llama/Llama-30b\"),\n",
        "            \"llama3_1\": LlamaTokenizer.from_pretrained(\"meta-llama/Llama-33b\")\n",
        "        },\n",
        "        \"llms\": {\n",
        "            \"gpt4\": ChatOpenAI(model=\"gpt-4\", temperature=0.2),\n",
        "            \"claude\": ChatOpenAI(model=\"claude-3-opus\", temperature=0.2)\n",
        "        }\n",
        "    }\n",
        "    return {\"device\": device, \"models\": models}"
      ],
      "metadata": {
        "id": "1zl-XfTIcdou"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Main Function**\n",
        "- Define the main function to run the deepfake detection system"
      ],
      "metadata": {
        "id": "iZRCoZgobKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_deepfake_detection(file_path: str, mode: str = \"all\") -> MultimodalAnalysisReport:\n",
        "    try:\n",
        "        env = setup_enhanced_detection_environment()\n",
        "        tools = create_deepfake_detection_tools(env[\"models\"], env[\"device\"])\n",
        "        agent = create_detection_agent(tools, env[\"models\"][\"llms\"][\"gpt4\"])\n",
        "        workflow = create_detection_graph()\n",
        "        initial_state = {\n",
        "            \"input\": file_path,\n",
        "            \"mode\": mode,\n",
        "            \"models\": env[\"models\"],\n",
        "            \"device\": env[\"device\"]\n",
        "        }\n",
        "        final_state = workflow.run(initial_state)\n",
        "        return final_state[\"final_report\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in deepfake detection: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"actual/path/to/your/content.mp4\"\n",
        "    report = asyncio.run(run_deepfake_detection(file_path))\n",
        "    print(\"\\nDeepfake Detection Report:\")\n",
        "    print(json.dumps(report.dict(), indent=2, default=str))"
      ],
      "metadata": {
        "id": "Xseky1jpaRK3",
        "outputId": "f71685a3-ca7f-482c-b44c-3b4d232a3b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in deepfake detection: name 'VideoMAEForVideoClassification' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'VideoMAEForVideoClassification' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c07fafecf925>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"actual/path/to/your/content.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_deepfake_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDeepfake Detection Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c07fafecf925>\u001b[0m in \u001b[0;36mrun_deepfake_detection\u001b[0;34m(file_path, mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_deepfake_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMultimodalAnalysisReport\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_enhanced_detection_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_deepfake_detection_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_detection_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gpt4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-6058bddeb3ee>\u001b[0m in \u001b[0;36msetup_enhanced_detection_environment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     models = {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m\"videomae\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVideoMAEForVideoClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MCG-NJU/videomae-base-finetuned-kinetics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"timesformer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimesformerForVideoClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/timesformer-base-finetuned-k400\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m\"llava\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlavaForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llava/llava-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VideoMAEForVideoClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yEbjYmL9aROX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}