{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/KG_Enhanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "#### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "#### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "#### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "#### Features Required\n",
        "\n",
        "##### User Interface (UI)\n",
        "- Chat UI with file upload capabilities.\n",
        "- Options to save and select prompts.\n",
        "- Configuration settings for connectors with enable/disable toggles.\n",
        "- Interface for configuring knowledge and variables (similar to Dify.ai).\n",
        "\n",
        "##### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "#### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "#### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "jBzZYldlbmJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation and Setup"
      ],
      "metadata": {
        "id": "kv12uPN1u42I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCq8zzvwSSdD",
        "outputId": "98518ca3-c612-4ab9-8b4d-3e5e0d2414d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -yq torch torchvision pandas\n",
        "!pip install -q torch==2.3.1 torchvision==0.18.1 pandas==2.0.3\n",
        "!pip install -qU langchain langchain-community faiss-cpu kuzu pyvis\n",
        "!pip install -qU sentence-transformers networkx pydantic\n",
        "!pip install -qU langchain-groq apify_client langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "iNhkT7HTvNu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "from apify_client import ApifyClient\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "wXnNIpzdS6jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding and LLM Initialization**"
      ],
      "metadata": {
        "id": "WCBIUmKBRFx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "HTY5wy9JRAJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Knowledge Graph Implementation**"
      ],
      "metadata": {
        "id": "GJLcPIqpROzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()\n",
        "\n",
        "    def add_entity(self, entity: str, entity_type: str):\n",
        "        self.graph.add_node(entity, type=entity_type)\n",
        "\n",
        "    def add_relation(self, entity1: str, entity2: str, relation: str):\n",
        "        self.graph.add_edge(entity1, entity2, relation=relation)\n",
        "\n",
        "    def get_related_entities(self, entity: str) -> List[Dict[str, str]]:\n",
        "        related = []\n",
        "        for neighbor in self.graph.neighbors(entity):\n",
        "            edge_data = self.graph.get_edge_data(entity, neighbor)\n",
        "            related.append({\n",
        "                \"entity\": neighbor,\n",
        "                \"relation\": edge_data[\"relation\"]\n",
        "            })\n",
        "        return related\n",
        "\n",
        "    def visualize(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        net = Network(notebook=True, width=\"100%\", height=\"500px\")\n",
        "        for node, node_data in self.graph.nodes(data=True):\n",
        "            net.add_node(node, label=node, title=f\"Type: {node_data['type']}\")\n",
        "        for edge in self.graph.edges(data=True):\n",
        "            net.add_edge(edge[0], edge[1], title=edge[2]['relation'])\n",
        "        net.show(output_file)\n",
        "\n",
        "# Initialize knowledge graph\n",
        "kg = KnowledgeGraph()"
      ],
      "metadata": {
        "id": "D81Iru-MRALk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection Functions**"
      ],
      "metadata": {
        "id": "v786AsBfRaUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apify_client = ApifyClient(\"YOUR_APIFY_API_TOKEN\")\n",
        "\n",
        "def scrape_websites(urls: List[str]) -> List[str]:\n",
        "    \"\"\"Scrape content from given websites using Apify.\"\"\"\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    run_input = {\n",
        "        \"startUrls\": [{\"url\": url} for url in urls],\n",
        "        \"maxCrawlPages\": 10,\n",
        "        \"maxCrawlDepth\": 1,\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        scraped_content = [item.get('text', '') for item in dataset_items if 'text' in item]\n",
        "        logger.info(f\"Successfully scraped {len(scraped_content)} pages.\")\n",
        "        return scraped_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping websites: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_scraped_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch tweets related to cybersecurity using Apify.\"\"\"\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"queries\": [query],\n",
        "        \"maxTweets\": max_tweets\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Cybersecurity-specific websites\n",
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://www.virustotal.com/gui/home/upload\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "]\n",
        "\n",
        "# Scrape websites\n",
        "scraped_content = scrape_websites(websites)\n",
        "\n",
        "# Fetch tweets\n",
        "tweets = fetch_scraped_tweets(\"#cybersecurity\")\n",
        "tweet_content = [tweet.get('full_text', '') for tweet in tweets]\n",
        "\n",
        "# Combine scraped content and tweets\n",
        "all_content = scraped_content + tweet_content"
      ],
      "metadata": {
        "id": "7Y7wleoORAOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Store and Retriever Setup Functions**"
      ],
      "metadata": {
        "id": "fEk4-XT2RoaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vectorstore(texts: List[str]) -> FAISS:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    documents = text_splitter.create_documents(texts)\n",
        "    return FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "def setup_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
        "    base_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "    compressor = LLMChainExtractor.from_llm(llm)\n",
        "    return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)\n",
        "\n",
        "# Create vector store and retriever\n",
        "vectorstore = create_vectorstore(all_content)\n",
        "retriever = setup_retriever(vectorstore)"
      ],
      "metadata": {
        "id": "AQu4_gqtRARA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pydantic Models for Structured Output**"
      ],
      "metadata": {
        "id": "UC43ZFtcR8NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreatAnalysis(BaseModel):\n",
        "    threat_type: str = Field(description=\"Type of cybersecurity threat\")\n",
        "    severity: str = Field(description=\"Severity level of the threat (Low, Medium, High, Critical)\")\n",
        "    description: str = Field(description=\"Brief description of the threat\")\n",
        "    potential_impact: str = Field(description=\"Potential impact on organizations\")\n",
        "    mitigation_steps: List[str] = Field(description=\"List of steps to mitigate the threat\")\n",
        "\n",
        "class VulnerabilityAssessment(BaseModel):\n",
        "    vulnerability_name: str = Field(description=\"Name or identifier of the vulnerability\")\n",
        "    affected_systems: List[str] = Field(description=\"List of affected systems or software\")\n",
        "    cvss_score: float = Field(description=\"CVSS score of the vulnerability\")\n",
        "    description: str = Field(description=\"Brief description of the vulnerability\")\n",
        "    remediation_steps: List[str] = Field(description=\"List of steps to remediate the vulnerability\")\n",
        "\n",
        "class SecurityRecommendation(BaseModel):\n",
        "    recommendation: str = Field(description=\"Security recommendation\")\n",
        "    priority: str = Field(description=\"Priority level (Low, Medium, High)\")\n",
        "    implementation_difficulty: str = Field(description=\"Difficulty of implementation (Easy, Moderate, Complex)\")\n",
        "    expected_impact: str = Field(description=\"Expected impact of implementing the recommendation\")"
      ],
      "metadata": {
        "id": "L2ovbk6CRATr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specialized Agent Tools**"
      ],
      "metadata": {
        "id": "I3swfJE0SQGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreatAnalyzerTool(BaseTool):\n",
        "    name = \"Threat Analyzer\"\n",
        "    description = \"Analyzes cybersecurity threats and provides detailed information\"\n",
        "\n",
        "    def _run(self, query: str) -> ThreatAnalysis:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a cybersecurity threat analyst. Provide a detailed analysis of the given threat.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=ThreatAnalysis))\n",
        "        return chain.run(query)\n",
        "\n",
        "class VulnerabilityAssessorTool(BaseTool):\n",
        "    name = \"Vulnerability Assessor\"\n",
        "    description = \"Assesses cybersecurity vulnerabilities and provides detailed information\"\n",
        "\n",
        "    def _run(self, query: str) -> VulnerabilityAssessment:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a vulnerability assessment specialist. Provide a detailed assessment of the given vulnerability.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=VulnerabilityAssessment))\n",
        "        return chain.run(query)\n",
        "\n",
        "class SecurityAdvisorTool(BaseTool):\n",
        "    name = \"Security Advisor\"\n",
        "    description = \"Provides security recommendations based on current threats and vulnerabilities\"\n",
        "\n",
        "    def _run(self, query: str) -> SecurityRecommendation:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a cybersecurity advisor. Provide a detailed security recommendation based on the given context.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=SecurityRecommendation))\n",
        "        return chain.run(query)\n",
        "\n",
        "class KnowledgeGraphQueryTool(BaseTool):\n",
        "    name = \"Knowledge Graph Query\"\n",
        "    description = \"Queries the knowledge graph for related information\"\n",
        "\n",
        "    def __init__(self, kg: KnowledgeGraph):\n",
        "        super().__init__()\n",
        "        self.kg = kg\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        entities = extract_entities(query)\n",
        "        results = []\n",
        "        for entity in entities:\n",
        "            related = self.kg.get_related_entities(entity)\n",
        "            results.extend([f\"{entity} is related to {r['entity']} via {r['relation']}\" for r in related])\n",
        "        return \"\\n\".join(results)"
      ],
      "metadata": {
        "id": "vMHBjtlRRAWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "zdf5NW13Scjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text: str) -> List[str]:\n",
        "    # This is a placeholder. In a real-world scenario, you'd use a named entity recognition model.\n",
        "    return [word.strip() for word in text.split() if len(word) > 5]\n",
        "\n",
        "def update_knowledge_graph(kg: KnowledgeGraph, text: str):\n",
        "    entities = extract_entities(text)\n",
        "    for i, entity in enumerate(entities):\n",
        "        kg.add_entity(entity, \"Concept\")\n",
        "        if i > 0:\n",
        "            kg.add_relation(entities[i-1], entity, \"related_to\")\n",
        "\n",
        "# Update knowledge graph with initial content\n",
        "for text in all_content:\n",
        "    update_knowledge_graph(kg, text)"
      ],
      "metadata": {
        "id": "L8KC6yBlRAYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph Nodes**"
      ],
      "metadata": {
        "id": "b7UosmCJSlXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import AgentState\n",
        "\n",
        "def retriever_node(state: AgentState, query: str) -> AgentState:\n",
        "    relevant_docs = state[\"retriever\"].get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Context: {context}\\n\\nQuery: {query}\"))\n",
        "    return state\n",
        "\n",
        "def knowledge_graph_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    kg_tool = KnowledgeGraphQueryTool(state[\"kg\"])\n",
        "    kg_info = kg_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Knowledge Graph Information:\\n{kg_info}\"))\n",
        "    return state\n",
        "\n",
        "def threat_analysis_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    threat_tool = ThreatAnalyzerTool()\n",
        "    analysis = threat_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Threat Analysis:\\n{analysis.json()}\"))\n",
        "    return state\n",
        "\n",
        "def vulnerability_assessment_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    vuln_tool = VulnerabilityAssessorTool()\n",
        "    assessment = vuln_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Vulnerability Assessment:\\n{assessment.json()}\"))\n",
        "    return state\n",
        "\n",
        "def security_recommendation_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    sec_tool = SecurityAdvisorTool()\n",
        "    recommendation = sec_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Security Recommendation:\\n{recommendation.json()}\"))\n",
        "    return state\n",
        "\n",
        "def agent_node(state: AgentState) -> AgentState:\n",
        "    tools = [\n",
        "        ThreatAnalyzerTool(),\n",
        "        VulnerabilityAssessorTool(),\n",
        "        SecurityAdvisorTool(),\n",
        "        KnowledgeGraphQueryTool(state[\"kg\"])\n",
        "    ]\n",
        "\n",
        "    agent = create_openai_functions_agent(llm, tools, \"\"\"You are a cybersecurity expert assistant.\n",
        "    Analyze the given information and provide a comprehensive response to the query.\"\"\")\n",
        "\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    response = agent_executor.invoke({\"input\": state[\"messages\"][-1].content})\n",
        "    state[\"messages\"].append(HumanMessage(content=response[\"output\"]))\n",
        "    return state\n",
        "\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    last_message = state[\"messages\"][-1].content\n",
        "    if \"FINAL RESPONSE:\" in last_message:\n",
        "        return \"end\"\n",
        "    return \"continue\""
      ],
      "metadata": {
        "id": "cLLJOFOTSXW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Workflow**"
      ],
      "metadata": {
        "id": "8G7vlt1zSuXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes\n",
        "workflow.add_node(\"retriever\", retriever_node)\n",
        "workflow.add_node(\"knowledge_graph\", knowledge_graph_node)\n",
        "workflow.add_node(\"threat_analysis\", threat_analysis_node)\n",
        "workflow.add_node(\"vulnerability_assessment\", vulnerability_assessment_node)\n",
        "workflow.add_node(\"security_recommendation\", security_recommendation_node)\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(\"retriever\", \"knowledge_graph\")\n",
        "workflow.add_edge(\"knowledge_graph\", \"threat_analysis\")\n",
        "workflow.add_edge(\"threat_analysis\", \"vulnerability_assessment\")\n",
        "workflow.add_edge(\"vulnerability_assessment\", \"security_recommendation\")\n",
        "workflow.add_edge(\"security_recommendation\", \"agent\")\n",
        "\n",
        "# Add conditional edges\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"retriever\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"retriever\")\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "EFnguZ51SXZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Agent State and Run Workflow**"
      ],
      "metadata": {
        "id": "OhbQtevES4T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agent state\n",
        "initial_state = AgentState(\n",
        "    messages=[HumanMessage(content=\"What are the latest cybersecurity threats and vulnerabilities?\")],\n",
        "    kg=kg,\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Run the workflow\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "# Print the final response\n",
        "print(\"Final Response:\")\n",
        "print(final_state[\"messages\"][-1].content)\n",
        "\n",
        "# Visualize the knowledge graph\n",
        "kg.visualize(\"cybersecurity_knowledge_graph.html\")\n",
        "\n",
        "# Optional: Print additional information or analysis\n",
        "print(\"\\nKnowledge Graph Statistics:\")\n",
        "print(f\"Number of entities: {len(kg.graph.nodes)}\")\n",
        "print(f\"Number of relationships: {len(kg.graph.edges)}\")\n",
        "\n",
        "print(\"\\nMost connected entities:\")\n",
        "sorted_nodes = sorted(kg.graph.degree, key=lambda x: x[1], reverse=True)[:5]\n",
        "for node, degree in sorted_nodes:\n",
        "    print(f\"{node}: {degree} connections\")\n",
        "\n",
        "print(\"\\nSample relationships:\")\n",
        "for i, (node1, node2, data) in enumerate(kg.graph.edges(data=True)):\n",
        "    if i >= 5:  # Print only first 5 relationships\n",
        "        break\n",
        "    print(f\"{node1} is {data['relation']} {node2}\")\n",
        "\n",
        "# Optional: Save the collected data for future use\n",
        "import json\n",
        "\n",
        "with open(\"collected_data.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"scraped_content\": scraped_content,\n",
        "        \"tweets\": tweets\n",
        "    }, f)\n",
        "\n",
        "print(\"\\nData collection and analysis complete. Results saved to 'cybersecurity_knowledge_graph.html' and 'collected_data.json'.\")"
      ],
      "metadata": {
        "id": "5XgTqqNrSXcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVWtofGaRAbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}