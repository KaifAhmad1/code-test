{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/KG_Enhanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "#### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "#### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "#### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "#### Features Required\n",
        "\n",
        "##### User Interface (UI)\n",
        "- Chat UI with file upload capabilities.\n",
        "- Options to save and select prompts.\n",
        "- Configuration settings for connectors with enable/disable toggles.\n",
        "- Interface for configuring knowledge and variables (similar to Dify.ai).\n",
        "\n",
        "##### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "#### Source Tools\n",
        "\n",
        "##### Website Crawling and Scraping\n",
        "- [Firecrawl](https://www.firecrawl.dev/playground)\n",
        "- [Crawl4AI](https://github.com/unclecode/crawl4ai)\n",
        "- [Apify](https://apify.com/apify/website-content-crawler)\n",
        "- [Exa](https://exa.ai/search)\n",
        "\n",
        "##### Twitter Sources\n",
        "- [Apify Tweet Scraper](https://apify.com/apidojo/tweet-scraper)\n",
        "- [Twitter API](https://developer.x.com/en/docs/twitter-api)\n",
        "\n",
        "##### Development Tools\n",
        "- [Flowise AI](https://flowiseai.com/)\n",
        "- [Langgenius Dify](https://github.com/langgenius/dify)\n",
        "\n",
        "#### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "#### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "jBzZYldlbmJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NCq8zzvwSSdD"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "%pip install -qU langchain langchain-community faiss-cpu kuzu pyvis\n",
        "%pip install -qU sentence-transformers torch plotly pandas scikit-learn networkx\n",
        "%pip install -qU torch torchvision\n",
        "%pip install -qU langchain-groq apify-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import kuzu\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from apify_client import ApifyClient"
      ],
      "metadata": {
        "id": "wXnNIpzdS6jp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "Gf9G1XucTdQs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})"
      ],
      "metadata": {
        "id": "EKlOL3pv6R_9",
        "outputId": "4434acb3-f535-4b34-e65b-c63b39f7f1bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Low latency is crucial for Large Language Models (LLMs) as it directly impacts the user experience, model performance, and overall efficiency. Here are some reasons why low latency is important for LLMs:\\n\\n1. **Real-time interactions**: LLMs are often used in applications that require real-time interactions, such as chatbots, virtual assistants, and language translation. Low latency ensures that the model responds quickly to user input, enabling a seamless and natural conversation flow.\\n2. **User experience**: High latency can lead to frustration and a poor user experience. When users have to wait too long for a response, they may abandon the application or lose interest. Low latency helps maintain user engagement and satisfaction.\\n3. **Conversational flow**: In conversational AI, latency can disrupt the natural flow of conversation. If the model takes too long to respond, the user may forget the context or lose their train of thought. Low latency helps maintain the conversational flow, allowing users to engage in a more natural and productive dialogue.\\n4. **Model performance**: LLMs rely on complex algorithms and large amounts of data to generate responses. High latency can indicate that the model is struggling to process the input or retrieve relevant information from its knowledge base. Low latency, on the other hand, suggests that the model is performing efficiently and effectively.\\n5. **Scalability**: As the number of users and requests increases, low latency becomes even more critical. A model with high latency may struggle to handle a large volume of requests, leading to decreased performance and increased errors. Low latency enables LLMs to scale more efficiently and handle a larger number of users.\\n6. **Edge cases and errors**: High latency can also lead to edge cases and errors, such as timeouts, connection losses, or incorrect responses. Low latency reduces the likelihood of these issues, ensuring that the model provides accurate and reliable responses.\\n7. **Competitive advantage**: In a competitive market, low latency can be a key differentiator for LLM-based applications. Users are more likely to choose an application that responds quickly and efficiently, rather than one that takes too long to respond.\\n\\nTo achieve low latency, LLM developers and engineers employ various techniques, such as:\\n\\n1. **Model optimization**: Optimizing the model architecture, weights, and hyperparameters to reduce computational complexity and improve inference speed.\\n2. **Hardware acceleration**: Leveraging specialized hardware, such as GPUs or TPUs, to accelerate model computations.\\n3. **Caching and memoization**: Storing frequently accessed data and intermediate results to reduce the computational overhead.\\n4. **Distributed computing**: Distributing the model across multiple machines or nodes to process requests in parallel and reduce latency.\\n5. **Efficient data structures**: Using efficient data structures and algorithms to store and retrieve data, reducing the time spent on data access and processing.\\n\\nBy prioritizing low latency, LLM developers can create more efficient, scalable, and user-friendly applications that provide a better experience for users.', response_metadata={'token_usage': {'completion_tokens': 604, 'prompt_tokens': 33, 'total_tokens': 637, 'completion_time': 2.416, 'prompt_time': 0.011289748, 'queue_time': None, 'total_time': 2.4272897479999997}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-df632c14-52a7-4f4a-a309-1c75e01ae836-0', usage_metadata={'input_tokens': 33, 'output_tokens': 604, 'total_tokens': 637})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Initialize Kuzu DB\n",
        "db = kuzu.Database(\"cybersecurity_knowledge_graph\")\n",
        "conn = kuzu.Connection(db)\n",
        "\n",
        "# Create schema for the graph\n",
        "conn.execute(\"CREATE NODE TABLE Entity (name STRING, type STRING, PRIMARY KEY (name))\")\n",
        "conn.execute(\"CREATE REL TABLE Relation (FROM Entity TO Entity, predicate STRING)\")\n",
        "'''"
      ],
      "metadata": {
        "id": "0HGnCtuBTdWP",
        "outputId": "c99700c6-958d-474e-c21d-a2feda660551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' # Initialize Kuzu DB\\ndb = kuzu.Database(\"cybersecurity_knowledge_graph\")\\nconn = kuzu.Connection(db)\\n\\n# Create schema for the graph\\nconn.execute(\"CREATE NODE TABLE Entity (name STRING, type STRING, PRIMARY KEY (name))\")\\nconn.execute(\"CREATE REL TABLE Relation (FROM Entity TO Entity, predicate STRING)\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cybersecurity-specific websites\n",
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://www.virustotal.com/gui/home/upload\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "]"
      ],
      "metadata": {
        "id": "M3PWVNlqTdcj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape and load web content\n",
        "def scrape_websites(urls):\n",
        "    documents = []\n",
        "    for url in urls:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        documents.append(text)\n",
        "    return documents\n",
        "\n",
        "# Apify client setup\n",
        "APIFY_API_TOKEN = 'apify_api_t9YCnrjquQgW4BCNM8yYZrX6Q2a1uF1ImYkB'\n",
        "apify_client = ApifyClient(APIFY_API_TOKEN)\n",
        "\n",
        "# Function to scrape tweets using Apify\n",
        "def scrape_tweets(query, max_results=100):\n",
        "    run_input = {\n",
        "        'searchQuery': query,\n",
        "        'maxResults': max_results\n",
        "    }\n",
        "\n",
        "    # Start a new actor task on Apify\n",
        "    run = apify_client.actor(\"microworlds/twitter-scraper\").call(run_input=run_input)\n",
        "\n",
        "    # Fetch the results from the run\n",
        "    tweets = run['data']\n",
        "\n",
        "    # Extract relevant text content from tweets\n",
        "    tweet_texts = [tweet['full_text'] for tweet in tweets]\n",
        "\n",
        "    return tweet_texts"
      ],
      "metadata": {
        "id": "1IzpQDCPpUN8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process documents\n",
        "documents = scrape_websites(websites)\n",
        "tweet_texts = scrape_tweets('cybersecurity')\n",
        "\n",
        "all_texts = documents + tweet_texts\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_text(\"\\n\\n\".join(all_texts))\n",
        "\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "I9NncGQxtIMY",
        "outputId": "e8f2bc52-ae13-4fc4-a533-a8528ae69797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1404a708cc94>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and process documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_websites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cybersecurity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtweet_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-8924cfb84391>\u001b[0m in \u001b[0;36mscrape_tweets\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Fetch the results from the run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Extract relevant text content from tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge extraction and graph population\n",
        "kg_triple_extract_template = \"\"\"\n",
        "Extract up to 5 cybersecurity-related knowledge triplets from the text below in the form (subject, predicate, object).\n",
        "Focus on threats, vulnerabilities, attack techniques, and security measures.\n",
        "Text: {text}\n",
        "Triplets:\n",
        "\"\"\"\n",
        "kg_triple_extract_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=kg_triple_extract_template,\n",
        ")\n",
        "\n",
        "kg_triple_extract_chain = LLMChain(llm=llm, prompt=kg_triple_extract_prompt)\n",
        "\n",
        "for text in texts:\n",
        "    triplets = kg_triple_extract_chain.invoke({\"text\": text})\n",
        "    for triplet in triplets['text'].split('\\n'):\n",
        "        if triplet.strip():\n",
        "            try:\n",
        "                subject, predicate, obj = eval(triplet.strip())\n",
        "                conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [subject, \"Cybersecurity_Entity\"])\n",
        "                conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [obj, \"Cybersecurity_Entity\"])\n",
        "                conn.execute(\"INSERT INTO Relation VALUES (?, ?, ?)\", [subject, obj, predicate])\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process triplet: {triplet}. Error: {e}\")"
      ],
      "metadata": {
        "id": "v5jU8n8atV-b",
        "outputId": "f844f088-1da9-4ae8-b7e7-059503172deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApifyApiError",
          "evalue": "User was not found or authentication token is not valid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApifyApiError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4c606dfc4dac>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and process documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_websites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cybersecurity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6199c9356708>\u001b[0m in \u001b[0;36mscrape_tweets\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Start a new actor task on Apify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapify_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apify/twitter-scraper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Fetch the results from the run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, webhooks, wait_secs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         started_run = self.start(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, wait_for_finish, webhooks)\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         response = self.http_client.call(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, method, url, headers, params, data, json, stream, parse_response)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         return retry_with_exp_backoff(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0m_make_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_utils.py\u001b[0m in \u001b[0;36mretry_with_exp_backoff\u001b[0;34m(func, max_retries, backoff_base_millis, backoff_factor, random_factor)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_retrying\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mswallow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(stop_retrying, attempt)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status code is not retryable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'status_code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mstop_retrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         return retry_with_exp_backoff(\n",
            "\u001b[0;31mApifyApiError\u001b[0m: User was not found or authentication token is not valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_data():\n",
        "    nodes_result = conn.execute(\"MATCH (e:Entity) RETURN e.name\")\n",
        "    edges_result = conn.execute(\"MATCH (e1:Entity)-[r:Relation]->(e2:Entity) RETURN e1.name, r.predicate, e2.name\")\n",
        "\n",
        "    nodes = [row.getString(0) for row in nodes_result]\n",
        "    edges = [(row.getString(0), row.getString(1), row.getString(2)) for row in edges_result]\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "# Enhanced graph visualization using Plotly\n",
        "def visualize_graph_plotly():\n",
        "    nodes, edges = get_graph_data()\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for node in nodes:\n",
        "        G.add_node(node)\n",
        "\n",
        "    for edge in edges:\n",
        "        G.add_edge(edge[0], edge[2], label=edge[1])\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in G.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines')\n",
        "\n",
        "    node_x = [pos[node][0] for node in G.nodes()]\n",
        "    node_y = [pos[node][1]]\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        marker=dict(\n",
        "            showscale=True,\n",
        "            colorscale='YlGnBu',\n",
        "            reversescale=True,\n",
        "            color=[],\n",
        "            size=10,\n",
        "            colorbar=dict(\n",
        "                thickness=15,\n",
        "                title='Node Connections',\n",
        "                xanchor='left',\n",
        "                titleside='right'\n",
        "            ),\n",
        "            line_width=2))\n",
        "\n",
        "    node_adjacencies = []\n",
        "    node_text = []\n",
        "    for node, adjacencies in G.adjacency():\n",
        "        node_adjacencies.append(len(adjacencies))\n",
        "        node_text.append(f'{node}# of connections: {len(adjacencies)}')\n",
        "\n",
        "    node_trace.marker.color = node_adjacencies\n",
        "    node_trace.text = node_text\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title='Knowledge Graph',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[ dict(\n",
        "                            text=\"\",\n",
        "                            showarrow=False,\n",
        "                            xref=\"paper\", yref=\"paper\",\n",
        "                            x=0.005, y=-0.002 ) ],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# Embedding visualization\n",
        "def visualize_embeddings():\n",
        "    # Get embeddings\n",
        "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
        "\n",
        "    # Reduce dimensionality for visualization\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    tsne_results = tsne.fit_transform(doc_embeddings)\n",
        "\n",
        "    # Create a DataFrame for Plotly\n",
        "    df = pd.DataFrame(tsne_results, columns=['x', 'y'])\n",
        "\n",
        "    fig = px.scatter(df, x='x', y='y', title='Document Embeddings Visualization')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "iqzsy5ckta_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example queries\n",
        "questions = [\n",
        "    \"What are the latest threats targeting the healthcare industry?\",\n",
        "    \"Can you provide details on recent ransomware attacks?\",\n",
        "    \"What are the most critical vulnerabilities discovered in the last month?\",\n",
        "    \"How can organizations protect against phishing attacks?\",\n",
        "    \"What are the emerging trends in cybersecurity for financial institutions?\"\n",
        "]"
      ],
      "metadata": {
        "id": "qAJZIObetgOE",
        "outputId": "a0c38a19-5ff8-420c-954e-a0d784eb40db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6bf09ae1e1eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the knowledge graph and visualize\n",
        "def query_graph(query):\n",
        "    return chain.invoke({\"text\": query})\n",
        "\n",
        "for query in questions:\n",
        "    answer = query_graph(query)\n",
        "    print(f\"Query: {query}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "# Visualize the graph and embeddings\n",
        "visualize_graph_plotly()\n",
        "visualize_embeddings()"
      ],
      "metadata": {
        "id": "aAgfogCoqNES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nht_Cjonp4Wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}