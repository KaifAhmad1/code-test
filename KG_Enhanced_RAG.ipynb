{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/KG_Enhanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "#### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "#### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "#### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "#### Features Required\n",
        "\n",
        "##### User Interface (UI)\n",
        "- Chat UI with file upload capabilities.\n",
        "- Options to save and select prompts.\n",
        "- Configuration settings for connectors with enable/disable toggles.\n",
        "- Interface for configuring knowledge and variables (similar to Dify.ai).\n",
        "\n",
        "##### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "#### Source Tools\n",
        "\n",
        "##### Website Crawling and Scraping\n",
        "- [Firecrawl](https://www.firecrawl.dev/playground)\n",
        "- [Crawl4AI](https://github.com/unclecode/crawl4ai)\n",
        "- [Apify](https://apify.com/apify/website-content-crawler)\n",
        "- [Exa](https://exa.ai/search)\n",
        "\n",
        "##### Twitter Sources\n",
        "- [Apify Tweet Scraper](https://apify.com/apidojo/tweet-scraper)\n",
        "- [Twitter API](https://developer.x.com/en/docs/twitter-api)\n",
        "\n",
        "##### Development Tools\n",
        "- [Flowise AI](https://flowiseai.com/)\n",
        "- [Langgenius Dify](https://github.com/langgenius/dify)\n",
        "\n",
        "#### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "#### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "jBzZYldlbmJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NCq8zzvwSSdD",
        "outputId": "3c732e8b-448f-4208-bd56-31d18137306f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m761.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "%pip install -qU langchain langchain-community faiss-cpu kuzu pyvis\n",
        "%pip install -qU sentence-transformers torch plotly pandas scikit-learn networkx\n",
        "%pip install -qU langchain-groq apify_client langgraph  python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import kuzu\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "from apify_client import ApifyClient\n",
        "from langgraph.graph import Graph, END\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "wXnNIpzdS6jp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66d8b2b-8dff-42eb-f63e-272b9c0f1a50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "Gf9G1XucTdQs",
        "outputId": "b211930f-13ef-4ef3-c679-8fe6a55ea690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.clip.processing_clip because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/processing_clip.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_custom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"boxes should be a 2d tensor, got {dets.dim()}D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0muse_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     30\u001b[0m             )\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-07dd7d781fc2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencode_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"normalize_embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m embeddings = HuggingFaceBgeEmbeddings(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CrossEncoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_device_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_from_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_npu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformerModelCardData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_model_card\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimilarityFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/model_card.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msentence_transformers_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformerTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mAsym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mBoW\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCLIPModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/CLIPModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"openai/clip-vit-base-patch32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLIPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/CLIPModel.py\u001b[0m in \u001b[0;36mCLIPModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1570\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.clip.processing_clip because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Apify client\n",
        "apify_client = ApifyClient(\"apify_api_t9YCnrjquQgW4BCNM8yYZrX6Q2a1uF1ImYkB\")"
      ],
      "metadata": {
        "id": "0HGnCtuBTdWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cybersecurity-specific websites\n",
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://www.virustotal.com/gui/home/upload\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "]"
      ],
      "metadata": {
        "id": "M3PWVNlqTdcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_websites(urls: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Scrape content from given websites using Apify.\n",
        "\n",
        "    Args:\n",
        "        urls (List[str]): List of URLs to scrape.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of scraped text content.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    run_input = {\n",
        "        \"startUrls\": [{\"url\": url} for url in urls],\n",
        "        \"maxCrawlPages\": 10,\n",
        "        \"maxCrawlDepth\": 1,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        scraped_content = [item.get('text', '') for item in dataset_items if 'text' in item]\n",
        "        logger.info(f\"Successfully scraped {len(scraped_content)} pages.\")\n",
        "        return scraped_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping websites: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "09mr6JGo8H2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_scraped_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fetch tweets related to cybersecurity using Apify.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query for tweets.\n",
        "        max_tweets (int): Maximum number of tweets to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: List of tweet data.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"queries\": [query],\n",
        "        \"maxTweets\": max_tweets\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "lPp-rrm3d-4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Kuzu DB\n",
        "db = kuzu.Database(\"cybersecurity_knowledge_graph\")\n",
        "conn = kuzu.Connection(db)"
      ],
      "metadata": {
        "id": "PZrI12i6sjOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_knowledge_graph():\n",
        "    \"\"\"Initialize the knowledge graph schema.\"\"\"\n",
        "    try:\n",
        "        conn.execute(\"CREATE NODE TABLE Entity (name STRING, type STRING, PRIMARY KEY (name))\")\n",
        "        conn.execute(\"CREATE REL TABLE Relation (FROM Entity TO Entity, predicate STRING)\")\n",
        "        logger.info(\"Knowledge graph schema initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error initializing knowledge graph: {str(e)}\")"
      ],
      "metadata": {
        "id": "PKZaTeaysjdY",
        "outputId": "0cbc6575-be7b-4a27-b494-4d337ad7bf52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 43324, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1030, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 22507, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 92506, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_knowledge_graph(triplets: List[tuple]):\n",
        "    \"\"\"\n",
        "    Update the knowledge graph with new triplets.\n",
        "\n",
        "    Args:\n",
        "        triplets (List[tuple]): List of (subject, predicate, object) triplets.\n",
        "    \"\"\"\n",
        "    for subject, predicate, obj in triplets:\n",
        "        try:\n",
        "            conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [subject, \"Cybersecurity_Entity\"])\n",
        "            conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [obj, \"Cybersecurity_Entity\"])\n",
        "            conn.execute(\"INSERT INTO Relation VALUES (?, ?, ?)\", [subject, obj, predicate])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating knowledge graph: {str(e)}\")"
      ],
      "metadata": {
        "id": "4ZnOfVbusjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triplets(text: str) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    Extract knowledge triplets from text using the LLM.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to extract triplets from.\n",
        "\n",
        "    Returns:\n",
        "        List[tuple]: List of extracted (subject, predicate, object) triplets.\n",
        "    \"\"\"\n",
        "    kg_triple_extract_template = \"\"\"\n",
        "    Extract up to 5 cybersecurity-related knowledge triplets from the text below in the form (subject, predicate, object).\n",
        "    Focus on threats, vulnerabilities, attack techniques, and security measures.\n",
        "    Text: {text}\n",
        "    Triplets:\n",
        "    \"\"\"\n",
        "    kg_triple_extract_prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "        template=kg_triple_extract_template,\n",
        "    )\n",
        "    kg_triple_extract_chain = LLMChain(llm=llm, prompt=kg_triple_extract_prompt)\n",
        "\n",
        "    try:\n",
        "        result = kg_triple_extract_chain.invoke({\"text\": text})\n",
        "        triplets = [eval(triplet.strip()) for triplet in result['text'].split('\\n') if triplet.strip()]\n",
        "        return triplets\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting triplets: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "vNpB3zd0sjkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d7ca39-8eca-455b-dd9a-959b9f7036bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<kuzu.query_result.QueryResult at 0x7d486599ae60>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_data():\n",
        "    \"\"\"Retrieve graph data from the knowledge graph.\"\"\"\n",
        "    try:\n",
        "        nodes_result = conn.execute(\"MATCH (e:Entity) RETURN e.name\")\n",
        "        edges_result = conn.execute(\"MATCH (e1:Entity)-[r:Relation]->(e2:Entity) RETURN e1.name, r.predicate, e2.name\")\n",
        "\n",
        "        nodes = [row.getString(0) for row in nodes_result]\n",
        "        edges = [(row.getString(0), row.getString(1), row.getString(2)) for row in edges_result]\n",
        "\n",
        "        return nodes, edges\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error retrieving graph data: {str(e)}\")\n",
        "        return [], []"
      ],
      "metadata": {
        "id": "p5r2hgWitZhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph_plotly():\n",
        "    \"\"\"Visualize the knowledge graph using Plotly.\"\"\"\n",
        "    nodes, edges = get_graph_data()\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for node in nodes:\n",
        "        G.add_node(node)\n",
        "\n",
        "    for edge in edges:\n",
        "        G.add_edge(edge[0], edge[2], label=edge[1])\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    edge_x, edge_y = [], []\n",
        "    for edge in G.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
        "\n",
        "    node_x, node_y = zip(*[pos[node] for node in G.nodes()])\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
        "        marker=dict(showscale=True, colorscale='YlGnBu', reversescale=True, color=[], size=10,\n",
        "                    colorbar=dict(thickness=15, title='Node Connections', xanchor='left', titleside='right'),\n",
        "                    line_width=2))\n",
        "\n",
        "    node_adjacencies = []\n",
        "    node_text = []\n",
        "    for node, adjacencies in G.adjacency():\n",
        "        node_adjacencies.append(len(adjacencies))\n",
        "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
        "\n",
        "    node_trace.marker.color = node_adjacencies\n",
        "    node_trace.text = node_text\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title='Cybersecurity Knowledge Graph',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[dict(text=\"\", showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.002)],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                    )\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "ui_duI_XtZkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ffaacb-f6b4-4b5f-e13f-59ae3c802356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process triplet: Here are 5 cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Threat Actor, Uses, Tactics, Techniques, and Procedures). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Vulnerability, Has, Indicators of Compromise). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 3. (Vendor, Publishes, Mitigations). Error: name 'Vendor' is not defined\n",
            "Failed to process triplet: 4. (Industrial Control System, Is Vulnerable To, Cyber Threats). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 5. (ICS Medical Vendor, Publishes, Mitigations for Vulnerabilities). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Note: These triplets are in the form (subject, predicate, object) as requested. Let me know if you'd like me to extract more!. Error: unterminated string literal (detected at line 1) (<string>, line 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process triplet: Here are 5 cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Phishing, uses, Malicious Link). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Exploitation of Remote Services, uses, Vulnerabilities). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 3. (Data Obfuscation, uses, Steganography). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 4. (Lateral Movement, uses, Remote Desktop Protocol). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 5. (Data Exfiltration, uses, Web Service). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Note: These triplets are in the format (subject, predicate, object) and represent relationships between different concepts in the text.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are a few more:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 6. (Account Manipulation, uses, Additional Cloud Credentials). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 7. (Process Injection, uses, Dynamic-link Library Injection). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 8. (Network Denial of Service, uses, Direct Network Flood). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 9. (Data Manipulation, uses, Stored Data Manipulation). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 10. (System Binary Proxy Execution, uses, CMSTP). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Let me know if you'd like me to extract more!. Error: unterminated string literal (detected at line 1) (<string>, line 1)\n",
            "Failed to process triplet: Here are 5 cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Ransomware Negotiators, interact with, Cyber Threat Actors). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Cybercriminals, hold for ransom, Victim Organizations' Systems and Data). Error: unterminated string literal (detected at line 1) (<string>, line 1)\n",
            "Failed to process triplet: 3. (Ransomware Negotiators, use negotiation to, Restore Operations). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 4. (Cyber Threat Actors, target, Hospitals and Churches). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 5. (Incident Response Experts, provide, Ransomware Negotiation Services). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Note that these triplets are not exhaustive and there may be other relevant triplets that can be extracted from the text.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are 5 cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Infosec Insider, is_written_by, Threatpost cybersecurity subject matter experts). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Infosec Insider, has_goal, bringing a unique voice to important cybersecurity topics). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 3. (Infosec Insider, strives_for, highest quality, objective and non-commercial content). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 4. (Threatpost, is_a, community of cybersecurity subject matter experts). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 5. (Infosec Insider, is_part_of, Threatpost). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Note that these triplets are not directly related to threats, vulnerabilities, attack techniques, and security measures, as the text is more focused on the Infosec Insider community and its goals, rather than specific cybersecurity topics.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are five cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Threat Actor, Uses, Compromised Account of Former Employee). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: * This triplet indicates that the threat actor used a compromised account of a former employee to gain access to the state government organization's network environment.. Error: unterminated string literal (detected at line 1) (<string>, line 1)\n",
            "Failed to process triplet: 2. (LDAP Queries, Conducted by, Threat Actor). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: * This triplet indicates that the threat actor conducted LDAP queries to collect user, host, and trust relationship information.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 3. (Untitled Goose Tool, Helps, Network Defenders). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: * This triplet indicates that the Untitled Goose Tool helps network defenders detect potentially malicious activity in Microsoft Azure, Azure Active Directory (AAD), and Microsoft 365 (M365) environments.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 4. (Multifactor Authentication, Prevents, Lateral Movement). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: * This triplet indicates that multifactor authentication can prevent lateral movement by threat actors, as it would have prevented the threat actor from moving laterally from the on-premises environment to the Azure environment.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 5. (Azure AD, Requires, Secure Configuration). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: * This triplet indicates that Azure AD requires a secure configuration to prevent threat actors from accessing sensitive information and moving laterally in the network.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are five cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Volt Typhoon, uses, Living off the Land (LOTL) techniques). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Volt Typhoon, exploits, zero-day vulnerabilities in public-facing network appliances). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 3. (Volt Typhoon, uses, valid administrator credentials to move laterally to the domain controller via Remote Desktop Protocol (RDP)). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 4. (Volt Typhoon, conducts, extensive pre-compromise reconnaissance to learn about the target organization's network architecture and operational protocols). Error: unterminated string literal (detected at line 1) (<string>, line 1)\n",
            "Failed to process triplet: 5. (Volt Typhoon, uses, Fast Reverse Proxy (FRP) for command and control). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: These triplets highlight some of the key tactics, techniques, and procedures (TTPs) used by the Volt Typhoon threat group, including their use of LOTL techniques, exploitation of zero-day vulnerabilities, lateral movement via RDP, pre-compromise reconnaissance, and use of FRP for command and control.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are 5 cybersecurity-related knowledge triplets extracted from the text:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 1. (Androxgh0st malware, uses, Python-scripted malware). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 2. (Androxgh0st malware, exploits, CVE-2017-9841). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 3. (Androxgh0st malware, targets, .env files). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 4. (Androxgh0st malware, uses, web shell deployment). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 5. (Androxgh0st malware, exploits, CVE-2018-15133). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: Additional triplets can be extracted, but these 5 provide a good starting point for understanding the threat posed by Androxgh0st malware.. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: Here are a few more triplets that can be extracted:. Error: invalid syntax (<string>, line 1)\n",
            "Failed to process triplet: 6. (Androxgh0st malware, uses, path traversal attack). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 7. (Androxgh0st malware, targets, Laravel web application framework). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 8. (Androxgh0st malware, exploits, CVE-2021-41773). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 9. (Androxgh0st malware, uses, command and scripting interpreter). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: 10. (Androxgh0st malware, exploits, unsecured credentials). Error: invalid syntax. Perhaps you forgot a comma? (<string>, line 1)\n",
            "Failed to process triplet: These triplets can be used to create a knowledge graph that represents the relationships between Androxgh0st malware and various cybersecurity concepts.. Error: invalid syntax (<string>, line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_embeddings(texts: List[str]):\n",
        "    \"\"\"Visualize document embeddings using t-SNE and Plotly.\"\"\"\n",
        "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    tsne_results = tsne.fit_transform(doc_embeddings)\n",
        "\n",
        "    df = pd.DataFrame(tsne_results, columns=['x', 'y'])\n",
        "    df['text'] = texts\n",
        "\n",
        "    fig = px.scatter(df, x='x', y='y', hover_data=['text'], title='Document Embeddings Visualization')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "He-lcLnDtZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced graph visualization using Plotly\n",
        "def visualize_graph_plotly():\n",
        "    nodes, edges = get_graph_data()\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for node in nodes:\n",
        "        G.add_node(node)\n",
        "\n",
        "    for edge in edges:\n",
        "        G.add_edge(edge[0], edge[2], label=edge[1])\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in G.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines')\n",
        "\n",
        "    node_x = [pos[node][0] for node in G.nodes()]\n",
        "    node_y = [pos[node][1] for node in G.nodes()]\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        marker=dict(\n",
        "            showscale=True,\n",
        "            colorscale='YlGnBu',\n",
        "            reversescale=True,\n",
        "            color=[],\n",
        "            size=10,\n",
        "            colorbar=dict(\n",
        "                thickness=15,\n",
        "                title='Node Connections',\n",
        "                xanchor='left',\n",
        "                titleside='right'\n",
        "            ),\n",
        "            line_width=2))\n",
        "\n",
        "    node_adjacencies = []\n",
        "    node_text = []\n",
        "    for node, adjacencies in G.adjacency():\n",
        "        node_adjacencies.append(len(adjacencies))\n",
        "        node_text.append(f'{node}# of connections: {len(adjacencies)}')\n",
        "\n",
        "    node_trace.marker.color = node_adjacencies\n",
        "    node_trace.text = node_text\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title='Knowledge Graph',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[ dict(\n",
        "                            text=\"\",\n",
        "                            showarrow=False,\n",
        "                            xref=\"paper\", yref=\"paper\",\n",
        "                            x=0.005, y=-0.002 ) ],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                    )\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "YcoLEog8t3_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding visualization\n",
        "def visualize_embeddings():\n",
        "    # Get embeddings\n",
        "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
        "\n",
        "    # Reduce dimensionality for visualization\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    tsne_results = tsne.fit_transform(doc_embeddings)\n",
        "\n",
        "    # Create a DataFrame for Plotly\n",
        "    df = pd.DataFrame(tsne_results, columns=['x', 'y'])\n",
        "\n",
        "    fig = px.scatter(df, x='x', y='y', title='Document Embeddings Visualization')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "9UjqcklSt4CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agent roles\n",
        "def threat_analyzer(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    query = \"Analyze the latest cybersecurity threats and provide a summary.\"\n",
        "    response = chain.invoke({\"text\": query})\n",
        "    return {\"threat_analysis\": response}\n",
        "\n",
        "def vulnerability_assessor(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    query = \"Identify and assess critical vulnerabilities in cybersecurity systems.\"\n",
        "    response = chain.invoke({\"text\": query})\n",
        "    return {\"vulnerability_assessment\": response}\n",
        "\n",
        "def security_advisor(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    query = \"Provide recommendations for improving cybersecurity based on current threats and vulnerabilities.\"\n",
        "    response = chain.invoke({\"text\": query})\n",
        "    return {\"security_advice\": response}\n",
        "\n",
        "def knowledge_graph_updater(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    threat_analysis = state.get(\"threat_analysis\", \"\")\n",
        "    vulnerability_assessment = state.get(\"vulnerability_assessment\", \"\")\n",
        "    security_advice = state.get(\"security_advice\", \"\")\n",
        "\n",
        "    combined_text = f\"{threat_analysis}\\n{vulnerability_assessment}\\n{security_advice}\"\n",
        "    triplets = extract_triplets(combined_text)\n",
        "    update_knowledge_graph(triplets)\n",
        "\n",
        "    return {\"graph_update\": f\"Knowledge graph updated with {len(triplets)} new triplets.\"}"
      ],
      "metadata": {
        "id": "J2j8mjTSt4E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LangGraph workflow\n",
        "workflow = Graph()\n",
        "\n",
        "workflow.add_node(\"threat_analyzer\", threat_analyzer)\n",
        "workflow.add_node(\"vulnerability_assessor\", vulnerability_assessor)\n",
        "workflow.add_node(\"security_advisor\", security_advisor)\n",
        "workflow.add_node(\"knowledge_graph_updater\", knowledge_graph_updater)\n",
        "\n",
        "workflow.add_edge(\"threat_analyzer\", \"vulnerability_assessor\")\n",
        "workflow.add_edge(\"vulnerability_assessor\", \"security_advisor\")\n",
        "workflow.add_edge(\"security_advisor\", \"knowledge_graph_updater\")\n",
        "\n",
        "workflow.set_entry_point(\"threat_analyzer\")\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "5r2QzDsmt4II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55389c26-8f37-4b7b-87de-a6672bae6082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the latest threats targeting the healthcare industry?\n",
            "Answer: content='The healthcare industry is a prime target for cyber attacks, and the threats are constantly evolving. Here are some of the latest threats targeting the healthcare industry:\\n\\n1. **Ransomware Attacks**: Ransomware attacks are still a major threat to the healthcare industry. These attacks involve encrypting sensitive data and demanding payment in exchange for the decryption key. Recent attacks have shown that ransomware can have devastating effects on healthcare organizations, including data breaches, system downtime, and even patient harm.\\n2. **Data Breaches**: Data breaches are a persistent threat to the healthcare industry, with hackers targeting sensitive patient data, including protected health information (PHI). These breaches can occur through various means, including phishing attacks, malware infections, and insider threats.\\n3. **Medical Device Hacking**: Medical devices, such as pacemakers, insulin pumps, and hospital equipment, are vulnerable to hacking. These devices can be compromised by hackers, potentially leading to patient harm or even death.\\n4. **Telehealth Threats**: With the rise of telehealth, there are new threats emerging, including unauthorized access to telehealth sessions, data breaches, and malware infections.\\n5. **Insider Threats**: Insider threats are a significant concern in the healthcare industry, with employees, contractors, and third-party vendors potentially compromising sensitive data.\\n6. **Cloud Security Threats**: As healthcare organizations move to the cloud, they are exposed to new threats, including data breaches, unauthorized access, and cloud-specific vulnerabilities.\\n7. **Artificial Intelligence (AI) and Machine Learning (ML) Threats**: The use of AI and ML in healthcare can introduce new threats, including data poisoning, model hijacking, and adversarial attacks.\\n8. **Internet of Things (IoT) Threats**: IoT devices, such as medical devices and wearables, are vulnerable to hacking and can be used as entry points for attacks.\\n9. **Phishing and Social Engineering**: Phishing and social engineering attacks are still a major threat to the healthcare industry, with hackers using tactics like spear phishing and business email compromise (BEC) to trick employees into divulging sensitive information.\\n10. **Supply Chain Threats**: The healthcare industry is vulnerable to supply chain threats, including attacks on medical device manufacturers, pharmaceutical companies, and other third-party vendors.\\n\\nTo mitigate these threats, healthcare organizations must prioritize cybersecurity and implement robust security measures, including:\\n\\n* Implementing robust security controls, such as firewalls, intrusion detection systems, and antivirus software\\n* Conducting regular security audits and risk assessments\\n* Providing employee training and awareness programs\\n* Implementing incident response plans\\n* Using secure communication protocols, such as encryption\\n* Monitoring for suspicious activity\\n* Implementing a robust patch management program\\n* Using secure cloud storage and backup solutions\\n\\nBy staying informed about the latest threats and taking proactive steps to protect against them, healthcare organizations can help ensure the security and integrity of sensitive patient data.' response_metadata={'token_usage': {'completion_tokens': 591, 'prompt_tokens': 31, 'total_tokens': 622, 'completion_time': 2.364, 'prompt_time': 0.011121403, 'queue_time': None, 'total_time': 2.375121403}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-166dd4a4-bdd3-4da5-b32a-000c9cd8daae-0' usage_metadata={'input_tokens': 31, 'output_tokens': 591, 'total_tokens': 622}\n",
            "\n",
            "Query: Can you provide details on recent ransomware attacks?\n",
            "Answer: content=\"Unfortunately, ransomware attacks have been on the rise in recent years, and 2022 has been no exception. Here are some notable recent ransomware attacks:\\n\\n1. **Colonial Pipeline Attack (May 2021)**: One of the most significant ransomware attacks in recent history, the Colonial Pipeline attack targeted the largest fuel pipeline in the United States. The attack, attributed to the DarkSide ransomware group, resulted in a shutdown of the pipeline, causing widespread fuel shortages and panic buying.\\n2. **JBS Foods Attack (May 2021)**: JBS Foods, a leading global meat processor, was hit by a ransomware attack attributed to the REvil (Sodinokibi) group. The attack affected operations in the United States, Australia, and Canada, leading to concerns about food supply chain disruptions.\\n3. **Kaseya Attack (July 2021)**: Kaseya, a software company that provides IT management solutions, was targeted by the REvil ransomware group. The attack affected over 1,500 organizations worldwide, including schools, hospitals, and government agencies.\\n4. **Accenture Attack (August 2021)**: Accenture, a global consulting firm, was hit by a ransomware attack attributed to the LockBit group. The attack resulted in the theft of sensitive data, including employee information and client data.\\n5. **Howard University Attack (September 2021)**: Howard University, a historically black college in Washington, D.C., was targeted by a ransomware attack attributed to the Babuk group. The attack resulted in the disruption of university operations and the theft of sensitive data.\\n6. **Grocery Store Chain Attack (October 2021)**: A ransomware attack attributed to the BlackMatter group targeted a grocery store chain in the United States, resulting in the disruption of operations and the theft of sensitive data.\\n7. **Ukrainian Government Attack (January 2022)**: The Ukrainian government was targeted by a ransomware attack attributed to the Emotet group. The attack resulted in the disruption of government operations and the theft of sensitive data.\\n8. **Red Cross Attack (January 2022)**: The International Committee of the Red Cross (ICRC) was hit by a ransomware attack attributed to the LockBit group. The attack resulted in the theft of sensitive data, including information about prisoners of war and missing persons.\\n9. **Toyota Attack (February 2022)**: Toyota, the Japanese automaker, was targeted by a ransomware attack attributed to the Conti group. The attack resulted in the disruption of operations and the theft of sensitive data.\\n10. **Costa Rica Government Attack (April 2022)**: The government of Costa Rica was targeted by a ransomware attack attributed to the Conti group. The attack resulted in the disruption of government operations and the theft of sensitive data.\\n\\nThese attacks highlight the ongoing threat of ransomware and the need for organizations to prioritize cybersecurity and implement robust defenses against these types of attacks.\\n\\n**Common trends and tactics:**\\n\\n* Many recent ransomware attacks have involved the use of phishing emails or exploited vulnerabilities to gain initial access to the targeted organization's network.\\n* Ransomware groups have increasingly used double extortion tactics, where they not only demand a ransom in exchange for the decryption key but also threaten to release stolen data if the ransom is not paid.\\n* The use of ransomware-as-a-service (RaaS) models has become more prevalent, allowing attackers to easily deploy and manage ransomware campaigns.\\n\\n**Mitigation strategies:**\\n\\n* Implement robust email security measures, including phishing detection and prevention tools.\\n* Regularly update and patch software vulnerabilities.\\n* Use strong passwords and multi-factor authentication.\\n* Implement a robust backup and disaster recovery plan.\\n* Conduct regular security awareness training for employees.\\n* Consider implementing a ransomware-specific incident response plan.\\n\\nI hope this information helps! Let me know if you have any further questions or concerns.\" response_metadata={'token_usage': {'completion_tokens': 805, 'prompt_tokens': 31, 'total_tokens': 836, 'completion_time': 3.22, 'prompt_time': 0.011906171, 'queue_time': None, 'total_time': 3.2319061710000003}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None} id='run-518dfddb-5a71-48df-b509-3a691178d27c-0' usage_metadata={'input_tokens': 31, 'output_tokens': 805, 'total_tokens': 836}\n",
            "\n",
            "Query: What are the most critical vulnerabilities discovered in the last month?\n",
            "Answer: content='I\\'ll provide you with an overview of some of the most critical vulnerabilities discovered in the last month, based on publicly available information. Please note that this is not an exhaustive list, and the severity of vulnerabilities can vary depending on the specific environment and context.\\n\\nHere are some of the most critical vulnerabilities discovered in the last month:\\n\\n1. **CVE-2023-20161**: A critical vulnerability in the popular open-source library \"Lodash\" was discovered, which could allow for arbitrary code execution. Lodash is widely used in many applications, including web applications and Node.js applications.\\n2. **CVE-2023-20144**: A critical vulnerability in the \"Apache Commons Text\" library was discovered, which could allow for arbitrary code execution. This library is widely used in many Java applications.\\n3. **CVE-2023-20132**: A critical vulnerability in the \"Git\" version control system was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Git prior to 2.35.4.\\n4. **CVE-2023-20125**: A critical vulnerability in the \"WordPress\" content management system was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of WordPress prior to 5.8.4.\\n5. **CVE-2023-20118**: A critical vulnerability in the \"VMware ESXi\" virtualization platform was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of ESXi prior to 7.0.0.\\n6. **CVE-2023-20114**: A critical vulnerability in the \"Microsoft Exchange Server\" was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Exchange Server prior to 2.0.1.\\n7. **CVE-2023-20108**: A critical vulnerability in the \"Google Chrome\" web browser was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Chrome prior to 104.0.0.184.\\n8. **CVE-2023-20104**: A critical vulnerability in the \"Mozilla Firefox\" web browser was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Firefox prior to 104.0.0.\\n9. **CVE-2023-20097**: A critical vulnerability in the \"Apple Safari\" web browser was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Safari prior to 15.6.0.\\n10. **CVE-2023-20092**: A critical vulnerability in the \"Microsoft Windows\" operating system was discovered, which could allow for arbitrary code execution. This vulnerability affects all versions of Windows prior to 10.0.225.0.\\n\\nThese vulnerabilities are considered critical because they could allow for arbitrary code execution, which could lead to a complete compromise of the affected system.\\n\\nIt\\'s essential to note that these vulnerabilities have been publicly disclosed, and patches are available for most of them. It\\'s crucial to apply these patches as soon as possible to prevent exploitation.\\n\\nIf you\\'re concerned about the security of your systems, I recommend:\\n\\n* Checking for updates and applying patches for the affected software\\n* Conducting a thorough vulnerability scan to identify potential vulnerabilities\\n* Implementing additional security measures, such as a web application firewall (WAF) and intrusion detection system (IDS)\\n* Staying informed about the latest security news and updates\\n\\nPlease let me know if you have any further questions or concerns!' response_metadata={'token_usage': {'completion_tokens': 711, 'prompt_tokens': 33, 'total_tokens': 744, 'completion_time': 2.844, 'prompt_time': 0.013510971, 'queue_time': None, 'total_time': 2.857510971}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-19fcf900-4fe9-44b5-9ecb-c66b79d12e0e-0' usage_metadata={'input_tokens': 33, 'output_tokens': 711, 'total_tokens': 744}\n",
            "\n",
            "Query: How can organizations protect against phishing attacks?\n",
            "Answer: content=\"Organizations can protect against phishing attacks by implementing a combination of technical, administrative, and educational measures. Here are some ways to protect against phishing attacks:\\n\\n**Technical Measures:**\\n\\n1. **Email Filtering**: Implement email filtering solutions that can detect and block phishing emails before they reach employees' inboxes.\\n2. **Two-Factor Authentication (2FA)**: Implement 2FA to add an extra layer of security to prevent attackers from accessing sensitive information even if they obtain login credentials.\\n3. **URL Filtering**: Block access to known phishing websites and malicious URLs.\\n4. **Anti-Virus Software**: Install anti-virus software that can detect and block malware and phishing attacks.\\n5. **Encryption**: Encrypt sensitive data to prevent unauthorized access.\\n\\n**Administrative Measures:**\\n\\n1. **Phishing Simulation**: Conduct regular phishing simulation exercises to test employees' awareness and response to phishing attacks.\\n2. **Incident Response Plan**: Develop an incident response plan to quickly respond to phishing attacks and minimize damage.\\n3. **Employee Training**: Provide regular training and awareness programs to educate employees on phishing attacks and how to identify and report them.\\n4. **Account Monitoring**: Monitor employee accounts for suspicious activity and implement account lockout policies to prevent brute-force attacks.\\n5. **Vendor Management**: Conduct thorough background checks on vendors and third-party contractors to prevent insider threats.\\n\\n**Educational Measures:**\\n\\n1. **Employee Awareness**: Educate employees on the risks of phishing attacks and how to identify and report them.\\n2. **Phishing Awareness Campaigns**: Conduct regular phishing awareness campaigns to remind employees of the risks and consequences of phishing attacks.\\n3. **Security Policies**: Develop and communicate security policies and procedures to employees, including guidelines for reporting phishing attacks.\\n4. **Training Programs**: Provide training programs for employees on how to identify and report phishing attacks, including how to verify the authenticity of emails and websites.\\n5. **Phishing Reporting**: Encourage employees to report phishing attacks to the IT department or security team.\\n\\n**Additional Measures:**\\n\\n1. **Implement DMARC**: Implement Domain-based Message Authentication, Reporting, and Conformance (DMARC) to prevent email spoofing.\\n2. **Use Secure Email Protocols**: Use secure email protocols such as Transport Layer Security (TLS) to encrypt email communications.\\n3. **Regularly Update Software**: Regularly update software and systems to prevent exploitation of known vulnerabilities.\\n4. **Conduct Regular Security Audits**: Conduct regular security audits to identify vulnerabilities and weaknesses in the organization's security posture.\\n\\nBy implementing these measures, organizations can significantly reduce the risk of phishing attacks and protect their sensitive information.\" response_metadata={'token_usage': {'completion_tokens': 530, 'prompt_tokens': 29, 'total_tokens': 559, 'completion_time': 2.12, 'prompt_time': 0.011020723, 'queue_time': None, 'total_time': 2.1310207230000002}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None} id='run-3c3c600c-73f5-4de5-9bf0-61d0a3cbb701-0' usage_metadata={'input_tokens': 29, 'output_tokens': 530, 'total_tokens': 559}\n",
            "\n",
            "Query: What are the emerging trends in cybersecurity for financial institutions?\n",
            "Answer: content='Here are some emerging trends in cybersecurity for financial institutions:\\n\\n1. **Cloud Security**: As financial institutions move to the cloud, they need to ensure that their cloud environments are secure. This includes implementing cloud security controls, such as encryption, access controls, and monitoring.\\n2. **Artificial Intelligence (AI) and Machine Learning (ML)**: AI and ML can help financial institutions detect and prevent cyber attacks by identifying patterns and anomalies in their systems.\\n3. **Advanced Threat Protection (ATP)**: ATP solutions can help detect and prevent advanced threats, such as ransomware and zero-day attacks.\\n4. **Endpoint Security**: With the rise of remote work, endpoint security is becoming increasingly important. Financial institutions need to ensure that their endpoints, including laptops, desktops, and mobile devices, are secure.\\n5. **Identity and Access Management (IAM)**: IAM solutions can help financial institutions manage access to their systems and data, ensuring that only authorized users have access.\\n6. **Blockchain Security**: As financial institutions explore the use of blockchain technology, they need to ensure that their blockchain environments are secure.\\n7. **Incident Response**: Financial institutions need to have an incident response plan in place to respond to cyber attacks quickly and effectively.\\n8. **Third-Party Risk Management**: Financial institutions need to ensure that their third-party vendors and partners are secure, as they can be a potential entry point for cyber attacks.\\n9. **Data Protection**: Financial institutions need to ensure that their data is protected, including sensitive customer data.\\n10. **Regulatory Compliance**: Financial institutions need to ensure that they are compliant with relevant regulations, such as the Payment Card Industry Data Security Standard (PCI DSS) and the General Data Protection Regulation (GDPR).\\n\\nSome other emerging trends in cybersecurity for financial institutions include:\\n\\n* **Zero-Trust Architecture**: A zero-trust architecture assumes that all users and devices are untrusted and verifies their identity and access rights before allowing access to sensitive data and systems.\\n* **Security Orchestation, Automation, and Response (SOAR)**: SOAR solutions can help financial institutions automate their security operations and respond to cyber attacks more quickly and effectively.\\n* **Threat Intelligence**: Threat intelligence solutions can help financial institutions stay informed about potential cyber threats and take proactive steps to prevent them.\\n* **Digital Forensics**: Digital forensics solutions can help financial institutions investigate and respond to cyber attacks more effectively.\\n\\nThese are just a few of the emerging trends in cybersecurity for financial institutions. The key is to stay informed and adapt to the ever-changing threat landscape.' response_metadata={'token_usage': {'completion_tokens': 519, 'prompt_tokens': 32, 'total_tokens': 551, 'completion_time': 2.076, 'prompt_time': 0.012642559, 'queue_time': None, 'total_time': 2.088642559}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-5e6e23af-9c6f-4eea-a391-000b990da1f0-0' usage_metadata={'input_tokens': 32, 'output_tokens': 519, 'total_tokens': 551}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cybersecurity_workflow():\n",
        "    \"\"\"Run the cybersecurity analysis workflow.\"\"\"\n",
        "    logger.info(\"Starting cybersecurity analysis workflow...\")\n",
        "    for step in app.stream({}, {\"recursion_limit\": 10}):\n",
        "        if isinstance(step, ToolInvocation):\n",
        "            logger.info(f\"Running: {step.tool}\")\n",
        "        else:\n",
        "            logger.info(f\"Result: {json.dumps(step, indent=2)}\")\n",
        "\n",
        "    logger.info(\"Workflow completed. Updating visualizations...\")\n",
        "    visualize_graph_plotly()\n",
        "    visualize_embeddings(texts)"
      ],
      "metadata": {
        "id": "NsuJ1aT8t4Le",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "0e583de6-aa66-4543-860e-a5c0762a1107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'QueryResult' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-059fcc34453c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the graph and embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_graph_plotly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvisualize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-ff0ec19a9a22>\u001b[0m in \u001b[0;36mvisualize_graph_plotly\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enhanced graph visualization using Plotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_graph_plotly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_graph_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-a857d7cb617c>\u001b[0m in \u001b[0;36mget_graph_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0medges_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MATCH (e1:Entity)-[r:Relation]->(e2:Entity) RETURN e1.name, r.predicate, e2.name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'QueryResult' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_graph(query: str) -> str:\n",
        "    \"\"\"Query the knowledge graph using the LLM.\"\"\"\n",
        "    return chain.invoke({\"text\": query})"
      ],
      "metadata": {
        "id": "M5fUPksE04wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize knowledge graph\n",
        "    initialize_knowledge_graph()\n",
        "\n",
        "    # Scrape websites and fetch tweets\n",
        "    documents = scrape_websites(websites)\n",
        "    tweets = fetch_scraped_tweets(\"#cybersecurity\")\n",
        "\n",
        "    # Combine all texts\n",
        "    all_texts = documents + [tweet.get('full_text', '') for tweet in tweets]\n",
        "\n",
        "    # Split texts into chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.split_text(\"\\n\\n\".join(all_texts))\n",
        "\n",
        "    # Create a vector store\n",
        "    vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "    # Run the cybersecurity workflow\n",
        "    run_cybersecurity_workflow()\n",
        "\n",
        "    # Example queries\n",
        "    questions = [\n",
        "        \"What are the latest threats targeting the healthcare industry?\",\n",
        "        \"Can you provide details on recent ransomware attacks?\",\n",
        "        \"What are the most critical vulnerabilities discovered in the last month?\",\n",
        "        \"How can organizations protect against phishing attacks?\",\n",
        "        \"What are the emerging trends in cybersecurity for financial institutions?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nExample queries:\")\n",
        "    for query in questions:\n",
        "        answer = query_graph(query)\n",
        "        print(f\"Query: {query}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "AP8Wl615cRO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}