{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/KG_Enhanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "#### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "#### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "#### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "#### Features Required\n",
        "\n",
        "##### User Interface (UI)\n",
        "- Chat UI with file upload capabilities.\n",
        "- Options to save and select prompts.\n",
        "- Configuration settings for connectors with enable/disable toggles.\n",
        "- Interface for configuring knowledge and variables (similar to Dify.ai).\n",
        "\n",
        "##### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "#### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "#### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "jBzZYldlbmJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation and Setup"
      ],
      "metadata": {
        "id": "kv12uPN1u42I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NCq8zzvwSSdD",
        "outputId": "956fa747-0015-4db4-82ae-b26ffc195646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -yq torch torchvision pandas\n",
        "!pip install -q torch==2.3.1 torchvision==0.18.1 pandas==2.0.3\n",
        "!pip install -qU langchain langchain-community faiss-cpu kuzu pyvis\n",
        "!pip install -qU sentence-transformers networkx pydantic\n",
        "!pip install -qU langchain-groq apify_client langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "iNhkT7HTvNu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "from apify_client import ApifyClient\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "wXnNIpzdS6jp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding and LLM Initialization**"
      ],
      "metadata": {
        "id": "WCBIUmKBRFx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "HTY5wy9JRAJM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Knowledge Graph Implementation**"
      ],
      "metadata": {
        "id": "GJLcPIqpROzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge Graph Implementation\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()\n",
        "\n",
        "    def add_entity(self, entity: str, entity_type: str):\n",
        "        self.graph.add_node(entity, type=entity_type)\n",
        "\n",
        "    def add_relation(self, entity1: str, entity2: str, relation: str):\n",
        "        self.graph.add_edge(entity1, entity2, relation=relation)\n",
        "\n",
        "    def get_related_entities(self, entity: str) -> List[Dict[str, str]]:\n",
        "        related = []\n",
        "        for neighbor in self.graph.neighbors(entity):\n",
        "            edge_data = self.graph.get_edge_data(entity, neighbor)\n",
        "            related.append({\n",
        "                \"entity\": neighbor,\n",
        "                \"relation\": edge_data[\"relation\"]\n",
        "            })\n",
        "        return related\n",
        "\n",
        "    def visualize(self, output_file: str = \"knowledge_graph.html\"):\n",
        "        net = Network(notebook=True, width=\"100%\", height=\"500px\")\n",
        "        for node, node_data in self.graph.nodes(data=True):\n",
        "            net.add_node(node, label=node, title=f\"Type: {node_data['type']}\")\n",
        "        for edge in self.graph.edges(data=True):\n",
        "            net.add_edge(edge[0], edge[1], title=edge[2]['relation'])\n",
        "        net.show(output_file)\n",
        "\n",
        "# Initialize knowledge graph\n",
        "kg = KnowledgeGraph()"
      ],
      "metadata": {
        "id": "D81Iru-MRALk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection Functions**"
      ],
      "metadata": {
        "id": "v786AsBfRaUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection Functions\n",
        "apify_client = ApifyClient(\"apify_api_yUkcz99gMX1pwNckRi7EyXLwhVTd0j3m4Mtt\")\n",
        "\n",
        "def scrape_websites(urls: List[str]) -> List[str]:\n",
        "    \"\"\"Scrape content from given websites using Apify.\"\"\"\n",
        "    logger.info(f\"Scraping {len(urls)} websites...\")\n",
        "    run_input = {\n",
        "        \"startUrls\": [{\"url\": url} for url in urls],\n",
        "        \"maxCrawlPages\": 10,\n",
        "        \"maxCrawlDepth\": 1,\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n",
        "        dataset_items = apify_client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
        "        scraped_content = [item.get('text', '') for item in dataset_items if 'text' in item]\n",
        "        logger.info(f\"Successfully scraped {len(scraped_content)} pages.\")\n",
        "        return scraped_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error scraping websites: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def fetch_scraped_tweets(query: str, max_tweets: int = 100) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch tweets related to cybersecurity using Apify.\"\"\"\n",
        "    logger.info(f\"Fetching tweets for query: {query}\")\n",
        "    actor_input = {\n",
        "        \"queries\": [query],\n",
        "        \"maxTweets\": max_tweets\n",
        "    }\n",
        "    try:\n",
        "        run = apify_client.actor(\"apidojo/tweet-scraper\").call(run_input=actor_input)\n",
        "        dataset_id = run[\"defaultDatasetId\"]\n",
        "        items = apify_client.dataset(dataset_id).list_items().items\n",
        "        logger.info(f\"Fetched {len(items)} tweets.\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching tweets: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "7Y7wleoORAOM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cybersecurity-specific websites\n",
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://www.virustotal.com/gui/home/upload\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "]\n",
        "\n",
        "# Scrape websites\n",
        "scraped_content = scrape_websites(websites)\n",
        "\n",
        "# Fetch tweets\n",
        "tweets = fetch_scraped_tweets(\"#cybersecurity\")\n",
        "tweet_content = [tweet.get('full_text', '') for tweet in tweets]\n",
        "\n",
        "# Combine scraped content and tweets\n",
        "all_content = scraped_content + tweet_content"
      ],
      "metadata": {
        "id": "GdutKj5ZrhGz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Store and Retriever Setup Functions**"
      ],
      "metadata": {
        "id": "fEk4-XT2RoaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Store and Retriever Setup Functions\n",
        "def create_vectorstore(texts: List[str]) -> FAISS:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    documents = text_splitter.create_documents(texts)\n",
        "    return FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "def setup_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
        "    base_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "    compressor = LLMChainExtractor.from_llm(llm)\n",
        "    return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)\n",
        "\n",
        "# Create vector store and retriever\n",
        "vectorstore = create_vectorstore(all_content)\n",
        "retriever = setup_retriever(vectorstore)"
      ],
      "metadata": {
        "id": "AQu4_gqtRARA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pydantic Models for Structured Output**"
      ],
      "metadata": {
        "id": "UC43ZFtcR8NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic Models for Structured Output\n",
        "class ThreatAnalysis(BaseModel):\n",
        "    threat_type: str = Field(description=\"Type of cybersecurity threat\")\n",
        "    severity: str = Field(description=\"Severity level of the threat (Low, Medium, High, Critical)\")\n",
        "    description: str = Field(description=\"Brief description of the threat\")\n",
        "    potential_impact: str = Field(description=\"Potential impact on organizations\")\n",
        "    mitigation_steps: List[str] = Field(description=\"List of steps to mitigate the threat\")\n",
        "\n",
        "class VulnerabilityAssessment(BaseModel):\n",
        "    vulnerability_name: str = Field(description=\"Name or identifier of the vulnerability\")\n",
        "    affected_systems: List[str] = Field(description=\"List of affected systems or software\")\n",
        "    cvss_score: float = Field(description=\"CVSS score of the vulnerability\")\n",
        "    description: str = Field(description=\"Brief description of the vulnerability\")\n",
        "    remediation_steps: List[str] = Field(description=\"List of steps to remediate the vulnerability\")\n",
        "\n",
        "class SecurityRecommendation(BaseModel):\n",
        "    recommendation: str = Field(description=\"Security recommendation\")\n",
        "    priority: str = Field(description=\"Priority level (Low, Medium, High)\")\n",
        "    implementation_difficulty: str = Field(description=\"Difficulty of implementation (Easy, Moderate, Complex)\")\n",
        "    expected_impact: str = Field(description=\"Expected impact of implementing the recommendation\")"
      ],
      "metadata": {
        "id": "L2ovbk6CRATr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specialized Agent Tools**"
      ],
      "metadata": {
        "id": "I3swfJE0SQGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specialized Agent Tools\n",
        "class ThreatAnalyzerTool(BaseTool):\n",
        "    name = \"Threat Analyzer\"\n",
        "    description = \"Analyzes cybersecurity threats and provides detailed information\"\n",
        "\n",
        "    def _run(self, query: str) -> ThreatAnalysis:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a cybersecurity threat analyst. Provide a detailed analysis of the given threat.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=ThreatAnalysis))\n",
        "        return chain.run(query)\n",
        "\n",
        "class VulnerabilityAssessorTool(BaseTool):\n",
        "    name = \"Vulnerability Assessor\"\n",
        "    description = \"Assesses cybersecurity vulnerabilities and provides detailed information\"\n",
        "\n",
        "    def _run(self, query: str) -> VulnerabilityAssessment:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a vulnerability assessment specialist. Provide a detailed assessment of the given vulnerability.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=VulnerabilityAssessment))\n",
        "        return chain.run(query)\n",
        "\n",
        "class SecurityAdvisorTool(BaseTool):\n",
        "    name = \"Security Advisor\"\n",
        "    description = \"Provides security recommendations based on current threats and vulnerabilities\"\n",
        "\n",
        "    def _run(self, query: str) -> SecurityRecommendation:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"You are a cybersecurity advisor. Provide a detailed security recommendation based on the given context.\"),\n",
        "            HumanMessage(content=query)\n",
        "        ])\n",
        "        chain = LLMChain(llm=llm, prompt=prompt, output_parser=PydanticOutputParser(pydantic_object=SecurityRecommendation))\n",
        "        return chain.run(query)\n",
        "\n",
        "class KnowledgeGraphQueryTool(BaseTool):\n",
        "    name = \"Knowledge Graph Query\"\n",
        "    description = \"Queries the knowledge graph for related information\"\n",
        "\n",
        "    def __init__(self, kg: KnowledgeGraph):\n",
        "        super().__init__()\n",
        "        self.kg = kg\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        entities = extract_entities(query)\n",
        "        results = []\n",
        "        for entity in entities:\n",
        "            related = self.kg.get_related_entities(entity)\n",
        "            results.extend([f\"{entity} is related to {r['entity']} via {r['relation']}\" for r in related])\n",
        "        return \"\\n\".join(results)"
      ],
      "metadata": {
        "id": "vMHBjtlRRAWJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "zdf5NW13Scjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def extract_entities(text: str) -> List[str]:\n",
        "    # This is a placeholder. In a real-world scenario, you'd use a named entity recognition model.\n",
        "    return [word.strip() for word in text.split() if len(word) > 5]\n",
        "\n",
        "def update_knowledge_graph(kg: KnowledgeGraph, text: str):\n",
        "    entities = extract_entities(text)\n",
        "    for i, entity in enumerate(entities):\n",
        "        kg.add_entity(entity, \"Concept\")\n",
        "        if i > 0:\n",
        "            kg.add_relation(entities[i-1], entity, \"related_to\")"
      ],
      "metadata": {
        "id": "L8KC6yBlRAYv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update knowledge graph with initial content\n",
        "for text in all_content:\n",
        "    update_knowledge_graph(kg, text)"
      ],
      "metadata": {
        "id": "VGcZ4jc2sPVx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph Nodes**"
      ],
      "metadata": {
        "id": "b7UosmCJSlXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Nodes\n",
        "class AgentState(Dict[str, Any]):\n",
        "    def __init__(self, messages: List[BaseMessage], kg: KnowledgeGraph, retriever: ContextualCompressionRetriever):\n",
        "        super().__init__()\n",
        "        self[\"messages\"] = messages\n",
        "        self[\"kg\"] = kg\n",
        "        self[\"retriever\"] = retriever\n",
        "\n",
        "def retriever_node(state: AgentState, query: str) -> AgentState:\n",
        "    relevant_docs = state[\"retriever\"].get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Context: {context}\\n\\nQuery: {query}\"))\n",
        "    return state\n",
        "\n",
        "def knowledge_graph_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    kg_tool = KnowledgeGraphQueryTool(state[\"kg\"])\n",
        "    kg_info = kg_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Knowledge Graph Information:\\n{kg_info}\"))\n",
        "    return state\n",
        "\n",
        "def threat_analysis_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    threat_tool = ThreatAnalyzerTool()\n",
        "    analysis = threat_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Threat Analysis:\\n{analysis.json()}\"))\n",
        "    return state\n",
        "\n",
        "def vulnerability_assessment_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    vuln_tool = VulnerabilityAssessorTool()\n",
        "    assessment = vuln_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Vulnerability Assessment:\\n{assessment.json()}\"))\n",
        "    return state\n",
        "\n",
        "def security_recommendation_node(state: AgentState) -> AgentState:\n",
        "    query = state[\"messages\"][-1].content\n",
        "    sec_tool = SecurityAdvisorTool()\n",
        "    recommendation = sec_tool._run(query)\n",
        "    state[\"messages\"].append(HumanMessage(content=f\"Security Recommendation:\\n{recommendation.json()}\"))\n",
        "    return state\n",
        "\n",
        "def agent_node(state: AgentState) -> AgentState:\n",
        "    tools = [\n",
        "        ThreatAnalyzerTool(),\n",
        "        VulnerabilityAssessorTool(),\n",
        "        SecurityAdvisorTool(),\n",
        "        KnowledgeGraphQueryTool(state[\"kg\"])\n",
        "    ]\n",
        "\n",
        "    agent = create_openai_functions_agent(llm, tools, \"\"\"You are a cybersecurity expert assistant.\n",
        "    Analyze the given information and provide a comprehensive response to the query.\"\"\")\n",
        "\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    response = agent_executor.invoke({\"input\": state[\"messages\"][-1].content})\n",
        "    state[\"messages\"].append(HumanMessage(content=response[\"output\"]))\n",
        "    return state\n",
        "\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    last_message = state[\"messages\"][-1].content\n",
        "    if \"FINAL RESPONSE:\" in last_message:\n",
        "        return \"end\"\n",
        "    return \"continue\""
      ],
      "metadata": {
        "id": "QdYvhNypsMKg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes\n",
        "workflow.add_node(\"retriever\", retriever_node)\n",
        "workflow.add_node(\"knowledge_graph\", knowledge_graph_node)\n",
        "workflow.add_node(\"threat_analysis\", threat_analysis_node)\n",
        "workflow.add_node(\"vulnerability_assessment\", vulnerability_assessment_node)\n",
        "workflow.add_node(\"security_recommendation\", security_recommendation_node)\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(\"retriever\", \"knowledge_graph\")\n",
        "workflow.add_edge(\"knowledge_graph\", \"threat_analysis\")\n",
        "workflow.add_edge(\"threat_analysis\", \"vulnerability_assessment\")\n",
        "workflow.add_edge(\"vulnerability_assessment\", \"security_recommendation\")\n",
        "workflow.add_edge(\"security_recommendation\", \"agent\")\n",
        "\n",
        "# Add conditional edges\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"retriever\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"retriever\")\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "cLLJOFOTSXW2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agent state\n",
        "initial_state = AgentState(\n",
        "    messages=[HumanMessage(content=\"What are the latest cybersecurity threats and vulnerabilities?\")],\n",
        "    kg=kg,\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Run the workflow\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "# Print the final response\n",
        "print(\"Final Response:\")\n",
        "print(final_state[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "nm0kRyBAswtT",
        "outputId": "af7799fd-5f1e-4e84-99cf-ae4e84452a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Must write to at least one of []",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-91b7e649bd12>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print the final response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0;31m# panic on failure or timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                     \u001b[0m_panic_or_proceed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minflight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                     \u001b[0;31m# don't keep futures around in memory longer than needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minflight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0minflight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;31m# raise the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minflight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/executor.py\u001b[0m in \u001b[0;36mdone\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGraphInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# This exception is an interruption signal, not an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m# if successful, end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             ret = self._call_with_config(\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             output = cast(\n\u001b[1;32m   1783\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1785\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m         ]\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# write packets and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mwrites\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, values, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire_at_least_one_of\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mchan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequire_at_least_one_of\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 raise InvalidUpdateError(\n\u001b[0m\u001b[1;32m    158\u001b[0m                     \u001b[0;34mf\"Must write to at least one of {require_at_least_one_of}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 )\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of []"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the knowledge graph\n",
        "kg.visualize(\"cybersecurity_knowledge_graph.html\")\n",
        "\n",
        "# Optional: Print additional information or analysis\n",
        "print(\"\\nKnowledge Graph Statistics:\")\n",
        "print(f\"Number of entities: {len(kg.graph.nodes)}\")\n",
        "print(f\"Number of relationships: {len(kg.graph.edges)}\")\n",
        "\n",
        "print(\"\\nMost connected entities:\")\n",
        "sorted_nodes = sorted(kg.graph.degree, key=lambda x: x[1], reverse=True)[:5]\n",
        "for node, degree in sorted_nodes:\n",
        "    print(f\"{node}: {degree} connections\")\n",
        "\n",
        "print(\"\\nSample relationships:\")\n",
        "for i, (node1, node2, data) in enumerate(kg.graph.edges(data=True)):\n",
        "    if i >= 5:  # Print only first 5 relationships\n",
        "        break\n",
        "    print(f\"{node1} is {data['relation']} {node2}\")\n",
        "\n",
        "# Optional: Save the collected data for future use\n",
        "import json\n",
        "\n",
        "with open(\"collected_data.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"scraped_content\": scraped_content,\n",
        "        \"tweets\": tweets\n",
        "    }, f)\n",
        "\n",
        "print(\"\\nData collection and analysis complete. Results saved to 'cybersecurity_knowledge_graph.html' and 'collected_data.json'.\")"
      ],
      "metadata": {
        "id": "eGTyVNkmsw_n",
        "outputId": "a3a4b556-095a-4c1d-b531-0167e4628b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "cybersecurity_knowledge_graph.html\n",
            "\n",
            "Knowledge Graph Statistics:\n",
            "Number of entities: 4154\n",
            "Number of relationships: 10070\n",
            "\n",
            "Most connected entities:\n",
            "Typhoon: 219 connections\n",
            "access: 157 connections\n",
            "network: 154 connections\n",
            "Phobos: 139 connections\n",
            "actors: 138 connections\n",
            "\n",
            "Sample relationships:\n",
            "Cybersecurity is related_to Alerts\n",
            "Cybersecurity is related_to Widget\n",
            "Cybersecurity is related_to Advisory:\n",
            "Cybersecurity is related_to religion.\"\n",
            "Cybersecurity is related_to Features\n",
            "\n",
            "Data collection and analysis complete. Results saved to 'cybersecurity_knowledge_graph.html' and 'collected_data.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SqH0AWsMsxBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hilJeoxsxEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}