{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/KG_Enhanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "#### Task\n",
        "Develop a co-pilot for threat researchers, security analysts, and professionals that addresses the limitations of current AI solutions like ChatGPT and Perplexity.\n",
        "\n",
        "#### Current Challenges\n",
        "1. **Generic Data**: Existing AI solutions provide generic information that lacks specificity.\n",
        "2. **Context Understanding**: These solutions fail to understand and maintain context.\n",
        "3. **Limited Information**: The data sources are often limited and not comprehensive.\n",
        "4. **Single Source Dependency**: Relying on a single source of information reduces reliability and accuracy.\n",
        "5. **Inadequate AI Models**: Current models do not meet the specialized needs of cybersecurity professionals.\n",
        "\n",
        "#### Requirement\n",
        "Create a chatbot capable of collecting and curating data from multiple sources, starting with search engines, and expanding to website crawling and Twitter scraping.\n",
        "\n",
        "#### Features Required\n",
        "\n",
        "##### User Interface (UI)\n",
        "- Chat UI with file upload capabilities.\n",
        "- Options to save and select prompts.\n",
        "- Configuration settings for connectors with enable/disable toggles.\n",
        "- Interface for configuring knowledge and variables (similar to Dify.ai).\n",
        "\n",
        "##### Technical Specifications\n",
        "- **No Hallucinations**: Ensure the chatbot provides accurate and reliable information.\n",
        "- **RAG (Retrieval-Augmented Generation)**: Use RAG to determine which connectors to use based on user inputs.\n",
        "- **Query Chunking and Distribution**: Optimize the process of breaking down queries and distributing them across different sources.\n",
        "- **Data Curation Steps**:\n",
        "  1. Collect links from approximately 50 sources.\n",
        "  2. Aggregate data from websites and Twitter.\n",
        "  3. Curate data using a knowledge graph to find relationships and generate responses.\n",
        "- **Chatbot Capabilities**: Answer queries such as:\n",
        "  - \"List all details on {{BFSI}} security incidents in {{India}}.\"\n",
        "  - \"List all ransomware attacks targeting the healthcare industry in {{last 7 days/last 3 months/last week/last month}}.\"\n",
        "  - \"Provide recent incidents related to Lockbit Ransomware gang / BlackBasta Ransomware.\"\n",
        "\n",
        "#### Source Tools\n",
        "\n",
        "##### Website Crawling and Scraping\n",
        "- [Firecrawl](https://www.firecrawl.dev/playground)\n",
        "- [Crawl4AI](https://github.com/unclecode/crawl4ai)\n",
        "- [Apify](https://apify.com/apify/website-content-crawler)\n",
        "- [Exa](https://exa.ai/search)\n",
        "\n",
        "##### Twitter Sources\n",
        "- [Apify Tweet Scraper](https://apify.com/apidojo/tweet-scraper)\n",
        "- [Twitter API](https://developer.x.com/en/docs/twitter-api)\n",
        "\n",
        "##### Development Tools\n",
        "- [Flowise AI](https://flowiseai.com/)\n",
        "- [Langgenius Dify](https://github.com/langgenius/dify)\n",
        "\n",
        "#### Goal\n",
        "Develop a data collector that integrates multiple specific sources to enrich the knowledge base, enabling the model to better understand context and deliver accurate results. The solution should be modular, allowing customization and configuration of sources.\n",
        "\n",
        "#### Summary\n",
        "The goal is to build an advanced, modular chatbot for cybersecurity professionals that overcomes the limitations of existing AI solutions by integrating multiple data sources and ensuring context-aware, accurate responses. The chatbot will utilize state-of-the-art techniques like RAG and knowledge graphs to provide comprehensive, curated information from diverse sources.\n"
      ],
      "metadata": {
        "id": "jBzZYldlbmJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NCq8zzvwSSdD",
        "outputId": "dc02c8dc-1cbe-4d80-979f-020e035f7e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "%pip install -qU langchain langchain-community faiss-cpu kuzu pyvis\n",
        "%pip install -qU sentence-transformers torch plotly pandas scikit-learn networkx\n",
        "%pip install -qU torch torchvision\n",
        "%pip install -qU langchain-groq apify-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import kuzu\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from apify_client import ApifyClient"
      ],
      "metadata": {
        "id": "wXnNIpzdS6jp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize Llama-3.1 from Meta using Groq LPU Inference\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "Gf9G1XucTdQs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})"
      ],
      "metadata": {
        "id": "EKlOL3pv6R_9",
        "outputId": "970177c6-d801-4e90-d810-1b4276dab143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Low latency is crucial for Large Language Models (LLMs) as it directly impacts the user experience, model performance, and overall efficiency. Here are some reasons why low latency is important for LLMs:\\n\\n1. **Real-time interactions**: LLMs are often used in applications that require real-time interactions, such as chatbots, virtual assistants, and language translation. Low latency ensures that the model responds quickly to user input, enabling a seamless and natural conversation flow.\\n2. **User experience**: High latency can lead to frustration and a poor user experience. When users have to wait too long for a response, they may abandon the application or lose interest. Low latency helps maintain user engagement and satisfaction.\\n3. **Conversational flow**: LLMs are designed to engage in conversations, and low latency is essential for maintaining a natural conversational flow. When the model responds quickly, users can respond in turn, creating a smooth and interactive dialogue.\\n4. **Model performance**: Low latency can also impact the performance of the LLM itself. When the model responds quickly, it can process more requests and handle a higher volume of conversations, leading to improved overall performance and efficiency.\\n5. **Resource utilization**: High latency can lead to inefficient resource utilization. When the model takes too long to respond, resources such as CPU, memory, and network bandwidth may be underutilized or wasted. Low latency helps optimize resource allocation and reduces waste.\\n6. **Scalability**: As LLMs are deployed in large-scale applications, low latency becomes increasingly important for scalability. When the model responds quickly, it can handle a larger volume of requests, making it easier to scale the application to meet growing demands.\\n7. **Edge computing**: With the increasing adoption of edge computing, low latency is critical for LLMs deployed at the edge. Edge computing requires fast processing and response times to minimize latency and ensure real-time interactions.\\n\\nTo achieve low latency in LLMs, developers and researchers employ various techniques, such as:\\n\\n1. **Model pruning**: Reducing the size of the model to decrease computational requirements.\\n2. **Knowledge distillation**: Transferring knowledge from a large model to a smaller one.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types.\\n4. **Parallel processing**: Utilizing multiple processing units or cores to speed up computations.\\n5. **Caching**: Storing frequently accessed data in memory to reduce access times.\\n6. **Optimized inference**: Using optimized inference algorithms and frameworks to reduce latency.\\n\\nBy prioritizing low latency, developers can create more efficient, scalable, and user-friendly LLM applications that provide a better experience for users.', response_metadata={'token_usage': {'completion_tokens': 544, 'prompt_tokens': 33, 'total_tokens': 577, 'completion_time': 2.176, 'prompt_time': 0.011767722, 'queue_time': None, 'total_time': 2.1877677220000002}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-e36e40a8-bfbc-46fc-8ba3-5bd11122a721-0', usage_metadata={'input_tokens': 33, 'output_tokens': 544, 'total_tokens': 577})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Kuzu DB\n",
        "db = kuzu.Database(\"cybersecurity_knowledge_graph\")\n",
        "conn = kuzu.Connection(db)\n",
        "\n",
        "# Create schema for the graph\n",
        "conn.execute(\"CREATE NODE TABLE Entity (name STRING, type STRING, PRIMARY KEY (name))\")\n",
        "conn.execute(\"CREATE REL TABLE Relation (FROM Entity TO Entity, predicate STRING)\")"
      ],
      "metadata": {
        "id": "0HGnCtuBTdWP",
        "outputId": "9063dae9-86a8-4363-a728-bbd266e12587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<kuzu.query_result.QueryResult at 0x7d7a3989c070>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cybersecurity-specific websites\n",
        "websites = [\n",
        "    \"https://www.cisa.gov/uscert/ncas/alerts\",\n",
        "    \"https://www.virustotal.com/gui/home/upload\",\n",
        "    \"https://attack.mitre.org/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "]"
      ],
      "metadata": {
        "id": "M3PWVNlqTdcj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape and load web content\n",
        "def scrape_websites(urls):\n",
        "    documents = []\n",
        "    for url in urls:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        documents.append(text)\n",
        "    return documents\n",
        "\n",
        "# Apify client setup\n",
        "APIFY_API_TOKEN = 'your_apify_api_token'\n",
        "apify_client = ApifyClient(APIFY_API_TOKEN)\n",
        "\n",
        "# Function to scrape tweets using Apify\n",
        "def scrape_tweets(query, max_results=100):\n",
        "    run_input = {\n",
        "        'searchQuery': query,\n",
        "        'maxResults': max_results\n",
        "    }\n",
        "\n",
        "    # Start a new actor task on Apify\n",
        "    run = apify_client.actor(\"apify/twitter-scraper\").call(run_input=run_input)\n",
        "\n",
        "    # Fetch the results from the run\n",
        "    tweets = run['data']\n",
        "\n",
        "    # Extract relevant text content from tweets\n",
        "    tweet_texts = [tweet['full_text'] for tweet in tweets]\n",
        "\n",
        "    return tweet_texts"
      ],
      "metadata": {
        "id": "1IzpQDCPpUN8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process documents\n",
        "documents = scrape_websites(websites)\n",
        "tweet_texts = scrape_tweets('cybersecurity')\n",
        "\n",
        "all_texts = documents + tweet_texts\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_text(\"\\n\\n\".join(all_texts))\n",
        "\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "I9NncGQxtIMY",
        "outputId": "947f7025-c187-4294-cf49-e87c8fcb3621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApifyApiError",
          "evalue": "User was not found or authentication token is not valid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApifyApiError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1404a708cc94>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and process documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_websites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cybersecurity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtweet_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-07c3c0708e4c>\u001b[0m in \u001b[0;36mscrape_tweets\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Start a new actor task on Apify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapify_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apify/twitter-scraper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Fetch the results from the run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, webhooks, wait_secs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         started_run = self.start(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, wait_for_finish, webhooks)\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         response = self.http_client.call(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, method, url, headers, params, data, json, stream, parse_response)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         return retry_with_exp_backoff(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0m_make_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_utils.py\u001b[0m in \u001b[0;36mretry_with_exp_backoff\u001b[0;34m(func, max_retries, backoff_base_millis, backoff_factor, random_factor)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_retrying\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mswallow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(stop_retrying, attempt)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status code is not retryable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'status_code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mstop_retrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         return retry_with_exp_backoff(\n",
            "\u001b[0;31mApifyApiError\u001b[0m: User was not found or authentication token is not valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge extraction and graph population\n",
        "kg_triple_extract_template = \"\"\"\n",
        "Extract up to 5 cybersecurity-related knowledge triplets from the text below in the form (subject, predicate, object).\n",
        "Focus on threats, vulnerabilities, attack techniques, and security measures.\n",
        "Text: {text}\n",
        "Triplets:\n",
        "\"\"\"\n",
        "kg_triple_extract_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=kg_triple_extract_template,\n",
        ")\n",
        "\n",
        "kg_triple_extract_chain = LLMChain(llm=llm, prompt=kg_triple_extract_prompt)\n",
        "\n",
        "for text in texts:\n",
        "    triplets = kg_triple_extract_chain.invoke({\"text\": text})\n",
        "    for triplet in triplets['text'].split('\\n'):\n",
        "        if triplet.strip():\n",
        "            try:\n",
        "                subject, predicate, obj = eval(triplet.strip())\n",
        "                conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [subject, \"Cybersecurity_Entity\"])\n",
        "                conn.execute(\"INSERT INTO Entity (name, type) VALUES (?, ?) ON CONFLICT DO NOTHING\", [obj, \"Cybersecurity_Entity\"])\n",
        "                conn.execute(\"INSERT INTO Relation VALUES (?, ?, ?)\", [subject, obj, predicate])\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process triplet: {triplet}. Error: {e}\")"
      ],
      "metadata": {
        "id": "v5jU8n8atV-b",
        "outputId": "f844f088-1da9-4ae8-b7e7-059503172deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApifyApiError",
          "evalue": "User was not found or authentication token is not valid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApifyApiError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4c606dfc4dac>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and process documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_websites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cybersecurity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6199c9356708>\u001b[0m in \u001b[0;36mscrape_tweets\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Start a new actor task on Apify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapify_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apify/twitter-scraper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Fetch the results from the run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, webhooks, wait_secs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         started_run = self.start(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mrun_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlog_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/clients/resource_clients/actor.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, wait_for_finish, webhooks)\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         response = self.http_client.call(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, method, url, headers, params, data, json, stream, parse_response)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         return retry_with_exp_backoff(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0m_make_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_utils.py\u001b[0m in \u001b[0;36mretry_with_exp_backoff\u001b[0;34m(func, max_retries, backoff_base_millis, backoff_factor, random_factor)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_retrying\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mswallow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/apify_client/_http_client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(stop_retrying, attempt)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status code is not retryable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'status_code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mstop_retrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApifyApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         return retry_with_exp_backoff(\n",
            "\u001b[0;31mApifyApiError\u001b[0m: User was not found or authentication token is not valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_data():\n",
        "    nodes_result = conn.execute(\"MATCH (e:Entity) RETURN e.name\")\n",
        "    edges_result = conn.execute(\"MATCH (e1:Entity)-[r:Relation]->(e2:Entity) RETURN e1.name, r.predicate, e2.name\")\n",
        "\n",
        "    nodes = [row.getString(0) for row in nodes_result]\n",
        "    edges = [(row.getString(0), row.getString(1), row.getString(2)) for row in edges_result]\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "# Enhanced graph visualization using Plotly\n",
        "def visualize_graph_plotly():\n",
        "    nodes, edges = get_graph_data()\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for node in nodes:\n",
        "        G.add_node(node)\n",
        "\n",
        "    for edge in edges:\n",
        "        G.add_edge(edge[0], edge[2], label=edge[1])\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in G.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines')\n",
        "\n",
        "    node_x = [pos[node][0] for node in G.nodes()]\n",
        "    node_y = [pos[node][1]]\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        marker=dict(\n",
        "            showscale=True,\n",
        "            colorscale='YlGnBu',\n",
        "            reversescale=True,\n",
        "            color=[],\n",
        "            size=10,\n",
        "            colorbar=dict(\n",
        "                thickness=15,\n",
        "                title='Node Connections',\n",
        "                xanchor='left',\n",
        "                titleside='right'\n",
        "            ),\n",
        "            line_width=2))\n",
        "\n",
        "    node_adjacencies = []\n",
        "    node_text = []\n",
        "    for node, adjacencies in G.adjacency():\n",
        "        node_adjacencies.append(len(adjacencies))\n",
        "        node_text.append(f'{node}# of connections: {len(adjacencies)}')\n",
        "\n",
        "    node_trace.marker.color = node_adjacencies\n",
        "    node_trace.text = node_text\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title='Knowledge Graph',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[ dict(\n",
        "                            text=\"\",\n",
        "                            showarrow=False,\n",
        "                            xref=\"paper\", yref=\"paper\",\n",
        "                            x=0.005, y=-0.002 ) ],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# Embedding visualization\n",
        "def visualize_embeddings():\n",
        "    # Get embeddings\n",
        "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
        "\n",
        "    # Reduce dimensionality for visualization\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    tsne_results = tsne.fit_transform(doc_embeddings)\n",
        "\n",
        "    # Create a DataFrame for Plotly\n",
        "    df = pd.DataFrame(tsne_results, columns=['x', 'y'])\n",
        "\n",
        "    fig = px.scatter(df, x='x', y='y', title='Document Embeddings Visualization')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "iqzsy5ckta_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example queries\n",
        "questions = [\n",
        "    \"What are the latest threats targeting the healthcare industry?\",\n",
        "    \"Can you provide details on recent ransomware attacks?\",\n",
        "    \"What are the most critical vulnerabilities discovered in the last month?\",\n",
        "    \"How can organizations protect against phishing attacks?\",\n",
        "    \"What are the emerging trends in cybersecurity for financial institutions?\"\n",
        "]"
      ],
      "metadata": {
        "id": "qAJZIObetgOE",
        "outputId": "a0c38a19-5ff8-420c-954e-a0d784eb40db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6bf09ae1e1eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the knowledge graph and visualize\n",
        "def query_graph(query):\n",
        "    return chain.invoke({\"text\": query})\n",
        "\n",
        "for query in questions:\n",
        "    answer = query_graph(query)\n",
        "    print(f\"Query: {query}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "# Visualize the graph and embeddings\n",
        "visualize_graph_plotly()\n",
        "visualize_embeddings()"
      ],
      "metadata": {
        "id": "aAgfogCoqNES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nht_Cjonp4Wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}