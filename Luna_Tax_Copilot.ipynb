{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Luna_Tax_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Luna Tax Copilot Implementation**"
      ],
      "metadata": {
        "id": "AhyFaRkR9PD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i_d39VaR9LNO",
        "outputId": "0de14dfd-b434-448f-ca49-fef463f8ae28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community langgraph openai chromadb neo4j python-dotenv pandas numpy pypdf docx2txt openpyxl\n",
        "!pip install -q langchain_neo4j\n",
        "!pip install -qU crawl4ai\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# LangGraph imports for agent orchestration\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Required environment variables\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "# os.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\n",
        "# os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "# os.environ[\"NEO4J_PASSWORD\"] = \"password\""
      ],
      "metadata": {
        "id": "JonjTdZA-YxZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n",
        "\n",
        "async def crawl_ato_legal_database(max_pages=100):\n",
        "    config = CrawlerRunConfig(\n",
        "        deep_crawl_strategy=BFSDeepCrawlStrategy(\n",
        "            max_depth=3,\n",
        "            include_external=False\n",
        "        ),\n",
        "        scraping_strategy=LXMLWebScrapingStrategy(),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    start_url = \"https://www.ato.gov.au/single-page-applications/legaldatabase#Law\"\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        results = []\n",
        "        try:\n",
        "            async for page in crawler.acrawl(start_url, config=config):\n",
        "                results.append(page)\n",
        "                print(f\"\\nCrawled Page {len(results)}: {page.url}\")\n",
        "                print(f\"Sample Content (First 300 chars):\\n{page.content[:300]}...\\n\")\n",
        "\n",
        "                # Save page content\n",
        "                with open(f\"page_{len(results)}.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(page.content)\n",
        "\n",
        "                if len(results) >= max_pages:\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during crawling: {str(e)}\")\n",
        "\n",
        "await crawl_ato_legal_database()"
      ],
      "metadata": {
        "id": "-00x2yVB396O",
        "outputId": "e14dafa9-2881-4d58-a91f-68f7d1f74540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TargetClosedError",
          "evalue": "BrowserType.launch: Target page, context or browser has been closed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTargetClosedError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-575f961e9ab2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error during crawling: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mcrawl_ato_legal_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-575f961e9ab2>\u001b[0m in \u001b[0;36mcrawl_ato_legal_database\u001b[0;34m(max_pages)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstart_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.ato.gov.au/single-page-applications/legaldatabase#Law\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAsyncWebCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crawl4ai/async_webcrawler.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aexit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crawl4ai/async_webcrawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_managed_browser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawler_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawarmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crawl4ai/async_crawler_strategy.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crawl4ai/async_crawler_strategy.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minitialize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         await self.execute_hook(\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m\"on_browser_created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crawl4ai/browser_manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaywright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbrowser_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaywright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchromium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbrowser_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/async_api/_generated.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, executable_path, channel, args, ignore_default_args, handle_sigint, handle_sigterm, handle_sighup, timeout, env, headless, devtools, proxy, downloads_path, slow_mo, traces_dir, chromium_sandbox, firefox_user_prefs)\u001b[0m\n\u001b[1;32m  14448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14449\u001b[0m         return mapping.from_impl(\n\u001b[0;32m> 14450\u001b[0;31m             await self._impl_obj.launch(\n\u001b[0m\u001b[1;32m  14451\u001b[0m                 \u001b[0mexecutablePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14452\u001b[0m                 \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_browser_type.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, executablePath, channel, args, ignoreDefaultArgs, handleSIGINT, handleSIGTERM, handleSIGHUP, timeout, env, headless, devtools, proxy, downloadsPath, slowMo, tracesDir, chromiumSandbox, firefoxUserPrefs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mnormalize_launch_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         browser = cast(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mBrowser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"launch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_did_launch_browser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         return await self._connection.wrap_api_call(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_internal_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py\u001b[0m in \u001b[0;36mwrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mrewrite_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{parsed_st['apiName']}: {error}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_zone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTargetClosedError\u001b[0m: BrowserType.launch: Target page, context or browser has been closed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model\n",
        "def get_llm(model_name=\"gpt-4o\", temperature=0):\n",
        "    \"\"\"Initialize the language model\"\"\"\n",
        "    return ChatOpenAI(model=model_name, temperature=temperature)"
      ],
      "metadata": {
        "id": "5PPq0N_W_AQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- DATA PROCESSING COMPONENTS -----\n",
        "\n",
        "def init_data_crawler(ato_db_connection_string=None):\n",
        "    \"\"\"\n",
        "    Initialize the data crawler that fetches data from the ATO database\n",
        "\n",
        "    Args:\n",
        "        ato_db_connection_string: Connection string to the ATO database\n",
        "\n",
        "    Returns:\n",
        "        Function that crawls data from the ATO database\n",
        "    \"\"\"\n",
        "    def crawl_data(query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Crawl data from the ATO database based on the query\n",
        "\n",
        "        Args:\n",
        "            query: Query to fetch data from the ATO database\n",
        "\n",
        "        Returns:\n",
        "            List of documents retrieved from the ATO database\n",
        "        \"\"\"\n",
        "        # This would be implemented with actual database connection code\n",
        "        # Here we're simulating retrieval with sample data\n",
        "        print(f\"Crawling data with query: {query}\")\n",
        "\n",
        "        # Simulated data retrieval\n",
        "        sample_data = [\n",
        "            {\"id\": \"1\", \"title\": \"Income Tax Assessment\", \"content\": \"Guidelines for assessing taxable income...\"},\n",
        "            {\"id\": \"2\", \"title\": \"GST Regulations\", \"content\": \"Goods and Services Tax regulations and exemptions...\"},\n",
        "            {\"id\": \"3\", \"title\": \"Tax Deductions\", \"content\": \"Eligible deductions for various business expenses...\"},\n",
        "        ]\n",
        "\n",
        "        return sample_data\n",
        "\n",
        "    return crawl_data\n",
        "\n",
        "def init_data_cleaner():\n",
        "    \"\"\"\n",
        "    Initialize the data cleaner that preprocesses crawled data\n",
        "\n",
        "    Returns:\n",
        "        Function that cleans raw data\n",
        "    \"\"\"\n",
        "    def clean_data(raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Clean raw data by removing irrelevant information, standardizing formats\n",
        "\n",
        "        Args:\n",
        "            raw_data: Raw data from the crawler\n",
        "\n",
        "        Returns:\n",
        "            Cleaned data\n",
        "        \"\"\"\n",
        "        cleaned_data = []\n",
        "\n",
        "        for item in raw_data:\n",
        "            # Implement cleaning logic: remove HTML, standardize dates, etc.\n",
        "            cleaned_item = {\n",
        "                \"id\": item[\"id\"],\n",
        "                \"title\": item[\"title\"],\n",
        "                \"content\": item[\"content\"].replace(\"...\", \"\"),  # Simple cleaning example\n",
        "                \"cleaned_timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "            cleaned_data.append(cleaned_item)\n",
        "\n",
        "        return cleaned_data\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "def init_data_chunker():\n",
        "    \"\"\"\n",
        "    Initialize the data chunker that splits documents into chunks for embedding\n",
        "\n",
        "    Returns:\n",
        "        Function that chunks cleaned data\n",
        "    \"\"\"\n",
        "    def chunk_data(cleaned_data: List[Dict[str, Any]]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Chunk cleaned data into smaller pieces for processing\n",
        "\n",
        "        Args:\n",
        "            cleaned_data: Cleaned data from the cleaner\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks\n",
        "        \"\"\"\n",
        "        # Initialize text splitter for chunking\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        document_chunks = []\n",
        "\n",
        "        for item in cleaned_data:\n",
        "            # Create a Document object\n",
        "            doc = Document(\n",
        "                page_content=item[\"content\"],\n",
        "                metadata={\n",
        "                    \"id\": item[\"id\"],\n",
        "                    \"title\": item[\"title\"],\n",
        "                    \"source\": \"ATO Database\",\n",
        "                    \"timestamp\": item.get(\"cleaned_timestamp\")\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Split the document into chunks\n",
        "            chunks = text_splitter.split_documents([doc])\n",
        "            document_chunks.extend(chunks)\n",
        "\n",
        "        return document_chunks\n",
        "\n",
        "    return chunk_data\n",
        "\n",
        "def init_data_encoder():\n",
        "    \"\"\"\n",
        "    Initialize the data encoder that creates embeddings from document chunks\n",
        "\n",
        "    Returns:\n",
        "        Function that encodes document chunks into embeddings\n",
        "    \"\"\"\n",
        "    def encode_data(document_chunks: List[Document]) -> Tuple[List[Document], List[List[float]]]:\n",
        "        \"\"\"\n",
        "        Encode document chunks into embeddings\n",
        "\n",
        "        Args:\n",
        "            document_chunks: Document chunks from the chunker\n",
        "\n",
        "        Returns:\n",
        "            Tuple of document chunks and their embeddings\n",
        "        \"\"\"\n",
        "        # Initialize embeddings\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        # Create embeddings for each document chunk\n",
        "        texts = [doc.page_content for doc in document_chunks]\n",
        "        embedded_vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "        return document_chunks, embedded_vectors\n",
        "\n",
        "    return encode_data"
      ],
      "metadata": {
        "id": "D-9M_Dk7AhhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- DATABASE COMPONENTS -----\n",
        "\n",
        "def init_vector_database():\n",
        "    \"\"\"\n",
        "    Initialize the vector database for storing embeddings\n",
        "\n",
        "    Returns:\n",
        "        Vector database instance and functions to interact with it\n",
        "    \"\"\"\n",
        "    # Initialize vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vector_db = Chroma(embedding_function=embeddings, collection_name=\"tax_regulations\")\n",
        "\n",
        "    def store_in_vector_db(documents: List[Document], embeddings: List[List[float]]) -> None:\n",
        "        \"\"\"Store documents and their embeddings in the vector database\"\"\"\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        metadatas = [doc.metadata for doc in documents]\n",
        "\n",
        "        # Add documents to the vector store\n",
        "        vector_db.add_texts(texts=texts, metadatas=metadatas, embeddings=embeddings)\n",
        "        print(f\"Stored {len(documents)} documents in the vector database\")\n",
        "\n",
        "    def search_vector_db(query: str, top_k: int = 5) -> List[Document]:\n",
        "        \"\"\"Search the vector database for relevant documents\"\"\"\n",
        "        results = vector_db.similarity_search(query, k=top_k)\n",
        "        return results\n",
        "\n",
        "    return {\n",
        "        \"vector_db\": vector_db,\n",
        "        \"store\": store_in_vector_db,\n",
        "        \"search\": search_vector_db\n",
        "    }\n",
        "\n",
        "def init_relationship_database():\n",
        "    \"\"\"\n",
        "    Initialize the graph database for storing relationships\n",
        "\n",
        "    Returns:\n",
        "        Graph database instance and functions to interact with it\n",
        "    \"\"\"\n",
        "    # Initialize Neo4j graph store\n",
        "    graph_db = Neo4j(\n",
        "        url=os.environ.get(\"NEO4J_URI\", \"neo4j://localhost:7687\"),\n",
        "        username=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n",
        "        password=os.environ.get(\"NEO4J_PASSWORD\", \"password\")\n",
        "    )\n",
        "\n",
        "    def store_relationship(source_id: str, target_id: str, relationship_type: str, properties: Dict = None) -> None:\n",
        "        \"\"\"Store a relationship in the graph database\"\"\"\n",
        "        if properties is None:\n",
        "            properties = {}\n",
        "\n",
        "        # Create Cypher query to add relationship\n",
        "        query = f\"\"\"\n",
        "        MATCH (source) WHERE source.id = $source_id\n",
        "        MATCH (target) WHERE target.id = $target_id\n",
        "        CREATE (source)-[r:{relationship_type} $properties]->(target)\n",
        "        RETURN source, r, target\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            \"source_id\": source_id,\n",
        "            \"target_id\": target_id,\n",
        "            \"properties\": properties\n",
        "        }\n",
        "\n",
        "        graph_db.query(query, params)\n",
        "\n",
        "    def store_document_node(document: Document) -> None:\n",
        "        \"\"\"Store a document as a node in the graph database\"\"\"\n",
        "        # Create Cypher query to add node\n",
        "        query = \"\"\"\n",
        "        CREATE (d:Document {\n",
        "            id: $id,\n",
        "            title: $title,\n",
        "            content: $content,\n",
        "            source: $source\n",
        "        })\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            \"id\": document.metadata.get(\"id\"),\n",
        "            \"title\": document.metadata.get(\"title\"),\n",
        "            \"content\": document.page_content,\n",
        "            \"source\": document.metadata.get(\"source\")\n",
        "        }\n",
        "\n",
        "        graph_db.query(query, params)\n",
        "\n",
        "    def query_graph(cypher_query: str, params: Dict = None) -> List[Dict]:\n",
        "        \"\"\"Query the graph database with a Cypher query\"\"\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "\n",
        "        results = graph_db.query(cypher_query, params)\n",
        "        return results\n",
        "\n",
        "    return {\n",
        "        \"graph_db\": graph_db,\n",
        "        \"store_relationship\": store_relationship,\n",
        "        \"store_node\": store_document_node,\n",
        "        \"query\": query_graph\n",
        "    }"
      ],
      "metadata": {
        "id": "wh8mR5icAnpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- AGENT COMPONENTS -----\n",
        "\n",
        "def init_context_engine(vector_db_search):\n",
        "    \"\"\"\n",
        "    Initialize the context engine that processes prompts and provides context\n",
        "\n",
        "    Args:\n",
        "        vector_db_search: Function to search the vector database\n",
        "\n",
        "    Returns:\n",
        "        Function that processes prompts and provides context\n",
        "    \"\"\"\n",
        "    def process_prompt(prompt: str, goal: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process the prompt, extract context, and prepare for optimization\n",
        "\n",
        "        Args:\n",
        "            prompt: User prompt\n",
        "            goal: Optional goal to guide the context engine\n",
        "\n",
        "        Returns:\n",
        "            Context information for the prompt optimizer\n",
        "        \"\"\"\n",
        "        # Retrieve relevant documents from vector DB\n",
        "        relevant_docs = vector_db_search(prompt, top_k=3)\n",
        "\n",
        "        # Extract and format context from retrieved documents\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "        return {\n",
        "            \"original_prompt\": prompt,\n",
        "            \"goal\": goal,\n",
        "            \"retrieved_context\": context,\n",
        "            \"document_ids\": [doc.metadata.get(\"id\") for doc in relevant_docs]\n",
        "        }\n",
        "\n",
        "    return process_prompt\n",
        "\n",
        "def init_prompt_optimizer(llm):\n",
        "    \"\"\"\n",
        "    Initialize the prompt optimizer that enhances prompts with context\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that optimizes prompts\n",
        "    \"\"\"\n",
        "    # Prompt template for optimizing user prompts\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a Tax Copilot that helps tax professionals.\n",
        "\n",
        "        Original prompt: {original_prompt}\n",
        "        Goal: {goal}\n",
        "\n",
        "        Relevant context from tax regulations:\n",
        "        {retrieved_context}\n",
        "\n",
        "        Rewrite the prompt to be more specific and include relevant tax regulations from the context.\n",
        "        Modified prompt:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create chain for prompt optimization\n",
        "    prompt_chain = (\n",
        "        prompt_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def optimize_prompt(context_info: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Optimize the prompt based on context and goal\n",
        "\n",
        "        Args:\n",
        "            context_info: Context information from the context engine\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with original and optimized prompts\n",
        "        \"\"\"\n",
        "        # Run the prompt through the chain\n",
        "        optimized_prompt = prompt_chain.invoke(context_info)\n",
        "\n",
        "        return {\n",
        "            \"original_prompt\": context_info[\"original_prompt\"],\n",
        "            \"optimized_prompt\": optimized_prompt,\n",
        "            \"retrieved_context\": context_info[\"retrieved_context\"],\n",
        "            \"document_ids\": context_info[\"document_ids\"]\n",
        "        }\n",
        "\n",
        "    return optimize_prompt\n",
        "\n",
        "def init_retriever(vector_db_search, graph_db_query):\n",
        "    \"\"\"\n",
        "    Initialize the retriever that fetches information from databases\n",
        "\n",
        "    Args:\n",
        "        vector_db_search: Function to search the vector database\n",
        "        graph_db_query: Function to query the graph database\n",
        "\n",
        "    Returns:\n",
        "        Function that retrieves information\n",
        "    \"\"\"\n",
        "    def retrieve_information(query: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Retrieve information from both vector and graph databases\n",
        "\n",
        "        Args:\n",
        "            query: Query to search for\n",
        "\n",
        "        Returns:\n",
        "            Retrieved information from both databases\n",
        "        \"\"\"\n",
        "        # Retrieve from vector DB\n",
        "        vector_results = vector_db_search(query, top_k=5)\n",
        "\n",
        "        # Retrieve from graph DB\n",
        "        # Look for related tax regulations via graph relationships\n",
        "        graph_query = \"\"\"\n",
        "        MATCH (d:Document)-[r]-(related)\n",
        "        WHERE d.title CONTAINS $keyword OR d.content CONTAINS $keyword\n",
        "        RETURN related.title, related.content, type(r) as relationship\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "        graph_results = graph_db_query(graph_query, {\"keyword\": query})\n",
        "\n",
        "        # Combine results\n",
        "        return {\n",
        "            \"vector_results\": vector_results,\n",
        "            \"graph_results\": graph_results,\n",
        "            \"query\": query\n",
        "        }\n",
        "\n",
        "    return retrieve_information\n",
        "\n",
        "def init_reranker(llm):\n",
        "    \"\"\"\n",
        "    Initialize the reranker that prioritizes retrieved information\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that reranks retrieved information\n",
        "    \"\"\"\n",
        "    # Create reranking prompt\n",
        "    rerank_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        You need to rank the following documents based on their relevance to the query: {query}\n",
        "\n",
        "        Documents:\n",
        "        {documents}\n",
        "\n",
        "        Return a JSON list of document indices in order of relevance (most relevant first),\n",
        "        with a brief explanation for each ranking.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    rerank_chain = (\n",
        "        rerank_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def rerank_results(retrieval_results: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Rerank retrieved results based on relevance to query\n",
        "\n",
        "        Args:\n",
        "            retrieval_results: Results from the retriever\n",
        "\n",
        "        Returns:\n",
        "            Reranked results\n",
        "        \"\"\"\n",
        "        # Format documents for reranking\n",
        "        vector_docs = \"\\n\\n\".join([\n",
        "            f\"Document {i}: {doc.page_content}\"\n",
        "            for i, doc in enumerate(retrieval_results[\"vector_results\"])\n",
        "        ])\n",
        "\n",
        "        # Rerank using LLM\n",
        "        rerank_result = rerank_chain.invoke({\n",
        "            \"query\": retrieval_results[\"query\"],\n",
        "            \"documents\": vector_docs\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"original_results\": retrieval_results,\n",
        "            \"reranked_results\": rerank_result\n",
        "        }\n",
        "\n",
        "    return rerank_results\n",
        "\n",
        "def init_reasoning_and_planning(llm):\n",
        "    \"\"\"\n",
        "    Initialize the reasoning and planning module\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that creates workflows\n",
        "    \"\"\"\n",
        "    # Create reasoning prompt\n",
        "    reasoning_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Based on the following query and available information, create a workflow plan\n",
        "        to help the tax professional.\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Relevant information:\n",
        "        {context}\n",
        "\n",
        "        Create a step-by-step workflow plan to address this tax query. Include:\n",
        "        1. Key tax regulations to consider\n",
        "        2. Analysis steps\n",
        "        3. Documentation needed\n",
        "        4. Potential advice or recommendations\n",
        "\n",
        "        Workflow plan:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    reasoning_chain = (\n",
        "        reasoning_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def create_workflow(reranked_results: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Create a workflow plan based on the query and retrieved information\n",
        "\n",
        "        Args:\n",
        "            reranked_results: Reranked results from the reranker\n",
        "\n",
        "        Returns:\n",
        "            Workflow plan\n",
        "        \"\"\"\n",
        "        # Extract context from reranked results\n",
        "        query = reranked_results[\"original_results\"][\"query\"]\n",
        "\n",
        "        # Use vector results as context\n",
        "        context = \"\\n\\n\".join([\n",
        "            doc.page_content\n",
        "            for doc in reranked_results[\"original_results\"][\"vector_results\"]\n",
        "        ])\n",
        "\n",
        "        # Generate workflow\n",
        "        workflow_plan = reasoning_chain.invoke({\n",
        "            \"query\": query,\n",
        "            \"context\": context\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"workflow_plan\": workflow_plan,\n",
        "            \"context\": context\n",
        "        }\n",
        "\n",
        "    return create_workflow\n",
        "\n",
        "def init_workflow_execution(llm, graph_db_store_relationship):\n",
        "    \"\"\"\n",
        "    Initialize the workflow execution module\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "        graph_db_store_relationship: Function to store relationships in the graph database\n",
        "\n",
        "    Returns:\n",
        "        Function that executes workflows\n",
        "    \"\"\"\n",
        "    # Create execution prompt\n",
        "    execution_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Execute the following workflow plan to provide a detailed response to the tax query:\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Workflow plan:\n",
        "        {workflow_plan}\n",
        "\n",
        "        Context information:\n",
        "        {context}\n",
        "\n",
        "        Generate a comprehensive response that follows the workflow steps and addresses the query.\n",
        "        Include specific tax regulations, guidelines, and actionable advice.\n",
        "\n",
        "        Response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    execution_chain = (\n",
        "        execution_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def execute_workflow(workflow_info: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Execute a workflow plan to generate a response\n",
        "\n",
        "        Args:\n",
        "            workflow_info: Workflow plan and context\n",
        "\n",
        "        Returns:\n",
        "            Response from workflow execution\n",
        "        \"\"\"\n",
        "        # Execute workflow\n",
        "        response = execution_chain.invoke({\n",
        "            \"query\": workflow_info[\"query\"],\n",
        "            \"workflow_plan\": workflow_info[\"workflow_plan\"],\n",
        "            \"context\": workflow_info[\"context\"]\n",
        "        })\n",
        "\n",
        "        # Store workflow execution in graph DB as a relationship\n",
        "        # Between query and response (in a real system)\n",
        "        # This is just a placeholder for demonstration\n",
        "        try:\n",
        "            graph_db_store_relationship(\n",
        "                \"query_node\",  # This would be a real node id in production\n",
        "                \"response_node\",  # This would be a real node id in production\n",
        "                \"GENERATED\",\n",
        "                {\"timestamp\": datetime.now().isoformat()}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not store relationship in graph DB: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"query\": workflow_info[\"query\"],\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "    return execute_workflow\n",
        "\n",
        "def init_generator(llm):\n",
        "    \"\"\"\n",
        "    Initialize the generator that creates the final response\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that generates final responses\n",
        "    \"\"\"\n",
        "    # Create generator prompt\n",
        "    generator_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Format the following response to be clear, professional, and actionable for a tax professional:\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Response content:\n",
        "        {response}\n",
        "\n",
        "        Format this as a professional tax advisory response with proper headings, bullet points where appropriate,\n",
        "        and clear actionable steps.\n",
        "\n",
        "        Final response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    generator_chain = (\n",
        "        generator_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def generate_response(execution_result: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Generate the final response\n",
        "\n",
        "        Args:\n",
        "            execution_result: Result from workflow execution\n",
        "\n",
        "        Returns:\n",
        "            Formatted final response\n",
        "        \"\"\"\n",
        "        # Generate final response\n",
        "        final_response = generator_chain.invoke({\n",
        "            \"query\": execution_result[\"query\"],\n",
        "            \"response\": execution_result[\"response\"]\n",
        "        })\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    return generate_response\n",
        "\n",
        "def init_guardrails(llm):\n",
        "    \"\"\"\n",
        "    Initialize the guardrails that ensure response safety and quality\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that applies guardrails to responses\n",
        "    \"\"\"\n",
        "    # Create guardrail prompt\n",
        "    guardrail_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax compliance expert.\n",
        "\n",
        "        Review the following response to ensure it:\n",
        "        1. Only provides factually accurate tax information\n",
        "        2. Doesn't give absolute tax advice without caveats\n",
        "        3. Recommends consulting a tax professional for specific situations\n",
        "        4. Doesn't contain outdated tax regulations\n",
        "        5. Makes clear distinctions between federal and state/local tax matters\n",
        "\n",
        "        Response to review:\n",
        "        {response}\n",
        "\n",
        "        If the response meets all criteria, return it unchanged.\n",
        "        If it needs revision, provide the corrected version that addresses the issues.\n",
        "\n",
        "        Reviewed response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    guardrail_chain = (\n",
        "        guardrail_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def apply_guardrails(response: str) -> str:\n",
        "        \"\"\"\n",
        "        Apply guardrails to ensure response quality and safety\n",
        "\n",
        "        Args:\n",
        "            response: Generated response\n",
        "\n",
        "        Returns:\n",
        "            Response after applying guardrails\n",
        "        \"\"\"\n",
        "        # Apply guardrails\n",
        "        safe_response = guardrail_chain.invoke({\"response\": response})\n",
        "\n",
        "        return safe_response\n",
        "\n",
        "    return apply_guardrails"
      ],
      "metadata": {
        "id": "PTYkx0aXBEsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- LANGGRAPH STATE AND WORKFLOW -----\n",
        "\n",
        "def create_tax_copilot_graph():\n",
        "    \"\"\"\n",
        "    Create the LangGraph for the Tax Copilot pipeline\n",
        "\n",
        "    Returns:\n",
        "        Configured StateGraph for the Tax Copilot\n",
        "    \"\"\"\n",
        "    # Initialize the language model\n",
        "    llm = get_llm()\n",
        "\n",
        "    # Initialize data processing components\n",
        "    data_crawler = init_data_crawler()\n",
        "    data_cleaner = init_data_cleaner()\n",
        "    data_chunker = init_data_chunker()\n",
        "    data_encoder = init_data_encoder()\n",
        "\n",
        "    # Initialize databases\n",
        "    vector_db = init_vector_database()\n",
        "    graph_db = init_relationship_database()\n",
        "\n",
        "    # Initialize agent components\n",
        "    context_engine = init_context_engine(vector_db[\"search\"])\n",
        "    prompt_optimizer = init_prompt_optimizer(llm)\n",
        "    retriever = init_retriever(vector_db[\"search\"], graph_db[\"query\"])\n",
        "    reranker = init_reranker(llm)\n",
        "    reasoning_and_planning = init_reasoning_and_planning(llm)\n",
        "    workflow_execution = init_workflow_execution(llm, graph_db[\"store_relationship\"])\n",
        "    generator = init_generator(llm)\n",
        "    guardrails = init_guardrails(llm)\n",
        "\n",
        "    # Define the state\n",
        "    class State:\n",
        "        \"\"\"State for the Tax Copilot pipeline\"\"\"\n",
        "        prompt: str\n",
        "        goal: str = None\n",
        "        context_info: Dict = None\n",
        "        optimized_prompt: Dict = None\n",
        "        retrieval_results: Dict = None\n",
        "        reranked_results: Dict = None\n",
        "        workflow_plan: Dict = None\n",
        "        execution_result: Dict = None\n",
        "        generated_response: str = None\n",
        "        final_response: str = None\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(State)\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    workflow.add_node(\"context_engine\", lambda state: {\"context_info\": context_engine(state.prompt, state.goal)})\n",
        "    workflow.add_node(\"prompt_optimizer\", lambda state: {\"optimized_prompt\": prompt_optimizer(state.context_info)})\n",
        "    workflow.add_node(\"retriever\", lambda state: {\"retrieval_results\": retriever(state.optimized_prompt[\"optimized_prompt\"])})\n",
        "    workflow.add_node(\"reranker\", lambda state: {\"reranked_results\": reranker(state.retrieval_results)})\n",
        "    workflow.add_node(\"reasoning_and_planning\", lambda state: {\"workflow_plan\": reasoning_and_planning(state.reranked_results)})\n",
        "    workflow.add_node(\"workflow_execution\", lambda state: {\"execution_result\": workflow_execution(state.workflow_plan)})\n",
        "    workflow.add_node(\"generator\", lambda state: {\"generated_response\": generator(state.execution_result)})\n",
        "    workflow.add_node(\"guardrails\", lambda state: {\"final_response\": guardrails(state.generated_response)})\n",
        "\n",
        "    # Add edges to the graph\n",
        "    workflow.add_edge(\"context_engine\", \"prompt_optimizer\")\n",
        "    workflow.add_edge(\"prompt_optimizer\", \"retriever\")\n",
        "    workflow.add_edge(\"retriever\", \"reranker\")\n",
        "    workflow.add_edge(\"reranker\", \"reasoning_and_planning\")\n",
        "    workflow.add_edge(\"reasoning_and_planning\", \"workflow_execution\")\n",
        "    workflow.add_edge(\"workflow_execution\", \"generator\")\n",
        "    workflow.add_edge(\"generator\", \"guardrails\")\n",
        "    workflow.add_edge(\"guardrails\", END)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"context_engine\")\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "d_czVZhfBQHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- MAIN FUNCTION TO INITIALIZE THE SYSTEM -----\n",
        "\n",
        "def setup_tax_copilot_system():\n",
        "    \"\"\"\n",
        "    Set up the Tax Copilot system with data processing and agent components\n",
        "\n",
        "    Returns:\n",
        "        Initialized Tax Copilot system\n",
        "    \"\"\"\n",
        "    # Initialize LLM\n",
        "    llm = get_llm()\n",
        "\n",
        "    # Initialize databases\n",
        "    vector_db = init_vector_database()\n",
        "    graph_db = init_relationship_database()\n",
        "\n",
        "    # Set up data processing pipeline\n",
        "    data_crawler = init_data_crawler()\n",
        "    data_cleaner = init_data_cleaner()\n",
        "    data_chunker = init_data_chunker()\n",
        "    data_encoder = init_data_encoder()\n",
        "\n",
        "    # Process initial data (in a real system, this would fetch from ATO database)\n",
        "    raw_data = data_crawler(\"initial tax regulations\")\n",
        "    cleaned_data = data_cleaner(raw_data)\n",
        "    document_chunks = data_chunker(cleaned_data)\n",
        "    documents, embeddings = data_encoder(document_chunks)\n",
        "\n",
        "    # Store processed data in databases\n",
        "    vector_db[\"store\"](documents, embeddings)\n",
        "\n",
        "    # Store documents in graph database\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            graph_db[\"store_node\"](doc)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not store document in graph DB: {e}\")\n",
        "\n",
        "    # Create sample relationships (in a real system, these would be derived from the data)\n",
        "    try:\n",
        "        graph_db[\"store_relationship\"](\"1\", \"2\", \"RELATES_TO\", {\"type\": \"reference\"})\n",
        "        graph_db[\"store_relationship\"](\"2\", \"3\", \"SUPPLEMENTS\", {\"section\": \"deductions\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not store relationships in graph DB: {e}\")\n",
        "\n",
        "    # Create the LangGraph for the Tax Copilot\n",
        "    tax_copilot_graph = create_tax_copilot_graph()\n",
        "\n",
        "    return tax_copilot_graph\n",
        "\n",
        "def run_tax_copilot(graph, query, goal=None):\n",
        "    \"\"\"\n",
        "    Run the Tax Copilot with a user query\n",
        "\n",
        "    Args:\n",
        "        graph: Compiled StateGraph for the Tax Copilot\n",
        "        query: User query\n",
        "        goal: Optional goal to guide the context engine\n",
        "\n",
        "    Returns:\n",
        "        Final response\n",
        "    \"\"\"\n",
        "    # Create initial state\n",
        "    initial_state = {\"prompt\": query, \"goal\": goal}\n",
        "\n",
        "    # Run the graph\n",
        "    result = graph.invoke(initial_state)\n",
        "\n",
        "    return result[\"final_response\"]\n",
        "\n",
        "# ----- EXAMPLE USAGE -----\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the Tax Copilot system\"\"\"\n",
        "    # Setup the system\n",
        "    print(\"Setting up Tax Copilot system...\")\n",
        "    tax_copilot = setup_tax_copilot_system()\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What are the deduction limits for business expenses?\",\n",
        "        \"How do I report foreign income on my tax return?\",\n",
        "        \"Can I claim home office expenses if I'm working remotely?\"\n",
        "    ]\n",
        "\n",
        "    # Run the queries\n",
        "    for query in queries:\n",
        "        print(f\"\\nProcessing query: {query}\")\n",
        "        response = run_tax_copilot(tax_copilot, query)\n",
        "        print(f\"\\nResponse: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iem7ejo2BVFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crwKcBmtBZUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}