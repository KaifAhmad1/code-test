{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2LNJqTpKlkSZP21ZaPoKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Luna_Tax_Copilot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Luna Tax Copilot Implementation**"
      ],
      "metadata": {
        "id": "AhyFaRkR9PD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i_d39VaR9LNO",
        "outputId": "54b7a2e4-35e9-49ee-fcc8-b4d73f22513d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community langgraph openai chromadb neo4j python-dotenv pandas numpy pypdf docx2txt openpyxl\n",
        "!pip install -q langchain_neo4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# LangGraph imports for agent orchestration\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Required environment variables\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "# os.environ[\"NEO4J_URI\"] = \"neo4j://localhost:7687\"\n",
        "# os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "# os.environ[\"NEO4J_PASSWORD\"] = \"password\""
      ],
      "metadata": {
        "id": "JonjTdZA-YxZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model\n",
        "def get_llm(model_name=\"gpt-4o\", temperature=0):\n",
        "    \"\"\"Initialize the language model\"\"\"\n",
        "    return ChatOpenAI(model=model_name, temperature=temperature)"
      ],
      "metadata": {
        "id": "5PPq0N_W_AQS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- DATA PROCESSING COMPONENTS -----\n",
        "\n",
        "def init_data_crawler(ato_db_connection_string=None):\n",
        "    \"\"\"\n",
        "    Initialize the data crawler that fetches data from the ATO database\n",
        "\n",
        "    Args:\n",
        "        ato_db_connection_string: Connection string to the ATO database\n",
        "\n",
        "    Returns:\n",
        "        Function that crawls data from the ATO database\n",
        "    \"\"\"\n",
        "    def crawl_data(query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Crawl data from the ATO database based on the query\n",
        "\n",
        "        Args:\n",
        "            query: Query to fetch data from the ATO database\n",
        "\n",
        "        Returns:\n",
        "            List of documents retrieved from the ATO database\n",
        "        \"\"\"\n",
        "        # This would be implemented with actual database connection code\n",
        "        # Here we're simulating retrieval with sample data\n",
        "        print(f\"Crawling data with query: {query}\")\n",
        "\n",
        "        # Simulated data retrieval\n",
        "        sample_data = [\n",
        "            {\"id\": \"1\", \"title\": \"Income Tax Assessment\", \"content\": \"Guidelines for assessing taxable income...\"},\n",
        "            {\"id\": \"2\", \"title\": \"GST Regulations\", \"content\": \"Goods and Services Tax regulations and exemptions...\"},\n",
        "            {\"id\": \"3\", \"title\": \"Tax Deductions\", \"content\": \"Eligible deductions for various business expenses...\"},\n",
        "        ]\n",
        "\n",
        "        return sample_data\n",
        "\n",
        "    return crawl_data\n",
        "\n",
        "def init_data_cleaner():\n",
        "    \"\"\"\n",
        "    Initialize the data cleaner that preprocesses crawled data\n",
        "\n",
        "    Returns:\n",
        "        Function that cleans raw data\n",
        "    \"\"\"\n",
        "    def clean_data(raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Clean raw data by removing irrelevant information, standardizing formats\n",
        "\n",
        "        Args:\n",
        "            raw_data: Raw data from the crawler\n",
        "\n",
        "        Returns:\n",
        "            Cleaned data\n",
        "        \"\"\"\n",
        "        cleaned_data = []\n",
        "\n",
        "        for item in raw_data:\n",
        "            # Implement cleaning logic: remove HTML, standardize dates, etc.\n",
        "            cleaned_item = {\n",
        "                \"id\": item[\"id\"],\n",
        "                \"title\": item[\"title\"],\n",
        "                \"content\": item[\"content\"].replace(\"...\", \"\"),  # Simple cleaning example\n",
        "                \"cleaned_timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "            cleaned_data.append(cleaned_item)\n",
        "\n",
        "        return cleaned_data\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "def init_data_chunker():\n",
        "    \"\"\"\n",
        "    Initialize the data chunker that splits documents into chunks for embedding\n",
        "\n",
        "    Returns:\n",
        "        Function that chunks cleaned data\n",
        "    \"\"\"\n",
        "    def chunk_data(cleaned_data: List[Dict[str, Any]]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Chunk cleaned data into smaller pieces for processing\n",
        "\n",
        "        Args:\n",
        "            cleaned_data: Cleaned data from the cleaner\n",
        "\n",
        "        Returns:\n",
        "            List of document chunks\n",
        "        \"\"\"\n",
        "        # Initialize text splitter for chunking\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "        document_chunks = []\n",
        "\n",
        "        for item in cleaned_data:\n",
        "            # Create a Document object\n",
        "            doc = Document(\n",
        "                page_content=item[\"content\"],\n",
        "                metadata={\n",
        "                    \"id\": item[\"id\"],\n",
        "                    \"title\": item[\"title\"],\n",
        "                    \"source\": \"ATO Database\",\n",
        "                    \"timestamp\": item.get(\"cleaned_timestamp\")\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Split the document into chunks\n",
        "            chunks = text_splitter.split_documents([doc])\n",
        "            document_chunks.extend(chunks)\n",
        "\n",
        "        return document_chunks\n",
        "\n",
        "    return chunk_data\n",
        "\n",
        "def init_data_encoder():\n",
        "    \"\"\"\n",
        "    Initialize the data encoder that creates embeddings from document chunks\n",
        "\n",
        "    Returns:\n",
        "        Function that encodes document chunks into embeddings\n",
        "    \"\"\"\n",
        "    def encode_data(document_chunks: List[Document]) -> Tuple[List[Document], List[List[float]]]:\n",
        "        \"\"\"\n",
        "        Encode document chunks into embeddings\n",
        "\n",
        "        Args:\n",
        "            document_chunks: Document chunks from the chunker\n",
        "\n",
        "        Returns:\n",
        "            Tuple of document chunks and their embeddings\n",
        "        \"\"\"\n",
        "        # Initialize embeddings\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        # Create embeddings for each document chunk\n",
        "        texts = [doc.page_content for doc in document_chunks]\n",
        "        embedded_vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "        return document_chunks, embedded_vectors\n",
        "\n",
        "    return encode_data"
      ],
      "metadata": {
        "id": "D-9M_Dk7AhhD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- DATABASE COMPONENTS -----\n",
        "\n",
        "def init_vector_database():\n",
        "    \"\"\"\n",
        "    Initialize the vector database for storing embeddings\n",
        "\n",
        "    Returns:\n",
        "        Vector database instance and functions to interact with it\n",
        "    \"\"\"\n",
        "    # Initialize vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vector_db = Chroma(embedding_function=embeddings, collection_name=\"tax_regulations\")\n",
        "\n",
        "    def store_in_vector_db(documents: List[Document], embeddings: List[List[float]]) -> None:\n",
        "        \"\"\"Store documents and their embeddings in the vector database\"\"\"\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        metadatas = [doc.metadata for doc in documents]\n",
        "\n",
        "        # Add documents to the vector store\n",
        "        vector_db.add_texts(texts=texts, metadatas=metadatas, embeddings=embeddings)\n",
        "        print(f\"Stored {len(documents)} documents in the vector database\")\n",
        "\n",
        "    def search_vector_db(query: str, top_k: int = 5) -> List[Document]:\n",
        "        \"\"\"Search the vector database for relevant documents\"\"\"\n",
        "        results = vector_db.similarity_search(query, k=top_k)\n",
        "        return results\n",
        "\n",
        "    return {\n",
        "        \"vector_db\": vector_db,\n",
        "        \"store\": store_in_vector_db,\n",
        "        \"search\": search_vector_db\n",
        "    }\n",
        "\n",
        "def init_relationship_database():\n",
        "    \"\"\"\n",
        "    Initialize the graph database for storing relationships\n",
        "\n",
        "    Returns:\n",
        "        Graph database instance and functions to interact with it\n",
        "    \"\"\"\n",
        "    # Initialize Neo4j graph store\n",
        "    graph_db = Neo4j(\n",
        "        url=os.environ.get(\"NEO4J_URI\", \"neo4j://localhost:7687\"),\n",
        "        username=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n",
        "        password=os.environ.get(\"NEO4J_PASSWORD\", \"password\")\n",
        "    )\n",
        "\n",
        "    def store_relationship(source_id: str, target_id: str, relationship_type: str, properties: Dict = None) -> None:\n",
        "        \"\"\"Store a relationship in the graph database\"\"\"\n",
        "        if properties is None:\n",
        "            properties = {}\n",
        "\n",
        "        # Create Cypher query to add relationship\n",
        "        query = f\"\"\"\n",
        "        MATCH (source) WHERE source.id = $source_id\n",
        "        MATCH (target) WHERE target.id = $target_id\n",
        "        CREATE (source)-[r:{relationship_type} $properties]->(target)\n",
        "        RETURN source, r, target\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            \"source_id\": source_id,\n",
        "            \"target_id\": target_id,\n",
        "            \"properties\": properties\n",
        "        }\n",
        "\n",
        "        graph_db.query(query, params)\n",
        "\n",
        "    def store_document_node(document: Document) -> None:\n",
        "        \"\"\"Store a document as a node in the graph database\"\"\"\n",
        "        # Create Cypher query to add node\n",
        "        query = \"\"\"\n",
        "        CREATE (d:Document {\n",
        "            id: $id,\n",
        "            title: $title,\n",
        "            content: $content,\n",
        "            source: $source\n",
        "        })\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            \"id\": document.metadata.get(\"id\"),\n",
        "            \"title\": document.metadata.get(\"title\"),\n",
        "            \"content\": document.page_content,\n",
        "            \"source\": document.metadata.get(\"source\")\n",
        "        }\n",
        "\n",
        "        graph_db.query(query, params)\n",
        "\n",
        "    def query_graph(cypher_query: str, params: Dict = None) -> List[Dict]:\n",
        "        \"\"\"Query the graph database with a Cypher query\"\"\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "\n",
        "        results = graph_db.query(cypher_query, params)\n",
        "        return results\n",
        "\n",
        "    return {\n",
        "        \"graph_db\": graph_db,\n",
        "        \"store_relationship\": store_relationship,\n",
        "        \"store_node\": store_document_node,\n",
        "        \"query\": query_graph\n",
        "    }"
      ],
      "metadata": {
        "id": "wh8mR5icAnpB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- AGENT COMPONENTS -----\n",
        "\n",
        "def init_context_engine(vector_db_search):\n",
        "    \"\"\"\n",
        "    Initialize the context engine that processes prompts and provides context\n",
        "\n",
        "    Args:\n",
        "        vector_db_search: Function to search the vector database\n",
        "\n",
        "    Returns:\n",
        "        Function that processes prompts and provides context\n",
        "    \"\"\"\n",
        "    def process_prompt(prompt: str, goal: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process the prompt, extract context, and prepare for optimization\n",
        "\n",
        "        Args:\n",
        "            prompt: User prompt\n",
        "            goal: Optional goal to guide the context engine\n",
        "\n",
        "        Returns:\n",
        "            Context information for the prompt optimizer\n",
        "        \"\"\"\n",
        "        # Retrieve relevant documents from vector DB\n",
        "        relevant_docs = vector_db_search(prompt, top_k=3)\n",
        "\n",
        "        # Extract and format context from retrieved documents\n",
        "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "        return {\n",
        "            \"original_prompt\": prompt,\n",
        "            \"goal\": goal,\n",
        "            \"retrieved_context\": context,\n",
        "            \"document_ids\": [doc.metadata.get(\"id\") for doc in relevant_docs]\n",
        "        }\n",
        "\n",
        "    return process_prompt\n",
        "\n",
        "def init_prompt_optimizer(llm):\n",
        "    \"\"\"\n",
        "    Initialize the prompt optimizer that enhances prompts with context\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that optimizes prompts\n",
        "    \"\"\"\n",
        "    # Prompt template for optimizing user prompts\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a Tax Copilot that helps tax professionals.\n",
        "\n",
        "        Original prompt: {original_prompt}\n",
        "        Goal: {goal}\n",
        "\n",
        "        Relevant context from tax regulations:\n",
        "        {retrieved_context}\n",
        "\n",
        "        Rewrite the prompt to be more specific and include relevant tax regulations from the context.\n",
        "        Modified prompt:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create chain for prompt optimization\n",
        "    prompt_chain = (\n",
        "        prompt_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def optimize_prompt(context_info: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Optimize the prompt based on context and goal\n",
        "\n",
        "        Args:\n",
        "            context_info: Context information from the context engine\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with original and optimized prompts\n",
        "        \"\"\"\n",
        "        # Run the prompt through the chain\n",
        "        optimized_prompt = prompt_chain.invoke(context_info)\n",
        "\n",
        "        return {\n",
        "            \"original_prompt\": context_info[\"original_prompt\"],\n",
        "            \"optimized_prompt\": optimized_prompt,\n",
        "            \"retrieved_context\": context_info[\"retrieved_context\"],\n",
        "            \"document_ids\": context_info[\"document_ids\"]\n",
        "        }\n",
        "\n",
        "    return optimize_prompt\n",
        "\n",
        "def init_retriever(vector_db_search, graph_db_query):\n",
        "    \"\"\"\n",
        "    Initialize the retriever that fetches information from databases\n",
        "\n",
        "    Args:\n",
        "        vector_db_search: Function to search the vector database\n",
        "        graph_db_query: Function to query the graph database\n",
        "\n",
        "    Returns:\n",
        "        Function that retrieves information\n",
        "    \"\"\"\n",
        "    def retrieve_information(query: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Retrieve information from both vector and graph databases\n",
        "\n",
        "        Args:\n",
        "            query: Query to search for\n",
        "\n",
        "        Returns:\n",
        "            Retrieved information from both databases\n",
        "        \"\"\"\n",
        "        # Retrieve from vector DB\n",
        "        vector_results = vector_db_search(query, top_k=5)\n",
        "\n",
        "        # Retrieve from graph DB\n",
        "        # Look for related tax regulations via graph relationships\n",
        "        graph_query = \"\"\"\n",
        "        MATCH (d:Document)-[r]-(related)\n",
        "        WHERE d.title CONTAINS $keyword OR d.content CONTAINS $keyword\n",
        "        RETURN related.title, related.content, type(r) as relationship\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "        graph_results = graph_db_query(graph_query, {\"keyword\": query})\n",
        "\n",
        "        # Combine results\n",
        "        return {\n",
        "            \"vector_results\": vector_results,\n",
        "            \"graph_results\": graph_results,\n",
        "            \"query\": query\n",
        "        }\n",
        "\n",
        "    return retrieve_information\n",
        "\n",
        "def init_reranker(llm):\n",
        "    \"\"\"\n",
        "    Initialize the reranker that prioritizes retrieved information\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that reranks retrieved information\n",
        "    \"\"\"\n",
        "    # Create reranking prompt\n",
        "    rerank_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        You need to rank the following documents based on their relevance to the query: {query}\n",
        "\n",
        "        Documents:\n",
        "        {documents}\n",
        "\n",
        "        Return a JSON list of document indices in order of relevance (most relevant first),\n",
        "        with a brief explanation for each ranking.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    rerank_chain = (\n",
        "        rerank_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def rerank_results(retrieval_results: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Rerank retrieved results based on relevance to query\n",
        "\n",
        "        Args:\n",
        "            retrieval_results: Results from the retriever\n",
        "\n",
        "        Returns:\n",
        "            Reranked results\n",
        "        \"\"\"\n",
        "        # Format documents for reranking\n",
        "        vector_docs = \"\\n\\n\".join([\n",
        "            f\"Document {i}: {doc.page_content}\"\n",
        "            for i, doc in enumerate(retrieval_results[\"vector_results\"])\n",
        "        ])\n",
        "\n",
        "        # Rerank using LLM\n",
        "        rerank_result = rerank_chain.invoke({\n",
        "            \"query\": retrieval_results[\"query\"],\n",
        "            \"documents\": vector_docs\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"original_results\": retrieval_results,\n",
        "            \"reranked_results\": rerank_result\n",
        "        }\n",
        "\n",
        "    return rerank_results\n",
        "\n",
        "def init_reasoning_and_planning(llm):\n",
        "    \"\"\"\n",
        "    Initialize the reasoning and planning module\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that creates workflows\n",
        "    \"\"\"\n",
        "    # Create reasoning prompt\n",
        "    reasoning_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Based on the following query and available information, create a workflow plan\n",
        "        to help the tax professional.\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Relevant information:\n",
        "        {context}\n",
        "\n",
        "        Create a step-by-step workflow plan to address this tax query. Include:\n",
        "        1. Key tax regulations to consider\n",
        "        2. Analysis steps\n",
        "        3. Documentation needed\n",
        "        4. Potential advice or recommendations\n",
        "\n",
        "        Workflow plan:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    reasoning_chain = (\n",
        "        reasoning_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def create_workflow(reranked_results: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Create a workflow plan based on the query and retrieved information\n",
        "\n",
        "        Args:\n",
        "            reranked_results: Reranked results from the reranker\n",
        "\n",
        "        Returns:\n",
        "            Workflow plan\n",
        "        \"\"\"\n",
        "        # Extract context from reranked results\n",
        "        query = reranked_results[\"original_results\"][\"query\"]\n",
        "\n",
        "        # Use vector results as context\n",
        "        context = \"\\n\\n\".join([\n",
        "            doc.page_content\n",
        "            for doc in reranked_results[\"original_results\"][\"vector_results\"]\n",
        "        ])\n",
        "\n",
        "        # Generate workflow\n",
        "        workflow_plan = reasoning_chain.invoke({\n",
        "            \"query\": query,\n",
        "            \"context\": context\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"workflow_plan\": workflow_plan,\n",
        "            \"context\": context\n",
        "        }\n",
        "\n",
        "    return create_workflow\n",
        "\n",
        "def init_workflow_execution(llm, graph_db_store_relationship):\n",
        "    \"\"\"\n",
        "    Initialize the workflow execution module\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "        graph_db_store_relationship: Function to store relationships in the graph database\n",
        "\n",
        "    Returns:\n",
        "        Function that executes workflows\n",
        "    \"\"\"\n",
        "    # Create execution prompt\n",
        "    execution_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Execute the following workflow plan to provide a detailed response to the tax query:\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Workflow plan:\n",
        "        {workflow_plan}\n",
        "\n",
        "        Context information:\n",
        "        {context}\n",
        "\n",
        "        Generate a comprehensive response that follows the workflow steps and addresses the query.\n",
        "        Include specific tax regulations, guidelines, and actionable advice.\n",
        "\n",
        "        Response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    execution_chain = (\n",
        "        execution_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def execute_workflow(workflow_info: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Execute a workflow plan to generate a response\n",
        "\n",
        "        Args:\n",
        "            workflow_info: Workflow plan and context\n",
        "\n",
        "        Returns:\n",
        "            Response from workflow execution\n",
        "        \"\"\"\n",
        "        # Execute workflow\n",
        "        response = execution_chain.invoke({\n",
        "            \"query\": workflow_info[\"query\"],\n",
        "            \"workflow_plan\": workflow_info[\"workflow_plan\"],\n",
        "            \"context\": workflow_info[\"context\"]\n",
        "        })\n",
        "\n",
        "        # Store workflow execution in graph DB as a relationship\n",
        "        # Between query and response (in a real system)\n",
        "        # This is just a placeholder for demonstration\n",
        "        try:\n",
        "            graph_db_store_relationship(\n",
        "                \"query_node\",  # This would be a real node id in production\n",
        "                \"response_node\",  # This would be a real node id in production\n",
        "                \"GENERATED\",\n",
        "                {\"timestamp\": datetime.now().isoformat()}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not store relationship in graph DB: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"query\": workflow_info[\"query\"],\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "    return execute_workflow\n",
        "\n",
        "def init_generator(llm):\n",
        "    \"\"\"\n",
        "    Initialize the generator that creates the final response\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that generates final responses\n",
        "    \"\"\"\n",
        "    # Create generator prompt\n",
        "    generator_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax expert assistant.\n",
        "\n",
        "        Format the following response to be clear, professional, and actionable for a tax professional:\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Response content:\n",
        "        {response}\n",
        "\n",
        "        Format this as a professional tax advisory response with proper headings, bullet points where appropriate,\n",
        "        and clear actionable steps.\n",
        "\n",
        "        Final response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    generator_chain = (\n",
        "        generator_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def generate_response(execution_result: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Generate the final response\n",
        "\n",
        "        Args:\n",
        "            execution_result: Result from workflow execution\n",
        "\n",
        "        Returns:\n",
        "            Formatted final response\n",
        "        \"\"\"\n",
        "        # Generate final response\n",
        "        final_response = generator_chain.invoke({\n",
        "            \"query\": execution_result[\"query\"],\n",
        "            \"response\": execution_result[\"response\"]\n",
        "        })\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    return generate_response\n",
        "\n",
        "def init_guardrails(llm):\n",
        "    \"\"\"\n",
        "    Initialize the guardrails that ensure response safety and quality\n",
        "\n",
        "    Args:\n",
        "        llm: Language model\n",
        "\n",
        "    Returns:\n",
        "        Function that applies guardrails to responses\n",
        "    \"\"\"\n",
        "    # Create guardrail prompt\n",
        "    guardrail_template = PromptTemplate.from_template(\n",
        "        \"\"\"You are a tax compliance expert.\n",
        "\n",
        "        Review the following response to ensure it:\n",
        "        1. Only provides factually accurate tax information\n",
        "        2. Doesn't give absolute tax advice without caveats\n",
        "        3. Recommends consulting a tax professional for specific situations\n",
        "        4. Doesn't contain outdated tax regulations\n",
        "        5. Makes clear distinctions between federal and state/local tax matters\n",
        "\n",
        "        Response to review:\n",
        "        {response}\n",
        "\n",
        "        If the response meets all criteria, return it unchanged.\n",
        "        If it needs revision, provide the corrected version that addresses the issues.\n",
        "\n",
        "        Reviewed response:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    guardrail_chain = (\n",
        "        guardrail_template\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    def apply_guardrails(response: str) -> str:\n",
        "        \"\"\"\n",
        "        Apply guardrails to ensure response quality and safety\n",
        "\n",
        "        Args:\n",
        "            response: Generated response\n",
        "\n",
        "        Returns:\n",
        "            Response after applying guardrails\n",
        "        \"\"\"\n",
        "        # Apply guardrails\n",
        "        safe_response = guardrail_chain.invoke({\"response\": response})\n",
        "\n",
        "        return safe_response\n",
        "\n",
        "    return apply_guardrails"
      ],
      "metadata": {
        "id": "PTYkx0aXBEsx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- LANGGRAPH STATE AND WORKFLOW -----\n",
        "\n",
        "def create_tax_copilot_graph():\n",
        "    \"\"\"\n",
        "    Create the LangGraph for the Tax Copilot pipeline\n",
        "\n",
        "    Returns:\n",
        "        Configured StateGraph for the Tax Copilot\n",
        "    \"\"\"\n",
        "    # Initialize the language model\n",
        "    llm = get_llm()\n",
        "\n",
        "    # Initialize data processing components\n",
        "    data_crawler = init_data_crawler()\n",
        "    data_cleaner = init_data_cleaner()\n",
        "    data_chunker = init_data_chunker()\n",
        "    data_encoder = init_data_encoder()\n",
        "\n",
        "    # Initialize databases\n",
        "    vector_db = init_vector_database()\n",
        "    graph_db = init_relationship_database()\n",
        "\n",
        "    # Initialize agent components\n",
        "    context_engine = init_context_engine(vector_db[\"search\"])\n",
        "    prompt_optimizer = init_prompt_optimizer(llm)\n",
        "    retriever = init_retriever(vector_db[\"search\"], graph_db[\"query\"])\n",
        "    reranker = init_reranker(llm)\n",
        "    reasoning_and_planning = init_reasoning_and_planning(llm)\n",
        "    workflow_execution = init_workflow_execution(llm, graph_db[\"store_relationship\"])\n",
        "    generator = init_generator(llm)\n",
        "    guardrails = init_guardrails(llm)\n",
        "\n",
        "    # Define the state\n",
        "    class State:\n",
        "        \"\"\"State for the Tax Copilot pipeline\"\"\"\n",
        "        prompt: str\n",
        "        goal: str = None\n",
        "        context_info: Dict = None\n",
        "        optimized_prompt: Dict = None\n",
        "        retrieval_results: Dict = None\n",
        "        reranked_results: Dict = None\n",
        "        workflow_plan: Dict = None\n",
        "        execution_result: Dict = None\n",
        "        generated_response: str = None\n",
        "        final_response: str = None\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(State)\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    workflow.add_node(\"context_engine\", lambda state: {\"context_info\": context_engine(state.prompt, state.goal)})\n",
        "    workflow.add_node(\"prompt_optimizer\", lambda state: {\"optimized_prompt\": prompt_optimizer(state.context_info)})\n",
        "    workflow.add_node(\"retriever\", lambda state: {\"retrieval_results\": retriever(state.optimized_prompt[\"optimized_prompt\"])})\n",
        "    workflow.add_node(\"reranker\", lambda state: {\"reranked_results\": reranker(state.retrieval_results)})\n",
        "    workflow.add_node(\"reasoning_and_planning\", lambda state: {\"workflow_plan\": reasoning_and_planning(state.reranked_results)})\n",
        "    workflow.add_node(\"workflow_execution\", lambda state: {\"execution_result\": workflow_execution(state.workflow_plan)})\n",
        "    workflow.add_node(\"generator\", lambda state: {\"generated_response\": generator(state.execution_result)})\n",
        "    workflow.add_node(\"guardrails\", lambda state: {\"final_response\": guardrails(state.generated_response)})\n",
        "\n",
        "    # Add edges to the graph\n",
        "    workflow.add_edge(\"context_engine\", \"prompt_optimizer\")\n",
        "    workflow.add_edge(\"prompt_optimizer\", \"retriever\")\n",
        "    workflow.add_edge(\"retriever\", \"reranker\")\n",
        "    workflow.add_edge(\"reranker\", \"reasoning_and_planning\")\n",
        "    workflow.add_edge(\"reasoning_and_planning\", \"workflow_execution\")\n",
        "    workflow.add_edge(\"workflow_execution\", \"generator\")\n",
        "    workflow.add_edge(\"generator\", \"guardrails\")\n",
        "    workflow.add_edge(\"guardrails\", END)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"context_engine\")\n",
        "\n",
        "    # Compile the graph\n",
        "    return workflow.compile()"
      ],
      "metadata": {
        "id": "d_czVZhfBQHb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- MAIN FUNCTION TO INITIALIZE THE SYSTEM -----\n",
        "\n",
        "def setup_tax_copilot_system():\n",
        "    \"\"\"\n",
        "    Set up the Tax Copilot system with data processing and agent components\n",
        "\n",
        "    Returns:\n",
        "        Initialized Tax Copilot system\n",
        "    \"\"\"\n",
        "    # Initialize LLM\n",
        "    llm = get_llm()\n",
        "\n",
        "    # Initialize databases\n",
        "    vector_db = init_vector_database()\n",
        "    graph_db = init_relationship_database()\n",
        "\n",
        "    # Set up data processing pipeline\n",
        "    data_crawler = init_data_crawler()\n",
        "    data_cleaner = init_data_cleaner()\n",
        "    data_chunker = init_data_chunker()\n",
        "    data_encoder = init_data_encoder()\n",
        "\n",
        "    # Process initial data (in a real system, this would fetch from ATO database)\n",
        "    raw_data = data_crawler(\"initial tax regulations\")\n",
        "    cleaned_data = data_cleaner(raw_data)\n",
        "    document_chunks = data_chunker(cleaned_data)\n",
        "    documents, embeddings = data_encoder(document_chunks)\n",
        "\n",
        "    # Store processed data in databases\n",
        "    vector_db[\"store\"](documents, embeddings)\n",
        "\n",
        "    # Store documents in graph database\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            graph_db[\"store_node\"](doc)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not store document in graph DB: {e}\")\n",
        "\n",
        "    # Create sample relationships (in a real system, these would be derived from the data)\n",
        "    try:\n",
        "        graph_db[\"store_relationship\"](\"1\", \"2\", \"RELATES_TO\", {\"type\": \"reference\"})\n",
        "        graph_db[\"store_relationship\"](\"2\", \"3\", \"SUPPLEMENTS\", {\"section\": \"deductions\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not store relationships in graph DB: {e}\")\n",
        "\n",
        "    # Create the LangGraph for the Tax Copilot\n",
        "    tax_copilot_graph = create_tax_copilot_graph()\n",
        "\n",
        "    return tax_copilot_graph\n",
        "\n",
        "def run_tax_copilot(graph, query, goal=None):\n",
        "    \"\"\"\n",
        "    Run the Tax Copilot with a user query\n",
        "\n",
        "    Args:\n",
        "        graph: Compiled StateGraph for the Tax Copilot\n",
        "        query: User query\n",
        "        goal: Optional goal to guide the context engine\n",
        "\n",
        "    Returns:\n",
        "        Final response\n",
        "    \"\"\"\n",
        "    # Create initial state\n",
        "    initial_state = {\"prompt\": query, \"goal\": goal}\n",
        "\n",
        "    # Run the graph\n",
        "    result = graph.invoke(initial_state)\n",
        "\n",
        "    return result[\"final_response\"]\n",
        "\n",
        "# ----- EXAMPLE USAGE -----\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the Tax Copilot system\"\"\"\n",
        "    # Setup the system\n",
        "    print(\"Setting up Tax Copilot system...\")\n",
        "    tax_copilot = setup_tax_copilot_system()\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What are the deduction limits for business expenses?\",\n",
        "        \"How do I report foreign income on my tax return?\",\n",
        "        \"Can I claim home office expenses if I'm working remotely?\"\n",
        "    ]\n",
        "\n",
        "    # Run the queries\n",
        "    for query in queries:\n",
        "        print(f\"\\nProcessing query: {query}\")\n",
        "        response = run_tax_copilot(tax_copilot, query)\n",
        "        print(f\"\\nResponse: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iem7ejo2BVFs",
        "outputId": "9a19ce0e-0f71-4061-de7b-8612162002c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Tax Copilot system...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d4dae6b0c9b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-d4dae6b0c9b4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Setup the system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting up Tax Copilot system...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mtax_copilot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_tax_copilot_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Example queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d4dae6b0c9b4>\u001b[0m in \u001b[0;36msetup_tax_copilot_system\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Initialize LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Initialize databases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5f706415d72b>\u001b[0m in \u001b[0;36mget_llm\u001b[0;34m(model_name, temperature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Initialize the language model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m                 )\n\u001b[1;32m    621\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\"\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             raise OpenAIError(\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crwKcBmtBZUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}