{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN+7TGcVQwaqxScTIoSxP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Audio_deepfake_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensice Forensics**"
      ],
      "metadata": {
        "id": "AQCCjseftgtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ns3mLF6tZyn"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis speechbrain langchain openai langgraph transformers vllm requests ipywidgets\n",
        "!pip install -q audiomentations hmmlearn eyed3 langchain_community praat-parselmouth webrtcvad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os, json, asyncio\n",
        "import numpy as np, librosa, librosa.display, soundfile as sf, matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import webrtcvad\n",
        "from pydub import AudioSegment\n",
        "import noisereduce as nr\n",
        "from pyAudioAnalysis import audioSegmentation as aS, ShortTermFeatures\n",
        "import parselmouth\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# SpeechBrain modules for inference and augmentation\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "try:\n",
        "    from speechbrain.augment import AddNoise\n",
        "except ImportError:\n",
        "    AddNoise = None\n",
        "\n",
        "# LangChain modules for report generation\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import openai\n",
        "\n",
        "# vLLM modules (dummy integration example)\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW9Y5YDbtrFI",
        "outputId": "8a1a888b-0d10-4c14-815c-c9a9bf9bee87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pyAudioAnalysis/../moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-18 11:52:19 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Data Model -----\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file = kwargs.get(\"file\")\n",
        "        self.verdict = kwargs.get(\"verdict\")\n",
        "        self.mean_score = kwargs.get(\"mean_score\")\n",
        "        self.confidence = kwargs.get(\"confidence\")\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\")\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\")\n",
        "        self.natural_summary = kwargs.get(\"natural_summary\")\n",
        "        self.asr_transcript = kwargs.get(\"asr_transcript\")\n",
        "        self.asr_lang = kwargs.get(\"asr_lang\")\n",
        "        self.speaker_identities = kwargs.get(\"speaker_identities\")\n",
        "        self.speaker_spoof_score = kwargs.get(\"speaker_spoof_score\")\n",
        "        self.noise_quality_score = kwargs.get(\"noise_quality_score\")\n",
        "        self.gender_distribution = kwargs.get(\"gender_distribution\")\n",
        "        self.detailed_results = kwargs.get(\"detailed_results\")\n",
        "        self.timestamp = kwargs.get(\"timestamp\")\n",
        "        self.vllm_model_outputs = kwargs.get(\"vllm_model_outputs\", {})\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        return json.dumps(self.__dict__, indent=indent)\n",
        "\n",
        "# ----- Asynchronous Executors -----\n",
        "executor = ThreadPoolExecutor(max_workers=8)"
      ],
      "metadata": {
        "id": "IEWenhu-tvvp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Audio/Video Preprocessing -----\n",
        "async def preprocess_audio(audio_path, out_sr=16000, mono=True, reduce_noise=True):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Starting audio preprocessing...\")\n",
        "    ext = os.path.splitext(audio_path)[1].lower()\n",
        "    # If file is not WAV, try to convert. In addition, if the file is a video (e.g., .mp4, .avi), first extract audio.\n",
        "    if ext != \".wav\":\n",
        "        if ext in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "            print(\"[Async] Detected video file. Extracting audio using MoviePy...\")\n",
        "            def extract_audio():\n",
        "                clip = VideoFileClip(audio_path)\n",
        "                temp_wav = \"temp_extracted_audio.wav\"\n",
        "                clip.audio.write_audiofile(temp_wav, fps=out_sr, logger=None)\n",
        "                clip.close()\n",
        "                return temp_wav\n",
        "            audio_path = await loop.run_in_executor(executor, extract_audio)\n",
        "        else:\n",
        "            print(\"[Async] Converting non-WAV file to WAV using pydub...\")\n",
        "            def convert():\n",
        "                audio = AudioSegment.from_file(audio_path)\n",
        "                audio = audio.set_frame_rate(out_sr).set_channels(1 if mono else 2)\n",
        "                temp_wav = \"temp_input.wav\"\n",
        "                audio.export(temp_wav, format=\"wav\")\n",
        "                return temp_wav\n",
        "            audio_path = await loop.run_in_executor(executor, convert)\n",
        "    def load_and_normalize():\n",
        "        audio_data, sr = librosa.load(audio_path, sr=out_sr, mono=mono)\n",
        "        audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-8)\n",
        "        if reduce_noise:\n",
        "            try:\n",
        "                audio_data = nr.reduce_noise(y=audio_data, sr=sr)\n",
        "            except Exception as ex:\n",
        "                print(\"[Warning] Noise reduction failed:\", ex)\n",
        "        return audio_data, sr\n",
        "    audio_data, sr = await loop.run_in_executor(executor, load_and_normalize)\n",
        "    print(\"[Async] Preprocessing complete.\")\n",
        "    return audio_data, sr\n",
        "\n",
        "# ----- Feature Extraction -----\n",
        "async def extract_features(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Extracting audio features...\")\n",
        "    def compute_features():\n",
        "        feat = {}\n",
        "        feat['duration'] = len(audio)/sr\n",
        "        feat['energy'] = np.sqrt(np.mean(audio**2))\n",
        "        feat['zcr'] = np.mean(librosa.feature.zero_crossing_rate(y=audio))\n",
        "        feat['rmse'] = np.mean(librosa.feature.rms(y=audio))\n",
        "        feat['spec_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
        "        feat['spec_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr))\n",
        "        feat['spec_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "        feat['mfcc_mean'] = np.mean(mfccs)\n",
        "        feat['mfcc_std'] = np.std(mfccs)\n",
        "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "        feat['chroma_mean'] = np.mean(chroma)\n",
        "        feat['chroma_std'] = np.std(chroma)\n",
        "        st_feats, _ = ShortTermFeatures.feature_extraction(audio, sr, int(0.050*sr), int(0.025*sr))\n",
        "        feat['st_energy_std'] = np.std(st_feats[1, :])\n",
        "        try:\n",
        "            snd = parselmouth.Sound(audio, sr)\n",
        "            pitch = snd.to_pitch()\n",
        "            pitch_values = pitch.selected_array['frequency'][pitch.selected_array['frequency'] > 0]\n",
        "            feat['mean_pitch'] = np.mean(pitch_values) if len(pitch_values) > 0 else 0\n",
        "            feat['pitch_std'] = np.std(pitch_values) if len(pitch_values) > 0 else 0\n",
        "        except Exception as ex:\n",
        "            feat['mean_pitch'] = 0\n",
        "            feat['pitch_std'] = 0\n",
        "        try:\n",
        "            signal_power = np.mean(audio**2)\n",
        "            noise_est = audio - librosa.effects.hpss(audio)[1]\n",
        "            noise_power = np.mean(noise_est**2)\n",
        "            feat['snr_est'] = 10 * np.log10((signal_power+1e-6)/(noise_power+1e-6))\n",
        "        except Exception as ex:\n",
        "            feat['snr_est'] = 0\n",
        "        return feat\n",
        "    features = await loop.run_in_executor(executor, compute_features)\n",
        "    print(\"[Async] Feature extraction complete.\")\n",
        "    return features\n",
        "\n",
        "async def extract_vad_ratio(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Computing VAD ratio...\")\n",
        "    def compute_vad():\n",
        "        try:\n",
        "            vad = webrtcvad.Vad(2)\n",
        "            audio_bytes = (audio*32768).astype(np.int16).tobytes()\n",
        "            speech_frames = 0; total_frames = 0\n",
        "            for i in range(0, len(audio_bytes), 320):\n",
        "                if i+320 > len(audio_bytes): break\n",
        "                total_frames += 1\n",
        "                if vad.is_speech(audio_bytes[i:i+320], sr):\n",
        "                    speech_frames += 1\n",
        "            return speech_frames / (total_frames+1e-8)\n",
        "        except Exception as ex:\n",
        "            return 0\n",
        "    ratio = await loop.run_in_executor(executor, compute_vad)\n",
        "    print(f\"[Async] VAD ratio: {ratio:.3f}\")\n",
        "    return ratio\n",
        "\n",
        "async def extract_inaspeech_segments(audio_path):\n",
        "    print(\"[Async] Simulating segmentation for gender estimation...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    nb_segments = 5\n",
        "    nb_speech = 4\n",
        "    gender_dist = {\"male\": 0.75, \"female\": 0.25}\n",
        "    print(\"[Async] Simulated segmentation complete:\", gender_dist)\n",
        "    return nb_segments, nb_speech, gender_dist"
      ],
      "metadata": {
        "id": "nKUzAPOttyUk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Forensic Agents (Async Wrappers) -----\n",
        "async def run_speechbrain_speaker(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running speaker diarization (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            segs, classes = aS.speaker_diarization(audio_path, 2, plot_res=False)\n",
        "            speakers = set([s for s in classes if s in (\"male\", \"female\")])\n",
        "            return list(speakers), segs\n",
        "        except Exception as ex:\n",
        "            return [], []\n",
        "    result = await loop.run_in_executor(executor, task)\n",
        "    return result\n",
        "\n",
        "async def run_speechbrain_verification(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running speaker verification (SpeechBrain)...\")\n",
        "    spkr_model = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmp_spkrec\")\n",
        "    def task():\n",
        "        try:\n",
        "            return float(spkr_model.verify_files(audio_path, audio_path)['score'])\n",
        "        except Exception as ex:\n",
        "            return 0.0\n",
        "    score = await loop.run_in_executor(executor, task)\n",
        "    return score\n",
        "\n",
        "async def run_speechbrain_spoof(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running spoof detection (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            model = EncoderClassifier.from_hparams(source=\"speechbrain/anti-spoofing-ecapa-voxceleb\", savedir=\"tmp_spoof\")\n",
        "            tensor_output = model.classify_file(audio_path)[0].detach().cpu().numpy()\n",
        "            score = float(tensor_output[1])\n",
        "            anomalies = [\"SpeechBrain spoof detected\"] if score > 0.5 else []\n",
        "            return score, anomalies\n",
        "        except Exception as ex:\n",
        "            return 0.3, []\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_language_id(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running language identification (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            langid = LanguageIdentification.from_hparams(source=\"speechbrain/lang-id-commonlanguage_ecapa\", savedir=\"tmp_langid\")\n",
        "            result = langid.classify_file(audio_path)\n",
        "            lang = result[3][0]\n",
        "            conf = float(result[1][0])\n",
        "            return lang, conf\n",
        "        except Exception as ex:\n",
        "            return \"unknown\", 0.0\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_wave2vec_fake_detection(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running wave2vec-based fake detection...\")\n",
        "    def task():\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=16000)\n",
        "            zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "            if zcr > 0.2:\n",
        "                return 0.8, [\"High ZCR - possible synthetic voice\"], \"Wave2vec analysis indicated high ZCR.\"\n",
        "            else:\n",
        "                return 0.3, [], \"Wave2vec analysis normal.\"\n",
        "        except Exception as ex:\n",
        "            return 0.3, [], \"Wave2vec error.\"\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_emotion_analysis(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running emotion analysis (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            classifier = EncoderClassifier.from_hparams(\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"tmp_emotion\")\n",
        "            out_prob, score, index, text_lab = classifier.classify_batch(torch.tensor(audio).unsqueeze(0))\n",
        "            emotion = text_lab[0]\n",
        "            conf = float(score[0])\n",
        "            return conf, [f\"Detected emotion: {emotion}\"], f\"Emotion: {emotion} (conf: {conf:.2f})\"\n",
        "        except Exception as ex:\n",
        "            return 0.1, [], \"Emotion: unknown\"\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_replay_attack_detection(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running replay attack detection...\")\n",
        "    def task():\n",
        "        try:\n",
        "            rms = np.mean(librosa.feature.rms(y=audio))\n",
        "            if rms < 0.01:\n",
        "                return 0.7, [\"Low RMS - possible replay attack\"]\n",
        "            else:\n",
        "                return 0.2, []\n",
        "        except Exception as ex:\n",
        "            return 0.2, []\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_noise_quality_estimation(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Estimating noise quality...\")\n",
        "    def task():\n",
        "        try:\n",
        "            denoised = nr.reduce_noise(y=audio, sr=sr)\n",
        "            noise = audio - denoised\n",
        "            score = float(np.clip(np.mean(noise**2)/(np.mean(audio**2)+1e-8), 0, 1))\n",
        "            return score\n",
        "        except Exception as ex:\n",
        "            return 0.0\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_speechbrain_augmentation(audio, sr):\n",
        "    if AddNoise is None:\n",
        "        print(\"[Async] SpeechBrain augmentation not available; skipping.\")\n",
        "        return audio\n",
        "    print(\"[Async] Running SpeechBrain augmentation (AddNoise)...\")\n",
        "    def task():\n",
        "        augmenter = AddNoise(snr=10)\n",
        "        return augmenter(audio)\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(executor, task)"
      ],
      "metadata": {
        "id": "-Fu-BvBht0_I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- vLLM Audio LLM Integration (Async Wrappers) -----\n",
        "def get_vllm_audio_model_configs():\n",
        "    return {\n",
        "        \"minicpmo\": run_minicpmo,\n",
        "        \"phi4_mm\": run_phi4mm,\n",
        "        \"qwen2_audio\": run_qwen2_audio,\n",
        "        \"ultravox\": run_ultravox,\n",
        "        \"whisper\": run_whisper,\n",
        "    }\n",
        "\n",
        "class ModelRequestData:\n",
        "    def __init__(self, engine_args, prompt, stop_token_ids=None, lora_requests=None):\n",
        "        self.engine_args = engine_args\n",
        "        self.prompt = prompt\n",
        "        self.stop_token_ids = stop_token_ids\n",
        "        self.lora_requests = lora_requests\n",
        "\n",
        "def run_minicpmo(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for MiniCPMO...\")\n",
        "    model_name = \"openbmb/MiniCPM-o-2_6\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args.dtype = torch.float16\n",
        "    stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
        "    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n",
        "    audio_placeholder = \"()\" * audio_count\n",
        "    audio_chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{{'<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n'}}\"\n",
        "        \"{% endfor %}\"\n",
        "        \"{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\\n<|spk_bos|><|spk|><|spk_eos|><|tts_bos|>' }}{% endif %}\"\n",
        "    )\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': f'{audio_placeholder}\\\\n{question}'\n",
        "    }]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, chat_template=audio_chat_template)\n",
        "    return ModelRequestData(engine_args, prompt, stop_token_ids)\n",
        "\n",
        "def run_phi4mm(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Phi4_MM...\")\n",
        "    model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "    placeholders = \"\".join([f\"<|audio_{i+1}|>\" for i in range(audio_count)])\n",
        "    prompts = f\"<|user|>{placeholders}{question}<|end|><|assistant|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_path,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        enable_lora=True,\n",
        "        max_lora_rank=320,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompts)\n",
        "\n",
        "def run_qwen2_audio(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Qwen2-Audio...\")\n",
        "    model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args.dtype = torch.float16\n",
        "    audio_in_prompt = \"\".join([f\"Audio {i+1}: <|audio_bos|><|AUDIO|><|audio_eos|>\\\\n\" for i in range(audio_count)])\n",
        "    prompt = (\"<|im_start|>system\\\\nYou are a forensic audio assistant with deepfake expertise.\\\\n<|im_end|>\\\\n\"\n",
        "              \"<|im_start|>user\\\\n\" + audio_in_prompt + question + \"<|im_end|>\\\\n\"\n",
        "              \"<|im_start|>assistant\\\\n\")\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "def run_ultravox(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Ultravox...\")\n",
        "    model_name = \"fixie-ai/ultravox-v0_5-llama-3_2-1b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': (\"<|audio|>\\\\n\" * audio_count) + question\n",
        "    }]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        trust_remote_code=True,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "def run_whisper(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Whisper...\")\n",
        "    assert audio_count == 1, \"Whisper supports single audio input\"\n",
        "    model_name = \"openai/whisper-large-v3-turbo\"\n",
        "    prompt = \"<|startoftranscript|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=448,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "async def vllm_multimodal_inference(audio_path, question, models=None):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running vLLM multimodal inference...\")\n",
        "    if models is None:\n",
        "        models = [\"minicpmo\", \"phi4_mm\", \"qwen2_audio\", \"ultravox\", \"whisper\"]\n",
        "    results = {}\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    audio_assets = [(audio, sr)]\n",
        "    async def process_model(model_type):\n",
        "        constructor = get_vllm_audio_model_configs()[model_type]\n",
        "        req_data = constructor(question, audio_count=1)\n",
        "        req_data.engine_args.limit_mm_per_prompt = {\"audio\": 1}\n",
        "        llm = LLM(**req_data.engine_args.__dict__)\n",
        "        sampling_params = SamplingParams(temperature=0.2, max_tokens=128, stop_token_ids=req_data.stop_token_ids)\n",
        "        mm_data = {\"audio\": audio_assets}\n",
        "        inputs = {\"prompt\": req_data.prompt, \"multi_modal_data\": mm_data}\n",
        "        try:\n",
        "            outputs = await loop.run_in_executor(executor, lambda: llm.generate([inputs], sampling_params=sampling_params, lora_request=req_data.lora_requests if req_data.lora_requests else None))\n",
        "            text = outputs[0].outputs[0].text\n",
        "        except Exception as ex:\n",
        "            print(f\"[Warning] vLLM output error for {model_type}: {ex}\")\n",
        "            text = \"[vLLM ERROR: no output]\"\n",
        "        results[model_type] = text\n",
        "        print(f\"[Async] vLLM {model_type} result: {text[:80]}...\")\n",
        "    await asyncio.gather(*(process_model(m) for m in models))\n",
        "    return results"
      ],
      "metadata": {
        "id": "ypHkg92ovyDA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- LLM Forensic Reporting -----\n",
        "async def langchain_llm_report(report_data):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Generating forensic report via LangChain...\")\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"verdict\", \"mean_score\", \"anomalies\", \"asr\", \"asr_lang\", \"speakers\", \"spoof\", \"features\", \"noise_quality\", \"gender_dist\", \"vllm_outputs\"],\n",
        "        template=(\n",
        "            \"You are a leading audio forensics expert. Based on the following details, provide a detailed forensic report with risk levels, evidence table, and actionable recommendations.\\n\\n\"\n",
        "            \"Verdict: {verdict}\\n\"\n",
        "            \"Mean Score: {mean_score:.3f}\\n\"\n",
        "            \"Detected Anomalies: {anomalies}\\n\"\n",
        "            \"ASR Transcript (first 400 chars): {asr}\\n\"\n",
        "            \"ASR Language: {asr_lang}\\n\"\n",
        "            \"Speaker Identities: {speakers}\\n\"\n",
        "            \"Spoof Score: {spoof}\\n\"\n",
        "            \"Extracted Audio Features: {features}\\n\"\n",
        "            \"Noise/Quality Score: {noise_quality}\\n\"\n",
        "            \"Gender Distribution: {gender_dist}\\n\"\n",
        "            \"Audio LLM Model Outputs: {vllm_outputs}\\n\\n\"\n",
        "            \"Also, include a Mermaid.js flowchart of the forensic analysis process.\"\n",
        "        ),\n",
        "    )\n",
        "    llm = OpenAI(temperature=0.2, max_tokens=700)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    def run_chain():\n",
        "        return chain.run(\n",
        "            verdict=report_data['verdict'],\n",
        "            mean_score=report_data['mean_score'],\n",
        "            anomalies=\", \".join(report_data['all_anomalies']),\n",
        "            asr=report_data['asr_transcript'][:400] + \"...\",\n",
        "            asr_lang=report_data.get('asr_lang', 'unknown'),\n",
        "            speakers=\", \".join(report_data['speaker_identities']),\n",
        "            spoof=str(report_data['speaker_spoof_score']),\n",
        "            features=\"; \".join([f\"{k}: {v:.3f}\" for k, v in report_data['features'].items() if isinstance(v, (int, float))]),\n",
        "            noise_quality=str(report_data.get('noise_quality_score', 'N/A')),\n",
        "            gender_dist=json.dumps(report_data.get('gender_distribution', {})),\n",
        "            vllm_outputs=json.dumps(report_data.get('vllm_model_outputs', {}))\n",
        "        )\n",
        "    summary = await loop.run_in_executor(executor, run_chain)\n",
        "    print(\"[Async] Forensic report generated.\")\n",
        "    return summary\n",
        "\n",
        "async def aggregate_and_report(audio_path, results, feats, asr, asr_lang, speaker_identities,\n",
        "                               spk_score, noise_quality_score, gender_dist, vllm_model_outputs, extra_results={}):\n",
        "    print(\"[Async] Aggregating results...\")\n",
        "    scores, all_anomalies, all_model_scores, detailed = [], [], {}, {}\n",
        "    for agent, (score, anomalies, detail) in results.items():\n",
        "        scores.append(score)\n",
        "        all_model_scores[agent] = score\n",
        "        all_anomalies += anomalies\n",
        "        detailed[agent] = {\"score\": score, \"anomalies\": anomalies, \"detail\": detail}\n",
        "    mean_score = float(np.mean(scores))\n",
        "    confidence = 1.0 - np.std(scores)\n",
        "    verdict = (\"Likely FAKE (deepfake/spoof detected)\" if mean_score > 0.7\n",
        "               else \"Possibly FAKE (review anomalies)\" if mean_score > 0.5 else \"Likely REAL\")\n",
        "    try:\n",
        "        natural_summary = await langchain_llm_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities,\n",
        "            \"speaker_spoof_score\": spk_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"vllm_model_outputs\": vllm_model_outputs\n",
        "        })\n",
        "    except Exception as e:\n",
        "        natural_summary = f\"Verdict: {verdict} (LangChain LLM error: {e})\"\n",
        "    detailed.update(extra_results)\n",
        "    print(\"[Async] Aggregation complete.\")\n",
        "    return ForensicReport(\n",
        "        file=audio_path,\n",
        "        verdict=verdict,\n",
        "        mean_score=mean_score,\n",
        "        confidence=confidence,\n",
        "        all_model_scores=all_model_scores,\n",
        "        all_anomalies=list(set(all_anomalies)),\n",
        "        natural_summary=natural_summary,\n",
        "        asr_transcript=asr,\n",
        "        asr_lang=asr_lang,\n",
        "        speaker_identities=speaker_identities,\n",
        "        speaker_spoof_score=spk_score,\n",
        "        noise_quality_score=noise_quality_score,\n",
        "        gender_distribution=gender_dist,\n",
        "        detailed_results=detailed,\n",
        "        timestamp=datetime.utcnow().isoformat(),\n",
        "        vllm_model_outputs=vllm_model_outputs\n",
        "    )"
      ],
      "metadata": {
        "id": "j4WbCxddv_iF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Main Pipeline -----\n",
        "async def deepfake_defensive_pipeline(audio_path: str, vllm_question=None):\n",
        "    print(\"[Pipeline] Initiating forensic analysis...\")\n",
        "    if vllm_question is None:\n",
        "        vllm_question = (\n",
        "            \"Analyze this audio sample for deepfake, spoof, scam or synthetic voice. \"\n",
        "            \"Extract evidence including speaker identity, replay markers, emotion tone, ASR transcript, and statistical features. \"\n",
        "            \"Provide a risk score (0-1), evidence table, and list anomalies.\"\n",
        "        )\n",
        "    audio, sr = await preprocess_audio(audio_path)\n",
        "    features, vad_ratio = await asyncio.gather(extract_features(audio, sr), extract_vad_ratio(audio, sr))\n",
        "    features['vad_ratio'] = vad_ratio\n",
        "    nb_segments, nb_speech, gender_dist = await extract_inaspeech_segments(audio_path)\n",
        "    features['nb_segments'] = nb_segments\n",
        "    features['nb_speech_segments'] = nb_speech\n",
        "    langid_label, langid_conf = await run_language_id(audio_path)\n",
        "    features['langid_label'] = langid_label\n",
        "    features['langid_conf'] = langid_conf\n",
        "    speaker_identities, diar_segments = await run_speechbrain_speaker(audio_path)\n",
        "    spk_score = await run_speechbrain_verification(audio_path)\n",
        "    spoof_score, spoof_anom = await run_speechbrain_spoof(audio_path)\n",
        "    emo_score, emo_anom, emo_desc = await run_emotion_analysis(audio, sr)\n",
        "    wave_score, wave_anom, wave_detail = await run_wave2vec_fake_detection(audio_path)\n",
        "    replay_score, replay_anom = await run_replay_attack_detection(audio, sr)\n",
        "    noise_quality = await run_noise_quality_estimation(audio, sr)\n",
        "    augmented_audio = await run_speechbrain_augmentation(audio, sr)\n",
        "    aug_feats = await extract_features(augmented_audio, sr)\n",
        "    aug_tests = {\"augmentation_feature_diff\": np.abs(features['zcr'] - aug_feats['zcr'])}\n",
        "    extra_results = {\"asr_langid_audio\": {\"label\": langid_label, \"conf\": langid_conf},\n",
        "                     \"speaker_diarization_segments\": diar_segments,\n",
        "                     \"augmentation_tests\": aug_tests}\n",
        "    vllm_outputs = await vllm_multimodal_inference(audio_path, vllm_question)\n",
        "    agents_results = {\"statistical_wav2vec\": (wave_score, wave_anom, wave_detail),\n",
        "                      \"replay_attack\": (replay_score, replay_anom, \"Replay detection via RMS\"),\n",
        "                      \"emotion_analysis\": (emo_score, emo_anom, emo_desc),\n",
        "                      \"speechbrain_spoof\": (spoof_score, spoof_anom, \"SpeechBrain anti-spoof analysis\")}\n",
        "    report = await aggregate_and_report(\n",
        "        audio_path, agents_results, features, asr=\"\", asr_lang=\"N/A\",\n",
        "        speaker_identities=speaker_identities, spk_score=spk_score,\n",
        "        noise_quality_score=noise_quality, gender_dist=gender_dist,\n",
        "        vllm_model_outputs=vllm_outputs, extra_results=extra_results\n",
        "    )\n",
        "    print(\"[Pipeline] Forensic analysis complete.\")\n",
        "    return report"
      ],
      "metadata": {
        "id": "qeMz0uMtwCh6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Visualization -----\n",
        "def display_audio_waveform(audio_path, sr=16000):\n",
        "    print(\"[Viz] Displaying waveform...\")\n",
        "    audio, sr = librosa.load(audio_path, sr=sr)\n",
        "    plt.figure(figsize=(14,3))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title(\"Audio Waveform\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "def display_mermaid_diagram():\n",
        "    print(\"[Viz] Rendering Mermaid.js flowchart...\")\n",
        "    mermaid_chart = \"\"\"\n",
        "    graph TD\n",
        "      A[Upload Audio File] --> B[Preprocess Audio]\n",
        "      B --> C[Extract Features & VAD]\n",
        "      C --> D[Speaker Diarization & Verification]\n",
        "      D --> E[Spoof, Emotion & Replay Analysis]\n",
        "      E --> F[SpeechBrain Augmentation]\n",
        "      F --> G[vLLM Multimodal Inference]\n",
        "      G --> H[Aggregate Results]\n",
        "      H --> I[Generate Forensic Report]\n",
        "    \"\"\"\n",
        "    html_str = f\"\"\"\n",
        "    <html>\n",
        "      <head>\n",
        "        <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
        "      </head>\n",
        "      <body>\n",
        "        <div class=\"mermaid\">\n",
        "          {mermaid_chart}\n",
        "        </div>\n",
        "        <script>mermaid.initialize({{startOnLoad:true}});</script>\n",
        "      </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    display(HTML(html_str))\n",
        "\n",
        "def pretty_print_report(report: ForensicReport):\n",
        "    print(f\"\\n\\033[1mForensic Verdict:\\033[0m {report.verdict}\")\n",
        "    print(f\"\\033[1mMean Deepfake Score:\\033[0m {report.mean_score:.2f}\")\n",
        "    print(f\"\\033[1mConfidence:\\033[0m {report.confidence:.2f}\")\n",
        "    print(f\"\\033[1mDetected Anomalies:\\033[0m {', '.join(report.all_anomalies) or 'None'}\")\n",
        "    print(f\"\\033[1mSpeaker Identities:\\033[0m {', '.join(report.speaker_identities)}\")\n",
        "    print(f\"\\033[1mSpoof Score:\\033[0m {report.speaker_spoof_score:.2f}\")\n",
        "    print(f\"\\033[1mNoise/Quality Score:\\033[0m {report.noise_quality_score:.2f}\")\n",
        "    print(f\"\\033[1mGender Distribution:\\033[0m {report.gender_distribution}\")\n",
        "    print(f\"\\033[1mvLLM Audio Model Outputs:\\033[0m\")\n",
        "    for m, t in report.vllm_model_outputs.items():\n",
        "        print(f\"   \\033[1m{m}:\\033[0m {t[:300]}{'...' if len(t)>300 else ''}\")\n",
        "    print(f\"\\033[1mTimestamp:\\033[0m {report.timestamp}\")\n",
        "    print(\"\\n\\033[1mNatural-Language LLM Summary:\\033[0m\\n\", report.natural_summary)\n",
        "    print(\"\\n\\033[1mRaw JSON Output:\\033[0m\")\n",
        "    print(report.json(indent=2))\n",
        "\n",
        "# ----- Colab UI -----\n",
        "def colab_audio_deepfake_ui():\n",
        "    from google.colab import files\n",
        "    clear_output()\n",
        "    print(\"## Deepfake Audio Forensic Pipeline (Async & Parallel Optimized) ##\")\n",
        "    print(\"Upload an audio or video file (wav/mp3/flac/m4a or video formats like mp4):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return\n",
        "    filename = next(iter(uploaded))\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    ext = ext.lower()\n",
        "    # Display file size in MB\n",
        "    file_size_bytes = os.path.getsize(filename)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "    print(f\"[UI] File '{filename}' size: {file_size_mb:.2f} MB\")\n",
        "    if ext != \".wav\":\n",
        "        print(\"[UI] Converting file to WAV (extracting audio if video)...\")\n",
        "        if ext in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "            clip = VideoFileClip(filename)\n",
        "            wav_path = base + \".wav\"\n",
        "            clip.audio.write_audiofile(wav_path, fps=16000, logger=None)\n",
        "            clip.close()\n",
        "            audio_path = wav_path\n",
        "        else:\n",
        "            audio = AudioSegment.from_file(filename)\n",
        "            wav_path = base + \".wav\"\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            audio_path = wav_path\n",
        "    else:\n",
        "        audio_path = filename\n",
        "    print(\"\\n== Displaying Audio Waveform ==\")\n",
        "    display_audio_waveform(audio_path)\n",
        "    print(\"\\n== Running Forensic Analysis (Async & Parallel) ==\")\n",
        "    report = asyncio.run(deepfake_defensive_pipeline(audio_path))\n",
        "    print(\"\\n== Displaying Pipeline Flow Diagram ==\")\n",
        "    display_mermaid_diagram()\n",
        "    print(\"\\n== Final Forensic Report ==\")\n",
        "    pretty_print_report(report)\n",
        "\n",
        "# Execute the UI in Colab\n",
        "colab_audio_deepfake_ui()"
      ],
      "metadata": {
        "id": "0oNPiih4wFvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}