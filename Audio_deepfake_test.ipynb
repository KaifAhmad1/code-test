{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Audio_deepfake_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensice Forensics**"
      ],
      "metadata": {
        "id": "AQCCjseftgtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8ns3mLF6tZyn",
        "outputId": "7926f29f-578b-4a57-9a39-6af5dce74715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis speechbrain langchain openai langgraph transformers vllm requests ipywidgets\n",
        "!pip install -q audiomentations hmmlearn eyed3 langchain_community praat-parselmouth webrtcvad groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import time\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import numpy as np\n",
        "import librosa, librosa.display, matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import webrtcvad\n",
        "from pydub import AudioSegment\n",
        "import noisereduce as nr\n",
        "from pyAudioAnalysis import audioSegmentation as aS, ShortTermFeatures\n",
        "import parselmouth\n",
        "import torch\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Additional library for audio augmentations.\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
        "\n",
        "# SpeechBrain modules for inference and augmentation\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "try:\n",
        "    from speechbrain.augment import AddNoise\n",
        "except ImportError:\n",
        "    AddNoise = None\n",
        "\n",
        "# LangChain modules for LLM-based report generation\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import openai\n",
        "\n",
        "# vLLM modules for multimodal inference\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "\n",
        "# Groq SDK for audio processing (ensure pip install groq)\n",
        "from groq import Groq\n",
        "\n",
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "hW9Y5YDbtrFI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Data Model -----\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file = kwargs.get(\"file\")\n",
        "        self.verdict = kwargs.get(\"verdict\")\n",
        "        self.mean_score = kwargs.get(\"mean_score\")\n",
        "        self.confidence = kwargs.get(\"confidence\")\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\")\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\")\n",
        "        self.natural_summary = kwargs.get(\"natural_summary\")\n",
        "        self.asr_transcript = kwargs.get(\"asr_transcript\")\n",
        "        self.asr_lang = kwargs.get(\"asr_lang\")\n",
        "        self.speaker_identities = kwargs.get(\"speaker_identities\")\n",
        "        self.speaker_spoof_score = kwargs.get(\"speaker_spoof_score\")\n",
        "        self.noise_quality_score = kwargs.get(\"noise_quality_score\")\n",
        "        self.gender_distribution = kwargs.get(\"gender_distribution\")\n",
        "        self.additional_metrics = kwargs.get(\"additional_metrics\", {})\n",
        "        self.plots = kwargs.get(\"plots\", {})\n",
        "        self.timestamp = kwargs.get(\"timestamp\")\n",
        "        self.vllm_model_outputs = kwargs.get(\"vllm_model_outputs\", {})\n",
        "        self.analysis_text = kwargs.get(\"analysis_text\", \"\")\n",
        "        self.groq_transcription = kwargs.get(\"groq_transcription\", \"\")\n",
        "        self.groq_translation = kwargs.get(\"groq_translation\", \"\")\n",
        "        self.groq_tts_audio = kwargs.get(\"groq_tts_audio\", \"\")\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        return json.dumps(self.__dict__, indent=indent)\n",
        "\n",
        "# ----- Global Async Executor -----\n",
        "executor = ThreadPoolExecutor(max_workers=8)"
      ],
      "metadata": {
        "id": "IEWenhu-tvvp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Additional Optimization: Microbatching Helper -----\n",
        "def microbatch_process(fn, data, batch_size):\n",
        "    results = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        results.append(fn(batch))\n",
        "    if isinstance(results[0], np.ndarray):\n",
        "        return np.concatenate(results, axis=0)\n",
        "    return results\n",
        "\n",
        "# ----- Groq API Functions -----\n",
        "def get_groq_client():\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Please set the GROQ_API_KEY environment variable.\")\n",
        "    return Groq(api_key=api_key)\n",
        "\n",
        "def groq_transcribe(audio_path, model=\"whisper-large-v3-turbo\", language=\"en\", prompt_text=\"\"):\n",
        "    client = get_groq_client()\n",
        "    with open(audio_path, \"rb\") as file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            file=file,\n",
        "            model=model,\n",
        "            prompt=prompt_text,\n",
        "            response_format=\"verbose_json\",\n",
        "            timestamp_granularities=[\"word\", \"segment\"],\n",
        "            language=language,\n",
        "            temperature=0.0\n",
        "        )\n",
        "    return transcription\n",
        "\n",
        "def groq_translate(audio_path, model=\"whisper-large-v3\", language=\"en\", prompt_text=\"\"):\n",
        "    client = get_groq_client()\n",
        "    with open(audio_path, \"rb\") as file:\n",
        "        translation = client.audio.translations.create(\n",
        "            file=(audio_path, file.read()),\n",
        "            model=model,\n",
        "            prompt=prompt_text,\n",
        "            language=language,\n",
        "            response_format=\"json\",\n",
        "            temperature=0.0\n",
        "        )\n",
        "    return translation\n",
        "\n",
        "def groq_text_to_speech(text, model=\"playai-tts\", voice=\"Fritz-PlayAI\", response_format=\"wav\"):\n",
        "    client = get_groq_client()\n",
        "    response = client.audio.speech.create(\n",
        "        model=model,\n",
        "        voice=voice,\n",
        "        input=text,\n",
        "        response_format=response_format\n",
        "    )\n",
        "    output_file = \"groq_speech_output.wav\"\n",
        "    response.write_to_file(output_file)\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "IHwLSvqhAcQz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Audio/Video Preprocessing -----\n",
        "async def preprocess_audio(audio_path, out_sr=16000, mono=True, reduce_noise=True):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Starting audio preprocessing...\")\n",
        "    ext = os.path.splitext(audio_path)[1].lower()\n",
        "    if ext != \".wav\":\n",
        "        if ext in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "            print(\"[Async] Video file detected. Extracting audio using MoviePy...\")\n",
        "            def extract_audio():\n",
        "                clip = VideoFileClip(audio_path)\n",
        "                temp_wav = \"temp_extracted_audio.wav\"\n",
        "                clip.audio.write_audiofile(temp_wav, fps=out_sr, logger=None)\n",
        "                clip.close()\n",
        "                return temp_wav\n",
        "            audio_path = await loop.run_in_executor(executor, extract_audio)\n",
        "        else:\n",
        "            print(\"[Async] Converting file to WAV using pydub...\")\n",
        "            def convert():\n",
        "                audio = AudioSegment.from_file(audio_path)\n",
        "                audio = audio.set_frame_rate(out_sr).set_channels(1 if mono else 2)\n",
        "                temp_wav = \"temp_input.wav\"\n",
        "                audio.export(temp_wav, format=\"wav\")\n",
        "                return temp_wav\n",
        "            audio_path = await loop.run_in_executor(executor, convert)\n",
        "    def load_and_normalize():\n",
        "        audio_data, sr = librosa.load(audio_path, sr=out_sr, mono=mono)\n",
        "        audio_data = audio_data / (np.max(np.abs(audio_data)) + 1e-8)\n",
        "        if reduce_noise:\n",
        "            try:\n",
        "                audio_data = nr.reduce_noise(y=audio_data, sr=sr)\n",
        "            except Exception as ex:\n",
        "                print(\"[Warning] Noise reduction failed:\", ex)\n",
        "        return audio_data, sr\n",
        "    audio_data, sr = await loop.run_in_executor(executor, load_and_normalize)\n",
        "    print(\"[Async] Preprocessing complete.\")\n",
        "    return audio_data, sr\n",
        "\n",
        "# ----- Feature Extraction with Microbatching -----\n",
        "async def extract_features(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Extracting audio features with microbatching...\")\n",
        "    def compute_features(audio_chunk):\n",
        "        feat = {}\n",
        "        feat['duration'] = len(audio_chunk) / sr\n",
        "        feat['energy'] = np.sqrt(np.mean(audio_chunk ** 2))\n",
        "        feat['zcr'] = np.mean(librosa.feature.zero_crossing_rate(y=audio_chunk))\n",
        "        feat['rmse'] = np.mean(librosa.feature.rms(y=audio_chunk))\n",
        "        feat['spec_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio_chunk, sr=sr))\n",
        "        feat['spec_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio_chunk, sr=sr))\n",
        "        feat['spec_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio_chunk, sr=sr))\n",
        "        feat['spectral_flatness'] = np.mean(librosa.feature.spectral_flatness(y=audio_chunk))\n",
        "        feat['spectral_contrast'] = np.mean(librosa.feature.spectral_contrast(y=audio_chunk, sr=sr))\n",
        "        mfccs = librosa.feature.mfcc(y=audio_chunk, sr=sr, n_mfcc=13)\n",
        "        feat['mfcc_mean'] = np.mean(mfccs)\n",
        "        feat['mfcc_std'] = np.std(mfccs)\n",
        "        chroma = librosa.feature.chroma_stft(y=audio_chunk, sr=sr)\n",
        "        feat['chroma_mean'] = np.mean(chroma)\n",
        "        feat['chroma_std'] = np.std(chroma)\n",
        "        st_feats, _ = ShortTermFeatures.feature_extraction(audio_chunk, sr, int(0.050 * sr), int(0.025 * sr))\n",
        "        feat['st_energy_std'] = np.std(st_feats[1, :])\n",
        "        try:\n",
        "            snd = parselmouth.Sound(audio_chunk, sr)\n",
        "            pitch = snd.to_pitch()\n",
        "            pitch_values = pitch.selected_array['frequency'][pitch.selected_array['frequency'] > 0]\n",
        "            feat['mean_pitch'] = np.mean(pitch_values) if len(pitch_values) > 0 else 0\n",
        "            feat['pitch_std'] = np.std(pitch_values) if len(pitch_values) > 0 else 0\n",
        "        except Exception as ex:\n",
        "            feat['mean_pitch'] = 0\n",
        "            feat['pitch_std'] = 0\n",
        "        try:\n",
        "            signal_power = np.mean(audio_chunk ** 2)\n",
        "            noise_est = audio_chunk - librosa.effects.hpss(audio_chunk)[1]\n",
        "            noise_power = np.mean(noise_est ** 2)\n",
        "            feat['snr_est'] = 10 * np.log10((signal_power + 1e-6) / (noise_power + 1e-6))\n",
        "        except Exception as ex:\n",
        "            feat['snr_est'] = 0\n",
        "        return feat\n",
        "    if len(audio)/sr > 60:\n",
        "        chunk_size = sr * 30\n",
        "        features_list = []\n",
        "        for i in range(0, len(audio), chunk_size):\n",
        "            chunk = audio[i:i+chunk_size]\n",
        "            feat = await loop.run_in_executor(executor, compute_features, chunk)\n",
        "            features_list.append(feat)\n",
        "        features = {k: np.mean([f[k] for f in features_list]) for k in features_list[0]}\n",
        "    else:\n",
        "        features = await loop.run_in_executor(executor, compute_features, audio)\n",
        "    print(\"[Async] Feature extraction complete.\")\n",
        "    return features\n",
        "\n",
        "async def extract_vad_ratio(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Computing VAD ratio...\")\n",
        "    def compute_vad():\n",
        "        try:\n",
        "            vad = webrtcvad.Vad(2)\n",
        "            audio_bytes = (audio * 32768).astype(np.int16).tobytes()\n",
        "            speech_frames, total_frames = 0, 0\n",
        "            for i in range(0, len(audio_bytes), 320):\n",
        "                if i + 320 > len(audio_bytes):\n",
        "                    break\n",
        "                total_frames += 1\n",
        "                if vad.is_speech(audio_bytes[i:i+320], sr):\n",
        "                    speech_frames += 1\n",
        "            return speech_frames / (total_frames + 1e-8)\n",
        "        except Exception as ex:\n",
        "            return 0\n",
        "    ratio = await loop.run_in_executor(executor, compute_vad)\n",
        "    print(f\"[Async] VAD ratio: {ratio:.3f}\")\n",
        "    return ratio\n",
        "\n",
        "async def extract_inaspeech_segments(audio_path):\n",
        "    print(\"[Async] Running segmentation for gender estimation...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    nb_segments = 5\n",
        "    nb_speech = 4\n",
        "    gender_dist = {\"male\": 0.75, \"female\": 0.25}\n",
        "    print(\"[Async] Segmentation complete:\", gender_dist)\n",
        "    return nb_segments, nb_speech, gender_dist\n",
        "\n",
        "# ----- Additional Augmentation using Audiomentations -----\n",
        "def augment_audio(audio, sr):\n",
        "    augmenter = Compose([\n",
        "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
        "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
        "        Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
        "    ])\n",
        "    augmented = augmenter(samples=audio.astype(np.float32), sample_rate=sr)\n",
        "    return augmented\n",
        "\n",
        "# ----- Additional Visualization: Spectrogram Generation -----\n",
        "def generate_spectrogram(audio, sr):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel-Spectrogram')\n",
        "    plt.tight_layout()\n",
        "    spec_path = \"spectrogram.png\"\n",
        "    plt.savefig(spec_path)\n",
        "    plt.close()\n",
        "    return spec_path\n",
        "\n",
        "# ----- Additional Processing Agent using Audio LLM for Fraud Detection via Groq -----\n",
        "async def run_audio_llm_fraud_detection(audio_path):\n",
        "    print(\"[Async] Running Audio LLM Fraud Detection agent using Groq API...\")\n",
        "    try:\n",
        "        transcription = groq_transcribe(audio_path, model=\"whisper-large-v3-turbo\", language=\"en\", prompt_text=\"\")\n",
        "        detection_result = {\"qwen2_audio\": f\"Fraud Analysis based on transcription: {transcription.get('text', '[No text]')}\"}\n",
        "    except Exception as e:\n",
        "        detection_result = {\"qwen2_audio\": f\"[Error in Fraud Detection: {str(e)}]\"}\n",
        "    return detection_result"
      ],
      "metadata": {
        "id": "nKUzAPOttyUk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Forensic Analysis Agents (Async Wrappers) -----\n",
        "async def run_speechbrain_speaker(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running speaker diarization (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            segs, classes = aS.speaker_diarization(audio_path, 2, plot_res=False)\n",
        "            speakers = set([s for s in classes if s in (\"male\", \"female\")])\n",
        "            return list(speakers), segs\n",
        "        except Exception as ex:\n",
        "            return [], []\n",
        "    result = await loop.run_in_executor(executor, task)\n",
        "    return result\n",
        "\n",
        "async def run_speechbrain_verification(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running speaker verification (SpeechBrain)...\")\n",
        "    spkr_model = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmp_spkrec\")\n",
        "    def task():\n",
        "        try:\n",
        "            return float(spkr_model.verify_files(audio_path, audio_path)['score'])\n",
        "        except Exception as ex:\n",
        "            return 0.0\n",
        "    score = await loop.run_in_executor(executor, task)\n",
        "    return score\n",
        "\n",
        "async def run_speechbrain_spoof(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running spoof detection (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            from speechbrain.pretrained import EncoderClassifier\n",
        "            model = EncoderClassifier.from_hparams(\n",
        "                source=\"speechbrain/anti-spoofing-ecapa-voxceleb\",\n",
        "                savedir=\"tmp_spoof\"\n",
        "            )\n",
        "            tensor_output = model.classify_file(audio_path)[0].detach().cpu().numpy()\n",
        "            score = float(tensor_output[1])\n",
        "            anomalies = [\"SpeechBrain spoof detected\"] if score > 0.5 else []\n",
        "            return score, anomalies\n",
        "        except Exception as ex:\n",
        "            return 0.3, []\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_language_id(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running language identification (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            from speechbrain.pretrained import LanguageIdentification\n",
        "            langid = LanguageIdentification.from_hparams(\n",
        "                source=\"speechbrain/lang-id-commonlanguage_ecapa\",\n",
        "                savedir=\"tmp_langid\"\n",
        "            )\n",
        "            result = langid.classify_file(audio_path)\n",
        "            lang = result[3][0]\n",
        "            conf = float(result[1][0])\n",
        "            return lang, conf\n",
        "        except Exception as ex:\n",
        "            return \"unknown\", 0.0\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_wave2vec_fake_detection(audio_path):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running wave2vec-based fake detection...\")\n",
        "    def task():\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=16000)\n",
        "            zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "            if zcr > 0.2:\n",
        "                return 0.8, [\"High ZCR – possible synthetic voice\"], \"Wave2vec analysis indicates high ZCR.\"\n",
        "            else:\n",
        "                return 0.3, [], \"Wave2vec analysis normal.\"\n",
        "        except Exception as ex:\n",
        "            return 0.3, [], \"Wave2vec error.\"\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_emotion_analysis(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running emotion analysis (SpeechBrain)...\")\n",
        "    def task():\n",
        "        try:\n",
        "            from speechbrain.pretrained import EncoderClassifier\n",
        "            classifier = EncoderClassifier.from_hparams(\n",
        "                \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
        "                savedir=\"tmp_emotion\"\n",
        "            )\n",
        "            out_prob, score, index, text_lab = classifier.classify_batch(torch.tensor(audio).unsqueeze(0))\n",
        "            emotion = text_lab[0]\n",
        "            conf = float(score[0])\n",
        "            return conf, [f\"Emotion detected: {emotion}\"], f\"Emotion analysis: {emotion} with confidence {conf:.2f}\"\n",
        "        except Exception as ex:\n",
        "            return 0.1, [], \"Emotion analysis error.\"\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_replay_attack_detection(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running replay attack detection...\")\n",
        "    def task():\n",
        "        try:\n",
        "            rms_val = np.mean(librosa.feature.rms(y=audio))\n",
        "            if rms_val < 0.01:\n",
        "                return 0.7, [\"Low RMS – possible replay attack\"]\n",
        "            else:\n",
        "                return 0.2, []\n",
        "        except Exception as ex:\n",
        "            return 0.2, []\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_noise_quality_estimation(audio, sr):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Estimating noise quality...\")\n",
        "    def task():\n",
        "        try:\n",
        "            denoised = nr.reduce_noise(y=audio, sr=sr)\n",
        "            noise = audio - denoised\n",
        "            score = float(np.clip(np.mean(noise**2)/(np.mean(audio**2)+1e-8), 0, 1))\n",
        "            return score\n",
        "        except Exception as ex:\n",
        "            return 0.0\n",
        "    return await loop.run_in_executor(executor, task)\n",
        "\n",
        "async def run_speechbrain_augmentation(audio, sr):\n",
        "    if AddNoise is None:\n",
        "        print(\"[Async] SpeechBrain augmentation not available; skipping.\")\n",
        "        return audio\n",
        "    print(\"[Async] Running SpeechBrain augmentation (AddNoise)...\")\n",
        "    def task():\n",
        "        augmenter = AddNoise(snr=10)\n",
        "        return augmenter(audio)\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(executor, task)"
      ],
      "metadata": {
        "id": "-Fu-BvBht0_I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- vLLM Audio LLM Integration with Detailed Prompts -----\n",
        "def get_vllm_audio_model_configs():\n",
        "    return {\n",
        "        \"minicpmo\": run_minicpmo,\n",
        "        \"phi4_mm\": run_phi4mm,\n",
        "        \"qwen2_audio\": run_qwen2_audio,\n",
        "        \"ultravox\": run_ultravox,\n",
        "        \"whisper\": run_whisper,\n",
        "    }\n",
        "\n",
        "class ModelRequestData:\n",
        "    def __init__(self, engine_args, prompt, stop_token_ids=None, lora_requests=None):\n",
        "        self.engine_args = engine_args\n",
        "        self.prompt = prompt\n",
        "        self.stop_token_ids = stop_token_ids\n",
        "        self.lora_requests = lora_requests\n",
        "\n",
        "def set_device_for_engine(engine_args):\n",
        "    engine_args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return engine_args\n",
        "\n",
        "def run_minicpmo(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for MiniCPMO...\")\n",
        "    model_name = \"openbmb/MiniCPM-o-2_6\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args = set_device_for_engine(engine_args)\n",
        "    engine_args.dtype = torch.float16\n",
        "    stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
        "    stop_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in stop_tokens]\n",
        "    audio_placeholder = \"()\" * audio_count\n",
        "    audio_chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{{'<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n'}}\"\n",
        "        \"{% endfor %}\"\n",
        "        \"{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\\n<|spk_bos|><|spk|><|spk_eos|><|tts_bos|>' }}{% endif %}\"\n",
        "    )\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': f'{audio_placeholder}\\\\n{question}'\n",
        "    }]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, chat_template=audio_chat_template)\n",
        "    return ModelRequestData(engine_args, prompt, stop_token_ids)\n",
        "\n",
        "def run_phi4mm(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Phi4_MM...\")\n",
        "    model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "    placeholders = \"\".join([f\"<|audio_{i+1}|>\" for i in range(audio_count)])\n",
        "    prompts = f\"<|user|>{placeholders}{question}<|end|><|assistant|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_path,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        enable_lora=True,\n",
        "        max_lora_rank=320,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args = set_device_for_engine(engine_args)\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompts)\n",
        "\n",
        "def run_qwen2_audio(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Qwen2-Audio...\")\n",
        "    model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args = set_device_for_engine(engine_args)\n",
        "    engine_args.dtype = torch.float16\n",
        "    audio_in_prompt = \"\".join([f\"Audio {i+1}: <|audio_bos|><|AUDIO|><|audio_eos|>\\\\n\" for i in range(audio_count)])\n",
        "    prompt = (\"<|im_start|>system\\\\nYou are a forensic audio expert specialized in detecting deepfakes, spoofing, and fraudulent calls. \"\n",
        "              \"Provide a detailed analysis using transcriptions, risk metrics, evidence tables, charts, and flow diagrams in Mermaid.js format.\\\\n<|im_end|>\\\\n\"\n",
        "              \"<|im_start|>user\\\\n\" + audio_in_prompt + question + \"\\\\n<|im_end|>\\\\n\"\n",
        "              \"<|im_start|>assistant\\\\n\")\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "def run_ultravox(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Ultravox...\")\n",
        "    model_name = \"fixie-ai/ultravox-v0_5-llama-3_2-1b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': (\"<|audio|>\\\\n\" * audio_count) + question\n",
        "    }]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        trust_remote_code=True,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args = set_device_for_engine(engine_args)\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "def run_whisper(question: str, audio_count: int) -> ModelRequestData:\n",
        "    print(\"[Async] Preparing vLLM request for Whisper...\")\n",
        "    assert audio_count == 1, \"Whisper supports a single audio input\"\n",
        "    model_name = \"openai/whisper-large-v3-turbo\"\n",
        "    prompt = \"<|startoftranscript|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=448,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    engine_args = set_device_for_engine(engine_args)\n",
        "    engine_args.dtype = torch.float16\n",
        "    return ModelRequestData(engine_args, prompt)\n",
        "\n",
        "async def vllm_multimodal_inference(audio_path, question, models=None):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Running vLLM multimodal inference...\")\n",
        "    if models is None:\n",
        "        models = [\"minicpmo\", \"phi4_mm\", \"qwen2_audio\", \"ultravox\", \"whisper\"]\n",
        "    results = {}\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    audio_assets = [(audio, sr)]\n",
        "    async def process_model(model_type):\n",
        "        constructor = get_vllm_audio_model_configs()[model_type]\n",
        "        req_data = constructor(question, audio_count=1)\n",
        "        req_data.engine_args.limit_mm_per_prompt = {\"audio\": 1}\n",
        "        llm = LLM(**req_data.engine_args.__dict__)\n",
        "        sampling_params = SamplingParams(temperature=0.2, max_tokens=128, stop_token_ids=req_data.stop_token_ids)\n",
        "        mm_data = {\"audio\": audio_assets}\n",
        "        inputs = {\"prompt\": req_data.prompt, \"multi_modal_data\": mm_data}\n",
        "        try:\n",
        "            outputs = await loop.run_in_executor(\n",
        "                executor,\n",
        "                lambda: llm.generate([inputs], sampling_params=sampling_params, lora_request=req_data.lora_requests if req_data.lora_requests else None)\n",
        "            )\n",
        "            text = outputs[0].outputs[0].text\n",
        "        except Exception as ex:\n",
        "            print(f\"[Warning] vLLM error for {model_type}: {ex}\")\n",
        "            text = \"[vLLM ERROR: no output]\"\n",
        "        results[model_type] = text\n",
        "        print(f\"[Async] vLLM {model_type} result: {text[:80]}...\")\n",
        "    await asyncio.gather(*(process_model(m) for m in models))\n",
        "    return results"
      ],
      "metadata": {
        "id": "ypHkg92ovyDA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Extended LLM Forensic Reporting Agent -----\n",
        "async def langchain_llm_report(report_data):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    print(\"[Async] Generating extended forensic report via LangChain...\")\n",
        "    detailed_template = (\n",
        "        \"You are a senior forensic audio analyst. Analyze the provided detailed audio forensic report. Your report must include:\\n\"\n",
        "        \"1. A clear verdict on whether the audio is real or deepfake.\\n\"\n",
        "        \"2. A numerical risk score (0-1) and a confidence level.\\n\"\n",
        "        \"3. A list of detected anomalies and an evidence table with metrics.\\n\"\n",
        "        \"4. Detailed textual analysis of speaker characteristics, background noise, language, and potential spoofing.\\n\"\n",
        "        \"5. Graphs and charts summarizing features such as energy, ZCR, spectral properties, etc.\\n\"\n",
        "        \"6. Flow diagrams in Mermaid.js format that illustrate the forensic workflow.\\n\"\n",
        "        \"\\nReport Template:\\n\"\n",
        "        \"Verdict: {verdict}\\n\"\n",
        "        \"Mean Risk Score: {mean_score:.3f}\\n\"\n",
        "        \"Confidence Level: {confidence:.3f}\\n\"\n",
        "        \"Detected Anomalies: {anomalies}\\n\"\n",
        "        \"ASR Transcript (snippet): {asr}\\n\"\n",
        "        \"ASR Language: {asr_lang}\\n\"\n",
        "        \"Speaker Identities: {speakers}\\n\"\n",
        "        \"Spoof Detection Score: {spoof}\\n\"\n",
        "        \"Extracted Audio Features: {features}\\n\"\n",
        "        \"Noise Quality: {noise_quality}\\n\"\n",
        "        \"Gender Distribution: {gender_dist}\\n\"\n",
        "        \"Additional Metrics: {metrics}\\n\"\n",
        "        \"vLLM Audio Model Outputs: {vllm_outputs}\\n\"\n",
        "        \"\\nProvide a detailed evidence table, risk analysis, recommendations, and include relevant charts and flow diagrams.\"\n",
        "    )\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"verdict\", \"mean_score\", \"confidence\", \"anomalies\", \"asr\", \"asr_lang\", \"speakers\", \"spoof\", \"features\", \"noise_quality\", \"gender_dist\", \"metrics\", \"vllm_outputs\"],\n",
        "        template=detailed_template,\n",
        "    )\n",
        "    llm = OpenAI(temperature=0.2, max_tokens=1000)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    def run_chain():\n",
        "        return chain.run(\n",
        "            verdict=report_data['verdict'],\n",
        "            mean_score=report_data['mean_score'],\n",
        "            confidence=report_data['confidence'],\n",
        "            anomalies=\", \".join(report_data['all_anomalies']),\n",
        "            asr=report_data['asr_transcript'][:400] + \"...\",\n",
        "            asr_lang=report_data.get('asr_lang', 'unknown'),\n",
        "            speakers=\", \".join(report_data['speaker_identities']),\n",
        "            spoof=str(report_data['speaker_spoof_score']),\n",
        "            features=\"; \".join([f\"{k}: {v:.3f}\" for k, v in report_data['features'].items() if isinstance(v, (int, float))]),\n",
        "            noise_quality=str(report_data.get('noise_quality_score', 'N/A')),\n",
        "            gender_dist=json.dumps(report_data.get('gender_distribution', {})),\n",
        "            metrics=json.dumps(report_data.get('metrics', {})),\n",
        "            vllm_outputs=json.dumps(report_data.get('vllm_model_outputs', {}))\n",
        "        )\n",
        "    summary = await loop.run_in_executor(executor, run_chain)\n",
        "    print(\"[Async] Extended forensic report generated.\")\n",
        "    return summary"
      ],
      "metadata": {
        "id": "j4WbCxddv_iF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Chart and Graph Visualization of Metrics -----\n",
        "def generate_metric_charts(features):\n",
        "    labels = list(features.keys())\n",
        "    values = [features[key] if isinstance(features[key], (int, float)) else 0 for key in labels]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(labels, values, color='skyblue')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Audio Feature Metrics')\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    chart_path = \"feature_metrics.png\"\n",
        "    plt.savefig(chart_path)\n",
        "    plt.close()\n",
        "    return chart_path"
      ],
      "metadata": {
        "id": "dQ8uAxeqBQbl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Aggregation and Final Report Generation -----\n",
        "async def aggregate_and_report(audio_path, results, feats, asr, asr_lang, speaker_identities,\n",
        "                               spk_score, noise_quality_score, gender_dist, vllm_model_outputs, extra_results={}):\n",
        "    print(\"[Async] Aggregating results...\")\n",
        "    scores, all_anomalies, all_model_scores, detailed = [], [], {}, {}\n",
        "    for agent, (score, anomalies, detail) in results.items():\n",
        "        scores.append(score)\n",
        "        all_model_scores[agent] = score\n",
        "        all_anomalies += anomalies\n",
        "        detailed[agent] = {\"score\": score, \"anomalies\": anomalies, \"detail\": detail}\n",
        "    mean_score = float(np.mean(scores))\n",
        "    confidence = 1.0 - np.std(scores)\n",
        "    verdict = (\"Likely FAKE (deepfake/spoof detected)\" if mean_score > 0.7\n",
        "               else \"Possibly FAKE (review anomalies)\" if mean_score > 0.5\n",
        "               else \"Likely REAL\")\n",
        "    chart_path = generate_metric_charts(feats)\n",
        "    audio, sr = await preprocess_audio(audio_path)\n",
        "    spec_path = generate_spectrogram(audio, sr)\n",
        "    metrics = {\n",
        "        \"mean_zcr\": feats.get(\"zcr\", 0),\n",
        "        \"energy\": feats.get(\"energy\", 0),\n",
        "        \"snr_est\": feats.get(\"snr_est\", 0),\n",
        "        \"vad_ratio\": feats.get(\"vad_ratio\", 0),\n",
        "        \"spectral_flatness\": feats.get(\"spectral_flatness\", 0),\n",
        "        \"spectrogram\": spec_path,\n",
        "        \"chart\": chart_path\n",
        "    }\n",
        "    extra_results.update({\n",
        "        \"asr_langid_audio\": {\"label\": feats.get(\"langid_label\", \"unknown\"), \"conf\": feats.get(\"langid_conf\", 0)},\n",
        "        \"speaker_diarization_segments\": extra_results.get(\"speaker_diarization_segments\", []),\n",
        "        \"augmentation_tests\": extra_results.get(\"augmentation_tests\", {})\n",
        "    })\n",
        "    try:\n",
        "        natural_summary = await langchain_llm_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"confidence\": confidence,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities,\n",
        "            \"speaker_spoof_score\": spk_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"metrics\": metrics,\n",
        "            \"vllm_outputs\": vllm_model_outputs\n",
        "        })\n",
        "    except Exception as e:\n",
        "        natural_summary = f\"Verdict: {verdict} (LangChain LLM error: {e})\"\n",
        "    detailed.update(extra_results)\n",
        "    additional_analysis = await run_additional_text_analysis(audio_path)\n",
        "    analysis_text = additional_analysis + \"\\nDetailed evidences, charts, and flow diagrams have been generated to support the assessment.\"\n",
        "\n",
        "    return ForensicReport(\n",
        "        file=audio_path,\n",
        "        verdict=verdict,\n",
        "        mean_score=mean_score,\n",
        "        confidence=confidence,\n",
        "        all_model_scores=all_model_scores,\n",
        "        all_anomalies=list(set(all_anomalies)),\n",
        "        natural_summary=natural_summary,\n",
        "        asr_transcript=asr,\n",
        "        asr_lang=asr_lang,\n",
        "        speaker_identities=speaker_identities,\n",
        "        speaker_spoof_score=spk_score,\n",
        "        noise_quality_score=noise_quality_score,\n",
        "        gender_distribution=gender_dist,\n",
        "        additional_metrics=metrics,\n",
        "        plots={\"feature_chart\": chart_path, \"spectrogram\": spec_path},\n",
        "        timestamp=datetime.utcnow().isoformat(),\n",
        "        vllm_model_outputs=vllm_model_outputs,\n",
        "        analysis_text=analysis_text\n",
        "    )\n",
        "\n",
        "# ----- Additional Text Analysis Agent -----\n",
        "async def run_additional_text_analysis(audio_path):\n",
        "    print(\"[Async] Running additional textual analysis agent...\")\n",
        "    await asyncio.sleep(0.5)\n",
        "    analysis = (\n",
        "        \"The audio exhibits consistent speech with clearly defined segments and moderate background noise. \"\n",
        "        \"Preliminary analysis indicates natural speech patterns; however, certain segments require further manual review due to slight anomalies that may be attributed to spoofing attempts. \"\n",
        "        \"Overall, advanced metrics and transcriptions suggest a low to moderate risk.\"\n",
        "    )\n",
        "    return analysis"
      ],
      "metadata": {
        "id": "2LJp71D3BYjB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Main Forensic Pipeline with All Audio LLMs Integration -----\n",
        "async def deepfake_defensive_pipeline(audio_path: str, vllm_question=None):\n",
        "    print(\"[Pipeline] Initiating enhanced forensic audio analysis using Groq and vLLM models...\")\n",
        "    if vllm_question is None:\n",
        "        vllm_question = (\n",
        "            \"Provide a detailed forensic analysis of this audio sample for deepfake, spoof, fraudulent, or synthetic call characteristics. \"\n",
        "            \"Include evidence such as speaker identification, replay markers, emotion tone, transcription, and advanced statistical audio features. \"\n",
        "            \"Generate a comprehensive report with transcriptions, risk metrics, evidence tables, charts, and flow diagrams (Mermaid.js) explaining every analytical step.\"\n",
        "        )\n",
        "    audio, sr = await preprocess_audio(audio_path)\n",
        "    features, vad_ratio = await asyncio.gather(extract_features(audio, sr), extract_vad_ratio(audio, sr))\n",
        "    features['vad_ratio'] = vad_ratio\n",
        "    nb_segments, nb_speech, gender_dist = await extract_inaspeech_segments(audio_path)\n",
        "    features['nb_segments'] = nb_segments\n",
        "    features['nb_speech_segments'] = nb_speech\n",
        "    langid_label, langid_conf = await run_language_id(audio_path)\n",
        "    features['langid_label'] = langid_label\n",
        "    features['langid_conf'] = langid_conf\n",
        "    speaker_identities, diar_segments = await run_speechbrain_speaker(audio_path)\n",
        "    spk_score = await run_speechbrain_verification(audio_path)\n",
        "    spoof_score, spoof_anom = await run_speechbrain_spoof(audio_path)\n",
        "    emo_score, emo_anom, emo_desc = await run_emotion_analysis(audio, sr)\n",
        "    wave_score, wave_anom, wave_detail = await run_wave2vec_fake_detection(audio_path)\n",
        "    replay_score, replay_anom = await run_replay_attack_detection(audio, sr)\n",
        "    noise_quality = await run_noise_quality_estimation(audio, sr)\n",
        "    augmented_audio = augment_audio(audio, sr)\n",
        "    aug_feats = await extract_features(augmented_audio, sr)\n",
        "    aug_tests = {\"augmentation_feature_diff\": np.abs(features['zcr'] - aug_feats['zcr'])}\n",
        "    extra_results = {\n",
        "        \"asr_langid_audio\": {\"label\": langid_label, \"conf\": langid_conf},\n",
        "        \"speaker_diarization_segments\": diar_segments,\n",
        "        \"augmentation_tests\": aug_tests\n",
        "    }\n",
        "    # Groq API integrations for transcription, translation and TTS.\n",
        "    try:\n",
        "        groq_transcription = groq_transcribe(audio_path, model=\"whisper-large-v3-turbo\", language=\"en\", prompt_text=\"\")\n",
        "    except Exception as ex:\n",
        "        groq_transcription = {\"error\": str(ex)}\n",
        "    try:\n",
        "        groq_translation = groq_translate(audio_path, model=\"whisper-large-v3\", language=\"en\", prompt_text=\"\")\n",
        "    except Exception as ex:\n",
        "        groq_translation = {\"error\": str(ex)}\n",
        "    try:\n",
        "        tts_text = \"This is a synthesized voice sample generated from the forensic analysis report.\"\n",
        "        groq_tts_audio = groq_text_to_speech(tts_text, model=\"playai-tts\", voice=\"Fritz-PlayAI\")\n",
        "    except Exception as ex:\n",
        "        groq_tts_audio = f\"[TTS error: {str(ex)}]\"\n",
        "    # Run primary vLLM multimodal inference.\n",
        "    vllm_outputs = await vllm_multimodal_inference(audio_path, vllm_question)\n",
        "    # Run additional fraud detection agent using Groq-based fraud detection.\n",
        "    fraud_detection = await run_audio_llm_fraud_detection(audio_path)\n",
        "    agents_results = {\n",
        "        \"statistical_wav2vec\": (wave_score, wave_anom, wave_detail),\n",
        "        \"replay_attack\": (replay_score, replay_anom, \"Replay detection via RMS\"),\n",
        "        \"emotion_analysis\": (emo_score, emo_anom, emo_desc),\n",
        "        \"speechbrain_spoof\": (spoof_score, spoof_anom, \"SpeechBrain anti-spoof analysis\"),\n",
        "        \"fraud_detection\": (0.0, [], fraud_detection.get(\"qwen2_audio\", \"[No Fraud Analysis]\"))\n",
        "    }\n",
        "    report = await aggregate_and_report(\n",
        "        audio_path, agents_results, features, asr=\"\", asr_lang=\"N/A\",\n",
        "        speaker_identities=speaker_identities, spk_score=spk_score,\n",
        "        noise_quality_score=noise_quality, gender_dist=gender_dist,\n",
        "        vllm_model_outputs=vllm_outputs, extra_results=extra_results\n",
        "    )\n",
        "    report.groq_transcription = json.dumps(groq_transcription, indent=2, default=str)\n",
        "    report.groq_translation = json.dumps(groq_translation, indent=2, default=str)\n",
        "    report.groq_tts_audio = groq_tts_audio\n",
        "    print(\"[Pipeline] Enhanced forensic audio analysis complete.\")\n",
        "    return report"
      ],
      "metadata": {
        "id": "qeMz0uMtwCh6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Visualization Functions -----\n",
        "def display_audio_waveform(audio_path, sr=16000):\n",
        "    print(\"[Visualization] Displaying audio waveform...\")\n",
        "    audio, sr = librosa.load(audio_path, sr=sr)\n",
        "    plt.figure(figsize=(14, 3))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title(\"Audio Waveform\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "def display_mermaid_diagram():\n",
        "    print(\"[Visualization] Rendering Mermaid.js flowchart...\")\n",
        "    mermaid_chart = \"\"\"\n",
        "    graph TD\n",
        "      A[Upload Audio File] --> B[Preprocess Audio]\n",
        "      B --> C[Microbatch Feature Extraction & VAD]\n",
        "      C --> D[Speaker Diarization & Verification]\n",
        "      D --> E[Spoof, Emotion & Replay Analysis]\n",
        "      E --> F[Optional SpeechBrain Augmentation]\n",
        "      F --> G[Augment Audio & Generate Spectrogram/Charts]\n",
        "      G --> H[vLLM Multimodal Inference]\n",
        "      H --> I[Groq Transcription, Translation & TTS]\n",
        "      I --> J[Audio LLM Fraud Detection]\n",
        "      J --> K[Aggregate Results & Generate Detailed Report]\n",
        "      K --> L[Additional Text Analysis]\n",
        "      L --> M[Display Final Forensic Report]\n",
        "    \"\"\"\n",
        "    html_str = f\"\"\"\n",
        "    <div class=\"mermaid\">\n",
        "    {mermaid_chart}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html_str))\n",
        "\n",
        "def pretty_print_report(report: ForensicReport):\n",
        "    print(f\"\\n\\033[1mForensic Verdict:\\033[0m {report.verdict}\")\n",
        "    print(f\"\\033[1mMean Risk Score:\\033[0m {report.mean_score:.2f}\")\n",
        "    print(f\"\\033[1mConfidence Level:\\033[0m {report.confidence:.2f}\")\n",
        "    print(f\"\\033[1mDetected Anomalies:\\033[0m {', '.join(report.all_anomalies) or 'None'}\")\n",
        "    print(f\"\\033[1mSpeaker Identities:\\033[0m {', '.join(report.speaker_identities)}\")\n",
        "    print(f\"\\033[1mSpoof Detection Score:\\033[0m {report.speaker_spoof_score:.2f}\")\n",
        "    print(f\"\\033[1mNoise/Quality Score:\\033[0m {report.noise_quality_score:.2f}\")\n",
        "    print(f\"\\033[1mGender Distribution:\\033[0m {report.gender_distribution}\")\n",
        "    print(f\"\\033[1mvLLM Audio Model Outputs:\\033[0m\")\n",
        "    for m, t in report.vllm_model_outputs.items():\n",
        "        print(f\"   \\033[1m{m}:\\033[0m {t[:300]}{'...' if len(t)>300 else ''}\")\n",
        "    print(f\"\\033[1mAdditional Metrics:\\033[0m {report.additional_metrics}\")\n",
        "    print(f\"\\033[1mTimestamp:\\033[0m {report.timestamp}\")\n",
        "    print(\"\\n\\033[1mForensic Report Summary:\\033[0m\\n\", report.natural_summary)\n",
        "    print(\"\\n\\033[1mAdditional Text Analysis:\\033[0m\\n\", report.analysis_text)\n",
        "    print(\"\\n\\033[1mGroq Transcription:\\033[0m\\n\", report.groq_transcription)\n",
        "    print(\"\\n\\033[1mGroq Translation:\\033[0m\\n\", report.groq_translation)\n",
        "    print(\"\\n\\033[1mGroq TTS Audio File:\\033[0m\\n\", report.groq_tts_audio)\n",
        "    print(\"\\n\\033[1mRaw JSON Output:\\033[0m\")\n",
        "    print(report.json(indent=2))\n",
        "    if \"feature_chart\" in report.plots:\n",
        "        print(\"\\n\\033[1mDisplaying Feature Metrics Chart:\\033[0m\")\n",
        "        from IPython.display import Image\n",
        "        display(Image(filename=report.plots[\"feature_chart\"]))\n",
        "    if \"spectrogram\" in report.plots:\n",
        "        print(\"\\n\\033[1mDisplaying Spectrogram:\\033[0m\")\n",
        "        from IPython.display import Image\n",
        "        display(Image(filename=report.plots[\"spectrogram\"]))\n",
        "\n",
        "# ----- Colab UI -----\n",
        "def colab_audio_deepfake_ui():\n",
        "    from google.colab import files\n",
        "    clear_output()\n",
        "    print(\"## Extended Deepfake Audio Forensic Pipeline with Groq & vLLM Models ##\")\n",
        "    print(\"Upload an audio or video file (wav/mp3/flac/m4a or video formats such as mp4):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return\n",
        "    filename = next(iter(uploaded))\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    ext = ext.lower()\n",
        "    file_size_bytes = os.path.getsize(filename)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "    print(f\"[UI] File '{filename}' size: {file_size_mb:.2f} MB\")\n",
        "    if ext != \".wav\":\n",
        "        print(\"[UI] Converting file to WAV / extracting audio...\")\n",
        "        if ext in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "            clip = VideoFileClip(filename)\n",
        "            wav_path = base + \".wav\"\n",
        "            clip.audio.write_audiofile(wav_path, fps=16000, logger=None)\n",
        "            clip.close()\n",
        "            audio_path = wav_path\n",
        "        else:\n",
        "            audio = AudioSegment.from_file(filename)\n",
        "            wav_path = base + \".wav\"\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            audio_path = wav_path\n",
        "    else:\n",
        "        audio_path = filename\n",
        "    print(\"\\n== Displaying Audio Waveform ==\")\n",
        "    display_audio_waveform(audio_path)\n",
        "    print(\"\\n== Running Enhanced Forensic Analysis (Asynchronous & Low-Latency) ==\")\n",
        "    report = asyncio.run(deepfake_defensive_pipeline(audio_path))\n",
        "    print(\"\\n== Displaying Pipeline Flow Diagram ==\")\n",
        "    display_mermaid_diagram()\n",
        "    print(\"\\n== Final Forensic Report ==\")\n",
        "    pretty_print_report(report)\n",
        "\n",
        "# Execute the UI in Colab\n",
        "colab_audio_deepfake_ui()"
      ],
      "metadata": {
        "id": "0oNPiih4wFvr",
        "outputId": "b91dfe19-f78f-40a1-b0f8-a2b42f3e08d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Extended Deepfake Audio Forensic Pipeline with Groq & vLLM Models ##\n",
            "Upload an audio or video file (wav/mp3/flac/m4a or video formats such as mp4):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-725ef204-e9c9-41e8-bcde-eb55d398711f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-725ef204-e9c9-41e8-bcde-eb55d398711f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSx8khaGBlWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}