{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Cyber_Knowledge_Graph_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installations**"
      ],
      "metadata": {
        "id": "5X4d_MMH2C9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP0Oq50u1SJA"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-groq pandas networkx pyvis ampligraph transformers relik"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Data Loading**\n",
        "- This function loads documents from a list of URLs.\n",
        "\n",
        "- Loading data from websites is the first step in any data processing pipeline. It ensures that we have the raw data needed for further analysis."
      ],
      "metadata": {
        "id": "s6oS1V38B9pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "def load_data_from_websites(urls):\n",
        "    \"\"\"\n",
        "    Load documents from a list of URLs.\n",
        "\n",
        "    Args:\n",
        "        urls (list): List of URLs to load documents from.\n",
        "\n",
        "    Returns:\n",
        "        list: List of loaded documents.\n",
        "    \"\"\"\n",
        "    web_base_loader = WebBaseLoader(urls)\n",
        "    documents = web_base_loader.load()\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    return documents\n",
        "\n",
        "websites = [\n",
        "    \"https://www.scmagazine.com/home/security-news/\",\n",
        "    \"https://thehackernews.com/\",\n",
        "    \"https://www.securityweek.com/\",\n",
        "    \"https://www.darkreading.com/\",\n",
        "    \"https://krebsonsecurity.com/\",\n",
        "    \"https://www.bleepingcomputer.com/\",\n",
        "    \"https://threatpost.com/\",\n",
        "    \"https://www.cyberscoop.com/\",\n",
        "    \"https://www.infosecurity-magazine.com/\",\n",
        "    \"https://www.zdnet.com/topic/security/\",\n",
        "    \"https://www.wired.com/category/security/\",\n",
        "    \"https://nakedsecurity.sophos.com/\",\n",
        "    \"https://www.cisomag.com/\",\n",
        "    \"https://www.cshub.com/\",\n",
        "    \"https://www.cybersecuritydive.com/\",\n",
        "    \"https://www.cybersecurity-insiders.com/\",\n",
        "    \"https://www.csoonline.com/\",\n",
        "    \"https://www.securitymagazine.com/topics/2236-cyber-security-news\",\n",
        "    \"https://www.helpnetsecurity.com/\"\n",
        "]\n",
        "\n",
        "documents = load_data_from_websites(websites)"
      ],
      "metadata": {
        "id": "Tbh4tGJ12ciJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Documents into Chunks**\n",
        "- This function splits the loaded documents into smaller chunks for easier processing.\n",
        "- Splitting documents into chunks helps in managing large texts and ensures that each chunk can be processed independently.\n",
        "- This function is used to break down long documents into manageable pieces for entity detection and triplet extraction."
      ],
      "metadata": {
        "id": "0p5As0niCDEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_data(documents, chunk_size=1000, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents to split.\n",
        "        chunk_size (int): Size of each chunk.\n",
        "        chunk_overlap (int): Overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: List of document chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Number of chunks created: {len(chunks)}\")\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_data(documents)"
      ],
      "metadata": {
        "id": "2Lw8dic-2Ueq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a DataFrame of Chunks**\n",
        "- This function converts the list of chunks into a pandas DataFrame.\n",
        "- Converting chunks into a DataFrame allows for easier data manipulation and analysis using pandas.\n",
        "- This function is used to create a structured format for the chunks, which will be used in subsequent steps."
      ],
      "metadata": {
        "id": "rIXDB0OyHbfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_chunks_dataframe(chunks):\n",
        "    \"\"\"\n",
        "    Convert list of chunks into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of document chunks.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the chunks.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for chunk in chunks:\n",
        "        data.append({\n",
        "            \"text\": chunk.page_content,\n",
        "            \"source\": chunk.metadata.get(\"source\", \"\"),\n",
        "            \"chunk_id\": chunk.metadata.get(\"chunk_id\", \"\")\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "chunks_df = create_chunks_dataframe(chunks)\n",
        "chunks_df.head()"
      ],
      "metadata": {
        "id": "yRuaG8A5Cjw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect and Classify Entities**\n",
        "- This function uses a Named Entity Recognition (NER) pipeline to detect entities in the text chunks.\n",
        "- Detecting entities helps in identifying key concepts and entities in the text, which is crucial for understanding the content.\n",
        "- This function is used to extract entities from the text chunks and create a DataFrame of detected entities."
      ],
      "metadata": {
        "id": "g3Yccjc_H6Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from relik import Relik\n",
        "from relik.inference.data.objects import RelikOutput\n",
        "\n",
        "def detect_entities_relik(chunks_df):\n",
        "    \"\"\"\n",
        "    Detect and classify entities using ReLiK.\n",
        "\n",
        "    Args:\n",
        "        chunks_df (pd.DataFrame): DataFrame containing the chunks.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the detected entities.\n",
        "    \"\"\"\n",
        "    relik = Relik.from_pretrained(\"sapienzanlp/relik-entity-linking-large\")\n",
        "\n",
        "    def extract_concepts(text):\n",
        "        \"\"\"\n",
        "        Extract concepts from text using ReLiK.\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to extract concepts from.\n",
        "\n",
        "        Returns:\n",
        "            list: List of extracted concepts.\n",
        "        \"\"\"\n",
        "        relik_out: RelikOutput = relik(text)\n",
        "        concepts = [span.text for span in relik_out.spans]\n",
        "        return concepts\n",
        "\n",
        "    def create_concepts_list(chunks_df):\n",
        "        \"\"\"\n",
        "        Create a list of concepts from the chunks DataFrame.\n",
        "\n",
        "        Args:\n",
        "            chunks_df (pd.DataFrame): DataFrame containing the chunks.\n",
        "\n",
        "        Returns:\n",
        "            list: List of concepts.\n",
        "        \"\"\"\n",
        "        concepts_list = []\n",
        "        for index, row in chunks_df.iterrows():\n",
        "            concepts = extract_concepts(row['text'])\n",
        "            for concept in concepts:\n",
        "                concepts_list.append({\n",
        "                    \"node_1\": concept,\n",
        "                    \"node_2\": row['source'],\n",
        "                    \"edge\": \"contains\",\n",
        "                    \"chunk_id\": row['chunk_id']\n",
        "                })\n",
        "        return concepts_list\n",
        "\n",
        "    concepts_list = create_concepts_list(chunks_df)\n",
        "    return pd.DataFrame(concepts_list)\n",
        "\n",
        "concepts_df = detect_entities_relik(chunks_df)\n",
        "concepts_df.head()"
      ],
      "metadata": {
        "id": "73tLsGglEwep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing LLM for Knowledge Extraction**\n",
        "- Initialize the Mistral LLM using Groq for knowledge extraction."
      ],
      "metadata": {
        "id": "AgnyFdO-Mbos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "GROQ_API_KEY = \"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\"\n",
        "\n",
        "# Initialize the Mistral LLM using Groq\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    api_key=GROQ_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "ZegD2vPXMbGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Triplets using ReLiK**\n",
        "- Use the ReLiK model for relation extraction.\n",
        "- Extracting triplets helps in understanding the relationships between entities, which is essential for building a knowledge graph.\n",
        "- This function is used to extract triplets from the text chunks and create a list of triplets."
      ],
      "metadata": {
        "id": "RFCsElheIw0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triplets_relik(chunks):\n",
        "    \"\"\"\n",
        "    Extract triplets using ReLiK.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of document chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: List of extracted triplets.\n",
        "    \"\"\"\n",
        "    relik = Relik.from_pretrained(\"sapienzanlp/relik-relation-extraction-nyt-large\")\n",
        "\n",
        "    def extract_triplets_from_text(text):\n",
        "        \"\"\"\n",
        "        Extract triplets from text using ReLiK.\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to extract triplets from.\n",
        "\n",
        "        Returns:\n",
        "            list: List of extracted triplets.\n",
        "        \"\"\"\n",
        "        relik_out: RelikOutput = relik(text)\n",
        "        triplets = [f\"({triplet.subject.text}, {triplet.label}, {triplet.object.text})\" for triplet in relik_out.triplets]\n",
        "        return triplets\n",
        "\n",
        "    all_triplets = []\n",
        "    for chunk in chunks:\n",
        "        triplets = extract_triplets_from_text(chunk.page_content)\n",
        "        all_triplets.extend(triplets)\n",
        "\n",
        "    return all_triplets\n",
        "\n",
        "triplets = extract_triplets_relik(chunks)"
      ],
      "metadata": {
        "id": "OP8N2_S7GAUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze Relationships**\n",
        "- This function converts the list of triplets into a DataFrame.\n",
        "- Converting triplets into a DataFrame allows for easier analysis and manipulation of the relationships between entities.\n",
        "- This function is used to create a structured format for the triplets, which will be used in subsequent steps."
      ],
      "metadata": {
        "id": "4zgeFnsoKJz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_relationships(triplets):\n",
        "    \"\"\"\n",
        "    Convert list of triplets into a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        triplets (list): List of triplets.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the triplets.\n",
        "    \"\"\"\n",
        "    def create_triplets_dataframe(triplets):\n",
        "        \"\"\"\n",
        "        Create a DataFrame from the list of triplets.\n",
        "\n",
        "        Args:\n",
        "            triplets (list): List of triplets.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing the triplets.\n",
        "        \"\"\"\n",
        "        triplet_data = []\n",
        "        for triplet in triplets:\n",
        "            subject, predicate, obj = triplet.strip(\"()\").split(\", \")\n",
        "            triplet_data.append({\n",
        "                \"subject\": subject.strip(),\n",
        "                \"predicate\": predicate.strip(),\n",
        "                \"object\": obj.strip()\n",
        "            })\n",
        "        return pd.DataFrame(triplet_data)\n",
        "\n",
        "    triplets_df = create_triplets_dataframe(triplets)\n",
        "    return triplets_df\n",
        "\n",
        "triplets_df = analyze_relationships(triplets)\n",
        "triplets_df.head()"
      ],
      "metadata": {
        "id": "AyxaTx47KLRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Contextual Proximity**\n",
        "- This function calculates the contextual proximity between nodes by merging the DataFrame with itself and counting the occurrences of node pairs within the same chunk.\n",
        "- Contextual proximity helps in understanding the relationships between entities that appear together in the same context.\n",
        "- This function is used to create a DataFrame of contextual proximity relationships between entities.\n"
      ],
      "metadata": {
        "id": "SibKxVNyKcdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_contextual_proximity(df):\n",
        "    \"\"\"\n",
        "    Calculate contextual proximity between nodes.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the nodes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the contextual proximity.\n",
        "    \"\"\"\n",
        "    long_format_df = pd.melt(\n",
        "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
        "    )\n",
        "    long_format_df.drop(columns=[\"variable\"], inplace=True)\n",
        "\n",
        "    merged_df = pd.merge(long_format_df, long_format_df, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
        "\n",
        "    self_loops_index = merged_df[merged_df[\"node_1\"] == merged_df[\"node_2\"]].index\n",
        "    merged_df = merged_df.drop(index=self_loops_index).reset_index(drop=True)\n",
        "\n",
        "    grouped_df = (\n",
        "        merged_df.groupby([\"node_1\", \"node_2\"])\n",
        "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
        "        .reset_index()\n",
        "    )\n",
        "    grouped_df.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
        "\n",
        "    grouped_df.replace(\"\", np.nan, inplace=True)\n",
        "    grouped_df.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
        "\n",
        "    grouped_df = grouped_df[grouped_df[\"count\"] != 1]\n",
        "\n",
        "    grouped_df[\"edge\"] = \"contextual proximity\"\n",
        "\n",
        "    return grouped_df\n",
        "\n",
        "contextual_proximity_df = calculate_contextual_proximity(concepts_df)\n",
        "contextual_proximity_df.head()"
      ],
      "metadata": {
        "id": "XnPLhZpiKhBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge DataFrames**\n",
        "- This function merges the concepts DataFrame with the contextual proximity DataFrame and aggregates the data.\n",
        "- Merging the DataFrames allows for a comprehensive view of the relationships between entities.\n",
        "- This function is used to create a merged DataFrame that will be used to create the graph."
      ],
      "metadata": {
        "id": "e5ENW1Y8NfY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dataframes(concepts_df, contextual_proximity_df):\n",
        "    \"\"\"\n",
        "    Merge the concepts DataFrame with the contextual proximity DataFrame.\n",
        "\n",
        "    Args:\n",
        "        concepts_df (pd.DataFrame): DataFrame containing the concepts.\n",
        "        contextual_proximity_df (pd.DataFrame): DataFrame containing the contextual proximity.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Merged DataFrame.\n",
        "    \"\"\"\n",
        "    merged_df = pd.concat([concepts_df, contextual_proximity_df], axis=0)\n",
        "    merged_df = (\n",
        "        merged_df.groupby([\"node_1\", \"node_2\"])\n",
        "        .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n",
        "        .reset_index()\n",
        "    )\n",
        "    return merged_df\n",
        "\n",
        "merged_df = merge_dataframes(concepts_df, contextual_proximity_df)"
      ],
      "metadata": {
        "id": "U1JBGgrYNhJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create NetworkX Graph**\n",
        "- This function creates a NetworkX graph from the merged DataFrame, with nodes and edges representing the relationships between entities.\n",
        "- Creating a graph allows for visualizing and analyzing the relationships between entities.\n",
        "- This function is used to create a graph that will be used for further analysis and visualization."
      ],
      "metadata": {
        "id": "OKByy8TKNt5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def create_networkx_graph(merged_df):\n",
        "    \"\"\"\n",
        "    Create a NetworkX graph from the merged DataFrame.\n",
        "\n",
        "    Args:\n",
        "        merged_df (pd.DataFrame): Merged DataFrame containing the nodes and edges.\n",
        "\n",
        "    Returns:\n",
        "        nx.Graph: NetworkX graph.\n",
        "    \"\"\"\n",
        "    nodes = pd.concat([merged_df['node_1'], merged_df['node_2']], axis=0).unique()\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for node in nodes:\n",
        "        graph.add_node(str(node))\n",
        "\n",
        "    for index, row in merged_df.iterrows():\n",
        "        graph.add_edge(\n",
        "            str(row[\"node_1\"]),\n",
        "            str(row[\"node_2\"]),\n",
        "            title=row[\"edge\"],\n",
        "            weight=row['count']/4\n",
        "        )\n",
        "\n",
        "    return graph\n",
        "\n",
        "graph = create_networkx_graph(merged_df)"
      ],
      "metadata": {
        "id": "5VAzRS6VNyKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Graph Metrics**\n",
        "- This function calculates various centrality metrics for the graph and sets them as node attributes.\n",
        "- Centrality metrics help in understanding the importance and influence of nodes in the graph.\n",
        "- This function is used to enrich the graph with additional metrics that will be used for analysis and visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "3neQ1xTDOCZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx.algorithms.centrality import degree_centrality, betweenness_centrality, closeness_centrality, eigenvector_centrality, pagerank\n",
        "\n",
        "def calculate_graph_metrics(graph):\n",
        "    \"\"\"\n",
        "    Calculate various centrality metrics for the graph.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.Graph): NetworkX graph.\n",
        "    \"\"\"\n",
        "    degree_centrality_values = degree_centrality(graph)\n",
        "    betweenness_centrality_values = betweenness_centrality(graph)\n",
        "    closeness_centrality_values = closeness_centrality(graph)\n",
        "    eigenvector_centrality_values = eigenvector_centrality(graph)\n",
        "    pagerank_values = pagerank(graph)\n",
        "\n",
        "    nx.set_node_attributes(graph, degree_centrality_values, 'degree_centrality')\n",
        "    nx.set_node_attributes(graph, betweenness_centrality_values, 'betweenness_centrality')\n",
        "    nx.set_node_attributes(graph, closeness_centrality_values, 'closeness_centrality')\n",
        "    nx.set_node_attributes(graph, eigenvector_centrality_values, 'eigenvector_centrality')\n",
        "    nx.set_node_attributes(graph, pagerank_values, 'pagerank')\n",
        "\n",
        "calculate_graph_metrics(graph)"
      ],
      "metadata": {
        "id": "39VartY3ODvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Communities**\n",
        "- This function calculates communities in the graph using the Louvain method and assigns colors to each community.\n",
        "- Identifying communities helps in understanding the structure and organization of the graph.\n",
        "- This function is used to create a DataFrame of communities that will be used for visualization."
      ],
      "metadata": {
        "id": "CoA4jXeeOQa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from community import community_louvain\n",
        "\n",
        "def assign_colors_to_communities(communities):\n",
        "    \"\"\"\n",
        "    Assign colors to communities.\n",
        "\n",
        "    Args:\n",
        "        communities (dict): Dictionary containing the communities.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the community colors.\n",
        "    \"\"\"\n",
        "    palette = sns.color_palette(\"hls\", len(communities)).as_hex()\n",
        "    random.shuffle(palette)\n",
        "    rows = []\n",
        "    group = 0\n",
        "    for community in communities:\n",
        "        color = palette.pop()\n",
        "        group += 1\n",
        "        for node in community:\n",
        "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
        "    colors_df = pd.DataFrame(rows)\n",
        "    return colors_df\n",
        "\n",
        "def calculate_communities(graph):\n",
        "    \"\"\"\n",
        "    Calculate communities in the graph using the Louvain method.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.Graph): NetworkX graph.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the communities.\n",
        "    \"\"\"\n",
        "    communities_generator = community_louvain.best_partition(graph)\n",
        "    communities = {}\n",
        "    for node, community_id in communities_generator.items():\n",
        "        if community_id not in communities:\n",
        "            communities[community_id] = []\n",
        "        communities[community_id].append(node)\n",
        "\n",
        "    return communities\n",
        "\n",
        "communities = calculate_communities(graph)\n",
        "colors_df = assign_colors_to_communities(communities)"
      ],
      "metadata": {
        "id": "TfwNQN78OUEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced Graph Visualization with AmpliGraph**\n",
        "- This function converts the NetworkX graph to AmpliGraph format, trains a ComplEx model, evaluates the model, and plots the graph using AmpliGraph's visualization tools.\n",
        "- Using AmpliGraph for visualization provides advanced graph analysis and visualization capabilities.\n",
        "- This function is used to create an interactive graph visualization that will be saved to a file."
      ],
      "metadata": {
        "id": "GT-B75jLOhsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ampligraph.latent_features import ComplEx\n",
        "from ampligraph.evaluation import evaluate_performance\n",
        "from ampligraph.utils import restore_model\n",
        "from ampligraph.visualization import plot_2D_graph\n",
        "\n",
        "def create_interactive_graph(graph, output_path):\n",
        "    \"\"\"\n",
        "    Convert the NetworkX graph to AmpliGraph format and visualize it.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.Graph): NetworkX graph.\n",
        "        output_path (str): Path to save the visualization.\n",
        "    \"\"\"\n",
        "    # Convert NetworkX graph to AmpliGraph format\n",
        "    edges = [(u, v, d['title']) for u, v, d in graph.edges(data=True)]\n",
        "    nodes = list(graph.nodes)\n",
        "\n",
        "    # Train a ComplEx model\n",
        "    model = ComplEx(batches_count=10, seed=0, epochs=20, k=100, eta=5,\n",
        "                    optimizer='adam', optimizer_params={'lr': 1e-3},\n",
        "                    loss='multiclass_nll', regularizer=None,\n",
        "                    regularizer_params={}, verbose=True)\n",
        "\n",
        "    model.fit(edges)\n",
        "\n",
        "    # Evaluate the model\n",
        "    _, _, _, _ = evaluate_performance(edges, model=model,\n",
        "                                      filter_triples=edges,\n",
        "                                      use_default_protocol=True,\n",
        "                                      verbose=True)\n",
        "\n",
        "    # Plot the graph\n",
        "    plot_2D_graph(model, nodes, output_path)\n",
        "\n",
        "# Main Execution for Step 12\n",
        "output_path = os.path.join(output_directory, \"graph.html\")\n",
        "create_interactive_graph(graph, output_path)"
      ],
      "metadata": {
        "id": "09UWCU46OpHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}