{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4RIEqRQdGDhRgNHdMcXeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Audio_Deepfake_Detection_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensice Forensics**"
      ],
      "metadata": {
        "id": "dawnqiKf2WZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iPw2TLE0yLn"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 0. Setup: Installations (Quiet)\n",
        "# ==============================================================================\n",
        "print(\"STEP 0: Installing required libraries...\")\n",
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis praat-parselmouth\n",
        "!pip install -q speechbrain==0.5.14 torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118 # Pin versions for compatibility\n",
        "!pip install -q moviepy Pillow transformers==4.39.3 datasets huggingface_hub # Pin transformers\n",
        "!pip install -q openai langchain langchain_openai\n",
        "!pip install -q groq langchain-groq\n",
        "# Ensure vLLM version compatibility with Colab CUDA/PyTorch\n",
        "# Check https://github.com/vllm-project/vllm/releases for recent versions suitable for CUDA 11.8/12.x\n",
        "!pip install -q vllm==0.4.0.post1 # Adjust if necessary based on Colab environment\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "print(\"Library installation complete.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Setup: Imports and Configuration\n",
        "# ==============================================================================\n",
        "print(\"\\nSTEP 1: Importing libraries and configuring settings...\")\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os, json, asyncio, traceback, time, warnings, io\n",
        "import numpy as np, librosa, librosa.display, soundfile as sf, matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import webrtcvad\n",
        "from pydub import AudioSegment\n",
        "import noisereduce as nr\n",
        "from pyAudioAnalysis import audioSegmentation as aS, ShortTermFeatures\n",
        "import parselmouth\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import ipywidgets as widgets\n",
        "from google.colab import userdata, files\n",
        "from datetime import datetime\n",
        "import base64 # For embedding plots\n",
        "\n",
        "# Filter common warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"Chunk.*not understood\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Using a non-tuple sequence\") # From HuggingFace/Datasets\n",
        "\n",
        "# --- SpeechBrain ---\n",
        "try:\n",
        "    from speechbrain.inference.speaker import SpeakerRecognition\n",
        "    from speechbrain.inference.classifiers import EncoderClassifier, LanguageIdentification\n",
        "    from speechbrain.augment.time_domain import AddNoise\n",
        "    SPEECHBRAIN_AVAILABLE = True\n",
        "    print(\"SpeechBrain components imported.\")\n",
        "except ImportError as e:\n",
        "    print(f\"[Warning] Failed to import SpeechBrain: {e}. Related analyses will be skipped.\")\n",
        "    SPEECHBRAIN_AVAILABLE = False\n",
        "    SpeakerRecognition = EncoderClassifier = LanguageIdentification = AddNoise = None\n",
        "\n",
        "# --- LangChain & LLMs ---\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "# OpenAI (Optional Fallback)\n",
        "OPENAI_ENABLED = False\n",
        "try:\n",
        "    from langchain_openai import OpenAI\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if openai_api_key:\n",
        "        os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "        OPENAI_ENABLED = True\n",
        "        print(\"OpenAI API Key loaded.\")\n",
        "    else:\n",
        "        print(\"[Config] OpenAI API Key not found in Colab secrets.\")\n",
        "except ImportError: pass\n",
        "except Exception as e: print(f\"[Config Error] OpenAI: {e}\")\n",
        "\n",
        "# Groq (Primary for Reporting)\n",
        "GROQ_ENABLED = False\n",
        "try:\n",
        "    from groq import Groq as GroqClient\n",
        "    groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "    if groq_api_key:\n",
        "        os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "        groq_client = GroqClient(api_key=groq_api_key)\n",
        "        GROQ_ENABLED = True\n",
        "        print(\"Groq API Key loaded and client initialized.\")\n",
        "    else:\n",
        "        print(\"[Config] Groq API Key not found in Colab secrets. Reporting LLM disabled.\")\n",
        "except ImportError: print(\"[Config Error] groq library not installed.\")\n",
        "except Exception as e: print(f\"[Config Error] Groq: {e}\")\n",
        "\n",
        "# --- vLLM ---\n",
        "VLLM_AVAILABLE = False\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        from transformers import AutoTokenizer, AutoProcessor\n",
        "        from vllm import LLM, EngineArgs, SamplingParams, RequestOutput, MultiModalData\n",
        "        # Check CUDA capability for FP8 etc.\n",
        "        capability = torch.cuda.get_device_capability()\n",
        "        print(f\"CUDA available. Device Capability: {capability}\")\n",
        "        if capability >= (8,0): print(\"   (Supports FP8 KV Cache)\")\n",
        "        VLLM_AVAILABLE = True\n",
        "        print(\"vLLM components imported.\")\n",
        "    else:\n",
        "        print(\"[Config Warning] CUDA not available. vLLM analysis will be skipped.\")\n",
        "except ImportError as e: print(f\"[Config Error] Failed to import vLLM: {e}. vLLM analysis disabled.\")\n",
        "except Exception as e: print(f\"[Config Error] vLLM Import: {e}. vLLM analysis disabled.\")\n",
        "\n",
        "\n",
        "# --- Global Settings ---\n",
        "MAX_WORKERS = 4 # Keep low for Colab stability\n",
        "executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)\n",
        "TARGET_SR = 16000 # Standard for speech models\n",
        "\n",
        "print(\"Imports and configuration complete.\")"
      ]
    }
  ]
}