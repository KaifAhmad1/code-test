{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMl2PjwrPokkQf432QXW89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/code-test/blob/main/Product_Marketing_AI_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Marketing AI System\n",
        "\n",
        "## Overview\n",
        "This system helps create high-quality marketing images automatically. It takes in photos and optional audio or video, then processes, refines, and enhances them to produce beautiful marketing visuals for many different industries.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Easy Input:** Upload main and supplementary images, plus optional multimedia for extra context.\n",
        "- **Smart Processing:** The system automatically cuts out key parts, improves image details, and boosts overall clarity.\n",
        "- **Creative Prompts:** Custom prompts are generated to guide the image creation process, making it tailored to your needs.\n",
        "- **Fast Generation:** Uses multiple AI models working together to generate and improve images quickly.\n",
        "- **Quality Check:** Compares final images to the originals and provides simple quality feedback.\n",
        "- **Simple Reports:** Automatically produces a brief report with the final prompt and quality scores.\n",
        "\n",
        "## Benefits\n",
        "- Saves time by automating the creation of professional marketing images.\n",
        "- Provides consistent and attractive visuals optimized for your business.\n",
        "- Easy to use with straightforward input and clear feedback.\n",
        "\n",
        "Enjoy a seamless experience in making your marketing visuals stand out!"
      ],
      "metadata": {
        "id": "Y8kVOa8p2KCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UNuaT-u16cA",
        "outputId": "399f5a69-6b61-4c98-d437-410fb94909be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/ChaoningZhang/Real-ESRGAN.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-3pjgdb1p\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/ChaoningZhang/Real-ESRGAN.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-3pjgdb1p\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31mERROR: git+https://github.com/advimman/lama.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement u2net (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for u2net\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Step 1: Install dependencies (for Colab Notebook; not needed if pre-installed)\n",
        "%pip install --q git+https://github.com/facebookresearch/segment-anything.git\n",
        "%pip install -q diffusers transformers accelerate\n",
        "%pip install -q git+https://github.com/ChaoningZhang/Real-ESRGAN.git\n",
        "%pip install -q git+https://github.com/advimman/lama.git\n",
        "%pip install -q langchain langchain-google-genai groq openai u2net langchain_community\n",
        "%pip install -q scikit-image\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O sam_vit_h.pth -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Import Libraries ---\n",
        "import os\n",
        "import math\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- Step 3: Utility Functions ---\n",
        "def free_memory(model):\n",
        "    \"\"\"Frees up GPU memory after use.\"\"\"\n",
        "    try:\n",
        "        del model\n",
        "        import torch\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f\"Memory cleanup error: {e}\")\n",
        "\n",
        "def verify_checksum(file_path, expected_hash):\n",
        "    sha256 = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        sha256.update(f.read())\n",
        "    return sha256.hexdigest() == expected_hash\n",
        "\n",
        "def psnr(target, ref):\n",
        "    \"\"\"Compute Peak Signal-to-Noise Ratio.\"\"\"\n",
        "    mse = np.mean((target - ref) ** 2)\n",
        "    return 100 if mse == 0 else 20 * math.log10(255.0 / math.sqrt(mse))\n",
        "\n",
        "# --- Step 4: Initialize Environment ---\n",
        "print(\"Starting Product Marketing AI Pipeline...\")\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Step 5: Load Models ---\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# Load SAM\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "sam_ckpt = \"sam_vit_h.pth\"\n",
        "expected_hash = \"your_real_checksum_here\"  # Replace with actual SHA256\n",
        "if not verify_checksum(sam_ckpt, expected_hash):\n",
        "    raise ValueError(\"SAM checkpoint corrupted\")\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_ckpt)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Load Stable Diffusion with ControlNet\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "import torch\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/control_v11p_sd15_seg\", torch_dtype=torch.float16\n",
        ")\n",
        "sd_pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Load Real-ESRGAN\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "\n",
        "model = RRDBNet(num_in_ch=3, num_out_ch=3, nf=64, nb=23,\n",
        "                gc=32, sf=4, norm_type='batch', act_type='leakyrelu',\n",
        "                mode='CNA', convtype='Conv2D')\n",
        "upscaler = RealESRGANer(scale=4, model_path='RealESRGAN_x4plus.pth', model=model, tile=0)\n",
        "\n",
        "# --- Step 6: LLMs and Prompt Generation ---\n",
        "from groq import Groq\n",
        "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "from openai import OpenAI\n",
        "xai_client = OpenAI(base_url=\"https://api.x.ai/v1\", api_key=os.environ.get(\"XAI_API_KEY\"))\n",
        "\n",
        "# HuggingFace LLM\n",
        "text_generator = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n",
        "llm = HuggingFacePipeline(pipeline=text_generator)\n",
        "\n",
        "print(\"✅ All models initialized successfully.\")"
      ],
      "metadata": {
        "id": "NISUlqbM3aXU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "07d9dbe2-87b3-4ecb-fb6a-44e14b9d881b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Product Marketing AI Pipeline...\n",
            "Loading models...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "SAM checkpoint corrupted",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b358489e180e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mexpected_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"your_real_checksum_here\"\u001b[0m  \u001b[0;31m# Replace with actual SHA256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msam_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SAM checkpoint corrupted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0msam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_model_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vit_h\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msam_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSamPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: SAM checkpoint corrupted"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_templates = {\n",
        "    \"clothing\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"A model wearing {product} in {setting}, {style} aesthetic, {lighting} lighting, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"food\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} on a table in {setting}, {style} ambiance, {lighting} lighting, delicious and appetizing, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"electronics\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} in {setting}, {style} tech vibe, {lighting} lighting, sleek and modern, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"hotels\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} in {setting}, {style} luxury ambiance, {lighting} lighting, inviting and upscale, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"cosmetics\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} in {setting}, {style} glamorous setup, {lighting} lighting, vibrant and luxurious, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"furniture\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} in {setting}, {style} cozy setting, {lighting} lighting, warm and inviting, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"automotive\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} on {setting}, {style} adventurous vibe, {lighting} lighting, dynamic and bold, with {context}, {caption}, {external}.\"\n",
        "    ),\n",
        "    \"travel\": PromptTemplate(\n",
        "        input_variables=[\"product\", \"setting\", \"style\", \"lighting\", \"context\", \"caption\", \"external\"],\n",
        "        template=\"{product} in {setting}, {style} vacation mood, {lighting} lighting, scenic and aspirational, with {context}, {caption}, {external}.\"\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "PFsY_K0IJu-I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Input Collection\n",
        "def collect_inputs():\n",
        "    try:\n",
        "        print(\"Upload base image (e.g., model, table, room):\")\n",
        "        from google.colab import files\n",
        "        base_upload = files.upload()\n",
        "        base_image_path = list(base_upload.keys())[0]\n",
        "        base_image = cv2.imread(base_image_path)\n",
        "        if base_image is None:\n",
        "            raise ValueError(\"Failed to load base image\")\n",
        "        base_image_rgb = cv2.cvtColor(base_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        print(\"Upload secondary image (e.g., clothing, product, food):\")\n",
        "        secondary_upload = files.upload()\n",
        "        secondary_image_path = list(secondary_upload.keys())[0]\n",
        "        secondary_image = cv2.imread(secondary_image_path)\n",
        "        if secondary_image is None:\n",
        "            raise ValueError(\"Failed to load secondary image\")\n",
        "        secondary_image_rgb = cv2.cvtColor(secondary_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        multimedia = None\n",
        "        if input(\"Upload audio/video for context (y/n)? \").lower() == 'y':\n",
        "            print(\"Upload audio/video file:\")\n",
        "            media_upload = files.upload()\n",
        "            media_path = list(media_upload.keys())[0]\n",
        "            multimedia = {'path': media_path, 'type': 'audio' if media_path.endswith(('.mp3', '.wav')) else 'video'}\n",
        "\n",
        "        domain = input(\"Enter domain (e.g., clothing, food, electronics, hotels, cosmetics, furniture, automotive, travel): \")\n",
        "        initial_prompt = input(\"Enter initial prompt (e.g., 'Model in a jacket, urban rooftop, edgy vibe'): \")\n",
        "\n",
        "        return base_image_rgb, secondary_image_rgb, multimedia, domain.lower(), initial_prompt\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting inputs: {e}\")\n",
        "        return None, None, None, None, None"
      ],
      "metadata": {
        "id": "BcG76-suJ5MV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Preprocess Images\n",
        "def preprocess_images(base_image, secondary_image):\n",
        "    try:\n",
        "        predictor.set_image(base_image)\n",
        "        h, w = base_image.shape[:2]\n",
        "        input_point = np.array([[w // 2, h // 2]])\n",
        "        input_label = np.array([1])\n",
        "        masks, scores, _ = predictor.predict(point_coords=input_point, point_labels=input_label)\n",
        "        mask = masks[np.argmax(scores)]\n",
        "        mask_path = os.path.join(OUTPUT_DIR, \"mask.png\")\n",
        "        cv2.imwrite(mask_path, (mask * 255).astype(np.uint8))\n",
        "\n",
        "        if u2net:\n",
        "            secondary_mask = u2net.predict(secondary_image)\n",
        "            secondary_extracted = cv2.bitwise_and(secondary_image, secondary_image, mask=secondary_mask)\n",
        "        else:\n",
        "            gray = cv2.cvtColor(secondary_image, cv2.COLOR_BGR2GRAY)\n",
        "            _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
        "            secondary_extracted = cv2.bitwise_and(secondary_image, secondary_image, mask=thresh)\n",
        "        secondary_path = os.path.join(OUTPUT_DIR, \"secondary_extracted.png\")\n",
        "        cv2.imwrite(secondary_path, secondary_extracted)\n",
        "\n",
        "        def normalize(image, size=(512, 512)):\n",
        "            image = cv2.resize(image, size)\n",
        "            image = cv2.convertScaleAbs(image, alpha=1.1, beta=10)\n",
        "            return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        base_norm = normalize(base_image)\n",
        "        secondary_norm = normalize(secondary_extracted)\n",
        "        cv2.imwrite(os.path.join(OUTPUT_DIR, \"base_normalized.png\"), base_norm)\n",
        "        cv2.imwrite(os.path.join(OUTPUT_DIR, \"secondary_normalized.png\"), secondary_norm)\n",
        "\n",
        "        return mask, base_norm, secondary_norm\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing images: {e}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "XbE9PwrMMvgr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Context Analysis Agent using Async/Await and Threads\n",
        "async def analyze_context(base_image_rgb, secondary_image_rgb, multimedia, domain):\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    def encode_image(image):\n",
        "        _, buffer = cv2.imencode('.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "        b64 = base64.b64encode(buffer).decode('utf-8')\n",
        "        if len(b64) > 4 * 1024 * 1024:\n",
        "            raise ValueError(\"Image exceeds 4MB limit\")\n",
        "        return b64\n",
        "\n",
        "    base_b64 = encode_image(base_image_rgb)\n",
        "    secondary_b64 = encode_image(secondary_image_rgb)\n",
        "    base_context = \"\"\n",
        "    secondary_context = \"\"\n",
        "    multimedia_context = \"\"\n",
        "    caption = \"\"\n",
        "    external_context = \"\"\n",
        "\n",
        "    # Wrap sync LLM calls in executor for asynchronous execution\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        async def gemini_base():\n",
        "            nonlocal base_context\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            if gemini_llm:\n",
        "                msg = HumanMessage(content=[\n",
        "                    {\"type\": \"text\", \"text\": f\"Describe the setting for a {domain} marketing image.\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base_b64}\"}\n",
        "                ])\n",
        "                result = await loop.run_in_executor(executor, gemini_llm.invoke, [msg])\n",
        "                base_context = result.content\n",
        "            else:\n",
        "                base_context = \"Default setting context\"\n",
        "\n",
        "        async def gemini_secondary():\n",
        "            nonlocal secondary_context\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            if gemini_llm:\n",
        "                msg = HumanMessage(content=[\n",
        "                    {\"type\": \"text\", \"text\": f\"Identify the product features for a {domain} marketing image.\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{secondary_b64}\"}\n",
        "                ])\n",
        "                result = await loop.run_in_executor(executor, gemini_llm.invoke, [msg])\n",
        "                secondary_context = result.content\n",
        "            else:\n",
        "                secondary_context = \"Default product context\"\n",
        "\n",
        "        async def gemini_multimedia():\n",
        "            nonlocal multimedia_context\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            if multimedia and gemini_llm:\n",
        "                with open(multimedia['path'], \"rb\") as f:\n",
        "                    media_data = f.read()\n",
        "                encoded_media = base64.b64encode(media_data).decode(\"utf-8\")\n",
        "                mime = \"audio/mpeg\" if multimedia['type'] == 'audio' else \"video/mp4\"\n",
        "                msg = HumanMessage(content=[\n",
        "                    {\"type\": \"text\", \"text\": f\"Extract ambiance for a {domain} marketing campaign.\"},\n",
        "                    {\"type\": \"media\", \"data\": encoded_media, \"mime_type\": mime}\n",
        "                ])\n",
        "                result = await loop.run_in_executor(executor, gemini_llm.invoke, [msg])\n",
        "                multimedia_context = result.content\n",
        "            else:\n",
        "                multimedia_context = \"\"\n",
        "\n",
        "        async def groq_caption():\n",
        "            nonlocal caption\n",
        "            response = await loop.run_in_executor(\n",
        "                executor,\n",
        "                lambda: groq_client.chat.completions.create(\n",
        "                    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": [\n",
        "                                {\"type\": \"text\", \"text\": f\"Generate a JSON caption for a {domain} marketing image.\"},\n",
        "                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base_b64}\"}},\n",
        "                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{secondary_b64}\"}}\n",
        "                            ]\n",
        "                        }\n",
        "                    ],\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "            )\n",
        "            try:\n",
        "                caption = json.loads(response.choices[0].message.content).get(\"caption\", \"\")\n",
        "            except Exception as ex:\n",
        "                caption = \"Default caption\"\n",
        "\n",
        "        async def groq_external():\n",
        "            nonlocal external_context\n",
        "            if domain in [\"travel\", \"automotive\"]:\n",
        "                response = await loop.run_in_executor(\n",
        "                    executor,\n",
        "                    lambda: groq_client.chat.completions.create(\n",
        "                        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                        messages=[\n",
        "                            {\n",
        "                                \"role\": \"user\",\n",
        "                                \"content\": [\n",
        "                                    {\"type\": \"text\", \"text\": f\"What's the current weather for a {domain} marketing scene?\"},\n",
        "                                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base_b64}\"}}\n",
        "                                ]\n",
        "                            }\n",
        "                        ],\n",
        "                        tools=[{\n",
        "                            \"type\": \"function\",\n",
        "                            \"function\": {\n",
        "                                \"name\": \"get_current_weather\",\n",
        "                                \"description\": \"Get the current weather in a given location\",\n",
        "                                \"parameters\": {\n",
        "                                    \"type\": \"object\",\n",
        "                                    \"properties\": {\n",
        "                                        \"location\": {\"type\": \"string\"},\n",
        "                                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
        "                                    },\n",
        "                                    \"required\": [\"location\"]\n",
        "                                }\n",
        "                            }\n",
        "                        }],\n",
        "                        tool_choice=\"auto\"\n",
        "                    )\n",
        "                )\n",
        "                if response.choices[0].message.tool_calls:\n",
        "                    external_context = response.choices[0].message.tool_calls[0][\"function\"][\"arguments\"]\n",
        "            else:\n",
        "                external_context = \"\"\n",
        "\n",
        "        await asyncio.gather(gemini_base(), gemini_secondary(), gemini_multimedia(), groq_caption(), groq_external())\n",
        "\n",
        "    return base_context, secondary_context, multimedia_context, caption, external_context"
      ],
      "metadata": {
        "id": "MaoiQh4-MzOa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Prompt Refinement Agent using Async Await\n",
        "async def refine_prompt_async(domain, initial_prompt, base_context, secondary_context, multimedia_context, caption, external_context):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    product = secondary_context.split(\" \")[0] if secondary_context else initial_prompt.split(\" \")[0]\n",
        "    setting = (base_context.split(\" \")[0] if base_context\n",
        "               else (initial_prompt.split(\" in \")[1].split(\",\")[0] if \" in \" in initial_prompt else \"studio\"))\n",
        "    style = initial_prompt.split(\",\")[1].strip() if len(initial_prompt.split(\",\")) > 1 else \"modern\"\n",
        "    lighting = initial_prompt.split(\",\")[2].strip() if len(initial_prompt.split(\",\")) > 2 else \"natural\"\n",
        "    context = multimedia_context or base_context or \"professional marketing aesthetic\"\n",
        "    caption = caption or \"\"\n",
        "    external = external_context or \"\"\n",
        "\n",
        "    from langchain.chains import LLMChain\n",
        "    prompt_chain = LLMChain(llm=llm, prompt=prompt_templates.get(domain, prompt_templates[\"clothing\"]))\n",
        "    # Wrap synchronous prompt refinement in executor so it becomes async\n",
        "    refined_prompt = await loop.run_in_executor(None, lambda: prompt_chain.run(\n",
        "        product=product, setting=setting, style=style, lighting=lighting,\n",
        "        context=context, caption=caption, external=external\n",
        "    ))\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Refine this prompt for a high-quality {domain} marketing image: {refined_prompt}\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let’s refine further. Suggest improvements.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Emphasize vibrant {domain} aesthetics and detail.\"}\n",
        "    ]\n",
        "\n",
        "    if groq_client:\n",
        "        # Wrap Groq call in executor for async execution\n",
        "        response = await loop.run_in_executor(\n",
        "            None,\n",
        "            lambda: groq_client.chat.completions.create(\n",
        "                model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "                messages=messages,\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "        )\n",
        "        try:\n",
        "            final_prompt = json.loads(response.choices[0].message.content).get(\"refined_prompt\", refined_prompt)\n",
        "        except Exception as ex:\n",
        "            final_prompt = refined_prompt\n",
        "    else:\n",
        "        final_prompt = refined_prompt\n",
        "\n",
        "    print(\"Refined Prompt:\", final_prompt)\n",
        "    user_input = input(\"Approve or enter new prompt (press Enter to approve): \")\n",
        "    if user_input.strip():\n",
        "        final_prompt = user_input\n",
        "\n",
        "    return final_prompt"
      ],
      "metadata": {
        "id": "sdRa7dfuM5Bm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Generation Agent using Async/Await and ThreadPoolExecutor\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "async def generate_composite_async(base_image, mask, prompt, domain):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        try:\n",
        "            # Try xAI generation first\n",
        "            response = await loop.run_in_executor(\n",
        "                executor,\n",
        "                lambda: xai_client.images.generate(\n",
        "                    model=\"grok-2-image\",\n",
        "                    prompt=f\"{prompt}, high-quality {domain} marketing image\",\n",
        "                    n=2,\n",
        "                    response_format=\"b64_json\"\n",
        "                )\n",
        "            )\n",
        "            images = []\n",
        "            for idx, img_data in enumerate(response.data):\n",
        "                image_b64 = img_data.b64_json\n",
        "                image_data = base64.b64decode(image_b64)\n",
        "                composite = cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR)\n",
        "                filename = os.path.join(OUTPUT_DIR, f\"composite_image_{idx}.png\")\n",
        "                cv2.imwrite(filename, composite)\n",
        "                images.append(composite)\n",
        "            return images\n",
        "        except Exception as e:\n",
        "            print(f\"xAI generation error: {e}. Falling back to Stable Diffusion.\")\n",
        "            if sd_pipeline:\n",
        "                mask_img = Image.open(os.path.join(OUTPUT_DIR, \"mask.png\")).convert(\"L\")\n",
        "                base_pil = Image.fromarray(base_image)\n",
        "                sd_pipeline.enable_model_cpu_offload()\n",
        "                output = await loop.run_in_executor(\n",
        "                    executor,\n",
        "                    lambda: sd_pipeline(\n",
        "                        prompt,\n",
        "                        image=base_pil,\n",
        "                        controlnet_conditioning_image=mask_img,\n",
        "                        num_inference_steps=30,\n",
        "                        guidance_scale=7.5\n",
        "                    ).images[0]\n",
        "                )\n",
        "                filename = os.path.join(OUTPUT_DIR, \"composite_image_0.png\")\n",
        "                output.save(filename)\n",
        "                return [cv2.imread(filename)]\n",
        "            else:\n",
        "                print(\"Stable Diffusion unavailable, generation failed.\")\n",
        "                return []"
      ],
      "metadata": {
        "id": "1HS0LzWeM-yT",
        "outputId": "0a6a1679-e6a6-4493-f100-9980c6a91a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'retry' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-072e6bcbbc56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 8: Generation Agent using Async/Await and ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_after_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_exponential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_composite_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'retry' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Enhancement Agent using Parallel Processing\n",
        "def enhance_image(composite_image, domain):\n",
        "    try:\n",
        "        if esrgan_model:\n",
        "            upscaled = esrgan_model.predict(composite_image)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_DIR, \"upscaled_image.png\"), upscaled)\n",
        "        else:\n",
        "            upscaled = composite_image\n",
        "\n",
        "        if lama_model:\n",
        "            h, w = upscaled.shape[:2]\n",
        "            artifact_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "            artifact_mask[h//4:h//2, w//4:w//2] = 255\n",
        "            cv2.imwrite(os.path.join(OUTPUT_DIR, \"artifact_mask.png\"), artifact_mask)\n",
        "            inpainted = lama_model.predict(upscaled, artifact_mask)\n",
        "            cv2.imwrite(os.path.join(OUTPUT_DIR, \"inpainted_image.png\"), inpainted)\n",
        "        else:\n",
        "            inpainted = upscaled\n",
        "\n",
        "        if gemini_llm:\n",
        "            _, buffer = cv2.imencode('.jpg', inpainted)\n",
        "            image_b64 = base64.b64encode(buffer).decode('utf-8')\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            style_msg = HumanMessage(content=[\n",
        "                {\"type\": \"text\", \"text\": f\"Apply a vibrant {domain} style to this image.\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{image_b64}\"}\n",
        "            ])\n",
        "            style_response = gemini_llm.invoke([style_msg], generation_config={\"response_modalities\": [\"TEXT\", \"IMAGE\"]})\n",
        "            style_b64 = style_response.content[0].get(\"image_url\", {}).get(\"url\", \"\").split(\",\")[-1]\n",
        "            if style_b64:\n",
        "                style_decoded = base64.b64decode(style_b64)\n",
        "                style_image = cv2.imdecode(np.frombuffer(style_decoded, np.uint8), cv2.IMREAD_COLOR)\n",
        "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"style_image.png\"), style_image)\n",
        "            else:\n",
        "                style_image = inpainted\n",
        "        else:\n",
        "            style_image = inpainted\n",
        "\n",
        "        final_image = cv2.addWeighted(inpainted, 0.7, style_image, 0.3, 0)\n",
        "        cv2.imwrite(os.path.join(OUTPUT_DIR, \"final_image.png\"), final_image)\n",
        "        return final_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error during enhancement: {e}\")\n",
        "        return composite_image"
      ],
      "metadata": {
        "id": "4H_KX9rYNDf7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Quality Evaluation Agent\n",
        "def evaluate_quality(base_image, final_image):\n",
        "    try:\n",
        "        min_dim = (min(base_image.shape[1], final_image.shape[1]),\n",
        "                   min(base_image.shape[0], final_image.shape[0]))\n",
        "        base_resized = cv2.resize(base_image, min_dim)\n",
        "        final_resized = cv2.resize(final_image, min_dim)\n",
        "        ssim_score = ssim(base_resized, final_resized, multichannel=True)\n",
        "        psnr_score = psnr(base_resized, final_resized)\n",
        "        feedback = \"Good quality\" if ssim_score > 0.7 and psnr_score > 30 else \"Low quality, consider revising prompt or style transfer.\"\n",
        "        return ssim_score, psnr_score, feedback\n",
        "    except Exception as e:\n",
        "        print(f\"Quality evaluation error: {e}\")\n",
        "        return 0, 0, \"Evaluation failed\"\n",
        "\n",
        "def log_metrics(metric_name, value):\n",
        "    print(f\"Metric - {metric_name}: {value}\")"
      ],
      "metadata": {
        "id": "MuTur2_HNIMe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New Feature: Generate Report (Markdown)\n",
        "def generate_report(final_prompt, ssim_scores, psnr_scores, feedbacks):\n",
        "    report_file = os.path.join(OUTPUT_DIR, \"report.md\")\n",
        "    with open(report_file, \"w\") as f:\n",
        "        f.write(\"# Product Marketing AI Pipeline Report\\n\\n\")\n",
        "        f.write(\"## Final Prompt\\n\")\n",
        "        f.write(f\"{final_prompt}\\n\\n\")\n",
        "        f.write(\"## Quality Evaluation Metrics\\n\")\n",
        "        for idx, (ssim_score, psnr_score, feedback) in enumerate(zip(ssim_scores, psnr_scores, feedbacks)):\n",
        "            f.write(f\"### Image {idx}\\n\")\n",
        "            f.write(f\"- SSIM: {ssim_score}\\n\")\n",
        "            f.write(f\"- PSNR: {psnr_score}\\n\")\n",
        "            f.write(f\"- Feedback: {feedback}\\n\\n\")\n",
        "    print(f\"Report generated at: {report_file}\")"
      ],
      "metadata": {
        "id": "LiR5BiYJNNT2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Main Pipeline Execution using asyncio for parallelism\n",
        "async def run_pipeline_async():\n",
        "    base_image_rgb, secondary_image_rgb, multimedia, domain, initial_prompt = collect_inputs()\n",
        "    if base_image_rgb is None:\n",
        "        print(\"Input collection failed, exiting.\")\n",
        "        return\n",
        "\n",
        "    mask, base_normalized, secondary_normalized = preprocess_images(base_image_rgb, secondary_image_rgb)\n",
        "    if mask is None:\n",
        "        print(\"Preprocessing failed, exiting.\")\n",
        "        return\n",
        "    free_memory(sam)\n",
        "\n",
        "    # Asynchronously analyze context (multiple LLM calls in parallel)\n",
        "    base_context, secondary_context, multimedia_context, caption, external_context = await analyze_context(\n",
        "        base_image_rgb, secondary_image_rgb, multimedia, domain\n",
        "    )\n",
        "\n",
        "    # Asynchronously refine prompt using gathered context\n",
        "    final_prompt = await refine_prompt_async(domain, initial_prompt, base_context, secondary_context, multimedia_context, caption, external_context)\n",
        "    print(f\"Final Prompt: {final_prompt}\")\n",
        "\n",
        "    # Asynchronously generate composite images (LLM calls and image generation in parallel)\n",
        "    composite_images = await generate_composite_async(base_normalized, mask, final_prompt, domain)\n",
        "    if not composite_images:\n",
        "        print(\"Image generation failed, exiting.\")\n",
        "        return\n",
        "    free_memory(sd_pipeline)\n",
        "\n",
        "    final_images = []\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    feedbacks = []\n",
        "    # Enhance images in parallel using ThreadPoolExecutor\n",
        "    loop = asyncio.get_event_loop()\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        tasks = []\n",
        "        for idx, composite_image in enumerate(composite_images):\n",
        "            task = loop.run_in_executor(executor, enhance_image, composite_image, domain)\n",
        "            tasks.append(task)\n",
        "        enhanced_results = await asyncio.gather(*tasks)\n",
        "\n",
        "    for idx, final_img in enumerate(enhanced_results):\n",
        "        final_images.append(final_img)\n",
        "        fname = os.path.join(OUTPUT_DIR, f\"final_image_{idx}.png\")\n",
        "        cv2.imwrite(fname, final_img)\n",
        "        ssim_score, psnr_score, feedback = evaluate_quality(base_image_rgb, final_img)\n",
        "        ssim_scores.append(ssim_score)\n",
        "        psnr_scores.append(psnr_score)\n",
        "        feedbacks.append(feedback)\n",
        "        log_metrics(f\"Final Image {idx} SSIM\", ssim_score)\n",
        "        log_metrics(f\"Final Image {idx} PSNR\", psnr_score)\n",
        "        print(f\"Image {idx} - SSIM: {ssim_score}, PSNR: {psnr_score}, Feedback: {feedback}\")\n",
        "\n",
        "    # Display images for user review\n",
        "    print(\"Displaying final images...\")\n",
        "    for idx, final_img in enumerate(final_images):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Final Image {idx}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    selected_index_input = input(\"Enter the index number of the final image you prefer (press Enter to keep all): \")\n",
        "    if selected_index_input.strip():\n",
        "        try:\n",
        "            selected_index = int(selected_index_input.strip())\n",
        "            final_images = [final_images[selected_index]]\n",
        "            ssim_scores = [ssim_scores[selected_index]]\n",
        "            psnr_scores = [psnr_scores[selected_index]]\n",
        "            feedbacks = [feedbacks[selected_index]]\n",
        "            print(f\"Selected Image {selected_index} as final output.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Invalid input, proceeding with all images. Error: {e}\")\n",
        "\n",
        "    if input(\"Generate quality report? (y/n): \").lower() == 'y':\n",
        "        generate_report(final_prompt, ssim_scores, psnr_scores, feedbacks)\n",
        "\n",
        "    # Optionally, list output files available in OUTPUT_DIR\n",
        "    print(\"Final outputs saved in the 'outputs' folder.\")\n",
        "    # Cleanup: Optionally remove temporary files if desired (omitted here)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        asyncio.run(run_pipeline_async())\n",
        "        print(\"Pipeline completed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eNVkZ9ogNSvj",
        "outputId": "c7de5f67-f052-4e31-be30-bc37e6b15875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline failed: name 'asyncio' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XCoLO3DlNW5D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}